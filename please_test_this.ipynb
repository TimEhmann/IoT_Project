{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r'C:\\Studium\\Semester_6\\IoT_Project\\data\\hka-aqm-am-combined-RAW.parquet')\n",
    "# drop channel_rssi and channel_index\n",
    "df = df.drop(columns=['channel_rssi', 'channel_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     673.999996  730.695447\n",
      "1     431.000001  426.697602\n",
      "2     527.000002  523.131507\n",
      "3     425.999998  430.128496\n",
      "4     424.000002  429.542042\n",
      "...          ...         ...\n",
      "1019  752.999998  798.811858\n",
      "1020  434.999998  439.417014\n",
      "1021  576.000002  589.208809\n",
      "1022  410.000003  422.979568\n",
      "1023  415.000002  425.354339\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.1263\n",
      "Epoch 1/25, Validation Loss: 0.0433\n",
      "          actual   predicted\n",
      "0     673.999996  735.362391\n",
      "1     431.000001  417.928557\n",
      "2     527.000002  529.200349\n",
      "3     425.999998  422.534752\n",
      "4     424.000002  421.984266\n",
      "...          ...         ...\n",
      "1019  752.999998  848.302429\n",
      "1020  434.999998  434.716461\n",
      "1021  576.000002  621.843544\n",
      "1022  410.000003  414.263083\n",
      "1023  415.000002  418.300558\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0709\n",
      "Epoch 2/25, Validation Loss: 0.0443\n",
      "          actual   predicted\n",
      "0     673.999996  690.982973\n",
      "1     431.000001  422.236219\n",
      "2     527.000002  514.806119\n",
      "3     425.999998  420.938844\n",
      "4     424.000002  421.008511\n",
      "...          ...         ...\n",
      "1019  752.999998  794.991904\n",
      "1020  434.999998  433.640395\n",
      "1021  576.000002  575.702198\n",
      "1022  410.000003  412.887462\n",
      "1023  415.000002  419.333057\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0624\n",
      "Epoch 3/25, Validation Loss: 0.0459\n",
      "          actual   predicted\n",
      "0     673.999996  696.234301\n",
      "1     431.000001  419.192619\n",
      "2     527.000002  526.645802\n",
      "3     425.999998  425.222720\n",
      "4     424.000002  417.086051\n",
      "...          ...         ...\n",
      "1019  752.999998  790.177872\n",
      "1020  434.999998  431.504553\n",
      "1021  576.000002  597.461027\n",
      "1022  410.000003  406.282645\n",
      "1023  415.000002  416.891700\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0576\n",
      "Epoch 4/25, Validation Loss: 0.0383\n",
      "          actual   predicted\n",
      "0     673.999996  704.511635\n",
      "1     431.000001  418.326207\n",
      "2     527.000002  512.191775\n",
      "3     425.999998  420.557835\n",
      "4     424.000002  417.387349\n",
      "...          ...         ...\n",
      "1019  752.999998  798.066517\n",
      "1020  434.999998  427.932454\n",
      "1021  576.000002  590.787091\n",
      "1022  410.000003  407.702232\n",
      "1023  415.000002  413.530372\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0548\n",
      "Epoch 5/25, Validation Loss: 0.0396\n",
      "          actual   predicted\n",
      "0     673.999996  718.975774\n",
      "1     431.000001  424.847248\n",
      "2     527.000002  523.597431\n",
      "3     425.999998  425.850480\n",
      "4     424.000002  425.663871\n",
      "...          ...         ...\n",
      "1019  752.999998  822.749491\n",
      "1020  434.999998  436.813731\n",
      "1021  576.000002  584.743541\n",
      "1022  410.000003  413.489366\n",
      "1023  415.000002  424.058633\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0525\n",
      "Epoch 6/25, Validation Loss: 0.0367\n",
      "          actual   predicted\n",
      "0     673.999996  709.320032\n",
      "1     431.000001  422.678473\n",
      "2     527.000002  529.104537\n",
      "3     425.999998  424.761029\n",
      "4     424.000002  422.677512\n",
      "...          ...         ...\n",
      "1019  752.999998  824.407019\n",
      "1020  434.999998  432.858368\n",
      "1021  576.000002  614.120919\n",
      "1022  410.000003  411.825910\n",
      "1023  415.000002  420.374820\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0512\n",
      "Epoch 7/25, Validation Loss: 0.0383\n",
      "          actual   predicted\n",
      "0     673.999996  715.475344\n",
      "1     431.000001  416.134677\n",
      "2     527.000002  545.048606\n",
      "3     425.999998  421.871061\n",
      "4     424.000002  417.822136\n",
      "...          ...         ...\n",
      "1019  752.999998  846.870018\n",
      "1020  434.999998  433.355407\n",
      "1021  576.000002  627.931856\n",
      "1022  410.000003  407.472329\n",
      "1023  415.000002  415.600223\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0499\n",
      "Epoch 8/25, Validation Loss: 0.0447\n",
      "          actual   predicted\n",
      "0     673.999996  687.651043\n",
      "1     431.000001  420.302621\n",
      "2     527.000002  519.719889\n",
      "3     425.999998  420.434134\n",
      "4     424.000002  418.168819\n",
      "...          ...         ...\n",
      "1019  752.999998  817.398075\n",
      "1020  434.999998  429.027145\n",
      "1021  576.000002  594.609532\n",
      "1022  410.000003  407.506070\n",
      "1023  415.000002  414.737929\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0493\n",
      "Epoch 9/25, Validation Loss: 0.0392\n",
      "          actual   predicted\n",
      "0     673.999996  704.377003\n",
      "1     431.000001  420.177336\n",
      "2     527.000002  522.251520\n",
      "3     425.999998  424.552959\n",
      "4     424.000002  417.477106\n",
      "...          ...         ...\n",
      "1019  752.999998  809.897998\n",
      "1020  434.999998  428.477700\n",
      "1021  576.000002  597.015534\n",
      "1022  410.000003  405.787372\n",
      "1023  415.000002  416.960563\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0557\n",
      "Epoch 10/25, Validation Loss: 0.0383\n",
      "          actual   predicted\n",
      "0     673.999996  714.242776\n",
      "1     431.000001  422.037743\n",
      "2     527.000002  518.259432\n",
      "3     425.999998  423.341232\n",
      "4     424.000002  419.908336\n",
      "...          ...         ...\n",
      "1019  752.999998  821.480560\n",
      "1020  434.999998  432.871373\n",
      "1021  576.000002  587.718932\n",
      "1022  410.000003  410.624649\n",
      "1023  415.000002  420.252194\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[-0.0220]], device='cuda:0')\n",
      "[486.02]\n",
      "           actual   predicted\n",
      "0      673.999996  714.242776\n",
      "1      431.000001  422.037743\n",
      "2      527.000002  518.259432\n",
      "3      425.999998  423.341232\n",
      "4      424.000002  419.908336\n",
      "...           ...         ...\n",
      "79080  521.000001  524.703643\n",
      "79081  416.999999  417.986744\n",
      "79082  431.000001  438.357319\n",
      "79083  447.000001  454.276378\n",
      "79084  466.999999  467.367626\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 24.2366\n",
      "Score (MAE): 8.9472\n",
      "Score (ME): 0.3160\n",
      "Score (MAPE): 1.6143%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual    predicted\n",
      "0     602.000003   590.630930\n",
      "1     609.999991   596.222229\n",
      "2     931.500003  1057.108645\n",
      "3     643.000007   625.188452\n",
      "4     751.999999   707.215600\n",
      "...          ...          ...\n",
      "1019  677.999997   685.174873\n",
      "1020  653.999997   613.263022\n",
      "1021  791.000000   768.254592\n",
      "1022  606.000006   637.019336\n",
      "1023  619.000007   632.606469\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.1156\n",
      "Epoch 1/25, Validation Loss: 0.0569\n",
      "          actual    predicted\n",
      "0     602.000003   580.532887\n",
      "1     609.999991   592.399228\n",
      "2     931.500003  1027.097500\n",
      "3     643.000007   628.696818\n",
      "4     751.999999   716.378862\n",
      "...          ...          ...\n",
      "1019  677.999997   685.434059\n",
      "1020  653.999997   628.686730\n",
      "1021  791.000000   791.544497\n",
      "1022  606.000006   625.122626\n",
      "1023  619.000007   633.089386\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0644\n",
      "Epoch 2/25, Validation Loss: 0.0448\n",
      "          actual    predicted\n",
      "0     602.000003   587.284846\n",
      "1     609.999991   601.268172\n",
      "2     931.500003  1039.823515\n",
      "3     643.000007   622.696709\n",
      "4     751.999999   721.363458\n",
      "...          ...          ...\n",
      "1019  677.999997   680.727725\n",
      "1020  653.999997   609.920236\n",
      "1021  791.000000   788.800083\n",
      "1022  606.000006   615.573308\n",
      "1023  619.000007   629.512387\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0580\n",
      "Epoch 3/25, Validation Loss: 0.0413\n",
      "          actual    predicted\n",
      "0     602.000003   586.300100\n",
      "1     609.999991   598.418180\n",
      "2     931.500003  1040.248274\n",
      "3     643.000007   637.391147\n",
      "4     751.999999   711.876355\n",
      "...          ...          ...\n",
      "1019  677.999997   662.774107\n",
      "1020  653.999997   608.449415\n",
      "1021  791.000000   748.157182\n",
      "1022  606.000006   609.451092\n",
      "1023  619.000007   610.931176\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0545\n",
      "Epoch 4/25, Validation Loss: 0.0425\n",
      "          actual   predicted\n",
      "0     602.000003  571.137499\n",
      "1     609.999991  589.272286\n",
      "2     931.500003  992.306871\n",
      "3     643.000007  621.418714\n",
      "4     751.999999  714.900445\n",
      "...          ...         ...\n",
      "1019  677.999997  684.998713\n",
      "1020  653.999997  604.132655\n",
      "1021  791.000000  788.663841\n",
      "1022  606.000006  627.437578\n",
      "1023  619.000007  614.037065\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0522\n",
      "Epoch 5/25, Validation Loss: 0.0409\n",
      "          actual    predicted\n",
      "0     602.000003   568.070225\n",
      "1     609.999991   592.534089\n",
      "2     931.500003  1016.665681\n",
      "3     643.000007   634.051378\n",
      "4     751.999999   723.508825\n",
      "...          ...          ...\n",
      "1019  677.999997   686.663685\n",
      "1020  653.999997   607.950814\n",
      "1021  791.000000   778.459018\n",
      "1022  606.000006   624.263987\n",
      "1023  619.000007   608.947841\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0504\n",
      "Epoch 6/25, Validation Loss: 0.0416\n",
      "          actual    predicted\n",
      "0     602.000003   584.991403\n",
      "1     609.999991   604.180340\n",
      "2     931.500003  1050.839269\n",
      "3     643.000007   635.143371\n",
      "4     751.999999   730.513650\n",
      "...          ...          ...\n",
      "1019  677.999997   672.524217\n",
      "1020  653.999997   624.499006\n",
      "1021  791.000000   811.292343\n",
      "1022  606.000006   626.895433\n",
      "1023  619.000007   616.881127\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0490\n",
      "Epoch 7/25, Validation Loss: 0.0424\n",
      "          actual    predicted\n",
      "0     602.000003   561.370178\n",
      "1     609.999991   590.059942\n",
      "2     931.500003  1022.690370\n",
      "3     643.000007   627.743002\n",
      "4     751.999999   735.550259\n",
      "...          ...          ...\n",
      "1019  677.999997   677.796285\n",
      "1020  653.999997   606.114446\n",
      "1021  791.000000   774.787152\n",
      "1022  606.000006   625.337506\n",
      "1023  619.000007   615.537869\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0484\n",
      "Epoch 8/25, Validation Loss: 0.0406\n",
      "          actual   predicted\n",
      "0     602.000003  580.456184\n",
      "1     609.999991  603.223348\n",
      "2     931.500003  997.862538\n",
      "3     643.000007  637.166162\n",
      "4     751.999999  718.419018\n",
      "...          ...         ...\n",
      "1019  677.999997  695.025449\n",
      "1020  653.999997  627.853687\n",
      "1021  791.000000  794.204424\n",
      "1022  606.000006  643.577047\n",
      "1023  619.000007  629.866040\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0476\n",
      "Epoch 9/25, Validation Loss: 0.0402\n",
      "          actual    predicted\n",
      "0     602.000003   574.731990\n",
      "1     609.999991   591.615308\n",
      "2     931.500003  1020.104767\n",
      "3     643.000007   631.846796\n",
      "4     751.999999   733.080403\n",
      "...          ...          ...\n",
      "1019  677.999997   676.163050\n",
      "1020  653.999997   609.362336\n",
      "1021  791.000000   779.952839\n",
      "1022  606.000006   624.512182\n",
      "1023  619.000007   617.876961\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0462\n",
      "Epoch 10/25, Validation Loss: 0.0409\n",
      "          actual    predicted\n",
      "0     602.000003   574.972957\n",
      "1     609.999991   602.199707\n",
      "2     931.500003  1042.774052\n",
      "3     643.000007   636.664736\n",
      "4     751.999999   728.721997\n",
      "...          ...          ...\n",
      "1019  677.999997   692.497241\n",
      "1020  653.999997   620.009315\n",
      "1021  791.000000   799.475764\n",
      "1022  606.000006   637.117723\n",
      "1023  619.000007   624.026546\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0461\n",
      "Epoch 11/25, Validation Loss: 0.0418\n",
      "          actual   predicted\n",
      "0     602.000003  574.778095\n",
      "1     609.999991  594.422615\n",
      "2     931.500003  995.804202\n",
      "3     643.000007  628.596116\n",
      "4     751.999999  710.473727\n",
      "...          ...         ...\n",
      "1019  677.999997  676.411289\n",
      "1020  653.999997  610.177870\n",
      "1021  791.000000  789.948138\n",
      "1022  606.000006  623.191468\n",
      "1023  619.000007  618.731548\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0461\n",
      "Epoch 12/25, Validation Loss: 0.0407\n",
      "          actual    predicted\n",
      "0     602.000003   562.529102\n",
      "1     609.999991   588.627648\n",
      "2     931.500003  1034.109549\n",
      "3     643.000007   629.488808\n",
      "4     751.999999   720.934638\n",
      "...          ...          ...\n",
      "1019  677.999997   684.894738\n",
      "1020  653.999997   608.085254\n",
      "1021  791.000000   785.427459\n",
      "1022  606.000006   622.101528\n",
      "1023  619.000007   608.721997\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0454\n",
      "Epoch 13/25, Validation Loss: 0.0428\n",
      "          actual    predicted\n",
      "0     602.000003   572.916218\n",
      "1     609.999991   594.335299\n",
      "2     931.500003  1013.110314\n",
      "3     643.000007   627.954443\n",
      "4     751.999999   724.074142\n",
      "...          ...          ...\n",
      "1019  677.999997   684.083301\n",
      "1020  653.999997   599.955580\n",
      "1021  791.000000   788.733739\n",
      "1022  606.000006   631.194368\n",
      "1023  619.000007   610.295784\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0440\n",
      "Epoch 14/25, Validation Loss: 0.0394\n",
      "          actual    predicted\n",
      "0     602.000003   571.312624\n",
      "1     609.999991   594.803987\n",
      "2     931.500003  1022.104101\n",
      "3     643.000007   627.678545\n",
      "4     751.999999   729.781494\n",
      "...          ...          ...\n",
      "1019  677.999997   686.716343\n",
      "1020  653.999997   603.189102\n",
      "1021  791.000000   805.747216\n",
      "1022  606.000006   633.907482\n",
      "1023  619.000007   613.303725\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0435\n",
      "Epoch 15/25, Validation Loss: 0.0394\n",
      "          actual    predicted\n",
      "0     602.000003   566.491911\n",
      "1     609.999991   595.192851\n",
      "2     931.500003  1003.824279\n",
      "3     643.000007   631.680285\n",
      "4     751.999999   724.743968\n",
      "...          ...          ...\n",
      "1019  677.999997   683.000019\n",
      "1020  653.999997   605.531160\n",
      "1021  791.000000   808.125537\n",
      "1022  606.000006   632.118149\n",
      "1023  619.000007   613.031126\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0437\n",
      "Epoch 16/25, Validation Loss: 0.0396\n",
      "          actual   predicted\n",
      "0     602.000003  576.201846\n",
      "1     609.999991  598.861342\n",
      "2     931.500003  998.526211\n",
      "3     643.000007  638.976249\n",
      "4     751.999999  732.368887\n",
      "...          ...         ...\n",
      "1019  677.999997  699.018180\n",
      "1020  653.999997  612.891142\n",
      "1021  791.000000  800.042532\n",
      "1022  606.000006  635.073002\n",
      "1023  619.000007  616.440684\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0434\n",
      "Epoch 17/25, Validation Loss: 0.0390\n",
      "          actual    predicted\n",
      "0     602.000003   562.021518\n",
      "1     609.999991   581.745425\n",
      "2     931.500003  1021.481464\n",
      "3     643.000007   626.574482\n",
      "4     751.999999   719.715811\n",
      "...          ...          ...\n",
      "1019  677.999997   677.036901\n",
      "1020  653.999997   596.246387\n",
      "1021  791.000000   804.091692\n",
      "1022  606.000006   625.510542\n",
      "1023  619.000007   600.193179\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0432\n",
      "Epoch 18/25, Validation Loss: 0.0408\n",
      "          actual    predicted\n",
      "0     602.000003   571.295974\n",
      "1     609.999991   594.846444\n",
      "2     931.500003  1006.598726\n",
      "3     643.000007   629.979795\n",
      "4     751.999999   727.458016\n",
      "...          ...          ...\n",
      "1019  677.999997   688.239963\n",
      "1020  653.999997   605.882847\n",
      "1021  791.000000   808.228891\n",
      "1022  606.000006   630.125165\n",
      "1023  619.000007   613.531253\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0433\n",
      "Epoch 19/25, Validation Loss: 0.0394\n",
      "          actual    predicted\n",
      "0     602.000003   564.865405\n",
      "1     609.999991   591.517412\n",
      "2     931.500003  1017.898623\n",
      "3     643.000007   636.539875\n",
      "4     751.999999   736.857614\n",
      "...          ...          ...\n",
      "1019  677.999997   689.788917\n",
      "1020  653.999997   602.604325\n",
      "1021  791.000000   792.893344\n",
      "1022  606.000006   630.126270\n",
      "1023  619.000007   606.885664\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0432\n",
      "Epoch 20/25, Validation Loss: 0.0400\n",
      "          actual   predicted\n",
      "0     602.000003  569.239201\n",
      "1     609.999991  587.223143\n",
      "2     931.500003  996.467472\n",
      "3     643.000007  635.402514\n",
      "4     751.999999  723.730537\n",
      "...          ...         ...\n",
      "1019  677.999997  682.739814\n",
      "1020  653.999997  598.623673\n",
      "1021  791.000000  802.258953\n",
      "1022  606.000006  625.801668\n",
      "1023  619.000007  607.011893\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0430\n",
      "Epoch 21/25, Validation Loss: 0.0386\n",
      "          actual    predicted\n",
      "0     602.000003   568.427649\n",
      "1     609.999991   587.003019\n",
      "2     931.500003  1027.943297\n",
      "3     643.000007   634.423925\n",
      "4     751.999999   734.159351\n",
      "...          ...          ...\n",
      "1019  677.999997   695.901369\n",
      "1020  653.999997   611.535778\n",
      "1021  791.000000   818.327344\n",
      "1022  606.000006   631.684355\n",
      "1023  619.000007   618.293369\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0432\n",
      "Epoch 22/25, Validation Loss: 0.0398\n",
      "          actual    predicted\n",
      "0     602.000003   565.005195\n",
      "1     609.999991   585.815674\n",
      "2     931.500003  1007.539858\n",
      "3     643.000007   628.702292\n",
      "4     751.999999   726.144622\n",
      "...          ...          ...\n",
      "1019  677.999997   682.939053\n",
      "1020  653.999997   598.188791\n",
      "1021  791.000000   803.794559\n",
      "1022  606.000006   623.995721\n",
      "1023  619.000007   606.259587\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0427\n",
      "Epoch 23/25, Validation Loss: 0.0394\n",
      "          actual    predicted\n",
      "0     602.000003   570.479054\n",
      "1     609.999991   593.220709\n",
      "2     931.500003  1041.914728\n",
      "3     643.000007   626.611728\n",
      "4     751.999999   727.267400\n",
      "...          ...          ...\n",
      "1019  677.999997   682.381495\n",
      "1020  653.999997   600.298496\n",
      "1021  791.000000   815.116092\n",
      "1022  606.000006   629.091348\n",
      "1023  619.000007   607.739619\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0427\n",
      "Epoch 24/25, Validation Loss: 0.0401\n",
      "          actual    predicted\n",
      "0     602.000003   567.083180\n",
      "1     609.999991   586.148993\n",
      "2     931.500003  1003.946631\n",
      "3     643.000007   628.077672\n",
      "4     751.999999   713.209709\n",
      "...          ...          ...\n",
      "1019  677.999997   672.844852\n",
      "1020  653.999997   604.750714\n",
      "1021  791.000000   800.168686\n",
      "1022  606.000006   619.485310\n",
      "1023  619.000007   613.104337\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0425\n",
      "Epoch 25/25, Validation Loss: 0.0394\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[-0.5286]], device='cuda:0')\n",
      "[634.92]\n",
      "            actual    predicted\n",
      "0       602.000003   567.083180\n",
      "1       609.999991   586.148993\n",
      "2       931.500003  1003.946631\n",
      "3       643.000007   628.077672\n",
      "4       751.999999   713.209709\n",
      "...            ...          ...\n",
      "79080  1028.999992   971.014195\n",
      "79081  1348.000006  1245.443487\n",
      "79082  1707.999989  1706.526151\n",
      "79083   673.499999   675.468938\n",
      "79084   571.000007   602.452236\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 58.3096\n",
      "Score (MAE): 30.3717\n",
      "Score (ME): 3.6802\n",
      "Score (MAPE): 3.7532%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     44.940000  45.546735\n",
      "1     29.280000  28.337173\n",
      "2     55.040001  59.794765\n",
      "3     33.920000  33.392429\n",
      "4     25.190000  23.560368\n",
      "...         ...        ...\n",
      "1019  32.113333  24.083751\n",
      "1020  28.430000  27.486172\n",
      "1021  29.750000  28.989299\n",
      "1022  39.990000  40.487031\n",
      "1023  25.260000  26.019968\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.0830\n",
      "Epoch 1/25, Validation Loss: 0.0144\n",
      "         actual  predicted\n",
      "0     44.940000  45.318379\n",
      "1     29.280000  27.789476\n",
      "2     55.040001  57.631450\n",
      "3     33.920000  32.890606\n",
      "4     25.190000  22.879643\n",
      "...         ...        ...\n",
      "1019  32.113333  23.336304\n",
      "1020  28.430000  27.759951\n",
      "1021  29.750000  29.016136\n",
      "1022  39.990000  41.071296\n",
      "1023  25.260000  25.704204\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0264\n",
      "Epoch 2/25, Validation Loss: 0.0160\n",
      "         actual  predicted\n",
      "0     44.940000  45.635654\n",
      "1     29.280000  29.346908\n",
      "2     55.040001  56.903289\n",
      "3     33.920000  34.190796\n",
      "4     25.190000  24.707740\n",
      "...         ...        ...\n",
      "1019  32.113333  24.404649\n",
      "1020  28.430000  28.426169\n",
      "1021  29.750000  29.657853\n",
      "1022  39.990000  40.617872\n",
      "1023  25.260000  25.434177\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0185\n",
      "Epoch 3/25, Validation Loss: 0.0106\n",
      "         actual  predicted\n",
      "0     44.940000  45.375408\n",
      "1     29.280000  29.023414\n",
      "2     55.040001  55.591675\n",
      "3     33.920000  33.867066\n",
      "4     25.190000  23.298427\n",
      "...         ...        ...\n",
      "1019  32.113333  24.969631\n",
      "1020  28.430000  29.201707\n",
      "1021  29.750000  30.372390\n",
      "1022  39.990000  40.785088\n",
      "1023  25.260000  25.627132\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0150\n",
      "Epoch 4/25, Validation Loss: 0.0059\n",
      "         actual  predicted\n",
      "0     44.940000  44.480061\n",
      "1     29.280000  29.500613\n",
      "2     55.040001  53.764900\n",
      "3     33.920000  34.410297\n",
      "4     25.190000  24.434155\n",
      "...         ...        ...\n",
      "1019  32.113333  24.434830\n",
      "1020  28.430000  28.575463\n",
      "1021  29.750000  29.943494\n",
      "1022  39.990000  40.479497\n",
      "1023  25.260000  25.611848\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0131\n",
      "Epoch 5/25, Validation Loss: 0.0049\n",
      "         actual  predicted\n",
      "0     44.940000  44.970543\n",
      "1     29.280000  28.550276\n",
      "2     55.040001  55.846983\n",
      "3     33.920000  34.001171\n",
      "4     25.190000  24.223594\n",
      "...         ...        ...\n",
      "1019  32.113333  23.914616\n",
      "1020  28.430000  27.764251\n",
      "1021  29.750000  29.249939\n",
      "1022  39.990000  40.611196\n",
      "1023  25.260000  24.708011\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0115\n",
      "Epoch 6/25, Validation Loss: 0.0053\n",
      "         actual  predicted\n",
      "0     44.940000  45.039372\n",
      "1     29.280000  28.372183\n",
      "2     55.040001  55.925604\n",
      "3     33.920000  33.886157\n",
      "4     25.190000  23.930104\n",
      "...         ...        ...\n",
      "1019  32.113333  24.004216\n",
      "1020  28.430000  27.827425\n",
      "1021  29.750000  29.270503\n",
      "1022  39.990000  40.148004\n",
      "1023  25.260000  24.892308\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0107\n",
      "Epoch 7/25, Validation Loss: 0.0085\n",
      "         actual  predicted\n",
      "0     44.940000  45.019127\n",
      "1     29.280000  28.621019\n",
      "2     55.040001  56.083458\n",
      "3     33.920000  33.976077\n",
      "4     25.190000  24.077388\n",
      "...         ...        ...\n",
      "1019  32.113333  24.343069\n",
      "1020  28.430000  28.110605\n",
      "1021  29.750000  29.676019\n",
      "1022  39.990000  39.979156\n",
      "1023  25.260000  25.298081\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0098\n",
      "Epoch 8/25, Validation Loss: 0.0062\n",
      "         actual  predicted\n",
      "0     44.940000  44.794807\n",
      "1     29.280000  28.765114\n",
      "2     55.040001  55.174480\n",
      "3     33.920000  34.099356\n",
      "4     25.190000  24.453489\n",
      "...         ...        ...\n",
      "1019  32.113333  24.266042\n",
      "1020  28.430000  27.899886\n",
      "1021  29.750000  29.304493\n",
      "1022  39.990000  40.140380\n",
      "1023  25.260000  24.918757\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0089\n",
      "Epoch 9/25, Validation Loss: 0.0041\n",
      "         actual  predicted\n",
      "0     44.940000  44.838854\n",
      "1     29.280000  29.122607\n",
      "2     55.040001  54.910542\n",
      "3     33.920000  34.291330\n",
      "4     25.190000  25.328218\n",
      "...         ...        ...\n",
      "1019  32.113333  25.136318\n",
      "1020  28.430000  28.928538\n",
      "1021  29.750000  30.339972\n",
      "1022  39.990000  40.118139\n",
      "1023  25.260000  25.893367\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0088\n",
      "Epoch 10/25, Validation Loss: 0.0034\n",
      "         actual  predicted\n",
      "0     44.940000  45.459165\n",
      "1     29.280000  28.645750\n",
      "2     55.040001  55.444662\n",
      "3     33.920000  34.050070\n",
      "4     25.190000  24.529863\n",
      "...         ...        ...\n",
      "1019  32.113333  24.388233\n",
      "1020  28.430000  28.094978\n",
      "1021  29.750000  29.664530\n",
      "1022  39.990000  40.589918\n",
      "1023  25.260000  25.167757\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0081\n",
      "Epoch 11/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     44.940000  44.971938\n",
      "1     29.280000  28.938887\n",
      "2     55.040001  55.187804\n",
      "3     33.920000  34.074333\n",
      "4     25.190000  24.341537\n",
      "...         ...        ...\n",
      "1019  32.113333  24.344005\n",
      "1020  28.430000  27.983369\n",
      "1021  29.750000  29.400382\n",
      "1022  39.990000  40.109933\n",
      "1023  25.260000  24.978998\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0077\n",
      "Epoch 12/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     44.940000  44.885583\n",
      "1     29.280000  28.722030\n",
      "2     55.040001  54.855628\n",
      "3     33.920000  33.771714\n",
      "4     25.190000  24.760797\n",
      "...         ...        ...\n",
      "1019  32.113333  24.620347\n",
      "1020  28.430000  28.168012\n",
      "1021  29.750000  29.505118\n",
      "1022  39.990000  40.222579\n",
      "1023  25.260000  25.103893\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0074\n",
      "Epoch 13/25, Validation Loss: 0.0036\n",
      "         actual  predicted\n",
      "0     44.940000  44.628276\n",
      "1     29.280000  28.847759\n",
      "2     55.040001  54.656582\n",
      "3     33.920000  33.508990\n",
      "4     25.190000  24.577553\n",
      "...         ...        ...\n",
      "1019  32.113333  24.626948\n",
      "1020  28.430000  28.466994\n",
      "1021  29.750000  29.781342\n",
      "1022  39.990000  40.634989\n",
      "1023  25.260000  25.499239\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0074\n",
      "Epoch 14/25, Validation Loss: 0.0032\n",
      "         actual  predicted\n",
      "0     44.940000  45.003296\n",
      "1     29.280000  29.236503\n",
      "2     55.040001  55.321537\n",
      "3     33.920000  34.055875\n",
      "4     25.190000  25.260105\n",
      "...         ...        ...\n",
      "1019  32.113333  25.056977\n",
      "1020  28.430000  28.662301\n",
      "1021  29.750000  30.073485\n",
      "1022  39.990000  40.701537\n",
      "1023  25.260000  25.733482\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0071\n",
      "Epoch 15/25, Validation Loss: 0.0039\n",
      "         actual  predicted\n",
      "0     44.940000  44.916870\n",
      "1     29.280000  29.037889\n",
      "2     55.040001  54.657573\n",
      "3     33.920000  33.878626\n",
      "4     25.190000  24.937106\n",
      "...         ...        ...\n",
      "1019  32.113333  24.769553\n",
      "1020  28.430000  28.422078\n",
      "1021  29.750000  29.827152\n",
      "1022  39.990000  40.593659\n",
      "1023  25.260000  25.139302\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0068\n",
      "Epoch 16/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     44.940000  44.755020\n",
      "1     29.280000  28.824588\n",
      "2     55.040001  54.331087\n",
      "3     33.920000  33.616291\n",
      "4     25.190000  24.889277\n",
      "...         ...        ...\n",
      "1019  32.113333  25.011214\n",
      "1020  28.430000  28.481604\n",
      "1021  29.750000  29.779239\n",
      "1022  39.990000  40.183226\n",
      "1023  25.260000  25.507101\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0062\n",
      "Epoch 17/25, Validation Loss: 0.0027\n",
      "         actual  predicted\n",
      "0     44.940000  44.508691\n",
      "1     29.280000  29.037851\n",
      "2     55.040001  54.265131\n",
      "3     33.920000  34.053346\n",
      "4     25.190000  24.965860\n",
      "...         ...        ...\n",
      "1019  32.113333  24.956193\n",
      "1020  28.430000  28.553258\n",
      "1021  29.750000  29.948428\n",
      "1022  39.990000  40.388219\n",
      "1023  25.260000  25.564710\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0061\n",
      "Epoch 18/25, Validation Loss: 0.0025\n",
      "         actual  predicted\n",
      "0     44.940000  44.887575\n",
      "1     29.280000  28.941356\n",
      "2     55.040001  54.842483\n",
      "3     33.920000  33.911271\n",
      "4     25.190000  24.844710\n",
      "...         ...        ...\n",
      "1019  32.113333  24.359055\n",
      "1020  28.430000  28.253460\n",
      "1021  29.750000  29.790531\n",
      "1022  39.990000  40.464794\n",
      "1023  25.260000  25.234300\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0060\n",
      "Epoch 19/25, Validation Loss: 0.0028\n",
      "         actual  predicted\n",
      "0     44.940000  44.784636\n",
      "1     29.280000  28.954754\n",
      "2     55.040001  54.523346\n",
      "3     33.920000  34.002041\n",
      "4     25.190000  25.067517\n",
      "...         ...        ...\n",
      "1019  32.113333  24.596211\n",
      "1020  28.430000  28.366789\n",
      "1021  29.750000  29.742963\n",
      "1022  39.990000  40.309967\n",
      "1023  25.260000  25.177183\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0060\n",
      "Epoch 20/25, Validation Loss: 0.0024\n",
      "         actual  predicted\n",
      "0     44.940000  44.868081\n",
      "1     29.280000  29.031081\n",
      "2     55.040001  54.903362\n",
      "3     33.920000  33.938428\n",
      "4     25.190000  24.768940\n",
      "...         ...        ...\n",
      "1019  32.113333  24.573714\n",
      "1020  28.430000  28.336447\n",
      "1021  29.750000  29.744927\n",
      "1022  39.990000  40.364088\n",
      "1023  25.260000  25.266231\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0059\n",
      "Epoch 21/25, Validation Loss: 0.0032\n",
      "         actual  predicted\n",
      "0     44.940000  44.920262\n",
      "1     29.280000  28.865922\n",
      "2     55.040001  54.926470\n",
      "3     33.920000  33.939052\n",
      "4     25.190000  24.857566\n",
      "...         ...        ...\n",
      "1019  32.113333  24.752529\n",
      "1020  28.430000  28.434604\n",
      "1021  29.750000  29.864997\n",
      "1022  39.990000  40.605370\n",
      "1023  25.260000  25.309580\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0059\n",
      "Epoch 22/25, Validation Loss: 0.0028\n",
      "         actual  predicted\n",
      "0     44.940000  45.014299\n",
      "1     29.280000  29.336238\n",
      "2     55.040001  55.002325\n",
      "3     33.920000  34.350266\n",
      "4     25.190000  25.078240\n",
      "...         ...        ...\n",
      "1019  32.113333  24.762621\n",
      "1020  28.430000  28.425434\n",
      "1021  29.750000  29.937024\n",
      "1022  39.990000  40.546535\n",
      "1023  25.260000  25.240132\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0057\n",
      "Epoch 23/25, Validation Loss: 0.0029\n",
      "         actual  predicted\n",
      "0     44.940000  44.258521\n",
      "1     29.280000  28.922931\n",
      "2     55.040001  54.590374\n",
      "3     33.920000  33.783024\n",
      "4     25.190000  24.826418\n",
      "...         ...        ...\n",
      "1019  32.113333  24.668617\n",
      "1020  28.430000  28.315243\n",
      "1021  29.750000  29.726873\n",
      "1022  39.990000  40.410398\n",
      "1023  25.260000  25.305116\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0057\n",
      "Epoch 24/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     44.940000  44.563486\n",
      "1     29.280000  29.261181\n",
      "2     55.040001  54.249101\n",
      "3     33.920000  34.029398\n",
      "4     25.190000  25.157274\n",
      "...         ...        ...\n",
      "1019  32.113333  24.874087\n",
      "1020  28.430000  28.601381\n",
      "1021  29.750000  29.878898\n",
      "1022  39.990000  40.441634\n",
      "1023  25.260000  25.520356\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[1.0842]], device='cuda:0')\n",
      "[45.94]\n",
      "          actual  predicted\n",
      "0      44.940000  44.563486\n",
      "1      29.280000  29.261181\n",
      "2      55.040001  54.249101\n",
      "3      33.920000  34.029398\n",
      "4      25.190000  25.157274\n",
      "...          ...        ...\n",
      "79080  33.690000  34.499226\n",
      "79081  32.850000  33.759214\n",
      "79082  30.630000  31.809131\n",
      "79083  41.850000  41.272957\n",
      "79084  41.430000  40.715063\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 0.4889\n",
      "Score (MAE): 0.2768\n",
      "Score (ME): 0.0552\n",
      "Score (MAPE): 0.7478%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     25.600000  25.687175\n",
      "1     29.352857  29.488539\n",
      "2     34.480000  36.322834\n",
      "3     22.680000  22.393383\n",
      "4     23.980000  23.857641\n",
      "...         ...        ...\n",
      "1019  28.880000  29.020846\n",
      "1020  26.450000  26.097740\n",
      "1021  14.800000  13.923285\n",
      "1022  28.600000  29.219292\n",
      "1023  24.747500  24.904024\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.0813\n",
      "Epoch 1/25, Validation Loss: 0.0230\n",
      "         actual  predicted\n",
      "0     25.600000  25.752666\n",
      "1     29.352857  29.675496\n",
      "2     34.480000  36.333737\n",
      "3     22.680000  22.640705\n",
      "4     23.980000  24.075335\n",
      "...         ...        ...\n",
      "1019  28.880000  28.716491\n",
      "1020  26.450000  26.035867\n",
      "1021  14.800000  14.249632\n",
      "1022  28.600000  28.544965\n",
      "1023  24.747500  24.473137\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0253\n",
      "Epoch 2/25, Validation Loss: 0.0118\n",
      "         actual  predicted\n",
      "0     25.600000  25.723710\n",
      "1     29.352857  29.541265\n",
      "2     34.480000  35.258566\n",
      "3     22.680000  22.571644\n",
      "4     23.980000  23.988614\n",
      "...         ...        ...\n",
      "1019  28.880000  28.698578\n",
      "1020  26.450000  26.152849\n",
      "1021  14.800000  14.032501\n",
      "1022  28.600000  28.705412\n",
      "1023  24.747500  24.482429\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0188\n",
      "Epoch 3/25, Validation Loss: 0.0073\n",
      "         actual  predicted\n",
      "0     25.600000  25.691726\n",
      "1     29.352857  29.547774\n",
      "2     34.480000  35.759628\n",
      "3     22.680000  22.728255\n",
      "4     23.980000  24.196565\n",
      "...         ...        ...\n",
      "1019  28.880000  28.801992\n",
      "1020  26.450000  26.256372\n",
      "1021  14.800000  13.848850\n",
      "1022  28.600000  28.518265\n",
      "1023  24.747500  24.722072\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0163\n",
      "Epoch 4/25, Validation Loss: 0.0072\n",
      "         actual  predicted\n",
      "0     25.600000  25.781672\n",
      "1     29.352857  29.451972\n",
      "2     34.480000  35.935877\n",
      "3     22.680000  22.611055\n",
      "4     23.980000  24.067101\n",
      "...         ...        ...\n",
      "1019  28.880000  28.982509\n",
      "1020  26.450000  26.131376\n",
      "1021  14.800000  14.030909\n",
      "1022  28.600000  28.743356\n",
      "1023  24.747500  24.712900\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0132\n",
      "Epoch 5/25, Validation Loss: 0.0041\n",
      "         actual  predicted\n",
      "0     25.600000  26.044597\n",
      "1     29.352857  29.738755\n",
      "2     34.480000  34.349406\n",
      "3     22.680000  22.706891\n",
      "4     23.980000  24.290078\n",
      "...         ...        ...\n",
      "1019  28.880000  29.090982\n",
      "1020  26.450000  26.354016\n",
      "1021  14.800000  14.453784\n",
      "1022  28.600000  28.972899\n",
      "1023  24.747500  24.895453\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0111\n",
      "Epoch 6/25, Validation Loss: 0.0065\n",
      "         actual  predicted\n",
      "0     25.600000  25.855075\n",
      "1     29.352857  29.728082\n",
      "2     34.480000  35.934603\n",
      "3     22.680000  22.601909\n",
      "4     23.980000  24.113981\n",
      "...         ...        ...\n",
      "1019  28.880000  28.937901\n",
      "1020  26.450000  26.288941\n",
      "1021  14.800000  14.256919\n",
      "1022  28.600000  28.829420\n",
      "1023  24.747500  24.742470\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0103\n",
      "Epoch 7/25, Validation Loss: 0.0056\n",
      "         actual  predicted\n",
      "0     25.600000  25.615441\n",
      "1     29.352857  29.276144\n",
      "2     34.480000  34.380693\n",
      "3     22.680000  22.600548\n",
      "4     23.980000  23.957764\n",
      "...         ...        ...\n",
      "1019  28.880000  28.738419\n",
      "1020  26.450000  26.115414\n",
      "1021  14.800000  14.641892\n",
      "1022  28.600000  28.545554\n",
      "1023  24.747500  24.663127\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0101\n",
      "Epoch 8/25, Validation Loss: 0.0028\n",
      "         actual  predicted\n",
      "0     25.600000  25.716598\n",
      "1     29.352857  29.487093\n",
      "2     34.480000  34.814708\n",
      "3     22.680000  22.650288\n",
      "4     23.980000  24.055246\n",
      "...         ...        ...\n",
      "1019  28.880000  28.797347\n",
      "1020  26.450000  26.295263\n",
      "1021  14.800000  14.370427\n",
      "1022  28.600000  28.636511\n",
      "1023  24.747500  24.724354\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0093\n",
      "Epoch 9/25, Validation Loss: 0.0029\n",
      "         actual  predicted\n",
      "0     25.600000  25.711151\n",
      "1     29.352857  29.467519\n",
      "2     34.480000  34.843990\n",
      "3     22.680000  22.682832\n",
      "4     23.980000  23.987915\n",
      "...         ...        ...\n",
      "1019  28.880000  28.864574\n",
      "1020  26.450000  26.191717\n",
      "1021  14.800000  13.989153\n",
      "1022  28.600000  28.583386\n",
      "1023  24.747500  24.774658\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0102\n",
      "Epoch 10/25, Validation Loss: 0.0038\n",
      "         actual  predicted\n",
      "0     25.600000  25.918311\n",
      "1     29.352857  29.839702\n",
      "2     34.480000  35.591742\n",
      "3     22.680000  22.644872\n",
      "4     23.980000  24.074982\n",
      "...         ...        ...\n",
      "1019  28.880000  29.140648\n",
      "1020  26.450000  26.475045\n",
      "1021  14.800000  14.553333\n",
      "1022  28.600000  28.976951\n",
      "1023  24.747500  25.001570\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0084\n",
      "Epoch 11/25, Validation Loss: 0.0063\n",
      "         actual  predicted\n",
      "0     25.600000  25.878890\n",
      "1     29.352857  29.732881\n",
      "2     34.480000  35.169614\n",
      "3     22.680000  22.633157\n",
      "4     23.980000  24.044193\n",
      "...         ...        ...\n",
      "1019  28.880000  28.810941\n",
      "1020  26.450000  26.264620\n",
      "1021  14.800000  14.854186\n",
      "1022  28.600000  28.597395\n",
      "1023  24.747500  24.738337\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0075\n",
      "Epoch 12/25, Validation Loss: 0.0037\n",
      "         actual  predicted\n",
      "0     25.600000  25.743647\n",
      "1     29.352857  29.683029\n",
      "2     34.480000  35.440117\n",
      "3     22.680000  22.642966\n",
      "4     23.980000  23.957702\n",
      "...         ...        ...\n",
      "1019  28.880000  28.879418\n",
      "1020  26.450000  26.216986\n",
      "1021  14.800000  14.837862\n",
      "1022  28.600000  28.632794\n",
      "1023  24.747500  24.733338\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0068\n",
      "Epoch 13/25, Validation Loss: 0.0027\n",
      "         actual  predicted\n",
      "0     25.600000  25.715096\n",
      "1     29.352857  29.468258\n",
      "2     34.480000  35.289610\n",
      "3     22.680000  22.747150\n",
      "4     23.980000  24.055154\n",
      "...         ...        ...\n",
      "1019  28.880000  28.762714\n",
      "1020  26.450000  26.240789\n",
      "1021  14.800000  14.622807\n",
      "1022  28.600000  28.558404\n",
      "1023  24.747500  24.731728\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0066\n",
      "Epoch 14/25, Validation Loss: 0.0021\n",
      "         actual  predicted\n",
      "0     25.600000  25.655691\n",
      "1     29.352857  29.422069\n",
      "2     34.480000  35.022791\n",
      "3     22.680000  22.702862\n",
      "4     23.980000  23.983933\n",
      "...         ...        ...\n",
      "1019  28.880000  28.712830\n",
      "1020  26.450000  26.249784\n",
      "1021  14.800000  14.429452\n",
      "1022  28.600000  28.612062\n",
      "1023  24.747500  24.774742\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0064\n",
      "Epoch 15/25, Validation Loss: 0.0024\n",
      "         actual  predicted\n",
      "0     25.600000  25.842594\n",
      "1     29.352857  29.624583\n",
      "2     34.480000  35.531261\n",
      "3     22.680000  22.732141\n",
      "4     23.980000  24.139867\n",
      "...         ...        ...\n",
      "1019  28.880000  28.821464\n",
      "1020  26.450000  26.252085\n",
      "1021  14.800000  14.526046\n",
      "1022  28.600000  28.560634\n",
      "1023  24.747500  24.768835\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0064\n",
      "Epoch 16/25, Validation Loss: 0.0026\n",
      "         actual  predicted\n",
      "0     25.600000  25.645884\n",
      "1     29.352857  29.597996\n",
      "2     34.480000  35.108691\n",
      "3     22.680000  22.589131\n",
      "4     23.980000  23.909997\n",
      "...         ...        ...\n",
      "1019  28.880000  28.711756\n",
      "1020  26.450000  26.136636\n",
      "1021  14.800000  14.565837\n",
      "1022  28.600000  28.557798\n",
      "1023  24.747500  24.710529\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0062\n",
      "Epoch 17/25, Validation Loss: 0.0027\n",
      "         actual  predicted\n",
      "0     25.600000  25.638158\n",
      "1     29.352857  29.587055\n",
      "2     34.480000  35.514588\n",
      "3     22.680000  22.647077\n",
      "4     23.980000  23.992172\n",
      "...         ...        ...\n",
      "1019  28.880000  28.782849\n",
      "1020  26.450000  26.231336\n",
      "1021  14.800000  14.245408\n",
      "1022  28.600000  28.633689\n",
      "1023  24.747500  24.762309\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0062\n",
      "Epoch 18/25, Validation Loss: 0.0027\n",
      "         actual  predicted\n",
      "0     25.600000  25.718691\n",
      "1     29.352857  29.447826\n",
      "2     34.480000  34.599394\n",
      "3     22.680000  22.656438\n",
      "4     23.980000  24.009926\n",
      "...         ...        ...\n",
      "1019  28.880000  28.865434\n",
      "1020  26.450000  26.274546\n",
      "1021  14.800000  14.699607\n",
      "1022  28.600000  28.699903\n",
      "1023  24.747500  24.778625\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0058\n",
      "Epoch 19/25, Validation Loss: 0.0017\n",
      "         actual  predicted\n",
      "0     25.600000  25.677487\n",
      "1     29.352857  29.656790\n",
      "2     34.480000  35.423084\n",
      "3     22.680000  22.701161\n",
      "4     23.980000  24.057556\n",
      "...         ...        ...\n",
      "1019  28.880000  28.826869\n",
      "1020  26.450000  26.189966\n",
      "1021  14.800000  14.667037\n",
      "1022  28.600000  28.622779\n",
      "1023  24.747500  24.747450\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0056\n",
      "Epoch 20/25, Validation Loss: 0.0019\n",
      "         actual  predicted\n",
      "0     25.600000  25.744668\n",
      "1     29.352857  29.601804\n",
      "2     34.480000  34.848288\n",
      "3     22.680000  22.724023\n",
      "4     23.980000  24.073241\n",
      "...         ...        ...\n",
      "1019  28.880000  28.925454\n",
      "1020  26.450000  26.340810\n",
      "1021  14.800000  14.518656\n",
      "1022  28.600000  28.757586\n",
      "1023  24.747500  24.821494\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0056\n",
      "Epoch 21/25, Validation Loss: 0.0022\n",
      "         actual  predicted\n",
      "0     25.600000  25.658602\n",
      "1     29.352857  29.592064\n",
      "2     34.480000  35.252381\n",
      "3     22.680000  22.682537\n",
      "4     23.980000  23.990979\n",
      "...         ...        ...\n",
      "1019  28.880000  28.782924\n",
      "1020  26.450000  26.282258\n",
      "1021  14.800000  14.524377\n",
      "1022  28.600000  28.667094\n",
      "1023  24.747500  24.791162\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0056\n",
      "Epoch 22/25, Validation Loss: 0.0024\n",
      "         actual  predicted\n",
      "0     25.600000  25.647067\n",
      "1     29.352857  29.502058\n",
      "2     34.480000  34.975061\n",
      "3     22.680000  22.705744\n",
      "4     23.980000  24.020713\n",
      "...         ...        ...\n",
      "1019  28.880000  28.873372\n",
      "1020  26.450000  26.319582\n",
      "1021  14.800000  14.487950\n",
      "1022  28.600000  28.683544\n",
      "1023  24.747500  24.805039\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0056\n",
      "Epoch 23/25, Validation Loss: 0.0018\n",
      "         actual  predicted\n",
      "0     25.600000  25.638507\n",
      "1     29.352857  29.442178\n",
      "2     34.480000  34.935518\n",
      "3     22.680000  22.735756\n",
      "4     23.980000  24.036755\n",
      "...         ...        ...\n",
      "1019  28.880000  28.813897\n",
      "1020  26.450000  26.270450\n",
      "1021  14.800000  14.492872\n",
      "1022  28.600000  28.681705\n",
      "1023  24.747500  24.818647\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[0.2321]], device='cuda:0')\n",
      "[24.9]\n",
      "          actual  predicted\n",
      "0      25.600000  25.638507\n",
      "1      29.352857  29.442178\n",
      "2      34.480000  34.935518\n",
      "3      22.680000  22.735756\n",
      "4      23.980000  24.036755\n",
      "...          ...        ...\n",
      "79080  22.700000  22.757748\n",
      "79081  23.250000  23.281401\n",
      "79082  24.450000  24.264581\n",
      "79083  15.270000  15.683490\n",
      "79084  21.940000  22.338152\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 0.1515\n",
      "Score (MAE): 0.0882\n",
      "Score (ME): 0.0061\n",
      "Score (MAPE): 0.3633%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0       5.999995   -0.488680\n",
      "1      17.999995   30.922443\n",
      "2       7.000006   29.680927\n",
      "3       3.999997  -11.456685\n",
      "4      16.999996    1.704646\n",
      "...          ...         ...\n",
      "1019  235.999999  107.457327\n",
      "1020    7.000006    2.033799\n",
      "1021   73.999996   37.655155\n",
      "1022    4.999996  -10.241427\n",
      "1023   68.000002   28.405523\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.5611\n",
      "Epoch 1/25, Validation Loss: 0.5046\n",
      "          actual   predicted\n",
      "0       5.999995   45.814720\n",
      "1      17.999995  153.696615\n",
      "2       7.000006  112.647203\n",
      "3       3.999997   30.637836\n",
      "4      16.999996   53.889674\n",
      "...          ...         ...\n",
      "1019  235.999999  323.839180\n",
      "1020    7.000006   74.918712\n",
      "1021   73.999996  231.885907\n",
      "1022    4.999996   32.651529\n",
      "1023   68.000002  188.016397\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.4658\n",
      "Epoch 2/25, Validation Loss: 0.5089\n",
      "          actual   predicted\n",
      "0       5.999995   47.301237\n",
      "1      17.999995  102.651511\n",
      "2       7.000006  133.786468\n",
      "3       3.999997   31.334761\n",
      "4      16.999996   50.524976\n",
      "...          ...         ...\n",
      "1019  235.999999  225.047714\n",
      "1020    7.000006   37.696945\n",
      "1021   73.999996  118.038179\n",
      "1022    4.999996   31.240109\n",
      "1023   68.000002  104.316921\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.4530\n",
      "Epoch 3/25, Validation Loss: 0.5085\n",
      "          actual   predicted\n",
      "0       5.999995   -4.050840\n",
      "1      17.999995   62.632112\n",
      "2       7.000006   65.349296\n",
      "3       3.999997  -16.152171\n",
      "4      16.999996   12.413170\n",
      "...          ...         ...\n",
      "1019  235.999999  136.556814\n",
      "1020    7.000006   -5.673014\n",
      "1021   73.999996   58.668549\n",
      "1022    4.999996  -13.998371\n",
      "1023   68.000002   31.244894\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.4495\n",
      "Epoch 4/25, Validation Loss: 0.4623\n",
      "          actual   predicted\n",
      "0       5.999995   41.833932\n",
      "1      17.999995   93.833713\n",
      "2       7.000006   83.264208\n",
      "3       3.999997   37.259748\n",
      "4      16.999996   55.297239\n",
      "...          ...         ...\n",
      "1019  235.999999  245.872205\n",
      "1020    7.000006   56.991675\n",
      "1021   73.999996  125.124755\n",
      "1022    4.999996   37.549207\n",
      "1023   68.000002  104.745718\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.4423\n",
      "Epoch 5/25, Validation Loss: 0.4770\n",
      "          actual   predicted\n",
      "0       5.999995   11.511466\n",
      "1      17.999995   36.297538\n",
      "2       7.000006   33.590877\n",
      "3       3.999997    5.744616\n",
      "4      16.999996   17.538714\n",
      "...          ...         ...\n",
      "1019  235.999999  114.817022\n",
      "1020    7.000006   16.487680\n",
      "1021   73.999996   44.981612\n",
      "1022    4.999996    7.121318\n",
      "1023   68.000002   39.725576\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.4356\n",
      "Epoch 6/25, Validation Loss: 0.4629\n",
      "          actual   predicted\n",
      "0       5.999995   18.581680\n",
      "1      17.999995   53.610111\n",
      "2       7.000006   78.722759\n",
      "3       3.999997   12.442442\n",
      "4      16.999996   22.197162\n",
      "...          ...         ...\n",
      "1019  235.999999  236.212159\n",
      "1020    7.000006   18.160811\n",
      "1021   73.999996   81.966368\n",
      "1022    4.999996   12.120932\n",
      "1023   68.000002   42.996518\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.4339\n",
      "Epoch 7/25, Validation Loss: 0.4894\n",
      "          actual   predicted\n",
      "0       5.999995   11.992391\n",
      "1      17.999995   31.788611\n",
      "2       7.000006   40.523137\n",
      "3       3.999997    4.822168\n",
      "4      16.999996   15.365034\n",
      "...          ...         ...\n",
      "1019  235.999999  172.449172\n",
      "1020    7.000006   11.510143\n",
      "1021   73.999996   44.222770\n",
      "1022    4.999996    2.704342\n",
      "1023   68.000002   25.700487\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.4358\n",
      "Epoch 8/25, Validation Loss: 0.4621\n",
      "          actual   predicted\n",
      "0       5.999995   42.918520\n",
      "1      17.999995  111.908355\n",
      "2       7.000006  123.145596\n",
      "3       3.999997   38.044880\n",
      "4      16.999996   54.166716\n",
      "...          ...         ...\n",
      "1019  235.999999  314.395279\n",
      "1020    7.000006   47.518928\n",
      "1021   73.999996  131.834858\n",
      "1022    4.999996   37.543503\n",
      "1023   68.000002  100.798965\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.4318\n",
      "Epoch 9/25, Validation Loss: 0.4558\n",
      "          actual   predicted\n",
      "0       5.999995   11.336002\n",
      "1      17.999995   24.320950\n",
      "2       7.000006   27.071955\n",
      "3       3.999997    7.906294\n",
      "4      16.999996   16.341567\n",
      "...          ...         ...\n",
      "1019  235.999999  109.553439\n",
      "1020    7.000006   12.833400\n",
      "1021   73.999996   26.663134\n",
      "1022    4.999996    8.464781\n",
      "1023   68.000002   25.936097\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.4357\n",
      "Epoch 10/25, Validation Loss: 0.4683\n",
      "          actual  predicted\n",
      "0       5.999995  15.027488\n",
      "1      17.999995  27.259657\n",
      "2       7.000006  29.115537\n",
      "3       3.999997  12.861944\n",
      "4      16.999996  19.462171\n",
      "...          ...        ...\n",
      "1019  235.999999  67.109728\n",
      "1020    7.000006  15.981786\n",
      "1021   73.999996  27.660096\n",
      "1022    4.999996  13.267695\n",
      "1023   68.000002  25.245068\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.4310\n",
      "Epoch 11/25, Validation Loss: 0.4463\n",
      "          actual   predicted\n",
      "0       5.999995   36.062074\n",
      "1      17.999995  139.341939\n",
      "2       7.000006  166.163061\n",
      "3       3.999997   28.227963\n",
      "4      16.999996   53.862823\n",
      "...          ...         ...\n",
      "1019  235.999999  258.915203\n",
      "1020    7.000006   32.868917\n",
      "1021   73.999996   96.860617\n",
      "1022    4.999996   24.459297\n",
      "1023   68.000002   83.534727\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.4281\n",
      "Epoch 12/25, Validation Loss: 0.4583\n",
      "          actual   predicted\n",
      "0       5.999995   25.662944\n",
      "1      17.999995   79.182896\n",
      "2       7.000006   87.026509\n",
      "3       3.999997   20.170178\n",
      "4      16.999996   36.077640\n",
      "...          ...         ...\n",
      "1019  235.999999  256.013557\n",
      "1020    7.000006   26.792437\n",
      "1021   73.999996   50.103939\n",
      "1022    4.999996   16.300046\n",
      "1023   68.000002   49.293805\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.4273\n",
      "Epoch 13/25, Validation Loss: 0.4475\n",
      "          actual   predicted\n",
      "0       5.999995   42.364649\n",
      "1      17.999995   75.626586\n",
      "2       7.000006  110.517442\n",
      "3       3.999997   38.602931\n",
      "4      16.999996   55.014090\n",
      "...          ...         ...\n",
      "1019  235.999999  233.782053\n",
      "1020    7.000006   39.969614\n",
      "1021   73.999996   84.794947\n",
      "1022    4.999996   35.553579\n",
      "1023   68.000002   61.939693\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.4739\n",
      "Epoch 14/25, Validation Loss: 0.5968\n",
      "          actual   predicted\n",
      "0       5.999995    1.974459\n",
      "1      17.999995   16.563303\n",
      "2       7.000006   17.605686\n",
      "3       3.999997   -1.420474\n",
      "4      16.999996    8.368616\n",
      "...          ...         ...\n",
      "1019  235.999999  197.242313\n",
      "1020    7.000006    4.194669\n",
      "1021   73.999996   17.245053\n",
      "1022    4.999996   -2.536228\n",
      "1023   68.000002   15.593473\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.4651\n",
      "Epoch 15/25, Validation Loss: 0.4415\n",
      "          actual   predicted\n",
      "0       5.999995   15.306076\n",
      "1      17.999995   64.541492\n",
      "2       7.000006   48.675832\n",
      "3       3.999997   12.208535\n",
      "4      16.999996   18.912424\n",
      "...          ...         ...\n",
      "1019  235.999999  347.881955\n",
      "1020    7.000006   18.995063\n",
      "1021   73.999996   93.539379\n",
      "1022    4.999996   10.579323\n",
      "1023   68.000002   65.048922\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.4265\n",
      "Epoch 16/25, Validation Loss: 0.4571\n",
      "          actual   predicted\n",
      "0       5.999995   38.561376\n",
      "1      17.999995   41.606021\n",
      "2       7.000006   42.182686\n",
      "3       3.999997   36.991448\n",
      "4      16.999996   39.811061\n",
      "...          ...         ...\n",
      "1019  235.999999  112.772691\n",
      "1020    7.000006   38.751532\n",
      "1021   73.999996   41.550548\n",
      "1022    4.999996   36.223518\n",
      "1023   68.000002   41.008086\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.4197\n",
      "Epoch 17/25, Validation Loss: 0.4513\n",
      "          actual   predicted\n",
      "0       5.999995   12.172876\n",
      "1      17.999995   26.440949\n",
      "2       7.000006   34.514547\n",
      "3       3.999997    8.426365\n",
      "4      16.999996   16.960615\n",
      "...          ...         ...\n",
      "1019  235.999999  274.020005\n",
      "1020    7.000006   13.046698\n",
      "1021   73.999996   46.086416\n",
      "1022    4.999996    7.141625\n",
      "1023   68.000002   24.076542\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.4133\n",
      "Epoch 18/25, Validation Loss: 0.4113\n",
      "          actual   predicted\n",
      "0       5.999995    7.307014\n",
      "1      17.999995   17.864987\n",
      "2       7.000006   23.402480\n",
      "3       3.999997    3.580350\n",
      "4      16.999996   11.523569\n",
      "...          ...         ...\n",
      "1019  235.999999  173.001173\n",
      "1020    7.000006    8.353510\n",
      "1021   73.999996   30.468099\n",
      "1022    4.999996    2.997331\n",
      "1023   68.000002   21.576253\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.4263\n",
      "Epoch 19/25, Validation Loss: 0.4454\n",
      "          actual   predicted\n",
      "0       5.999995   10.213513\n",
      "1      17.999995   19.574350\n",
      "2       7.000006   25.814740\n",
      "3       3.999997    6.778908\n",
      "4      16.999996   17.082578\n",
      "...          ...         ...\n",
      "1019  235.999999  153.727073\n",
      "1020    7.000006   10.677304\n",
      "1021   73.999996   29.398977\n",
      "1022    4.999996    6.132784\n",
      "1023   68.000002   20.642643\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.4110\n",
      "Epoch 20/25, Validation Loss: 0.4478\n",
      "          actual   predicted\n",
      "0       5.999995   27.308384\n",
      "1      17.999995   34.066020\n",
      "2       7.000006   37.312453\n",
      "3       3.999997   24.316523\n",
      "4      16.999996   29.489404\n",
      "...          ...         ...\n",
      "1019  235.999999  302.813425\n",
      "1020    7.000006   27.958105\n",
      "1021   73.999996  107.082820\n",
      "1022    4.999996   24.560863\n",
      "1023   68.000002   39.407776\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.4245\n",
      "Epoch 21/25, Validation Loss: 0.4560\n",
      "          actual   predicted\n",
      "0       5.999995    6.721160\n",
      "1      17.999995   13.284426\n",
      "2       7.000006   14.816174\n",
      "3       3.999997    2.801157\n",
      "4      16.999996   10.763439\n",
      "...          ...         ...\n",
      "1019  235.999999  212.595540\n",
      "1020    7.000006    7.364538\n",
      "1021   73.999996   22.666074\n",
      "1022    4.999996    2.586134\n",
      "1023   68.000002   12.784494\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.5076\n",
      "Epoch 22/25, Validation Loss: 0.4341\n",
      "          actual   predicted\n",
      "0       5.999995   10.022696\n",
      "1      17.999995   20.114111\n",
      "2       7.000006   43.154433\n",
      "3       3.999997    6.175672\n",
      "4      16.999996   16.210515\n",
      "...          ...         ...\n",
      "1019  235.999999  237.002946\n",
      "1020    7.000006   12.283385\n",
      "1021   73.999996   38.811264\n",
      "1022    4.999996    5.767635\n",
      "1023   68.000002   23.963466\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[-0.1120]], device='cuda:0')\n",
      "[95.86]\n",
      "            actual    predicted\n",
      "0         5.999995    10.022696\n",
      "1        17.999995    20.114111\n",
      "2         7.000006    43.154433\n",
      "3         3.999997     6.175672\n",
      "4        16.999996    16.210515\n",
      "...            ...          ...\n",
      "79080  1860.000013  1677.822498\n",
      "79081    31.000004    16.599052\n",
      "79082     5.999995     7.752190\n",
      "79083   102.000002    98.824166\n",
      "79084     5.999995     9.944496\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 495.2475\n",
      "Score (MAE): 53.4388\n",
      "Score (ME): 10.2329\n",
      "Score (MAPE): 537041.0096%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     493.000000  466.735051\n",
      "1     451.500000  440.218148\n",
      "2     406.500003  390.837389\n",
      "3     696.999994  787.018361\n",
      "4     423.000003  417.898235\n",
      "...          ...         ...\n",
      "1019  426.500001  443.096209\n",
      "1020  440.500002  429.074214\n",
      "1021  431.499999  418.600563\n",
      "1022  434.500001  437.850825\n",
      "1023  449.500000  445.037454\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2304\n",
      "Epoch 1/25, Validation Loss: 0.0890\n",
      "          actual   predicted\n",
      "0     493.000000  470.986962\n",
      "1     451.500000  431.270339\n",
      "2     406.500003  395.649302\n",
      "3     696.999994  802.013836\n",
      "4     423.000003  422.052627\n",
      "...          ...         ...\n",
      "1019  426.500001  437.085244\n",
      "1020  440.500002  443.359152\n",
      "1021  431.499999  418.265854\n",
      "1022  434.500001  427.239602\n",
      "1023  449.500000  443.320056\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.1212\n",
      "Epoch 2/25, Validation Loss: 0.0993\n",
      "          actual   predicted\n",
      "0     493.000000  479.291618\n",
      "1     451.500000  444.571862\n",
      "2     406.500003  400.101056\n",
      "3     696.999994  759.628491\n",
      "4     423.000003  428.007793\n",
      "...          ...         ...\n",
      "1019  426.500001  451.578535\n",
      "1020  440.500002  445.865574\n",
      "1021  431.499999  422.585981\n",
      "1022  434.500001  424.176294\n",
      "1023  449.500000  446.155933\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1117\n",
      "Epoch 3/25, Validation Loss: 0.0899\n",
      "          actual   predicted\n",
      "0     493.000000  476.021250\n",
      "1     451.500000  436.747812\n",
      "2     406.500003  396.652955\n",
      "3     696.999994  753.381402\n",
      "4     423.000003  422.938634\n",
      "...          ...         ...\n",
      "1019  426.500001  442.115934\n",
      "1020  440.500002  440.390346\n",
      "1021  431.499999  417.205690\n",
      "1022  434.500001  422.358171\n",
      "1023  449.500000  437.664936\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1081\n",
      "Epoch 4/25, Validation Loss: 0.0836\n",
      "          actual   predicted\n",
      "0     493.000000  488.739186\n",
      "1     451.500000  441.824578\n",
      "2     406.500003  404.421709\n",
      "3     696.999994  693.394236\n",
      "4     423.000003  430.042114\n",
      "...          ...         ...\n",
      "1019  426.500001  438.619885\n",
      "1020  440.500002  439.051002\n",
      "1021  431.499999  424.581126\n",
      "1022  434.500001  428.917513\n",
      "1023  449.500000  434.428009\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.1018\n",
      "Epoch 5/25, Validation Loss: 0.0829\n",
      "          actual   predicted\n",
      "0     493.000000  493.728219\n",
      "1     451.500000  442.292535\n",
      "2     406.500003  388.241512\n",
      "3     696.999994  787.162788\n",
      "4     423.000003  427.214566\n",
      "...          ...         ...\n",
      "1019  426.500001  439.760897\n",
      "1020  440.500002  434.823982\n",
      "1021  431.499999  420.632192\n",
      "1022  434.500001  429.762460\n",
      "1023  449.500000  440.087205\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0998\n",
      "Epoch 6/25, Validation Loss: 0.0926\n",
      "          actual   predicted\n",
      "0     493.000000  485.976262\n",
      "1     451.500000  441.562670\n",
      "2     406.500003  402.144379\n",
      "3     696.999994  734.504280\n",
      "4     423.000003  431.654068\n",
      "...          ...         ...\n",
      "1019  426.500001  438.907360\n",
      "1020  440.500002  444.351562\n",
      "1021  431.499999  422.170402\n",
      "1022  434.500001  432.180051\n",
      "1023  449.500000  437.538633\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0967\n",
      "Epoch 7/25, Validation Loss: 0.0773\n",
      "          actual   predicted\n",
      "0     493.000000  477.619735\n",
      "1     451.500000  444.051207\n",
      "2     406.500003  403.876134\n",
      "3     696.999994  753.467619\n",
      "4     423.000003  425.478151\n",
      "...          ...         ...\n",
      "1019  426.500001  441.604654\n",
      "1020  440.500002  441.969623\n",
      "1021  431.499999  419.224135\n",
      "1022  434.500001  426.965705\n",
      "1023  449.500000  442.902931\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0947\n",
      "Epoch 8/25, Validation Loss: 0.0828\n",
      "          actual   predicted\n",
      "0     493.000000  485.501953\n",
      "1     451.500000  438.614613\n",
      "2     406.500003  396.204789\n",
      "3     696.999994  765.519611\n",
      "4     423.000003  420.418551\n",
      "...          ...         ...\n",
      "1019  426.500001  436.984959\n",
      "1020  440.500002  435.136099\n",
      "1021  431.499999  415.595658\n",
      "1022  434.500001  426.711784\n",
      "1023  449.500000  444.321652\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0931\n",
      "Epoch 9/25, Validation Loss: 0.0800\n",
      "          actual   predicted\n",
      "0     493.000000  482.860031\n",
      "1     451.500000  436.581864\n",
      "2     406.500003  401.335362\n",
      "3     696.999994  736.077657\n",
      "4     423.000003  421.225057\n",
      "...          ...         ...\n",
      "1019  426.500001  435.623707\n",
      "1020  440.500002  427.381886\n",
      "1021  431.499999  412.532843\n",
      "1022  434.500001  424.631147\n",
      "1023  449.500000  435.921550\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0919\n",
      "Epoch 10/25, Validation Loss: 0.0790\n",
      "          actual   predicted\n",
      "0     493.000000  489.876289\n",
      "1     451.500000  440.649494\n",
      "2     406.500003  398.684568\n",
      "3     696.999994  770.884521\n",
      "4     423.000003  424.940864\n",
      "...          ...         ...\n",
      "1019  426.500001  443.077618\n",
      "1020  440.500002  438.875138\n",
      "1021  431.499999  420.006221\n",
      "1022  434.500001  428.790917\n",
      "1023  449.500000  446.079933\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0910\n",
      "Epoch 11/25, Validation Loss: 0.0753\n",
      "          actual   predicted\n",
      "0     493.000000  492.229457\n",
      "1     451.500000  444.484701\n",
      "2     406.500003  400.293578\n",
      "3     696.999994  732.997913\n",
      "4     423.000003  425.218852\n",
      "...          ...         ...\n",
      "1019  426.500001  435.924754\n",
      "1020  440.500002  435.842887\n",
      "1021  431.499999  420.118054\n",
      "1022  434.500001  428.285646\n",
      "1023  449.500000  439.817264\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0890\n",
      "Epoch 12/25, Validation Loss: 0.0746\n",
      "          actual   predicted\n",
      "0     493.000000  495.672525\n",
      "1     451.500000  441.368399\n",
      "2     406.500003  404.014688\n",
      "3     696.999994  727.448564\n",
      "4     423.000003  427.023118\n",
      "...          ...         ...\n",
      "1019  426.500001  434.129313\n",
      "1020  440.500002  441.596697\n",
      "1021  431.499999  421.613802\n",
      "1022  434.500001  427.831752\n",
      "1023  449.500000  436.583180\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0884\n",
      "Epoch 13/25, Validation Loss: 0.0756\n",
      "          actual   predicted\n",
      "0     493.000000  490.450658\n",
      "1     451.500000  447.815768\n",
      "2     406.500003  404.692244\n",
      "3     696.999994  726.127838\n",
      "4     423.000003  425.141509\n",
      "...          ...         ...\n",
      "1019  426.500001  443.598277\n",
      "1020  440.500002  442.802195\n",
      "1021  431.499999  423.081926\n",
      "1022  434.500001  432.658045\n",
      "1023  449.500000  443.971010\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0884\n",
      "Epoch 14/25, Validation Loss: 0.0918\n",
      "          actual   predicted\n",
      "0     493.000000  486.165956\n",
      "1     451.500000  444.574837\n",
      "2     406.500003  400.403418\n",
      "3     696.999994  747.443654\n",
      "4     423.000003  422.749466\n",
      "...          ...         ...\n",
      "1019  426.500001  437.217514\n",
      "1020  440.500002  438.648693\n",
      "1021  431.499999  422.341738\n",
      "1022  434.500001  431.794867\n",
      "1023  449.500000  437.843583\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0870\n",
      "Epoch 15/25, Validation Loss: 0.0764\n",
      "          actual   predicted\n",
      "0     493.000000  493.330438\n",
      "1     451.500000  444.488168\n",
      "2     406.500003  404.107058\n",
      "3     696.999994  736.548272\n",
      "4     423.000003  429.281436\n",
      "...          ...         ...\n",
      "1019  426.500001  441.862227\n",
      "1020  440.500002  432.086622\n",
      "1021  431.499999  422.242240\n",
      "1022  434.500001  432.087502\n",
      "1023  449.500000  443.012876\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0890\n",
      "Epoch 16/25, Validation Loss: 0.0714\n",
      "          actual   predicted\n",
      "0     493.000000  501.443982\n",
      "1     451.500000  452.132018\n",
      "2     406.500003  404.566054\n",
      "3     696.999994  760.741947\n",
      "4     423.000003  430.362575\n",
      "...          ...         ...\n",
      "1019  426.500001  441.778006\n",
      "1020  440.500002  450.300587\n",
      "1021  431.499999  428.548272\n",
      "1022  434.500001  437.629013\n",
      "1023  449.500000  443.326546\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0843\n",
      "Epoch 17/25, Validation Loss: 0.0754\n",
      "          actual   predicted\n",
      "0     493.000000  489.207374\n",
      "1     451.500000  444.787962\n",
      "2     406.500003  399.033183\n",
      "3     696.999994  769.215256\n",
      "4     423.000003  427.018614\n",
      "...          ...         ...\n",
      "1019  426.500001  432.193600\n",
      "1020  440.500002  440.078880\n",
      "1021  431.499999  423.212095\n",
      "1022  434.500001  433.481096\n",
      "1023  449.500000  434.943960\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0848\n",
      "Epoch 18/25, Validation Loss: 0.0736\n",
      "          actual   predicted\n",
      "0     493.000000  493.796520\n",
      "1     451.500000  441.983544\n",
      "2     406.500003  401.204817\n",
      "3     696.999994  744.364828\n",
      "4     423.000003  425.972412\n",
      "...          ...         ...\n",
      "1019  426.500001  432.628526\n",
      "1020  440.500002  440.538993\n",
      "1021  431.499999  422.310919\n",
      "1022  434.500001  434.846491\n",
      "1023  449.500000  436.395911\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0830\n",
      "Epoch 19/25, Validation Loss: 0.0750\n",
      "          actual   predicted\n",
      "0     493.000000  489.508130\n",
      "1     451.500000  440.923086\n",
      "2     406.500003  406.844331\n",
      "3     696.999994  730.018224\n",
      "4     423.000003  422.015009\n",
      "...          ...         ...\n",
      "1019  426.500001  431.902646\n",
      "1020  440.500002  433.395364\n",
      "1021  431.499999  416.113323\n",
      "1022  434.500001  425.968652\n",
      "1023  449.500000  438.600384\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0829\n",
      "Epoch 20/25, Validation Loss: 0.0766\n",
      "          actual   predicted\n",
      "0     493.000000  497.457413\n",
      "1     451.500000  445.364853\n",
      "2     406.500003  404.993794\n",
      "3     696.999994  760.942870\n",
      "4     423.000003  427.309563\n",
      "...          ...         ...\n",
      "1019  426.500001  438.659971\n",
      "1020  440.500002  444.532882\n",
      "1021  431.499999  426.650404\n",
      "1022  434.500001  435.930838\n",
      "1023  449.500000  444.357981\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[-0.0636]], device='cuda:0')\n",
      "[481.02]\n",
      "           actual   predicted\n",
      "0      493.000000  497.457413\n",
      "1      451.500000  445.364853\n",
      "2      406.500003  404.993794\n",
      "3      696.999994  760.942870\n",
      "4      423.000003  427.309563\n",
      "...           ...         ...\n",
      "39163  408.000001  396.372914\n",
      "39164  420.000001  422.624901\n",
      "39165  414.500004  412.145399\n",
      "39166  496.000000  496.989764\n",
      "39167  463.000000  441.102245\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 33.7962\n",
      "Score (MAE): 11.6178\n",
      "Score (ME): -2.1522\n",
      "Score (MAPE): 1.9892%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      619.500001   581.730798\n",
      "1      764.500000   779.263030\n",
      "2     1704.562493  2039.298163\n",
      "3      680.000001   810.982164\n",
      "4      676.999996   735.491971\n",
      "...           ...          ...\n",
      "1019   744.500000   689.649635\n",
      "1020   587.000004   610.758953\n",
      "1021   591.499998   609.263535\n",
      "1022   669.499997   609.896801\n",
      "1023   635.500003   654.624898\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.1805\n",
      "Epoch 1/25, Validation Loss: 0.0917\n",
      "           actual    predicted\n",
      "0      619.500001   598.169218\n",
      "1      764.500000   765.100389\n",
      "2     1704.562493  1803.874394\n",
      "3      680.000001   755.732815\n",
      "4      676.999996   680.749379\n",
      "...           ...          ...\n",
      "1019   744.500000   697.755696\n",
      "1020   587.000004   610.073640\n",
      "1021   591.499998   600.156220\n",
      "1022   669.499997   635.699080\n",
      "1023   635.500003   642.689566\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0949\n",
      "Epoch 2/25, Validation Loss: 0.0672\n",
      "           actual    predicted\n",
      "0      619.500001   603.906582\n",
      "1      764.500000   756.958150\n",
      "2     1704.562493  1794.814125\n",
      "3      680.000001   764.114627\n",
      "4      676.999996   681.853319\n",
      "...           ...          ...\n",
      "1019   744.500000   735.232795\n",
      "1020   587.000004   616.807704\n",
      "1021   591.499998   627.140833\n",
      "1022   669.499997   642.300362\n",
      "1023   635.500003   652.442516\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0855\n",
      "Epoch 3/25, Validation Loss: 0.0678\n",
      "           actual    predicted\n",
      "0      619.500001   594.551534\n",
      "1      764.500000   749.772192\n",
      "2     1704.562493  1777.528710\n",
      "3      680.000001   741.075174\n",
      "4      676.999996   657.768079\n",
      "...           ...          ...\n",
      "1019   744.500000   713.578955\n",
      "1020   587.000004   611.557746\n",
      "1021   591.499998   605.310616\n",
      "1022   669.499997   635.298198\n",
      "1023   635.500003   640.501651\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0810\n",
      "Epoch 4/25, Validation Loss: 0.0641\n",
      "           actual    predicted\n",
      "0      619.500001   571.393595\n",
      "1      764.500000   762.586335\n",
      "2     1704.562493  1681.590101\n",
      "3      680.000001   699.562860\n",
      "4      676.999996   666.480604\n",
      "...           ...          ...\n",
      "1019   744.500000   716.868642\n",
      "1020   587.000004   581.216503\n",
      "1021   591.499998   596.516297\n",
      "1022   669.499997   611.505158\n",
      "1023   635.500003   618.024636\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0783\n",
      "Epoch 5/25, Validation Loss: 0.0679\n",
      "           actual    predicted\n",
      "0      619.500001   609.532195\n",
      "1      764.500000   768.063713\n",
      "2     1704.562493  1719.694294\n",
      "3      680.000001   730.871751\n",
      "4      676.999996   683.632196\n",
      "...           ...          ...\n",
      "1019   744.500000   743.323781\n",
      "1020   587.000004   624.749452\n",
      "1021   591.499998   624.819069\n",
      "1022   669.499997   644.792322\n",
      "1023   635.500003   648.980159\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0762\n",
      "Epoch 6/25, Validation Loss: 0.0638\n",
      "           actual    predicted\n",
      "0      619.500001   599.477708\n",
      "1      764.500000   781.699882\n",
      "2     1704.562493  1766.377417\n",
      "3      680.000001   759.626045\n",
      "4      676.999996   671.278026\n",
      "...           ...          ...\n",
      "1019   744.500000   730.391435\n",
      "1020   587.000004   589.951066\n",
      "1021   591.499998   595.222650\n",
      "1022   669.499997   624.297799\n",
      "1023   635.500003   638.609039\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0750\n",
      "Epoch 7/25, Validation Loss: 0.0627\n",
      "           actual    predicted\n",
      "0      619.500001   596.670007\n",
      "1      764.500000   791.551958\n",
      "2     1704.562493  1749.361169\n",
      "3      680.000001   772.240924\n",
      "4      676.999996   682.858455\n",
      "...           ...          ...\n",
      "1019   744.500000   715.088148\n",
      "1020   587.000004   593.959003\n",
      "1021   591.499998   583.537236\n",
      "1022   669.499997   617.904179\n",
      "1023   635.500003   619.059694\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0718\n",
      "Epoch 8/25, Validation Loss: 0.0637\n",
      "           actual    predicted\n",
      "0      619.500001   567.648426\n",
      "1      764.500000   797.699799\n",
      "2     1704.562493  1836.098807\n",
      "3      680.000001   731.758305\n",
      "4      676.999996   679.242851\n",
      "...           ...          ...\n",
      "1019   744.500000   740.079466\n",
      "1020   587.000004   587.654739\n",
      "1021   591.499998   573.784976\n",
      "1022   669.499997   617.714979\n",
      "1023   635.500003   623.899363\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0718\n",
      "Epoch 9/25, Validation Loss: 0.0673\n",
      "           actual    predicted\n",
      "0      619.500001   586.075561\n",
      "1      764.500000   810.582762\n",
      "2     1704.562493  1759.692864\n",
      "3      680.000001   772.508354\n",
      "4      676.999996   699.147545\n",
      "...           ...          ...\n",
      "1019   744.500000   745.510451\n",
      "1020   587.000004   602.854548\n",
      "1021   591.499998   600.201081\n",
      "1022   669.499997   639.315191\n",
      "1023   635.500003   631.260696\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0704\n",
      "Epoch 10/25, Validation Loss: 0.0613\n",
      "           actual    predicted\n",
      "0      619.500001   587.239782\n",
      "1      764.500000   786.196648\n",
      "2     1704.562493  1729.742277\n",
      "3      680.000001   760.352234\n",
      "4      676.999996   687.878338\n",
      "...           ...          ...\n",
      "1019   744.500000   718.608943\n",
      "1020   587.000004   601.829823\n",
      "1021   591.499998   592.073735\n",
      "1022   669.499997   617.101190\n",
      "1023   635.500003   629.479031\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0700\n",
      "Epoch 11/25, Validation Loss: 0.0606\n",
      "           actual    predicted\n",
      "0      619.500001   605.021903\n",
      "1      764.500000   803.225090\n",
      "2     1704.562493  1763.735820\n",
      "3      680.000001   777.103669\n",
      "4      676.999996   676.746494\n",
      "...           ...          ...\n",
      "1019   744.500000   747.244903\n",
      "1020   587.000004   620.759766\n",
      "1021   591.499998   617.556934\n",
      "1022   669.499997   653.477452\n",
      "1023   635.500003   646.559730\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0688\n",
      "Epoch 12/25, Validation Loss: 0.0661\n",
      "           actual    predicted\n",
      "0      619.500001   583.616311\n",
      "1      764.500000   831.787092\n",
      "2     1704.562493  1664.524831\n",
      "3      680.000001   800.700041\n",
      "4      676.999996   703.530972\n",
      "...           ...          ...\n",
      "1019   744.500000   745.491403\n",
      "1020   587.000004   601.056974\n",
      "1021   591.499998   593.281453\n",
      "1022   669.499997   623.164015\n",
      "1023   635.500003   633.634664\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0686\n",
      "Epoch 13/25, Validation Loss: 0.0635\n",
      "           actual    predicted\n",
      "0      619.500001   592.703434\n",
      "1      764.500000   814.271847\n",
      "2     1704.562493  1673.855036\n",
      "3      680.000001   783.101890\n",
      "4      676.999996   690.312482\n",
      "...           ...          ...\n",
      "1019   744.500000   756.492893\n",
      "1020   587.000004   607.985797\n",
      "1021   591.499998   611.361990\n",
      "1022   669.499997   638.052576\n",
      "1023   635.500003   648.365390\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0684\n",
      "Epoch 14/25, Validation Loss: 0.0609\n",
      "           actual    predicted\n",
      "0      619.500001   596.053944\n",
      "1      764.500000   789.704347\n",
      "2     1704.562493  1741.700511\n",
      "3      680.000001   762.929127\n",
      "4      676.999996   680.202890\n",
      "...           ...          ...\n",
      "1019   744.500000   722.938085\n",
      "1020   587.000004   612.931168\n",
      "1021   591.499998   613.743179\n",
      "1022   669.499997   638.547674\n",
      "1023   635.500003   637.913884\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0676\n",
      "Epoch 15/25, Validation Loss: 0.0591\n",
      "           actual    predicted\n",
      "0      619.500001   608.516089\n",
      "1      764.500000   790.896554\n",
      "2     1704.562493  1806.575362\n",
      "3      680.000001   780.404536\n",
      "4      676.999996   708.912506\n",
      "...           ...          ...\n",
      "1019   744.500000   746.994181\n",
      "1020   587.000004   614.125512\n",
      "1021   591.499998   615.326168\n",
      "1022   669.499997   650.106197\n",
      "1023   635.500003   644.170368\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0659\n",
      "Epoch 16/25, Validation Loss: 0.0669\n",
      "           actual    predicted\n",
      "0      619.500001   582.147975\n",
      "1      764.500000   831.781707\n",
      "2     1704.562493  1762.286767\n",
      "3      680.000001   767.824151\n",
      "4      676.999996   703.604330\n",
      "...           ...          ...\n",
      "1019   744.500000   764.528248\n",
      "1020   587.000004   604.018595\n",
      "1021   591.499998   599.007226\n",
      "1022   669.499997   636.850434\n",
      "1023   635.500003   641.170215\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0660\n",
      "Epoch 17/25, Validation Loss: 0.0602\n",
      "           actual    predicted\n",
      "0      619.500001   595.285239\n",
      "1      764.500000   797.658820\n",
      "2     1704.562493  1722.597854\n",
      "3      680.000001   772.968617\n",
      "4      676.999996   694.493072\n",
      "...           ...          ...\n",
      "1019   744.500000   751.079192\n",
      "1020   587.000004   601.276157\n",
      "1021   591.499998   599.336202\n",
      "1022   669.499997   635.877913\n",
      "1023   635.500003   629.324692\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0651\n",
      "Epoch 18/25, Validation Loss: 0.0591\n",
      "           actual    predicted\n",
      "0      619.500001   579.666801\n",
      "1      764.500000   791.140450\n",
      "2     1704.562493  1703.544639\n",
      "3      680.000001   726.960664\n",
      "4      676.999996   672.499591\n",
      "...           ...          ...\n",
      "1019   744.500000   725.590241\n",
      "1020   587.000004   593.586198\n",
      "1021   591.499998   594.793008\n",
      "1022   669.499997   626.985270\n",
      "1023   635.500003   620.721042\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0651\n",
      "Epoch 19/25, Validation Loss: 0.0601\n",
      "           actual    predicted\n",
      "0      619.500001   603.889414\n",
      "1      764.500000   799.689698\n",
      "2     1704.562493  1713.568985\n",
      "3      680.000001   762.525374\n",
      "4      676.999996   696.470047\n",
      "...           ...          ...\n",
      "1019   744.500000   744.466582\n",
      "1020   587.000004   610.573372\n",
      "1021   591.499998   608.274160\n",
      "1022   669.499997   651.012004\n",
      "1023   635.500003   640.189109\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0630\n",
      "Epoch 20/25, Validation Loss: 0.0569\n",
      "           actual    predicted\n",
      "0      619.500001   597.589975\n",
      "1      764.500000   789.087498\n",
      "2     1704.562493  1741.134834\n",
      "3      680.000001   744.446748\n",
      "4      676.999996   693.500717\n",
      "...           ...          ...\n",
      "1019   744.500000   750.185501\n",
      "1020   587.000004   605.510988\n",
      "1021   591.499998   604.764817\n",
      "1022   669.499997   642.697608\n",
      "1023   635.500003   638.418808\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0626\n",
      "Epoch 21/25, Validation Loss: 0.0571\n",
      "           actual    predicted\n",
      "0      619.500001   589.159475\n",
      "1      764.500000   778.399600\n",
      "2     1704.562493  1728.316581\n",
      "3      680.000001   742.803298\n",
      "4      676.999996   687.311682\n",
      "...           ...          ...\n",
      "1019   744.500000   739.107850\n",
      "1020   587.000004   599.684723\n",
      "1021   591.499998   602.673530\n",
      "1022   669.499997   632.909177\n",
      "1023   635.500003   624.357013\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0624\n",
      "Epoch 22/25, Validation Loss: 0.0579\n",
      "           actual    predicted\n",
      "0      619.500001   587.024480\n",
      "1      764.500000   774.151259\n",
      "2     1704.562493  1731.233568\n",
      "3      680.000001   730.280584\n",
      "4      676.999996   681.172946\n",
      "...           ...          ...\n",
      "1019   744.500000   733.551382\n",
      "1020   587.000004   608.399827\n",
      "1021   591.499998   607.661367\n",
      "1022   669.499997   637.723880\n",
      "1023   635.500003   633.048865\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0622\n",
      "Epoch 23/25, Validation Loss: 0.0579\n",
      "           actual    predicted\n",
      "0      619.500001   587.115304\n",
      "1      764.500000   788.957063\n",
      "2     1704.562493  1704.475848\n",
      "3      680.000001   741.051157\n",
      "4      676.999996   683.902018\n",
      "...           ...          ...\n",
      "1019   744.500000   745.875704\n",
      "1020   587.000004   606.121998\n",
      "1021   591.499998   606.800002\n",
      "1022   669.499997   639.454862\n",
      "1023   635.500003   634.381202\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0625\n",
      "Epoch 24/25, Validation Loss: 0.0573\n",
      "           actual    predicted\n",
      "0      619.500001   592.371259\n",
      "1      764.500000   798.670781\n",
      "2     1704.562493  1746.308095\n",
      "3      680.000001   759.172088\n",
      "4      676.999996   692.594875\n",
      "...           ...          ...\n",
      "1019   744.500000   756.041408\n",
      "1020   587.000004   604.246240\n",
      "1021   591.499998   607.048259\n",
      "1022   669.499997   639.414459\n",
      "1023   635.500003   636.758142\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[-0.4242]], device='cuda:0')\n",
      "[666.37]\n",
      "            actual    predicted\n",
      "0       619.500001   592.371259\n",
      "1       764.500000   798.670781\n",
      "2      1704.562493  1746.308095\n",
      "3       680.000001   759.172088\n",
      "4       676.999996   692.594875\n",
      "...            ...          ...\n",
      "39163   741.500000   746.689008\n",
      "39164  1137.500009  1190.259070\n",
      "39165   620.999999   669.500268\n",
      "39166   631.999996   635.124295\n",
      "39167   701.500001   697.339376\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 70.8193\n",
      "Score (MAE): 34.0367\n",
      "Score (ME): -3.8245\n",
      "Score (MAPE): 4.0441%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     31.440000  31.519627\n",
      "1     31.680000  31.719563\n",
      "2     44.590000  46.924781\n",
      "3     65.950001  64.880549\n",
      "4     23.905000  23.790029\n",
      "...         ...        ...\n",
      "1019  29.210000  30.596909\n",
      "1020  45.050000  46.493202\n",
      "1021  29.045000  29.101068\n",
      "1022  26.495000  26.553963\n",
      "1023  35.923333  35.623502\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.1369\n",
      "Epoch 1/25, Validation Loss: 0.0147\n",
      "         actual  predicted\n",
      "0     31.440000  31.059300\n",
      "1     31.680000  31.147633\n",
      "2     44.590000  46.480117\n",
      "3     65.950001  70.523174\n",
      "4     23.905000  21.302007\n",
      "...         ...        ...\n",
      "1019  29.210000  29.330000\n",
      "1020  45.050000  46.429291\n",
      "1021  29.045000  28.529696\n",
      "1022  26.495000  25.890963\n",
      "1023  35.923333  35.289616\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0405\n",
      "Epoch 2/25, Validation Loss: 0.0169\n",
      "         actual  predicted\n",
      "0     31.440000  31.625567\n",
      "1     31.680000  32.274341\n",
      "2     44.590000  45.460170\n",
      "3     65.950001  66.548606\n",
      "4     23.905000  22.730815\n",
      "...         ...        ...\n",
      "1019  29.210000  29.675723\n",
      "1020  45.050000  45.993083\n",
      "1021  29.045000  29.125181\n",
      "1022  26.495000  26.567629\n",
      "1023  35.923333  35.233276\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0318\n",
      "Epoch 3/25, Validation Loss: 0.0088\n",
      "         actual  predicted\n",
      "0     31.440000  30.537305\n",
      "1     31.680000  30.924952\n",
      "2     44.590000  46.621373\n",
      "3     65.950001  69.948681\n",
      "4     23.905000  21.402985\n",
      "...         ...        ...\n",
      "1019  29.210000  29.821600\n",
      "1020  45.050000  47.118296\n",
      "1021  29.045000  28.646185\n",
      "1022  26.495000  25.954081\n",
      "1023  35.923333  35.455085\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0265\n",
      "Epoch 4/25, Validation Loss: 0.0201\n",
      "         actual  predicted\n",
      "0     31.440000  31.396032\n",
      "1     31.680000  31.618141\n",
      "2     44.590000  44.753454\n",
      "3     65.950001  67.014092\n",
      "4     23.905000  22.625093\n",
      "...         ...        ...\n",
      "1019  29.210000  29.293351\n",
      "1020  45.050000  45.715942\n",
      "1021  29.045000  28.733635\n",
      "1022  26.495000  25.921441\n",
      "1023  35.923333  35.384921\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0229\n",
      "Epoch 5/25, Validation Loss: 0.0078\n",
      "         actual  predicted\n",
      "0     31.440000  31.217967\n",
      "1     31.680000  31.137529\n",
      "2     44.590000  44.982925\n",
      "3     65.950001  68.941202\n",
      "4     23.905000  21.577665\n",
      "...         ...        ...\n",
      "1019  29.210000  29.704266\n",
      "1020  45.050000  45.313106\n",
      "1021  29.045000  29.131701\n",
      "1022  26.495000  26.163284\n",
      "1023  35.923333  35.066345\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0198\n",
      "Epoch 6/25, Validation Loss: 0.0074\n",
      "         actual  predicted\n",
      "0     31.440000  31.420497\n",
      "1     31.680000  31.226205\n",
      "2     44.590000  45.227172\n",
      "3     65.950001  69.599720\n",
      "4     23.905000  23.048293\n",
      "...         ...        ...\n",
      "1019  29.210000  29.638100\n",
      "1020  45.050000  45.774418\n",
      "1021  29.045000  28.988694\n",
      "1022  26.495000  26.017349\n",
      "1023  35.923333  34.785649\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0183\n",
      "Epoch 7/25, Validation Loss: 0.0068\n",
      "         actual  predicted\n",
      "0     31.440000  31.205123\n",
      "1     31.680000  31.092079\n",
      "2     44.590000  45.339229\n",
      "3     65.950001  70.790398\n",
      "4     23.905000  22.991178\n",
      "...         ...        ...\n",
      "1019  29.210000  30.338762\n",
      "1020  45.050000  46.234791\n",
      "1021  29.045000  29.489694\n",
      "1022  26.495000  26.438792\n",
      "1023  35.923333  36.043609\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0167\n",
      "Epoch 8/25, Validation Loss: 0.0097\n",
      "         actual  predicted\n",
      "0     31.440000  31.116355\n",
      "1     31.680000  31.248708\n",
      "2     44.590000  44.968473\n",
      "3     65.950001  68.118348\n",
      "4     23.905000  22.691579\n",
      "...         ...        ...\n",
      "1019  29.210000  29.710769\n",
      "1020  45.050000  45.863180\n",
      "1021  29.045000  29.434030\n",
      "1022  26.495000  26.321460\n",
      "1023  35.923333  35.505200\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0160\n",
      "Epoch 9/25, Validation Loss: 0.0070\n",
      "         actual  predicted\n",
      "0     31.440000  31.216168\n",
      "1     31.680000  31.243849\n",
      "2     44.590000  45.701291\n",
      "3     65.950001  65.019898\n",
      "4     23.905000  22.838207\n",
      "...         ...        ...\n",
      "1019  29.210000  29.286386\n",
      "1020  45.050000  45.930928\n",
      "1021  29.045000  28.752281\n",
      "1022  26.495000  26.070917\n",
      "1023  35.923333  35.401077\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0148\n",
      "Epoch 10/25, Validation Loss: 0.0075\n",
      "         actual  predicted\n",
      "0     31.440000  31.117480\n",
      "1     31.680000  31.399507\n",
      "2     44.590000  45.121493\n",
      "3     65.950001  66.729224\n",
      "4     23.905000  22.320830\n",
      "...         ...        ...\n",
      "1019  29.210000  29.620713\n",
      "1020  45.050000  45.918922\n",
      "1021  29.045000  29.048792\n",
      "1022  26.495000  26.138839\n",
      "1023  35.923333  35.793838\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0139\n",
      "Epoch 11/25, Validation Loss: 0.0066\n",
      "         actual  predicted\n",
      "0     31.440000  31.423254\n",
      "1     31.680000  31.578462\n",
      "2     44.590000  44.022773\n",
      "3     65.950001  64.999699\n",
      "4     23.905000  23.327325\n",
      "...         ...        ...\n",
      "1019  29.210000  29.408841\n",
      "1020  45.050000  44.866583\n",
      "1021  29.045000  28.890358\n",
      "1022  26.495000  26.043582\n",
      "1023  35.923333  35.055532\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0136\n",
      "Epoch 12/25, Validation Loss: 0.0075\n",
      "         actual  predicted\n",
      "0     31.440000  31.116286\n",
      "1     31.680000  31.259490\n",
      "2     44.590000  45.326707\n",
      "3     65.950001  68.685132\n",
      "4     23.905000  23.006374\n",
      "...         ...        ...\n",
      "1019  29.210000  29.597056\n",
      "1020  45.050000  46.158682\n",
      "1021  29.045000  29.399645\n",
      "1022  26.495000  26.723834\n",
      "1023  35.923333  35.753240\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0131\n",
      "Epoch 13/25, Validation Loss: 0.0085\n",
      "         actual  predicted\n",
      "0     31.440000  31.097211\n",
      "1     31.680000  31.446790\n",
      "2     44.590000  44.462719\n",
      "3     65.950001  64.613503\n",
      "4     23.905000  23.393272\n",
      "...         ...        ...\n",
      "1019  29.210000  29.861171\n",
      "1020  45.050000  45.201701\n",
      "1021  29.045000  29.129592\n",
      "1022  26.495000  26.605586\n",
      "1023  35.923333  35.790115\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0124\n",
      "Epoch 14/25, Validation Loss: 0.0051\n",
      "         actual  predicted\n",
      "0     31.440000  31.212995\n",
      "1     31.680000  31.528430\n",
      "2     44.590000  45.216809\n",
      "3     65.950001  67.338153\n",
      "4     23.905000  24.189324\n",
      "...         ...        ...\n",
      "1019  29.210000  30.731095\n",
      "1020  45.050000  45.801651\n",
      "1021  29.045000  29.841744\n",
      "1022  26.495000  27.186984\n",
      "1023  35.923333  35.776972\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0124\n",
      "Epoch 15/25, Validation Loss: 0.0060\n",
      "         actual  predicted\n",
      "0     31.440000  31.202040\n",
      "1     31.680000  31.624922\n",
      "2     44.590000  44.393277\n",
      "3     65.950001  67.704872\n",
      "4     23.905000  23.126426\n",
      "...         ...        ...\n",
      "1019  29.210000  29.782528\n",
      "1020  45.050000  45.333720\n",
      "1021  29.045000  29.727563\n",
      "1022  26.495000  26.687298\n",
      "1023  35.923333  35.902163\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0116\n",
      "Epoch 16/25, Validation Loss: 0.0058\n",
      "         actual  predicted\n",
      "0     31.440000  31.669617\n",
      "1     31.680000  31.851043\n",
      "2     44.590000  44.814133\n",
      "3     65.950001  67.392796\n",
      "4     23.905000  23.310948\n",
      "...         ...        ...\n",
      "1019  29.210000  30.272182\n",
      "1020  45.050000  46.148269\n",
      "1021  29.045000  29.483620\n",
      "1022  26.495000  26.689276\n",
      "1023  35.923333  35.864561\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0115\n",
      "Epoch 17/25, Validation Loss: 0.0060\n",
      "         actual  predicted\n",
      "0     31.440000  31.521644\n",
      "1     31.680000  31.767322\n",
      "2     44.590000  44.688231\n",
      "3     65.950001  66.518790\n",
      "4     23.905000  23.213970\n",
      "...         ...        ...\n",
      "1019  29.210000  30.091830\n",
      "1020  45.050000  45.896294\n",
      "1021  29.045000  29.635902\n",
      "1022  26.495000  26.668933\n",
      "1023  35.923333  35.754378\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0111\n",
      "Epoch 18/25, Validation Loss: 0.0053\n",
      "         actual  predicted\n",
      "0     31.440000  31.146277\n",
      "1     31.680000  31.495909\n",
      "2     44.590000  45.005472\n",
      "3     65.950001  66.213043\n",
      "4     23.905000  23.370758\n",
      "...         ...        ...\n",
      "1019  29.210000  29.707604\n",
      "1020  45.050000  45.792564\n",
      "1021  29.045000  29.139285\n",
      "1022  26.495000  26.114890\n",
      "1023  35.923333  35.570769\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[1.3041]], device='cuda:0')\n",
      "[48.04]\n",
      "          actual  predicted\n",
      "0      31.440000  31.146277\n",
      "1      31.680000  31.495909\n",
      "2      44.590000  45.005472\n",
      "3      65.950001  66.213043\n",
      "4      23.905000  23.370758\n",
      "...          ...        ...\n",
      "39163  60.445001  63.842505\n",
      "39164  53.435000  53.622984\n",
      "39165  30.580000  29.744092\n",
      "39166  44.420000  44.572866\n",
      "39167  49.695001  51.147302\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 0.7054\n",
      "Score (MAE): 0.4396\n",
      "Score (ME): -0.0217\n",
      "Score (MAPE): 1.2266%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     24.655000  24.937795\n",
      "1     19.725000  19.816856\n",
      "2     27.435000  27.289065\n",
      "3     29.510000  29.559356\n",
      "4     29.214546  28.765885\n",
      "...         ...        ...\n",
      "1019  26.960000  26.705164\n",
      "1020  22.820000  22.208190\n",
      "1021  29.925000  29.634433\n",
      "1022  23.250000  23.250832\n",
      "1023  22.860000  23.077929\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.1916\n",
      "Epoch 1/25, Validation Loss: 0.0184\n",
      "         actual  predicted\n",
      "0     24.655000  24.760662\n",
      "1     19.725000  19.500996\n",
      "2     27.435000  26.767789\n",
      "3     29.510000  29.131967\n",
      "4     29.214546  28.689531\n",
      "...         ...        ...\n",
      "1019  26.960000  26.718044\n",
      "1020  22.820000  22.571988\n",
      "1021  29.925000  29.887209\n",
      "1022  23.250000  23.465643\n",
      "1023  22.860000  23.201652\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0425\n",
      "Epoch 2/25, Validation Loss: 0.0104\n",
      "         actual  predicted\n",
      "0     24.655000  24.853730\n",
      "1     19.725000  19.248365\n",
      "2     27.435000  26.839895\n",
      "3     29.510000  29.203096\n",
      "4     29.214546  28.885910\n",
      "...         ...        ...\n",
      "1019  26.960000  26.756727\n",
      "1020  22.820000  22.521395\n",
      "1021  29.925000  29.978666\n",
      "1022  23.250000  23.450455\n",
      "1023  22.860000  23.038190\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0314\n",
      "Epoch 3/25, Validation Loss: 0.0107\n",
      "         actual  predicted\n",
      "0     24.655000  24.579120\n",
      "1     19.725000  19.230652\n",
      "2     27.435000  26.602847\n",
      "3     29.510000  29.062908\n",
      "4     29.214546  28.713391\n",
      "...         ...        ...\n",
      "1019  26.960000  26.486500\n",
      "1020  22.820000  22.254826\n",
      "1021  29.925000  29.877992\n",
      "1022  23.250000  23.354778\n",
      "1023  22.860000  23.000258\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0262\n",
      "Epoch 4/25, Validation Loss: 0.0123\n",
      "         actual  predicted\n",
      "0     24.655000  24.827295\n",
      "1     19.725000  19.809221\n",
      "2     27.435000  26.899195\n",
      "3     29.510000  29.638573\n",
      "4     29.214546  29.245168\n",
      "...         ...        ...\n",
      "1019  26.960000  26.942734\n",
      "1020  22.820000  22.611666\n",
      "1021  29.925000  30.169144\n",
      "1022  23.250000  23.368865\n",
      "1023  22.860000  22.944137\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0230\n",
      "Epoch 5/25, Validation Loss: 0.0090\n",
      "         actual  predicted\n",
      "0     24.655000  24.818363\n",
      "1     19.725000  19.860670\n",
      "2     27.435000  27.011295\n",
      "3     29.510000  29.652595\n",
      "4     29.214546  29.309352\n",
      "...         ...        ...\n",
      "1019  26.960000  26.986284\n",
      "1020  22.820000  22.448830\n",
      "1021  29.925000  30.581344\n",
      "1022  23.250000  23.299878\n",
      "1023  22.860000  22.912432\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0200\n",
      "Epoch 6/25, Validation Loss: 0.0127\n",
      "         actual  predicted\n",
      "0     24.655000  24.912633\n",
      "1     19.725000  19.570010\n",
      "2     27.435000  27.038837\n",
      "3     29.510000  29.788841\n",
      "4     29.214546  29.466456\n",
      "...         ...        ...\n",
      "1019  26.960000  26.896437\n",
      "1020  22.820000  22.448511\n",
      "1021  29.925000  30.170088\n",
      "1022  23.250000  23.334553\n",
      "1023  22.860000  22.815151\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0187\n",
      "Epoch 7/25, Validation Loss: 0.0083\n",
      "         actual  predicted\n",
      "0     24.655000  25.001785\n",
      "1     19.725000  19.613060\n",
      "2     27.435000  26.963450\n",
      "3     29.510000  29.864222\n",
      "4     29.214546  29.321391\n",
      "...         ...        ...\n",
      "1019  26.960000  26.739219\n",
      "1020  22.820000  22.619785\n",
      "1021  29.925000  30.111838\n",
      "1022  23.250000  23.281024\n",
      "1023  22.860000  22.859499\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0181\n",
      "Epoch 8/25, Validation Loss: 0.0063\n",
      "         actual  predicted\n",
      "0     24.655000  24.902656\n",
      "1     19.725000  19.615297\n",
      "2     27.435000  27.135429\n",
      "3     29.510000  29.806535\n",
      "4     29.214546  29.390906\n",
      "...         ...        ...\n",
      "1019  26.960000  26.869594\n",
      "1020  22.820000  22.698671\n",
      "1021  29.925000  29.838876\n",
      "1022  23.250000  23.396589\n",
      "1023  22.860000  22.962217\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0161\n",
      "Epoch 9/25, Validation Loss: 0.0067\n",
      "         actual  predicted\n",
      "0     24.655000  24.890695\n",
      "1     19.725000  19.556347\n",
      "2     27.435000  27.128701\n",
      "3     29.510000  29.714855\n",
      "4     29.214546  29.314401\n",
      "...         ...        ...\n",
      "1019  26.960000  27.022977\n",
      "1020  22.820000  22.528685\n",
      "1021  29.925000  30.101112\n",
      "1022  23.250000  23.190587\n",
      "1023  22.860000  22.724553\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0149\n",
      "Epoch 10/25, Validation Loss: 0.0073\n",
      "         actual  predicted\n",
      "0     24.655000  24.793049\n",
      "1     19.725000  19.442479\n",
      "2     27.435000  26.979496\n",
      "3     29.510000  29.790219\n",
      "4     29.214546  29.425341\n",
      "...         ...        ...\n",
      "1019  26.960000  27.049034\n",
      "1020  22.820000  22.552752\n",
      "1021  29.925000  30.300472\n",
      "1022  23.250000  23.251368\n",
      "1023  22.860000  22.731355\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0138\n",
      "Epoch 11/25, Validation Loss: 0.0090\n",
      "         actual  predicted\n",
      "0     24.655000  24.716105\n",
      "1     19.725000  19.316318\n",
      "2     27.435000  27.044212\n",
      "3     29.510000  29.802140\n",
      "4     29.214546  29.275294\n",
      "...         ...        ...\n",
      "1019  26.960000  27.014949\n",
      "1020  22.820000  22.508009\n",
      "1021  29.925000  30.031049\n",
      "1022  23.250000  23.222279\n",
      "1023  22.860000  22.734439\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0137\n",
      "Epoch 12/25, Validation Loss: 0.0111\n",
      "         actual  predicted\n",
      "0     24.655000  24.896291\n",
      "1     19.725000  19.490971\n",
      "2     27.435000  27.002561\n",
      "3     29.510000  29.881217\n",
      "4     29.214546  29.456949\n",
      "...         ...        ...\n",
      "1019  26.960000  27.151171\n",
      "1020  22.820000  22.555876\n",
      "1021  29.925000  30.218273\n",
      "1022  23.250000  23.419488\n",
      "1023  22.860000  22.919875\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[0.1640]], device='cuda:0')\n",
      "[24.65]\n",
      "          actual  predicted\n",
      "0      24.655000  24.896291\n",
      "1      19.725000  19.490971\n",
      "2      27.435000  27.002561\n",
      "3      29.510000  29.881217\n",
      "4      29.214546  29.456949\n",
      "...          ...        ...\n",
      "39163  26.010000  26.383662\n",
      "39164  23.530000  23.941356\n",
      "39165  22.425000  22.572705\n",
      "39166  28.130000  28.312165\n",
      "39167  24.920000  25.378163\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 0.3135\n",
      "Score (MAE): 0.2216\n",
      "Score (ME): -0.0841\n",
      "Score (MAPE): 0.9168%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual  predicted\n",
      "0     282.250001  60.510223\n",
      "1     185.000000   2.295987\n",
      "2       5.000000 -17.460113\n",
      "3      11.000004  -4.909323\n",
      "4       5.999997 -20.678889\n",
      "...          ...        ...\n",
      "1019    5.000000  11.244099\n",
      "1020    5.999997  62.542145\n",
      "1021    5.499999   5.091141\n",
      "1022   12.999998  37.187226\n",
      "1023    9.999996  15.807942\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.5561\n",
      "Epoch 1/25, Validation Loss: 0.8888\n",
      "          actual   predicted\n",
      "0     282.250001  181.854126\n",
      "1     185.000000   58.937800\n",
      "2       5.000000   42.780962\n",
      "3      11.000004   57.825455\n",
      "4       5.999997   41.761980\n",
      "...          ...         ...\n",
      "1019    5.000000   61.313150\n",
      "1020    5.999997  153.763266\n",
      "1021    5.499999   60.641863\n",
      "1022   12.999998   84.925197\n",
      "1023    9.999996   74.779912\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.5040\n",
      "Epoch 2/25, Validation Loss: 0.9110\n",
      "          actual   predicted\n",
      "0     282.250001  268.095763\n",
      "1     185.000000   64.073691\n",
      "2       5.000000   43.876367\n",
      "3      11.000004   64.304061\n",
      "4       5.999997   48.440115\n",
      "...          ...         ...\n",
      "1019    5.000000   70.305555\n",
      "1020    5.999997  170.933340\n",
      "1021    5.499999   62.363185\n",
      "1022   12.999998   81.951450\n",
      "1023    9.999996   77.923222\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.4896\n",
      "Epoch 3/25, Validation Loss: 0.8669\n",
      "          actual  predicted\n",
      "0     282.250001  85.563987\n",
      "1     185.000000  37.330947\n",
      "2       5.000000  27.511630\n",
      "3      11.000004  38.647656\n",
      "4       5.999997  29.523681\n",
      "...          ...        ...\n",
      "1019    5.000000  48.074618\n",
      "1020    5.999997  92.087821\n",
      "1021    5.499999  45.220276\n",
      "1022   12.999998  59.143452\n",
      "1023    9.999996  55.406190\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.4761\n",
      "Epoch 4/25, Validation Loss: 0.8880\n",
      "          actual   predicted\n",
      "0     282.250001  194.387879\n",
      "1     185.000000   62.099480\n",
      "2       5.000000   54.512122\n",
      "3      11.000004   67.595112\n",
      "4       5.999997   60.945690\n",
      "...          ...         ...\n",
      "1019    5.000000   93.680139\n",
      "1020    5.999997  169.117672\n",
      "1021    5.499999   76.484756\n",
      "1022   12.999998   99.926413\n",
      "1023    9.999996   85.177342\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.4854\n",
      "Epoch 5/25, Validation Loss: 0.9154\n",
      "          actual   predicted\n",
      "0     282.250001  196.220153\n",
      "1     185.000000   42.019113\n",
      "2       5.000000   29.996136\n",
      "3      11.000004   44.987979\n",
      "4       5.999997   27.337931\n",
      "...          ...         ...\n",
      "1019    5.000000   50.703188\n",
      "1020    5.999997  169.359128\n",
      "1021    5.499999   43.399644\n",
      "1022   12.999998   86.072485\n",
      "1023    9.999996   53.395935\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.4911\n",
      "Epoch 6/25, Validation Loss: 0.8909\n",
      "          actual   predicted\n",
      "0     282.250001  178.191216\n",
      "1     185.000000   34.384554\n",
      "2       5.000000   18.717312\n",
      "3      11.000004   36.387526\n",
      "4       5.999997   25.064290\n",
      "...          ...         ...\n",
      "1019    5.000000   50.715447\n",
      "1020    5.999997   78.696856\n",
      "1021    5.499999   38.154904\n",
      "1022   12.999998   54.782660\n",
      "1023    9.999996   52.589758\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.4791\n",
      "Epoch 7/25, Validation Loss: 0.8773\n",
      "          actual   predicted\n",
      "0     282.250001  230.124292\n",
      "1     185.000000   22.882724\n",
      "2       5.000000   11.312271\n",
      "3      11.000004   28.000280\n",
      "4       5.999997   14.252305\n",
      "...          ...         ...\n",
      "1019    5.000000   47.531981\n",
      "1020    5.999997  217.956825\n",
      "1021    5.499999   35.957155\n",
      "1022   12.999998   54.898992\n",
      "1023    9.999996   47.672203\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.4562\n",
      "Epoch 8/25, Validation Loss: 0.8477\n",
      "          actual  predicted\n",
      "0     282.250001  82.182695\n",
      "1     185.000000  -1.046248\n",
      "2       5.000000 -12.445519\n",
      "3      11.000004   4.590256\n",
      "4       5.999997 -12.168396\n",
      "...          ...        ...\n",
      "1019    5.000000  15.735810\n",
      "1020    5.999997  52.262502\n",
      "1021    5.499999   8.682495\n",
      "1022   12.999998  25.128679\n",
      "1023    9.999996  24.367871\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.4448\n",
      "Epoch 9/25, Validation Loss: 0.8198\n",
      "          actual   predicted\n",
      "0     282.250001  149.461521\n",
      "1     185.000000    5.521619\n",
      "2       5.000000   -6.362329\n",
      "3      11.000004    9.595016\n",
      "4       5.999997   -6.983317\n",
      "...          ...         ...\n",
      "1019    5.000000   10.858529\n",
      "1020    5.999997   66.673347\n",
      "1021    5.499999    7.192572\n",
      "1022   12.999998   19.967550\n",
      "1023    9.999996   15.952444\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.4445\n",
      "Epoch 10/25, Validation Loss: 0.8100\n",
      "          actual   predicted\n",
      "0     282.250001  281.514425\n",
      "1     185.000000   19.806558\n",
      "2       5.000000   12.583327\n",
      "3      11.000004   35.556666\n",
      "4       5.999997   12.080255\n",
      "...          ...         ...\n",
      "1019    5.000000   37.687520\n",
      "1020    5.999997  174.041442\n",
      "1021    5.499999   27.264876\n",
      "1022   12.999998   64.066256\n",
      "1023    9.999996   49.441306\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.4369\n",
      "Epoch 11/25, Validation Loss: 0.8136\n",
      "          actual   predicted\n",
      "0     282.250001  274.469184\n",
      "1     185.000000   15.236426\n",
      "2       5.000000    1.968507\n",
      "3      11.000004   23.947618\n",
      "4       5.999997    0.430152\n",
      "...          ...         ...\n",
      "1019    5.000000   30.646594\n",
      "1020    5.999997  160.289512\n",
      "1021    5.499999   10.108774\n",
      "1022   12.999998   44.024828\n",
      "1023    9.999996   31.861428\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.4294\n",
      "Epoch 12/25, Validation Loss: 0.8458\n",
      "          actual   predicted\n",
      "0     282.250001  229.963081\n",
      "1     185.000000   22.520500\n",
      "2       5.000000    9.037578\n",
      "3      11.000004   35.926845\n",
      "4       5.999997   10.923782\n",
      "...          ...         ...\n",
      "1019    5.000000   44.576414\n",
      "1020    5.999997  191.783745\n",
      "1021    5.499999   31.436778\n",
      "1022   12.999998   63.230939\n",
      "1023    9.999996   64.833412\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.4294\n",
      "Epoch 13/25, Validation Loss: 0.7997\n",
      "          actual   predicted\n",
      "0     282.250001  142.035242\n",
      "1     185.000000   17.822180\n",
      "2       5.000000    7.946630\n",
      "3      11.000004   26.004460\n",
      "4       5.999997    5.450775\n",
      "...          ...         ...\n",
      "1019    5.000000   17.832193\n",
      "1020    5.999997   74.471168\n",
      "1021    5.499999    9.654783\n",
      "1022   12.999998   28.278432\n",
      "1023    9.999996   28.782969\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.4323\n",
      "Epoch 14/25, Validation Loss: 0.8263\n",
      "          actual   predicted\n",
      "0     282.250001  165.617822\n",
      "1     185.000000   -7.012799\n",
      "2       5.000000  -20.701609\n",
      "3      11.000004    8.466145\n",
      "4       5.999997  -27.833274\n",
      "...          ...         ...\n",
      "1019    5.000000   -0.890682\n",
      "1020    5.999997  102.805934\n",
      "1021    5.499999  -17.190166\n",
      "1022   12.999998   17.883863\n",
      "1023    9.999996   16.169657\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.4258\n",
      "Epoch 15/25, Validation Loss: 0.7992\n",
      "          actual   predicted\n",
      "0     282.250001  149.056819\n",
      "1     185.000000   15.741330\n",
      "2       5.000000    7.615083\n",
      "3      11.000004   25.082069\n",
      "4       5.999997    3.975120\n",
      "...          ...         ...\n",
      "1019    5.000000   21.165988\n",
      "1020    5.999997  122.120492\n",
      "1021    5.499999    9.798292\n",
      "1022   12.999998   33.609844\n",
      "1023    9.999996   33.376720\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.4153\n",
      "Epoch 16/25, Validation Loss: 0.8298\n",
      "          actual   predicted\n",
      "0     282.250001  204.225367\n",
      "1     185.000000   34.131771\n",
      "2       5.000000   19.493676\n",
      "3      11.000004   41.653261\n",
      "4       5.999997   17.686119\n",
      "...          ...         ...\n",
      "1019    5.000000   44.282269\n",
      "1020    5.999997  189.757786\n",
      "1021    5.499999   29.895692\n",
      "1022   12.999998   58.745695\n",
      "1023    9.999996   66.340322\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.4369\n",
      "Epoch 17/25, Validation Loss: 0.8315\n",
      "          actual  predicted\n",
      "0     282.250001  90.729361\n",
      "1     185.000000  15.885997\n",
      "2       5.000000   7.075294\n",
      "3      11.000004  21.380660\n",
      "4       5.999997   3.904039\n",
      "...          ...        ...\n",
      "1019    5.000000  17.725471\n",
      "1020    5.999997  75.923749\n",
      "1021    5.499999   9.849737\n",
      "1022   12.999998  30.580171\n",
      "1023    9.999996  26.201896\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.4156\n",
      "Epoch 18/25, Validation Loss: 0.7587\n",
      "          actual   predicted\n",
      "0     282.250001  127.699641\n",
      "1     185.000000    0.310120\n",
      "2       5.000000  -14.692858\n",
      "3      11.000004    6.835975\n",
      "4       5.999997  -14.859606\n",
      "...          ...         ...\n",
      "1019    5.000000    1.258246\n",
      "1020    5.999997   80.507983\n",
      "1021    5.499999   -9.924568\n",
      "1022   12.999998   18.468158\n",
      "1023    9.999996   22.714863\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.4043\n",
      "Epoch 19/25, Validation Loss: 0.8288\n",
      "          actual   predicted\n",
      "0     282.250001  158.824558\n",
      "1     185.000000   36.898507\n",
      "2       5.000000   24.342409\n",
      "3      11.000004   40.500370\n",
      "4       5.999997   20.908311\n",
      "...          ...         ...\n",
      "1019    5.000000   42.390935\n",
      "1020    5.999997  129.131877\n",
      "1021    5.499999   29.197855\n",
      "1022   12.999998   58.195031\n",
      "1023    9.999996   55.243555\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.4176\n",
      "Epoch 20/25, Validation Loss: 0.7558\n",
      "          actual  predicted\n",
      "0     282.250001  78.591116\n",
      "1     185.000000  18.746663\n",
      "2       5.000000   9.685530\n",
      "3      11.000004  20.838673\n",
      "4       5.999997   8.454407\n",
      "...          ...        ...\n",
      "1019    5.000000  23.710475\n",
      "1020    5.999997  52.991274\n",
      "1021    5.499999  14.991610\n",
      "1022   12.999998  32.932801\n",
      "1023    9.999996  32.663492\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.3958\n",
      "Epoch 21/25, Validation Loss: 0.7218\n",
      "          actual   predicted\n",
      "0     282.250001  149.427423\n",
      "1     185.000000   18.285013\n",
      "2       5.000000    5.948871\n",
      "3      11.000004   19.356409\n",
      "4       5.999997    1.628755\n",
      "...          ...         ...\n",
      "1019    5.000000   26.580149\n",
      "1020    5.999997  134.080723\n",
      "1021    5.499999   13.502680\n",
      "1022   12.999998   45.606165\n",
      "1023    9.999996   42.200769\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.3812\n",
      "Epoch 22/25, Validation Loss: 0.7313\n",
      "          actual  predicted\n",
      "0     282.250001  71.781624\n",
      "1     185.000000   2.529360\n",
      "2       5.000000  -4.316493\n",
      "3      11.000004   4.349578\n",
      "4       5.999997  -6.563619\n",
      "...          ...        ...\n",
      "1019    5.000000  14.795249\n",
      "1020    5.999997  67.772996\n",
      "1021    5.499999   3.029914\n",
      "1022   12.999998  28.984342\n",
      "1023    9.999996  24.472063\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.4134\n",
      "Epoch 23/25, Validation Loss: 0.7630\n",
      "          actual  predicted\n",
      "0     282.250001  98.027029\n",
      "1     185.000000  16.890190\n",
      "2       5.000000  10.579787\n",
      "3      11.000004  18.526069\n",
      "4       5.999997   7.561994\n",
      "...          ...        ...\n",
      "1019    5.000000  24.899007\n",
      "1020    5.999997  75.620052\n",
      "1021    5.499999  15.419500\n",
      "1022   12.999998  41.723833\n",
      "1023    9.999996  36.202396\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.3873\n",
      "Epoch 24/25, Validation Loss: 0.6555\n",
      "          actual  predicted\n",
      "0     282.250001  96.712000\n",
      "1     185.000000   6.574776\n",
      "2       5.000000  -0.329663\n",
      "3      11.000004   7.782115\n",
      "4       5.999997  -1.109078\n",
      "...          ...        ...\n",
      "1019    5.000000  17.402388\n",
      "1020    5.999997  83.782129\n",
      "1021    5.499999   6.950109\n",
      "1022   12.999998  29.594194\n",
      "1023    9.999996  31.287630\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.3629\n",
      "Epoch 25/25, Validation Loss: 0.6831\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[-0.1421]], device='cuda:0')\n",
      "[73.27]\n",
      "           actual    predicted\n",
      "0      282.250001    96.712000\n",
      "1      185.000000     6.574776\n",
      "2        5.000000    -0.329663\n",
      "3       11.000004     7.782115\n",
      "4        5.999997    -1.109078\n",
      "...           ...          ...\n",
      "39163  951.888876  1052.597395\n",
      "39164    4.000003    -3.245913\n",
      "39165  519.142849   724.120269\n",
      "39166  339.333338   410.080715\n",
      "39167   69.500004    53.115751\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 659.7745\n",
      "Score (MAE): 76.3780\n",
      "Score (ME): 5.2534\n",
      "Score (MAPE): 63318.9951%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     398.500003  434.523404\n",
      "1     496.750000  493.886441\n",
      "2     429.000000  428.045147\n",
      "3     433.750001  451.217772\n",
      "4     389.249999  415.663975\n",
      "...          ...         ...\n",
      "1019  594.750003  605.261967\n",
      "1020  445.499999  432.276106\n",
      "1021  399.499999  408.375426\n",
      "1022  483.750000  500.929205\n",
      "1023  429.250001  430.164867\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.3260\n",
      "Epoch 1/25, Validation Loss: 0.1503\n",
      "          actual   predicted\n",
      "0     398.500003  413.406416\n",
      "1     496.750000  495.234009\n",
      "2     429.000000  408.658898\n",
      "3     433.750001  426.560730\n",
      "4     389.249999  392.019406\n",
      "...          ...         ...\n",
      "1019  594.750003  562.987934\n",
      "1020  445.499999  417.002170\n",
      "1021  399.499999  388.512773\n",
      "1022  483.750000  458.991302\n",
      "1023  429.250001  404.875624\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.1993\n",
      "Epoch 2/25, Validation Loss: 0.1552\n",
      "          actual   predicted\n",
      "0     398.500003  425.884226\n",
      "1     496.750000  513.883840\n",
      "2     429.000000  423.131653\n",
      "3     433.750001  434.289379\n",
      "4     389.249999  406.176664\n",
      "...          ...         ...\n",
      "1019  594.750003  638.313886\n",
      "1020  445.499999  424.361109\n",
      "1021  399.499999  404.780606\n",
      "1022  483.750000  499.774063\n",
      "1023  429.250001  413.902409\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1878\n",
      "Epoch 3/25, Validation Loss: 0.1729\n",
      "          actual   predicted\n",
      "0     398.500003  421.210144\n",
      "1     496.750000  484.575610\n",
      "2     429.000000  426.809480\n",
      "3     433.750001  438.863639\n",
      "4     389.249999  412.408668\n",
      "...          ...         ...\n",
      "1019  594.750003  596.340047\n",
      "1020  445.499999  437.723248\n",
      "1021  399.499999  415.778358\n",
      "1022  483.750000  488.284551\n",
      "1023  429.250001  426.395456\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1753\n",
      "Epoch 4/25, Validation Loss: 0.1351\n",
      "          actual   predicted\n",
      "0     398.500003  407.177713\n",
      "1     496.750000  488.081734\n",
      "2     429.000000  409.576029\n",
      "3     433.750001  420.778897\n",
      "4     389.249999  392.450138\n",
      "...          ...         ...\n",
      "1019  594.750003  565.076330\n",
      "1020  445.499999  425.064278\n",
      "1021  399.499999  399.149064\n",
      "1022  483.750000  479.819172\n",
      "1023  429.250001  417.937778\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.1717\n",
      "Epoch 5/25, Validation Loss: 0.1331\n",
      "          actual   predicted\n",
      "0     398.500003  410.502104\n",
      "1     496.750000  506.124439\n",
      "2     429.000000  421.447719\n",
      "3     433.750001  435.177642\n",
      "4     389.249999  396.579144\n",
      "...          ...         ...\n",
      "1019  594.750003  605.069490\n",
      "1020  445.499999  439.611376\n",
      "1021  399.499999  402.136697\n",
      "1022  483.750000  487.265449\n",
      "1023  429.250001  424.023626\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1699\n",
      "Epoch 6/25, Validation Loss: 0.1483\n",
      "          actual   predicted\n",
      "0     398.500003  415.516110\n",
      "1     496.750000  504.559537\n",
      "2     429.000000  419.507143\n",
      "3     433.750001  429.753388\n",
      "4     389.249999  404.179927\n",
      "...          ...         ...\n",
      "1019  594.750003  578.455654\n",
      "1020  445.499999  441.445486\n",
      "1021  399.499999  412.862176\n",
      "1022  483.750000  491.381380\n",
      "1023  429.250001  437.231133\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.1688\n",
      "Epoch 7/25, Validation Loss: 0.1304\n",
      "          actual   predicted\n",
      "0     398.500003  411.655541\n",
      "1     496.750000  494.629015\n",
      "2     429.000000  417.785830\n",
      "3     433.750001  430.000956\n",
      "4     389.249999  402.524092\n",
      "...          ...         ...\n",
      "1019  594.750003  562.436766\n",
      "1020  445.499999  435.647002\n",
      "1021  399.499999  407.378469\n",
      "1022  483.750000  480.624712\n",
      "1023  429.250001  424.070732\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1661\n",
      "Epoch 8/25, Validation Loss: 0.1370\n",
      "          actual   predicted\n",
      "0     398.500003  414.080732\n",
      "1     496.750000  506.355865\n",
      "2     429.000000  418.898051\n",
      "3     433.750001  432.694557\n",
      "4     389.249999  399.546814\n",
      "...          ...         ...\n",
      "1019  594.750003  576.033792\n",
      "1020  445.499999  438.250408\n",
      "1021  399.499999  405.620621\n",
      "1022  483.750000  494.355428\n",
      "1023  429.250001  425.424507\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1603\n",
      "Epoch 9/25, Validation Loss: 0.1452\n",
      "          actual   predicted\n",
      "0     398.500003  405.965136\n",
      "1     496.750000  491.390715\n",
      "2     429.000000  415.852764\n",
      "3     433.750001  427.032311\n",
      "4     389.249999  394.688309\n",
      "...          ...         ...\n",
      "1019  594.750003  600.329511\n",
      "1020  445.499999  425.214673\n",
      "1021  399.499999  406.244755\n",
      "1022  483.750000  462.708968\n",
      "1023  429.250001  419.144898\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.1661\n",
      "Epoch 10/25, Validation Loss: 0.1479\n",
      "          actual   predicted\n",
      "0     398.500003  420.221592\n",
      "1     496.750000  498.311844\n",
      "2     429.000000  425.147404\n",
      "3     433.750001  433.624167\n",
      "4     389.249999  409.663963\n",
      "...          ...         ...\n",
      "1019  594.750003  587.427054\n",
      "1020  445.499999  430.861276\n",
      "1021  399.499999  409.371816\n",
      "1022  483.750000  474.005598\n",
      "1023  429.250001  422.399235\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.1603\n",
      "Epoch 11/25, Validation Loss: 0.1292\n",
      "          actual   predicted\n",
      "0     398.500003  414.729046\n",
      "1     496.750000  499.063208\n",
      "2     429.000000  424.975650\n",
      "3     433.750001  437.889809\n",
      "4     389.249999  404.449185\n",
      "...          ...         ...\n",
      "1019  594.750003  581.238320\n",
      "1020  445.499999  440.406570\n",
      "1021  399.499999  405.923382\n",
      "1022  483.750000  484.210644\n",
      "1023  429.250001  424.012249\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1597\n",
      "Epoch 12/25, Validation Loss: 0.1278\n",
      "          actual   predicted\n",
      "0     398.500003  407.867347\n",
      "1     496.750000  491.306020\n",
      "2     429.000000  420.198099\n",
      "3     433.750001  435.566881\n",
      "4     389.249999  395.653965\n",
      "...          ...         ...\n",
      "1019  594.750003  556.520245\n",
      "1020  445.499999  423.588811\n",
      "1021  399.499999  402.402550\n",
      "1022  483.750000  480.870215\n",
      "1023  429.250001  417.851017\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1546\n",
      "Epoch 13/25, Validation Loss: 0.1404\n",
      "          actual   predicted\n",
      "0     398.500003  407.440341\n",
      "1     496.750000  496.537759\n",
      "2     429.000000  422.455366\n",
      "3     433.750001  440.916433\n",
      "4     389.249999  398.569528\n",
      "...          ...         ...\n",
      "1019  594.750003  587.959978\n",
      "1020  445.499999  441.931524\n",
      "1021  399.499999  402.740428\n",
      "1022  483.750000  482.154869\n",
      "1023  429.250001  427.658744\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.1591\n",
      "Epoch 14/25, Validation Loss: 0.1295\n",
      "          actual   predicted\n",
      "0     398.500003  398.525960\n",
      "1     496.750000  502.643296\n",
      "2     429.000000  416.997355\n",
      "3     433.750001  431.564486\n",
      "4     389.249999  391.402939\n",
      "...          ...         ...\n",
      "1019  594.750003  587.357889\n",
      "1020  445.499999  436.580210\n",
      "1021  399.499999  402.615542\n",
      "1022  483.750000  491.272874\n",
      "1023  429.250001  420.837492\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.1540\n",
      "Epoch 15/25, Validation Loss: 0.1292\n",
      "          actual   predicted\n",
      "0     398.500003  403.288678\n",
      "1     496.750000  499.907746\n",
      "2     429.000000  417.920242\n",
      "3     433.750001  430.145099\n",
      "4     389.249999  389.296560\n",
      "...          ...         ...\n",
      "1019  594.750003  612.787845\n",
      "1020  445.499999  431.495462\n",
      "1021  399.499999  405.644823\n",
      "1022  483.750000  500.444783\n",
      "1023  429.250001  418.463639\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.1522\n",
      "Epoch 16/25, Validation Loss: 0.1292\n",
      "          actual   predicted\n",
      "0     398.500003  409.429411\n",
      "1     496.750000  494.458862\n",
      "2     429.000000  423.927406\n",
      "3     433.750001  436.231197\n",
      "4     389.249999  405.394341\n",
      "...          ...         ...\n",
      "1019  594.750003  574.044953\n",
      "1020  445.499999  434.724167\n",
      "1021  399.499999  410.962338\n",
      "1022  483.750000  480.737390\n",
      "1023  429.250001  426.324319\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.1637]], device='cuda:0')\n",
      "[468.86]\n",
      "           actual   predicted\n",
      "0      398.500003  409.429411\n",
      "1      496.750000  494.458862\n",
      "2      429.000000  423.927406\n",
      "3      433.750001  436.231197\n",
      "4      389.249999  405.394341\n",
      "...           ...         ...\n",
      "19155  692.000002  679.390527\n",
      "19156  502.363636  446.876678\n",
      "19157  418.500003  428.095840\n",
      "19158  458.999999  466.812995\n",
      "19159  489.238095  655.660690\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 45.4747\n",
      "Score (MAE): 17.4599\n",
      "Score (ME): -3.0285\n",
      "Score (MAPE): 3.0686%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      639.250002   600.038970\n",
      "1      742.249999   687.055248\n",
      "2      639.500003   591.715230\n",
      "3      571.333342   583.237173\n",
      "4      693.499999   654.154716\n",
      "...           ...          ...\n",
      "1019   621.500007   793.692553\n",
      "1020  1196.499995  1243.749328\n",
      "1021   977.250004   916.305690\n",
      "1022  1089.999987  1186.067033\n",
      "1023   705.162794   656.330715\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.3627\n",
      "Epoch 1/25, Validation Loss: 0.1268\n",
      "           actual    predicted\n",
      "0      639.250002   613.156983\n",
      "1      742.249999   673.051064\n",
      "2      639.500003   604.174739\n",
      "3      571.333342   600.638061\n",
      "4      693.499999   666.167010\n",
      "...           ...          ...\n",
      "1019   621.500007   857.880073\n",
      "1020  1196.499995  1255.254490\n",
      "1021   977.250004   872.250885\n",
      "1022  1089.999987  1108.711123\n",
      "1023   705.162794   651.202371\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.1443\n",
      "Epoch 2/25, Validation Loss: 0.1183\n",
      "           actual    predicted\n",
      "0      639.250002   596.304686\n",
      "1      742.249999   731.563011\n",
      "2      639.500003   592.002145\n",
      "3      571.333342   590.661270\n",
      "4      693.499999   664.046931\n",
      "...           ...          ...\n",
      "1019   621.500007   879.937725\n",
      "1020  1196.499995  1212.835962\n",
      "1021   977.250004   992.018113\n",
      "1022  1089.999987  1125.649030\n",
      "1023   705.162794   658.656192\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1324\n",
      "Epoch 3/25, Validation Loss: 0.1092\n",
      "           actual    predicted\n",
      "0      639.250002   621.816472\n",
      "1      742.249999   708.679920\n",
      "2      639.500003   599.142280\n",
      "3      571.333342   587.955147\n",
      "4      693.499999   696.466129\n",
      "...           ...          ...\n",
      "1019   621.500007   826.759978\n",
      "1020  1196.499995  1168.852043\n",
      "1021   977.250004   845.400134\n",
      "1022  1089.999987  1049.629653\n",
      "1023   705.162794   641.114000\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1291\n",
      "Epoch 4/25, Validation Loss: 0.1135\n",
      "           actual    predicted\n",
      "0      639.250002   619.693267\n",
      "1      742.249999   736.149653\n",
      "2      639.500003   601.725461\n",
      "3      571.333342   589.194605\n",
      "4      693.499999   707.456506\n",
      "...           ...          ...\n",
      "1019   621.500007   860.653478\n",
      "1020  1196.499995  1213.529105\n",
      "1021   977.250004   939.210609\n",
      "1022  1089.999987  1109.410416\n",
      "1023   705.162794   665.439757\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.1211\n",
      "Epoch 5/25, Validation Loss: 0.1068\n",
      "           actual    predicted\n",
      "0      639.250002   619.111079\n",
      "1      742.249999   734.713475\n",
      "2      639.500003   613.429185\n",
      "3      571.333342   605.537503\n",
      "4      693.499999   691.780692\n",
      "...           ...          ...\n",
      "1019   621.500007   824.884903\n",
      "1020  1196.499995  1170.842075\n",
      "1021   977.250004   889.379128\n",
      "1022  1089.999987  1071.148121\n",
      "1023   705.162794   687.354064\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1228\n",
      "Epoch 6/25, Validation Loss: 0.1087\n",
      "           actual    predicted\n",
      "0      639.250002   608.710672\n",
      "1      742.249999   729.612301\n",
      "2      639.500003   606.831335\n",
      "3      571.333342   592.132244\n",
      "4      693.499999   705.573946\n",
      "...           ...          ...\n",
      "1019   621.500007   874.902234\n",
      "1020  1196.499995  1172.353856\n",
      "1021   977.250004   926.902774\n",
      "1022  1089.999987  1091.993459\n",
      "1023   705.162794   680.435275\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.1171\n",
      "Epoch 7/25, Validation Loss: 0.1012\n",
      "           actual    predicted\n",
      "0      639.250002   617.143266\n",
      "1      742.249999   759.483764\n",
      "2      639.500003   607.002327\n",
      "3      571.333342   576.330814\n",
      "4      693.499999   734.080539\n",
      "...           ...          ...\n",
      "1019   621.500007   918.520064\n",
      "1020  1196.499995  1234.819079\n",
      "1021   977.250004   934.596407\n",
      "1022  1089.999987  1136.907648\n",
      "1023   705.162794   683.060479\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1178\n",
      "Epoch 8/25, Validation Loss: 0.1010\n",
      "           actual    predicted\n",
      "0      639.250002   622.082090\n",
      "1      742.249999   741.043723\n",
      "2      639.500003   598.841553\n",
      "3      571.333342   587.316692\n",
      "4      693.499999   704.473482\n",
      "...           ...          ...\n",
      "1019   621.500007   873.686924\n",
      "1020  1196.499995  1206.958042\n",
      "1021   977.250004   903.546557\n",
      "1022  1089.999987  1108.110469\n",
      "1023   705.162794   676.997287\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1146\n",
      "Epoch 9/25, Validation Loss: 0.0992\n",
      "           actual    predicted\n",
      "0      639.250002   602.683423\n",
      "1      742.249999   757.753704\n",
      "2      639.500003   592.661482\n",
      "3      571.333342   580.608286\n",
      "4      693.499999   699.811863\n",
      "...           ...          ...\n",
      "1019   621.500007   840.329938\n",
      "1020  1196.499995  1182.327571\n",
      "1021   977.250004   937.180622\n",
      "1022  1089.999987  1068.589955\n",
      "1023   705.162794   691.405605\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.1114\n",
      "Epoch 10/25, Validation Loss: 0.1086\n",
      "           actual    predicted\n",
      "0      639.250002   615.606713\n",
      "1      742.249999   757.643399\n",
      "2      639.500003   599.091884\n",
      "3      571.333342   572.640133\n",
      "4      693.499999   721.504177\n",
      "...           ...          ...\n",
      "1019   621.500007   872.699213\n",
      "1020  1196.499995  1167.875945\n",
      "1021   977.250004   899.047114\n",
      "1022  1089.999987  1076.090715\n",
      "1023   705.162794   693.610164\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.1126\n",
      "Epoch 11/25, Validation Loss: 0.1008\n",
      "           actual    predicted\n",
      "0      639.250002   618.463225\n",
      "1      742.249999   727.067785\n",
      "2      639.500003   584.827267\n",
      "3      571.333342   559.175983\n",
      "4      693.499999   695.845610\n",
      "...           ...          ...\n",
      "1019   621.500007   854.356407\n",
      "1020  1196.499995  1180.701865\n",
      "1021   977.250004   896.672388\n",
      "1022  1089.999987  1063.815731\n",
      "1023   705.162794   676.669408\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1100\n",
      "Epoch 12/25, Validation Loss: 0.0986\n",
      "           actual    predicted\n",
      "0      639.250002   588.547672\n",
      "1      742.249999   743.909208\n",
      "2      639.500003   590.494134\n",
      "3      571.333342   568.230911\n",
      "4      693.499999   681.884178\n",
      "...           ...          ...\n",
      "1019   621.500007   848.885647\n",
      "1020  1196.499995  1119.422037\n",
      "1021   977.250004   874.777781\n",
      "1022  1089.999987  1022.607093\n",
      "1023   705.162794   696.251776\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1111\n",
      "Epoch 13/25, Validation Loss: 0.1122\n",
      "           actual    predicted\n",
      "0      639.250002   598.862313\n",
      "1      742.249999   780.308608\n",
      "2      639.500003   594.909593\n",
      "3      571.333342   564.624988\n",
      "4      693.499999   718.937143\n",
      "...           ...          ...\n",
      "1019   621.500007   889.392670\n",
      "1020  1196.499995  1173.761717\n",
      "1021   977.250004   935.907306\n",
      "1022  1089.999987  1076.919933\n",
      "1023   705.162794   727.329377\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.1083\n",
      "Epoch 14/25, Validation Loss: 0.0959\n",
      "           actual    predicted\n",
      "0      639.250002   596.711190\n",
      "1      742.249999   761.115668\n",
      "2      639.500003   581.529402\n",
      "3      571.333342   560.942646\n",
      "4      693.499999   713.776856\n",
      "...           ...          ...\n",
      "1019   621.500007   878.294673\n",
      "1020  1196.499995  1200.582345\n",
      "1021   977.250004   956.803371\n",
      "1022  1089.999987  1076.470762\n",
      "1023   705.162794   682.303390\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.1083\n",
      "Epoch 15/25, Validation Loss: 0.1028\n",
      "           actual    predicted\n",
      "0      639.250002   594.342883\n",
      "1      742.249999   740.372299\n",
      "2      639.500003   584.840835\n",
      "3      571.333342   563.740493\n",
      "4      693.499999   699.531618\n",
      "...           ...          ...\n",
      "1019   621.500007   835.794331\n",
      "1020  1196.499995  1152.926611\n",
      "1021   977.250004   925.914082\n",
      "1022  1089.999987  1047.657670\n",
      "1023   705.162794   681.829777\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.1075\n",
      "Epoch 16/25, Validation Loss: 0.1004\n",
      "           actual    predicted\n",
      "0      639.250002   598.350482\n",
      "1      742.249999   814.672511\n",
      "2      639.500003   599.016924\n",
      "3      571.333342   561.628041\n",
      "4      693.499999   758.621779\n",
      "...           ...          ...\n",
      "1019   621.500007   930.060700\n",
      "1020  1196.499995  1194.660856\n",
      "1021   977.250004   970.848145\n",
      "1022  1089.999987  1115.828292\n",
      "1023   705.162794   768.784828\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.1084\n",
      "Epoch 17/25, Validation Loss: 0.1045\n",
      "           actual    predicted\n",
      "0      639.250002   603.388187\n",
      "1      742.249999   791.007469\n",
      "2      639.500003   601.842236\n",
      "3      571.333342   565.221612\n",
      "4      693.499999   707.767222\n",
      "...           ...          ...\n",
      "1019   621.500007   885.128000\n",
      "1020  1196.499995  1217.047733\n",
      "1021   977.250004  1004.347924\n",
      "1022  1089.999987  1106.472533\n",
      "1023   705.162794   713.212579\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.1056\n",
      "Epoch 18/25, Validation Loss: 0.1048\n",
      "           actual    predicted\n",
      "0      639.250002   616.765164\n",
      "1      742.249999   762.148346\n",
      "2      639.500003   609.569926\n",
      "3      571.333342   576.586095\n",
      "4      693.499999   703.509718\n",
      "...           ...          ...\n",
      "1019   621.500007   873.812628\n",
      "1020  1196.499995  1185.940044\n",
      "1021   977.250004   964.746682\n",
      "1022  1089.999987  1086.095700\n",
      "1023   705.162794   710.772881\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.3370]], device='cuda:0')\n",
      "[692.84]\n",
      "            actual    predicted\n",
      "0       639.250002   616.765164\n",
      "1       742.249999   762.148346\n",
      "2       639.500003   609.569926\n",
      "3       571.333342   576.586095\n",
      "4       693.499999   703.509718\n",
      "...            ...          ...\n",
      "19155  1409.999976  1409.192039\n",
      "19156   643.999993   654.137631\n",
      "19157   902.285715   903.413991\n",
      "19158   614.999996   628.639186\n",
      "19159   660.999996   660.244270\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 90.7066\n",
      "Score (MAE): 43.6780\n",
      "Score (ME): -0.4348\n",
      "Score (MAPE): 5.1030%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     35.912750  36.460677\n",
      "1     26.970000  22.974661\n",
      "2     21.217500  18.177671\n",
      "3     43.427273  44.228983\n",
      "4     51.087500  55.928851\n",
      "...         ...        ...\n",
      "1019  28.910000  27.841693\n",
      "1020  37.315000  36.783651\n",
      "1021  51.635000  55.617749\n",
      "1022  37.430000  38.900836\n",
      "1023  32.208000  32.022481\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2761\n",
      "Epoch 1/25, Validation Loss: 0.0563\n",
      "         actual  predicted\n",
      "0     35.912750  36.076872\n",
      "1     26.970000  25.560059\n",
      "2     21.217500  18.830797\n",
      "3     43.427273  43.479255\n",
      "4     51.087500  52.767017\n",
      "...         ...        ...\n",
      "1019  28.910000  28.903266\n",
      "1020  37.315000  37.099206\n",
      "1021  51.635000  51.307701\n",
      "1022  37.430000  37.450528\n",
      "1023  32.208000  32.634202\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0592\n",
      "Epoch 2/25, Validation Loss: 0.0188\n",
      "         actual  predicted\n",
      "0     35.912750  36.015819\n",
      "1     26.970000  24.942319\n",
      "2     21.217500  18.806092\n",
      "3     43.427273  43.804131\n",
      "4     51.087500  52.157685\n",
      "...         ...        ...\n",
      "1019  28.910000  28.208304\n",
      "1020  37.315000  36.727036\n",
      "1021  51.635000  51.831699\n",
      "1022  37.430000  37.437462\n",
      "1023  32.208000  32.678111\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0474\n",
      "Epoch 3/25, Validation Loss: 0.0205\n",
      "         actual  predicted\n",
      "0     35.912750  37.037214\n",
      "1     26.970000  25.843173\n",
      "2     21.217500  19.281134\n",
      "3     43.427273  42.996808\n",
      "4     51.087500  52.975600\n",
      "...         ...        ...\n",
      "1019  28.910000  28.817445\n",
      "1020  37.315000  37.315188\n",
      "1021  51.635000  53.019223\n",
      "1022  37.430000  37.582399\n",
      "1023  32.208000  32.698045\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0402\n",
      "Epoch 4/25, Validation Loss: 0.0211\n",
      "         actual  predicted\n",
      "0     35.912750  36.991283\n",
      "1     26.970000  25.706699\n",
      "2     21.217500  18.358997\n",
      "3     43.427273  44.378353\n",
      "4     51.087500  52.843825\n",
      "...         ...        ...\n",
      "1019  28.910000  28.681944\n",
      "1020  37.315000  37.045461\n",
      "1021  51.635000  52.550900\n",
      "1022  37.430000  38.002193\n",
      "1023  32.208000  33.161858\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0356\n",
      "Epoch 5/25, Validation Loss: 0.0195\n",
      "         actual  predicted\n",
      "0     35.912750  37.289614\n",
      "1     26.970000  25.611534\n",
      "2     21.217500  18.447939\n",
      "3     43.427273  43.678427\n",
      "4     51.087500  52.612779\n",
      "...         ...        ...\n",
      "1019  28.910000  28.850839\n",
      "1020  37.315000  36.489640\n",
      "1021  51.635000  51.221247\n",
      "1022  37.430000  37.296743\n",
      "1023  32.208000  32.632115\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0341\n",
      "Epoch 6/25, Validation Loss: 0.0171\n",
      "         actual  predicted\n",
      "0     35.912750  36.492975\n",
      "1     26.970000  26.481684\n",
      "2     21.217500  20.195913\n",
      "3     43.427273  43.592475\n",
      "4     51.087500  52.345926\n",
      "...         ...        ...\n",
      "1019  28.910000  29.156002\n",
      "1020  37.315000  36.496631\n",
      "1021  51.635000  50.850239\n",
      "1022  37.430000  37.067102\n",
      "1023  32.208000  32.378282\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0302\n",
      "Epoch 7/25, Validation Loss: 0.0160\n",
      "         actual  predicted\n",
      "0     35.912750  38.244191\n",
      "1     26.970000  26.859272\n",
      "2     21.217500  18.707349\n",
      "3     43.427273  45.321097\n",
      "4     51.087500  53.398958\n",
      "...         ...        ...\n",
      "1019  28.910000  28.214509\n",
      "1020  37.315000  36.492910\n",
      "1021  51.635000  51.660767\n",
      "1022  37.430000  37.273959\n",
      "1023  32.208000  32.477916\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0280\n",
      "Epoch 8/25, Validation Loss: 0.0190\n",
      "         actual  predicted\n",
      "0     35.912750  37.309600\n",
      "1     26.970000  26.084113\n",
      "2     21.217500  18.680085\n",
      "3     43.427273  43.935984\n",
      "4     51.087500  52.581044\n",
      "...         ...        ...\n",
      "1019  28.910000  28.699644\n",
      "1020  37.315000  36.898451\n",
      "1021  51.635000  51.861573\n",
      "1022  37.430000  37.919914\n",
      "1023  32.208000  32.733001\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0269\n",
      "Epoch 9/25, Validation Loss: 0.0150\n",
      "         actual  predicted\n",
      "0     35.912750  37.372029\n",
      "1     26.970000  26.515534\n",
      "2     21.217500  20.654104\n",
      "3     43.427273  43.679013\n",
      "4     51.087500  51.327561\n",
      "...         ...        ...\n",
      "1019  28.910000  29.153943\n",
      "1020  37.315000  37.013020\n",
      "1021  51.635000  50.759123\n",
      "1022  37.430000  37.480327\n",
      "1023  32.208000  32.700918\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0256\n",
      "Epoch 10/25, Validation Loss: 0.0109\n",
      "         actual  predicted\n",
      "0     35.912750  37.163215\n",
      "1     26.970000  26.295057\n",
      "2     21.217500  19.378578\n",
      "3     43.427273  43.297707\n",
      "4     51.087500  50.689153\n",
      "...         ...        ...\n",
      "1019  28.910000  28.417025\n",
      "1020  37.315000  36.766859\n",
      "1021  51.635000  50.799840\n",
      "1022  37.430000  37.364797\n",
      "1023  32.208000  32.091088\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0245\n",
      "Epoch 11/25, Validation Loss: 0.0146\n",
      "         actual  predicted\n",
      "0     35.912750  37.371508\n",
      "1     26.970000  25.661006\n",
      "2     21.217500  17.433287\n",
      "3     43.427273  43.525172\n",
      "4     51.087500  51.608431\n",
      "...         ...        ...\n",
      "1019  28.910000  27.709484\n",
      "1020  37.315000  36.210391\n",
      "1021  51.635000  51.712523\n",
      "1022  37.430000  36.717521\n",
      "1023  32.208000  31.354122\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0239\n",
      "Epoch 12/25, Validation Loss: 0.0220\n",
      "         actual  predicted\n",
      "0     35.912750  38.277419\n",
      "1     26.970000  26.906415\n",
      "2     21.217500  20.186058\n",
      "3     43.427273  43.619363\n",
      "4     51.087500  51.788528\n",
      "...         ...        ...\n",
      "1019  28.910000  29.119302\n",
      "1020  37.315000  36.569764\n",
      "1021  51.635000  50.563494\n",
      "1022  37.430000  37.034770\n",
      "1023  32.208000  32.513946\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0228\n",
      "Epoch 13/25, Validation Loss: 0.0130\n",
      "         actual  predicted\n",
      "0     35.912750  36.913854\n",
      "1     26.970000  26.097973\n",
      "2     21.217500  18.689291\n",
      "3     43.427273  43.458179\n",
      "4     51.087500  51.716818\n",
      "...         ...        ...\n",
      "1019  28.910000  28.006795\n",
      "1020  37.315000  35.638581\n",
      "1021  51.635000  50.911605\n",
      "1022  37.430000  36.352914\n",
      "1023  32.208000  31.500598\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0219\n",
      "Epoch 14/25, Validation Loss: 0.0161\n",
      "         actual  predicted\n",
      "0     35.912750  37.477876\n",
      "1     26.970000  26.355564\n",
      "2     21.217500  19.056840\n",
      "3     43.427273  43.534002\n",
      "4     51.087500  52.170389\n",
      "...         ...        ...\n",
      "1019  28.910000  28.441256\n",
      "1020  37.315000  36.006459\n",
      "1021  51.635000  51.049672\n",
      "1022  37.430000  36.830241\n",
      "1023  32.208000  32.136579\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.7319]], device='cuda:0')\n",
      "[42.54]\n",
      "          actual  predicted\n",
      "0      35.912750  37.477876\n",
      "1      26.970000  26.355564\n",
      "2      21.217500  19.056840\n",
      "3      43.427273  43.534002\n",
      "4      51.087500  52.170389\n",
      "...          ...        ...\n",
      "19155  37.247500  36.913505\n",
      "19156  23.445000  22.726100\n",
      "19157  35.712500  35.459257\n",
      "19158  34.365000  34.466762\n",
      "19159  50.252500  50.048668\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 1.1527\n",
      "Score (MAE): 0.7572\n",
      "Score (ME): -0.0233\n",
      "Score (MAPE): 2.0982%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     22.660000  21.771682\n",
      "1     19.897500  18.649929\n",
      "2     22.169131  21.715549\n",
      "3     32.862500  34.881504\n",
      "4     25.902500  26.376281\n",
      "...         ...        ...\n",
      "1019  23.340000  23.682298\n",
      "1020  22.463333  22.985991\n",
      "1021  21.642500  21.826376\n",
      "1022  33.762500  34.925798\n",
      "1023  17.480000  18.394791\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2574\n",
      "Epoch 1/25, Validation Loss: 0.0280\n",
      "         actual  predicted\n",
      "0     22.660000  21.126926\n",
      "1     19.897500  18.720925\n",
      "2     22.169131  21.286178\n",
      "3     32.862500  36.442210\n",
      "4     25.902500  26.338114\n",
      "...         ...        ...\n",
      "1019  23.340000  23.456325\n",
      "1020  22.463333  22.586249\n",
      "1021  21.642500  21.410713\n",
      "1022  33.762500  35.920747\n",
      "1023  17.480000  17.850898\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0603\n",
      "Epoch 2/25, Validation Loss: 0.0292\n",
      "         actual  predicted\n",
      "0     22.660000  21.590724\n",
      "1     19.897500  19.146838\n",
      "2     22.169131  21.340931\n",
      "3     32.862500  37.779843\n",
      "4     25.902500  25.971035\n",
      "...         ...        ...\n",
      "1019  23.340000  23.212921\n",
      "1020  22.463333  22.523306\n",
      "1021  21.642500  21.482816\n",
      "1022  33.762500  36.626163\n",
      "1023  17.480000  17.393648\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0487\n",
      "Epoch 3/25, Validation Loss: 0.0424\n",
      "         actual  predicted\n",
      "0     22.660000  22.074929\n",
      "1     19.897500  19.255124\n",
      "2     22.169131  21.904535\n",
      "3     32.862500  35.842763\n",
      "4     25.902500  26.081618\n",
      "...         ...        ...\n",
      "1019  23.340000  23.597530\n",
      "1020  22.463333  22.895795\n",
      "1021  21.642500  22.014957\n",
      "1022  33.762500  36.762455\n",
      "1023  17.480000  17.134449\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0435\n",
      "Epoch 4/25, Validation Loss: 0.0303\n",
      "         actual  predicted\n",
      "0     22.660000  21.760833\n",
      "1     19.897500  18.756904\n",
      "2     22.169131  21.747539\n",
      "3     32.862500  35.445603\n",
      "4     25.902500  26.086500\n",
      "...         ...        ...\n",
      "1019  23.340000  23.596567\n",
      "1020  22.463333  22.674952\n",
      "1021  21.642500  21.547321\n",
      "1022  33.762500  35.713319\n",
      "1023  17.480000  16.408123\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0380\n",
      "Epoch 5/25, Validation Loss: 0.0295\n",
      "         actual  predicted\n",
      "0     22.660000  21.990224\n",
      "1     19.897500  19.141355\n",
      "2     22.169131  21.957029\n",
      "3     32.862500  35.026267\n",
      "4     25.902500  26.062172\n",
      "...         ...        ...\n",
      "1019  23.340000  23.585626\n",
      "1020  22.463333  22.956266\n",
      "1021  21.642500  21.909253\n",
      "1022  33.762500  35.563481\n",
      "1023  17.480000  16.889507\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0331\n",
      "Epoch 6/25, Validation Loss: 0.0216\n",
      "         actual  predicted\n",
      "0     22.660000  22.241690\n",
      "1     19.897500  19.289858\n",
      "2     22.169131  22.019644\n",
      "3     32.862500  34.488693\n",
      "4     25.902500  26.308218\n",
      "...         ...        ...\n",
      "1019  23.340000  23.782612\n",
      "1020  22.463333  23.148893\n",
      "1021  21.642500  22.058479\n",
      "1022  33.762500  36.361100\n",
      "1023  17.480000  17.501366\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0317\n",
      "Epoch 7/25, Validation Loss: 0.0206\n",
      "         actual  predicted\n",
      "0     22.660000  21.843804\n",
      "1     19.897500  19.212140\n",
      "2     22.169131  21.893042\n",
      "3     32.862500  34.747283\n",
      "4     25.902500  26.193212\n",
      "...         ...        ...\n",
      "1019  23.340000  23.424765\n",
      "1020  22.463333  22.806207\n",
      "1021  21.642500  21.769781\n",
      "1022  33.762500  35.698315\n",
      "1023  17.480000  16.945066\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0299\n",
      "Epoch 8/25, Validation Loss: 0.0199\n",
      "         actual  predicted\n",
      "0     22.660000  22.063573\n",
      "1     19.897500  19.505978\n",
      "2     22.169131  22.012363\n",
      "3     32.862500  34.343068\n",
      "4     25.902500  26.333177\n",
      "...         ...        ...\n",
      "1019  23.340000  23.617511\n",
      "1020  22.463333  23.050757\n",
      "1021  21.642500  22.001067\n",
      "1022  33.762500  35.237481\n",
      "1023  17.480000  17.860903\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0305\n",
      "Epoch 9/25, Validation Loss: 0.0236\n",
      "         actual  predicted\n",
      "0     22.660000  22.244521\n",
      "1     19.897500  19.329097\n",
      "2     22.169131  22.132311\n",
      "3     32.862500  33.489038\n",
      "4     25.902500  26.189221\n",
      "...         ...        ...\n",
      "1019  23.340000  23.601571\n",
      "1020  22.463333  23.030005\n",
      "1021  21.642500  22.053836\n",
      "1022  33.762500  34.692591\n",
      "1023  17.480000  17.540780\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0288\n",
      "Epoch 10/25, Validation Loss: 0.0171\n",
      "         actual  predicted\n",
      "0     22.660000  22.048332\n",
      "1     19.897500  19.435442\n",
      "2     22.169131  22.063547\n",
      "3     32.862500  34.659469\n",
      "4     25.902500  26.242252\n",
      "...         ...        ...\n",
      "1019  23.340000  23.556516\n",
      "1020  22.463333  22.893679\n",
      "1021  21.642500  21.931739\n",
      "1022  33.762500  35.839897\n",
      "1023  17.480000  17.188661\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0271\n",
      "Epoch 11/25, Validation Loss: 0.0208\n",
      "         actual  predicted\n",
      "0     22.660000  21.812800\n",
      "1     19.897500  18.899406\n",
      "2     22.169131  21.798345\n",
      "3     32.862500  34.033180\n",
      "4     25.902500  26.342273\n",
      "...         ...        ...\n",
      "1019  23.340000  23.181288\n",
      "1020  22.463333  22.669695\n",
      "1021  21.642500  21.598417\n",
      "1022  33.762500  34.793348\n",
      "1023  17.480000  16.221319\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0265\n",
      "Epoch 12/25, Validation Loss: 0.0225\n",
      "         actual  predicted\n",
      "0     22.660000  22.154711\n",
      "1     19.897500  19.301402\n",
      "2     22.169131  22.011699\n",
      "3     32.862500  34.172712\n",
      "4     25.902500  26.021233\n",
      "...         ...        ...\n",
      "1019  23.340000  23.574641\n",
      "1020  22.463333  22.872639\n",
      "1021  21.642500  21.877447\n",
      "1022  33.762500  35.888492\n",
      "1023  17.480000  17.045143\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0253\n",
      "Epoch 13/25, Validation Loss: 0.0183\n",
      "         actual  predicted\n",
      "0     22.660000  22.122399\n",
      "1     19.897500  19.440528\n",
      "2     22.169131  22.158475\n",
      "3     32.862500  34.053648\n",
      "4     25.902500  26.278775\n",
      "...         ...        ...\n",
      "1019  23.340000  23.423057\n",
      "1020  22.463333  22.936376\n",
      "1021  21.642500  21.890158\n",
      "1022  33.762500  35.171401\n",
      "1023  17.480000  16.958724\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0249\n",
      "Epoch 14/25, Validation Loss: 0.0196\n",
      "         actual  predicted\n",
      "0     22.660000  22.116175\n",
      "1     19.897500  19.437549\n",
      "2     22.169131  22.037529\n",
      "3     32.862500  34.058498\n",
      "4     25.902500  26.122827\n",
      "...         ...        ...\n",
      "1019  23.340000  23.515311\n",
      "1020  22.463333  22.972210\n",
      "1021  21.642500  21.887482\n",
      "1022  33.762500  35.198483\n",
      "1023  17.480000  17.083269\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0233\n",
      "Epoch 15/25, Validation Loss: 0.0144\n",
      "         actual  predicted\n",
      "0     22.660000  22.111254\n",
      "1     19.897500  19.307575\n",
      "2     22.169131  22.092678\n",
      "3     32.862500  34.058320\n",
      "4     25.902500  26.016690\n",
      "...         ...        ...\n",
      "1019  23.340000  23.469498\n",
      "1020  22.463333  22.867476\n",
      "1021  21.642500  21.815247\n",
      "1022  33.762500  35.268493\n",
      "1023  17.480000  16.760449\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0233\n",
      "Epoch 16/25, Validation Loss: 0.0171\n",
      "         actual  predicted\n",
      "0     22.660000  22.029338\n",
      "1     19.897500  19.270723\n",
      "2     22.169131  21.916368\n",
      "3     32.862500  34.190453\n",
      "4     25.902500  25.959028\n",
      "...         ...        ...\n",
      "1019  23.340000  23.510228\n",
      "1020  22.463333  22.870699\n",
      "1021  21.642500  21.727562\n",
      "1022  33.762500  35.386000\n",
      "1023  17.480000  17.108857\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0226\n",
      "Epoch 17/25, Validation Loss: 0.0179\n",
      "         actual  predicted\n",
      "0     22.660000  22.053654\n",
      "1     19.897500  19.450515\n",
      "2     22.169131  21.872654\n",
      "3     32.862500  33.859546\n",
      "4     25.902500  25.930967\n",
      "...         ...        ...\n",
      "1019  23.340000  23.483096\n",
      "1020  22.463333  22.851902\n",
      "1021  21.642500  21.708207\n",
      "1022  33.762500  35.180853\n",
      "1023  17.480000  17.056249\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0225\n",
      "Epoch 18/25, Validation Loss: 0.0151\n",
      "         actual  predicted\n",
      "0     22.660000  22.206436\n",
      "1     19.897500  19.386879\n",
      "2     22.169131  22.149376\n",
      "3     32.862500  33.770646\n",
      "4     25.902500  26.094954\n",
      "...         ...        ...\n",
      "1019  23.340000  23.504148\n",
      "1020  22.463333  22.981672\n",
      "1021  21.642500  21.780492\n",
      "1022  33.762500  35.086763\n",
      "1023  17.480000  17.004862\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0222\n",
      "Epoch 19/25, Validation Loss: 0.0159\n",
      "         actual  predicted\n",
      "0     22.660000  22.108975\n",
      "1     19.897500  19.318589\n",
      "2     22.169131  22.062961\n",
      "3     32.862500  34.043486\n",
      "4     25.902500  26.009820\n",
      "...         ...        ...\n",
      "1019  23.340000  23.446637\n",
      "1020  22.463333  22.853567\n",
      "1021  21.642500  21.711918\n",
      "1022  33.762500  35.498220\n",
      "1023  17.480000  17.001853\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.3162]], device='cuda:0')\n",
      "[25.19]\n",
      "          actual  predicted\n",
      "0      22.660000  22.108975\n",
      "1      19.897500  19.318589\n",
      "2      22.169131  22.062961\n",
      "3      32.862500  34.043486\n",
      "4      25.902500  26.009820\n",
      "...          ...        ...\n",
      "19155  28.260000  28.700259\n",
      "19156  28.440000  28.511646\n",
      "19157  23.783333  23.937428\n",
      "19158  27.240000  28.204079\n",
      "19159  18.467500  17.761024\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 0.4744\n",
      "Score (MAE): 0.3125\n",
      "Score (ME): -0.0745\n",
      "Score (MAPE): 1.2796%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0      12.000005  -24.137268\n",
      "1      75.749997   64.705997\n",
      "2       8.250001  -12.760422\n",
      "3      29.333328    3.186653\n",
      "4       6.000002  -14.369030\n",
      "...          ...         ...\n",
      "1019    6.499998  115.412241\n",
      "1020   15.249997    1.384554\n",
      "1021   29.500004    1.516573\n",
      "1022  391.750005  105.651197\n",
      "1023    4.999994   -1.148693\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.9005\n",
      "Epoch 1/25, Validation Loss: 0.6981\n",
      "          actual   predicted\n",
      "0      12.000005   14.233261\n",
      "1      75.749997   77.324098\n",
      "2       8.250001   27.365707\n",
      "3      29.333328   42.371889\n",
      "4       6.000002   23.800909\n",
      "...          ...         ...\n",
      "1019    6.499998   74.435309\n",
      "1020   15.249997   30.346791\n",
      "1021   29.500004   30.840684\n",
      "1022  391.750005  331.398357\n",
      "1023    4.999994   22.250531\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.7060\n",
      "Epoch 2/25, Validation Loss: 0.6513\n",
      "          actual   predicted\n",
      "0      12.000005   17.926015\n",
      "1      75.749997   63.715665\n",
      "2       8.250001   24.497305\n",
      "3      29.333328   71.977115\n",
      "4       6.000002   18.450012\n",
      "...          ...         ...\n",
      "1019    6.499998  107.535480\n",
      "1020   15.249997   23.031388\n",
      "1021   29.500004   21.401009\n",
      "1022  391.750005  488.936903\n",
      "1023    4.999994   14.887095\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.6809\n",
      "Epoch 3/25, Validation Loss: 0.6426\n",
      "          actual   predicted\n",
      "0      12.000005   17.959360\n",
      "1      75.749997   45.948435\n",
      "2       8.250001   25.811566\n",
      "3      29.333328   40.993627\n",
      "4       6.000002   22.041245\n",
      "...          ...         ...\n",
      "1019    6.499998   99.424517\n",
      "1020   15.249997   32.272292\n",
      "1021   29.500004   31.468589\n",
      "1022  391.750005  293.354429\n",
      "1023    4.999994   22.824079\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.6785\n",
      "Epoch 4/25, Validation Loss: 0.6869\n",
      "          actual   predicted\n",
      "0      12.000005   13.164303\n",
      "1      75.749997  114.847871\n",
      "2       8.250001   25.046852\n",
      "3      29.333328   80.903576\n",
      "4       6.000002   22.588358\n",
      "...          ...         ...\n",
      "1019    6.499998  171.312227\n",
      "1020   15.249997   30.379440\n",
      "1021   29.500004   22.748282\n",
      "1022  391.750005  461.580065\n",
      "1023    4.999994   15.894582\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.6798\n",
      "Epoch 5/25, Validation Loss: 0.6430\n",
      "          actual   predicted\n",
      "0      12.000005   17.553215\n",
      "1      75.749997   18.885011\n",
      "2       8.250001   20.929392\n",
      "3      29.333328   72.368294\n",
      "4       6.000002   18.049797\n",
      "...          ...         ...\n",
      "1019    6.499998  111.375684\n",
      "1020   15.249997   24.092394\n",
      "1021   29.500004   17.881761\n",
      "1022  391.750005  359.926908\n",
      "1023    4.999994   21.483588\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.6716\n",
      "Epoch 6/25, Validation Loss: 0.6720\n",
      "          actual   predicted\n",
      "0      12.000005   97.397985\n",
      "1      75.749997  113.677384\n",
      "2       8.250001  100.327827\n",
      "3      29.333328  105.488284\n",
      "4       6.000002  100.549160\n",
      "...          ...         ...\n",
      "1019    6.499998  111.887784\n",
      "1020   15.249997  104.202830\n",
      "1021   29.500004  103.648145\n",
      "1022  391.750005  118.901053\n",
      "1023    4.999994  101.216418\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.6671\n",
      "Epoch 7/25, Validation Loss: 0.6798\n",
      "          actual   predicted\n",
      "0      12.000005    4.962585\n",
      "1      75.749997  355.541647\n",
      "2       8.250001   23.590453\n",
      "3      29.333328  100.069512\n",
      "4       6.000002   14.705304\n",
      "...          ...         ...\n",
      "1019    6.499998  218.929925\n",
      "1020   15.249997   24.683697\n",
      "1021   29.500004   19.044714\n",
      "1022  391.750005  505.458285\n",
      "1023    4.999994   21.714360\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.6421\n",
      "Epoch 8/25, Validation Loss: 0.6145\n",
      "          actual   predicted\n",
      "0      12.000005  -17.456837\n",
      "1      75.749997    2.141869\n",
      "2       8.250001   -4.371523\n",
      "3      29.333328   35.094922\n",
      "4       6.000002  -10.065780\n",
      "...          ...         ...\n",
      "1019    6.499998  108.625372\n",
      "1020   15.249997   -5.441209\n",
      "1021   29.500004  -13.272830\n",
      "1022  391.750005  360.243690\n",
      "1023    4.999994   -4.652921\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.6258\n",
      "Epoch 9/25, Validation Loss: 0.5966\n",
      "          actual   predicted\n",
      "0      12.000005   14.279966\n",
      "1      75.749997   80.750308\n",
      "2       8.250001   23.964091\n",
      "3      29.333328   45.429276\n",
      "4       6.000002   20.323591\n",
      "...          ...         ...\n",
      "1019    6.499998   94.596652\n",
      "1020   15.249997   25.895363\n",
      "1021   29.500004   22.748251\n",
      "1022  391.750005  207.781445\n",
      "1023    4.999994   30.036175\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.6326\n",
      "Epoch 10/25, Validation Loss: 0.5911\n",
      "          actual   predicted\n",
      "0      12.000005    3.036041\n",
      "1      75.749997  140.532808\n",
      "2       8.250001   20.645385\n",
      "3      29.333328   81.240423\n",
      "4       6.000002   11.156270\n",
      "...          ...         ...\n",
      "1019    6.499998  318.484873\n",
      "1020   15.249997   20.377047\n",
      "1021   29.500004   14.622868\n",
      "1022  391.750005  526.633408\n",
      "1023    4.999994   28.914076\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.6160\n",
      "Epoch 11/25, Validation Loss: 0.5940\n",
      "          actual   predicted\n",
      "0      12.000005    0.505608\n",
      "1      75.749997   32.794598\n",
      "2       8.250001    8.086029\n",
      "3      29.333328   29.407179\n",
      "4       6.000002    5.139428\n",
      "...          ...         ...\n",
      "1019    6.499998   51.889828\n",
      "1020   15.249997    7.863699\n",
      "1021   29.500004    5.424035\n",
      "1022  391.750005  180.582087\n",
      "1023    4.999994   11.571916\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.6074\n",
      "Epoch 12/25, Validation Loss: 0.5767\n",
      "          actual   predicted\n",
      "0      12.000005   41.374664\n",
      "1      75.749997   62.487556\n",
      "2       8.250001   55.622219\n",
      "3      29.333328   98.307312\n",
      "4       6.000002   47.799041\n",
      "...          ...         ...\n",
      "1019    6.499998  164.577356\n",
      "1020   15.249997   54.958962\n",
      "1021   29.500004   45.820369\n",
      "1022  391.750005  307.296451\n",
      "1023    4.999994   58.178360\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.6344\n",
      "Epoch 13/25, Validation Loss: 0.5815\n",
      "          actual   predicted\n",
      "0      12.000005   21.824118\n",
      "1      75.749997   52.029168\n",
      "2       8.250001   27.731044\n",
      "3      29.333328   51.329815\n",
      "4       6.000002   25.021223\n",
      "...          ...         ...\n",
      "1019    6.499998   69.438587\n",
      "1020   15.249997   25.687105\n",
      "1021   29.500004   23.726836\n",
      "1022  391.750005  328.383414\n",
      "1023    4.999994   36.593868\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.6038\n",
      "Epoch 14/25, Validation Loss: 0.5719\n",
      "          actual   predicted\n",
      "0      12.000005    8.148766\n",
      "1      75.749997  232.697268\n",
      "2       8.250001   13.843007\n",
      "3      29.333328   38.466229\n",
      "4       6.000002   12.549173\n",
      "...          ...         ...\n",
      "1019    6.499998   54.401320\n",
      "1020   15.249997   12.556145\n",
      "1021   29.500004   10.058567\n",
      "1022  391.750005  253.965239\n",
      "1023    4.999994   22.729231\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.5858\n",
      "Epoch 15/25, Validation Loss: 0.5732\n",
      "          actual   predicted\n",
      "0      12.000005   -0.271138\n",
      "1      75.749997   31.014317\n",
      "2       8.250001    3.891002\n",
      "3      29.333328   28.784824\n",
      "4       6.000002    3.680467\n",
      "...          ...         ...\n",
      "1019    6.499998   56.837885\n",
      "1020   15.249997    5.395228\n",
      "1021   29.500004    2.242377\n",
      "1022  391.750005  476.742742\n",
      "1023    4.999994   20.972840\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.6009\n",
      "Epoch 16/25, Validation Loss: 0.5648\n",
      "          actual   predicted\n",
      "0      12.000005    4.409513\n",
      "1      75.749997   47.499729\n",
      "2       8.250001    7.183082\n",
      "3      29.333328   29.821450\n",
      "4       6.000002    8.887835\n",
      "...          ...         ...\n",
      "1019    6.499998   46.949835\n",
      "1020   15.249997   11.199765\n",
      "1021   29.500004    9.135398\n",
      "1022  391.750005  294.355102\n",
      "1023    4.999994   25.035231\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.6151\n",
      "Epoch 17/25, Validation Loss: 0.5546\n",
      "          actual   predicted\n",
      "0      12.000005  -18.979814\n",
      "1      75.749997   25.239236\n",
      "2       8.250001  -13.681883\n",
      "3      29.333328   13.013722\n",
      "4       6.000002  -15.338967\n",
      "...          ...         ...\n",
      "1019    6.499998   40.533916\n",
      "1020   15.249997   -9.721313\n",
      "1021   29.500004  -14.066462\n",
      "1022  391.750005  355.745368\n",
      "1023    4.999994    8.406606\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.5927\n",
      "Epoch 18/25, Validation Loss: 0.5712\n",
      "          actual   predicted\n",
      "0      12.000005   33.478472\n",
      "1      75.749997   68.578392\n",
      "2       8.250001   34.625915\n",
      "3      29.333328   45.615494\n",
      "4       6.000002   34.825161\n",
      "...          ...         ...\n",
      "1019    6.499998   71.374634\n",
      "1020   15.249997   36.545329\n",
      "1021   29.500004   36.075705\n",
      "1022  391.750005  267.562577\n",
      "1023    4.999994   44.359558\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.5886\n",
      "Epoch 19/25, Validation Loss: 0.5921\n",
      "          actual   predicted\n",
      "0      12.000005    7.843082\n",
      "1      75.749997   45.347013\n",
      "2       8.250001   15.228890\n",
      "3      29.333328   35.788283\n",
      "4       6.000002   14.438531\n",
      "...          ...         ...\n",
      "1019    6.499998   64.988265\n",
      "1020   15.249997   19.499919\n",
      "1021   29.500004   17.139371\n",
      "1022  391.750005  362.841018\n",
      "1023    4.999994   29.869515\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.6082\n",
      "Epoch 20/25, Validation Loss: 0.5574\n",
      "          actual   predicted\n",
      "0      12.000005   -3.807462\n",
      "1      75.749997   38.353784\n",
      "2       8.250001    1.672229\n",
      "3      29.333328   16.899128\n",
      "4       6.000002    1.680561\n",
      "...          ...         ...\n",
      "1019    6.499998   31.805577\n",
      "1020   15.249997    4.980957\n",
      "1021   29.500004    5.107221\n",
      "1022  391.750005  344.776801\n",
      "1023    4.999994   14.916883\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.5909\n",
      "Epoch 21/25, Validation Loss: 0.5518\n",
      "          actual   predicted\n",
      "0      12.000005  -26.848622\n",
      "1      75.749997   11.545417\n",
      "2       8.250001  -19.127612\n",
      "3      29.333328   10.667546\n",
      "4       6.000002  -20.548010\n",
      "...          ...         ...\n",
      "1019    6.499998   20.961773\n",
      "1020   15.249997  -15.522118\n",
      "1021   29.500004  -18.692788\n",
      "1022  391.750005  487.838790\n",
      "1023    4.999994   -5.915370\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.5996\n",
      "Epoch 22/25, Validation Loss: 0.5646\n",
      "          actual   predicted\n",
      "0      12.000005   -5.792743\n",
      "1      75.749997   26.854943\n",
      "2       8.250001   -1.928997\n",
      "3      29.333328   13.912842\n",
      "4       6.000002   -0.826614\n",
      "...          ...         ...\n",
      "1019    6.499998   22.098670\n",
      "1020   15.249997    2.085931\n",
      "1021   29.500004    1.741764\n",
      "1022  391.750005  342.364173\n",
      "1023    4.999994    8.097175\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.5664\n",
      "Epoch 23/25, Validation Loss: 0.5445\n",
      "          actual   predicted\n",
      "0      12.000005   12.117400\n",
      "1      75.749997   29.353976\n",
      "2       8.250001   18.207793\n",
      "3      29.333328   36.351870\n",
      "4       6.000002   19.166346\n",
      "...          ...         ...\n",
      "1019    6.499998   43.329083\n",
      "1020   15.249997   23.364108\n",
      "1021   29.500004   18.553097\n",
      "1022  391.750005  472.324705\n",
      "1023    4.999994   24.985301\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.5677\n",
      "Epoch 24/25, Validation Loss: 0.5855\n",
      "          actual   predicted\n",
      "0      12.000005    0.026560\n",
      "1      75.749997   35.448101\n",
      "2       8.250001    6.447460\n",
      "3      29.333328   36.153636\n",
      "4       6.000002    6.887755\n",
      "...          ...         ...\n",
      "1019    6.499998   80.025824\n",
      "1020   15.249997   10.026266\n",
      "1021   29.500004    8.555274\n",
      "1022  391.750005  401.625312\n",
      "1023    4.999994   17.313874\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.5616\n",
      "Epoch 25/25, Validation Loss: 0.5848\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.1524]], device='cuda:0')\n",
      "[42.6]\n",
      "          actual  predicted\n",
      "0      12.000005   0.026560\n",
      "1      75.749997  35.448101\n",
      "2       8.250001   6.447460\n",
      "3      29.333328  36.153636\n",
      "4       6.000002   6.887755\n",
      "...          ...        ...\n",
      "19155   6.499998   4.262648\n",
      "19156  71.000000  31.358199\n",
      "19157   4.499997  22.850704\n",
      "19158   5.500006   8.486181\n",
      "19159   5.250007  20.986311\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 815.4706\n",
      "Score (MAE): 105.5044\n",
      "Score (ME): 31.6958\n",
      "Score (MAPE): 209018.0814%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "performance_df = pd.read_csv('model_performances.csv')\n",
    "d_model=256\n",
    "nhead=8\n",
    "num_layers=5 \n",
    "dropout_pe=0.15\n",
    "dropout_encoder=0.15\n",
    "batch_size=1024\n",
    "learning_rate=0.00031\n",
    "epochs=25\n",
    "y_feature = 'CO2'\n",
    "aggregation_level = 'quarter_hour'\n",
    "freq = 15 if aggregation_level == 'quarter_hour' else 30 if aggregation_level == 'half_hour' else 60\n",
    "window_size = 5 * 60 // freq\n",
    "for aggregation_level in ['quarter_hour', 'half_hour', 'hour']:\n",
    "    for y_feature in ['CO2', 'VOC', 'hum', 'tmp', 'vis']:\n",
    "        \n",
    "        model, scaler, rmse, mae, me, mape, train_loss, val_loss, n_features = utils.create_multivariate_transformer_model_for_feature(df, d_model=d_model, nhead=nhead, num_layers=num_layers, dropout_pe=dropout_pe, dropout_encoder=dropout_encoder, batch_size=batch_size, learning_rate=learning_rate, epochs=epochs, y_feature=y_feature, aggregation_level=aggregation_level, window_size=window_size)\n",
    "        performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n",
    "        performance_df.to_csv('model_performances.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     475.000000  475.310204\n",
      "1     415.000002  415.023051\n",
      "2     473.000000  477.090989\n",
      "3     512.000001  529.613180\n",
      "4     554.000002  574.161563\n",
      "...          ...         ...\n",
      "4091  420.000001  414.515299\n",
      "4092  429.999999  430.388544\n",
      "4093  425.000000  416.999382\n",
      "4094  415.000002  413.225235\n",
      "4095  412.999998  416.412842\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.3546\n",
      "Epoch 1/25, Validation Loss: 0.0907\n",
      "          actual   predicted\n",
      "0     475.000000  474.009144\n",
      "1     415.000002  418.210591\n",
      "2     473.000000  470.695207\n",
      "3     512.000001  521.129846\n",
      "4     554.000002  574.822406\n",
      "...          ...         ...\n",
      "4091  420.000001  417.722072\n",
      "4092  429.999999  431.246147\n",
      "4093  425.000000  420.243562\n",
      "4094  415.000002  413.374141\n",
      "4095  412.999998  416.235154\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0706\n",
      "Epoch 2/25, Validation Loss: 0.0477\n",
      "          actual   predicted\n",
      "0     475.000000  473.969152\n",
      "1     415.000002  423.640817\n",
      "2     473.000000  470.905437\n",
      "3     512.000001  516.851436\n",
      "4     554.000002  566.636655\n",
      "...          ...         ...\n",
      "4091  420.000001  420.692721\n",
      "4092  429.999999  431.221576\n",
      "4093  425.000000  420.671918\n",
      "4094  415.000002  417.884756\n",
      "4095  412.999998  418.703701\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0544\n",
      "Epoch 3/25, Validation Loss: 0.0439\n",
      "          actual   predicted\n",
      "0     475.000000  476.187944\n",
      "1     415.000002  421.480360\n",
      "2     473.000000  476.091446\n",
      "3     512.000001  516.066173\n",
      "4     554.000002  578.300823\n",
      "...          ...         ...\n",
      "4091  420.000001  418.605380\n",
      "4092  429.999999  428.015712\n",
      "4093  425.000000  414.079558\n",
      "4094  415.000002  413.501747\n",
      "4095  412.999998  415.241046\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0480\n",
      "Epoch 4/25, Validation Loss: 0.0396\n",
      "          actual   predicted\n",
      "0     475.000000  476.852557\n",
      "1     415.000002  419.384025\n",
      "2     473.000000  478.255592\n",
      "3     512.000001  520.934221\n",
      "4     554.000002  581.129712\n",
      "...          ...         ...\n",
      "4091  420.000001  416.732441\n",
      "4092  429.999999  428.817317\n",
      "4093  425.000000  413.247991\n",
      "4094  415.000002  412.525475\n",
      "4095  412.999998  414.370007\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0455\n",
      "Epoch 5/25, Validation Loss: 0.0368\n",
      "          actual   predicted\n",
      "0     475.000000  471.456732\n",
      "1     415.000002  419.698035\n",
      "2     473.000000  472.298347\n",
      "3     512.000001  513.731031\n",
      "4     554.000002  566.642951\n",
      "...          ...         ...\n",
      "4091  420.000001  417.422953\n",
      "4092  429.999999  428.687584\n",
      "4093  425.000000  417.215356\n",
      "4094  415.000002  413.262897\n",
      "4095  412.999998  414.341676\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0426\n",
      "Epoch 6/25, Validation Loss: 0.0356\n",
      "          actual   predicted\n",
      "0     475.000000  471.908365\n",
      "1     415.000002  423.449343\n",
      "2     473.000000  480.877326\n",
      "3     512.000001  516.198466\n",
      "4     554.000002  566.835665\n",
      "...          ...         ...\n",
      "4091  420.000001  422.416611\n",
      "4092  429.999999  431.292915\n",
      "4093  425.000000  420.565821\n",
      "4094  415.000002  416.677334\n",
      "4095  412.999998  419.314906\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0406\n",
      "Epoch 7/25, Validation Loss: 0.0340\n",
      "          actual   predicted\n",
      "0     475.000000  471.660982\n",
      "1     415.000002  421.486332\n",
      "2     473.000000  476.205254\n",
      "3     512.000001  516.324334\n",
      "4     554.000002  565.693815\n",
      "...          ...         ...\n",
      "4091  420.000001  416.476446\n",
      "4092  429.999999  427.816350\n",
      "4093  425.000000  415.131921\n",
      "4094  415.000002  412.643653\n",
      "4095  412.999998  413.604907\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0404\n",
      "Epoch 8/25, Validation Loss: 0.0340\n",
      "          actual   predicted\n",
      "0     475.000000  472.897267\n",
      "1     415.000002  421.066872\n",
      "2     473.000000  477.083962\n",
      "3     512.000001  516.759566\n",
      "4     554.000002  565.416236\n",
      "...          ...         ...\n",
      "4091  420.000001  420.009467\n",
      "4092  429.999999  429.693998\n",
      "4093  425.000000  418.527456\n",
      "4094  415.000002  416.560478\n",
      "4095  412.999998  417.070267\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0389\n",
      "Epoch 9/25, Validation Loss: 0.0330\n",
      "          actual   predicted\n",
      "0     475.000000  470.934735\n",
      "1     415.000002  421.661744\n",
      "2     473.000000  477.706277\n",
      "3     512.000001  516.896910\n",
      "4     554.000002  565.757345\n",
      "...          ...         ...\n",
      "4091  420.000001  422.756998\n",
      "4092  429.999999  429.581218\n",
      "4093  425.000000  421.032778\n",
      "4094  415.000002  417.616185\n",
      "4095  412.999998  419.918576\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0368\n",
      "Epoch 10/25, Validation Loss: 0.0336\n",
      "          actual   predicted\n",
      "0     475.000000  473.848163\n",
      "1     415.000002  417.272875\n",
      "2     473.000000  481.258723\n",
      "3     512.000001  522.342347\n",
      "4     554.000002  575.472716\n",
      "...          ...         ...\n",
      "4091  420.000001  418.826214\n",
      "4092  429.999999  430.122287\n",
      "4093  425.000000  418.104794\n",
      "4094  415.000002  412.696326\n",
      "4095  412.999998  415.465306\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0363\n",
      "Epoch 11/25, Validation Loss: 0.0368\n",
      "          actual   predicted\n",
      "0     475.000000  471.368899\n",
      "1     415.000002  420.542275\n",
      "2     473.000000  475.799395\n",
      "3     512.000001  516.216257\n",
      "4     554.000002  568.303733\n",
      "...          ...         ...\n",
      "4091  420.000001  421.431909\n",
      "4092  429.999999  431.021342\n",
      "4093  425.000000  420.336662\n",
      "4094  415.000002  416.580958\n",
      "4095  412.999998  418.377776\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0357\n",
      "Epoch 12/25, Validation Loss: 0.0336\n",
      "          actual   predicted\n",
      "0     475.000000  473.494772\n",
      "1     415.000002  423.722558\n",
      "2     473.000000  480.851534\n",
      "3     512.000001  516.068662\n",
      "4     554.000002  566.753804\n",
      "...          ...         ...\n",
      "4091  420.000001  420.745357\n",
      "4092  429.999999  430.807862\n",
      "4093  425.000000  420.054680\n",
      "4094  415.000002  415.980962\n",
      "4095  412.999998  416.674081\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0358\n",
      "Epoch 13/25, Validation Loss: 0.0342\n",
      "          actual   predicted\n",
      "0     475.000000  473.171225\n",
      "1     415.000002  418.903860\n",
      "2     473.000000  478.170028\n",
      "3     512.000001  519.961161\n",
      "4     554.000002  569.752222\n",
      "...          ...         ...\n",
      "4091  420.000001  419.593034\n",
      "4092  429.999999  429.475343\n",
      "4093  425.000000  418.076531\n",
      "4094  415.000002  415.885398\n",
      "4095  412.999998  416.599914\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0341\n",
      "Epoch 14/25, Validation Loss: 0.0315\n",
      "          actual   predicted\n",
      "0     475.000000  473.142002\n",
      "1     415.000002  421.399754\n",
      "2     473.000000  480.965882\n",
      "3     512.000001  519.515778\n",
      "4     554.000002  568.237927\n",
      "...          ...         ...\n",
      "4091  420.000001  421.422157\n",
      "4092  429.999999  430.250671\n",
      "4093  425.000000  420.737265\n",
      "4094  415.000002  418.237285\n",
      "4095  412.999998  418.444934\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0338\n",
      "Epoch 15/25, Validation Loss: 0.0316\n",
      "          actual   predicted\n",
      "0     475.000000  472.524751\n",
      "1     415.000002  421.814474\n",
      "2     473.000000  478.087492\n",
      "3     512.000001  516.642100\n",
      "4     554.000002  566.025721\n",
      "...          ...         ...\n",
      "4091  420.000001  419.435571\n",
      "4092  429.999999  428.063092\n",
      "4093  425.000000  417.220578\n",
      "4094  415.000002  417.975828\n",
      "4095  412.999998  416.299149\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0338\n",
      "Epoch 16/25, Validation Loss: 0.0316\n",
      "          actual   predicted\n",
      "0     475.000000  471.655259\n",
      "1     415.000002  421.312056\n",
      "2     473.000000  478.362434\n",
      "3     512.000001  517.196650\n",
      "4     554.000002  567.982721\n",
      "...          ...         ...\n",
      "4091  420.000001  420.426268\n",
      "4092  429.999999  429.583480\n",
      "4093  425.000000  420.011684\n",
      "4094  415.000002  416.829410\n",
      "4095  412.999998  417.560432\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0337\n",
      "Epoch 17/25, Validation Loss: 0.0339\n",
      "          actual   predicted\n",
      "0     475.000000  471.678904\n",
      "1     415.000002  422.764834\n",
      "2     473.000000  477.785784\n",
      "3     512.000001  516.260179\n",
      "4     554.000002  557.798025\n",
      "...          ...         ...\n",
      "4091  420.000001  421.576735\n",
      "4092  429.999999  429.584167\n",
      "4093  425.000000  422.888429\n",
      "4094  415.000002  418.433206\n",
      "4095  412.999998  418.763451\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0334\n",
      "Epoch 18/25, Validation Loss: 0.0321\n",
      "          actual   predicted\n",
      "0     475.000000  471.599179\n",
      "1     415.000002  421.966896\n",
      "2     473.000000  478.942017\n",
      "3     512.000001  519.341828\n",
      "4     554.000002  562.012662\n",
      "...          ...         ...\n",
      "4091  420.000001  420.811493\n",
      "4092  429.999999  429.502750\n",
      "4093  425.000000  421.480097\n",
      "4094  415.000002  417.906859\n",
      "4095  412.999998  418.044308\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0327\n",
      "Epoch 19/25, Validation Loss: 0.0311\n",
      "          actual   predicted\n",
      "0     475.000000  472.710890\n",
      "1     415.000002  420.219964\n",
      "2     473.000000  477.868394\n",
      "3     512.000001  518.386451\n",
      "4     554.000002  563.711166\n",
      "...          ...         ...\n",
      "4091  420.000001  420.134189\n",
      "4092  429.999999  428.704289\n",
      "4093  425.000000  419.586498\n",
      "4094  415.000002  416.896380\n",
      "4095  412.999998  416.839095\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0329\n",
      "Epoch 20/25, Validation Loss: 0.0312\n",
      "          actual   predicted\n",
      "0     475.000000  473.471369\n",
      "1     415.000002  419.720679\n",
      "2     473.000000  479.366135\n",
      "3     512.000001  521.296160\n",
      "4     554.000002  568.849884\n",
      "...          ...         ...\n",
      "4091  420.000001  418.922004\n",
      "4092  429.999999  428.812839\n",
      "4093  425.000000  418.996494\n",
      "4094  415.000002  416.433014\n",
      "4095  412.999998  415.979377\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0326\n",
      "Epoch 21/25, Validation Loss: 0.0310\n",
      "          actual   predicted\n",
      "0     475.000000  473.357333\n",
      "1     415.000002  420.567045\n",
      "2     473.000000  482.003689\n",
      "3     512.000001  520.304219\n",
      "4     554.000002  569.750614\n",
      "...          ...         ...\n",
      "4091  420.000001  420.168223\n",
      "4092  429.999999  429.720594\n",
      "4093  425.000000  419.234864\n",
      "4094  415.000002  415.935081\n",
      "4095  412.999998  416.847997\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0328\n",
      "Epoch 22/25, Validation Loss: 0.0314\n",
      "          actual   predicted\n",
      "0     475.000000  472.058481\n",
      "1     415.000002  421.525467\n",
      "2     473.000000  476.513219\n",
      "3     512.000001  518.545520\n",
      "4     554.000002  560.646853\n",
      "...          ...         ...\n",
      "4091  420.000001  420.451805\n",
      "4092  429.999999  429.969189\n",
      "4093  425.000000  421.396005\n",
      "4094  415.000002  417.294490\n",
      "4095  412.999998  417.076953\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0323\n",
      "Epoch 23/25, Validation Loss: 0.0318\n",
      "          actual   predicted\n",
      "0     475.000000  471.328136\n",
      "1     415.000002  420.604873\n",
      "2     473.000000  478.664001\n",
      "3     512.000001  518.066660\n",
      "4     554.000002  563.440559\n",
      "...          ...         ...\n",
      "4091  420.000001  420.849884\n",
      "4092  429.999999  429.707210\n",
      "4093  425.000000  420.754222\n",
      "4094  415.000002  416.924366\n",
      "4095  412.999998  417.289839\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0322\n",
      "Epoch 24/25, Validation Loss: 0.0311\n",
      "          actual   predicted\n",
      "0     475.000000  470.934805\n",
      "1     415.000002  420.203676\n",
      "2     473.000000  477.926399\n",
      "3     512.000001  516.481958\n",
      "4     554.000002  562.597648\n",
      "...          ...         ...\n",
      "4091  420.000001  419.390426\n",
      "4092  429.999999  428.817584\n",
      "4093  425.000000  419.877383\n",
      "4094  415.000002  415.686922\n",
      "4095  412.999998  416.053237\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0323\n",
      "Epoch 25/25, Validation Loss: 0.0319\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[0.7085]], device='cuda:0')\n",
      "[578.1]\n",
      "           actual   predicted\n",
      "0      475.000000  470.934805\n",
      "1      415.000002  420.203676\n",
      "2      473.000000  477.926399\n",
      "3      512.000001  516.481958\n",
      "4      554.000002  562.597648\n",
      "...           ...         ...\n",
      "79080  460.000000  455.841745\n",
      "79081  587.999997  588.255098\n",
      "79082  490.000000  480.200046\n",
      "79083  447.000001  449.638561\n",
      "79084  438.000001  427.009009\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 22.5269\n",
      "Score (MAE): 7.6593\n",
      "Score (ME): 0.5431\n",
      "Score (MAPE): 1.3813%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      584.999999   632.771541\n",
      "1      742.000000   744.016520\n",
      "2     1669.999979  1930.202522\n",
      "3      973.999995   936.825870\n",
      "4      515.999993   560.142307\n",
      "...           ...          ...\n",
      "4091   678.117648   684.709631\n",
      "4092   643.999999   693.356222\n",
      "4093   608.000008   618.960392\n",
      "4094   547.999999   588.418523\n",
      "4095  1370.000005  1203.730845\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.3455\n",
      "Epoch 1/25, Validation Loss: 0.1252\n",
      "           actual    predicted\n",
      "0      584.999999   616.638002\n",
      "1      742.000000   749.331039\n",
      "2     1669.999979  1756.890840\n",
      "3      973.999995   919.297423\n",
      "4      515.999993   561.492285\n",
      "...           ...          ...\n",
      "4091   678.117648   622.930307\n",
      "4092   643.999999   678.846742\n",
      "4093   608.000008   611.195038\n",
      "4094   547.999999   584.928437\n",
      "4095  1370.000005  1297.190260\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0968\n",
      "Epoch 2/25, Validation Loss: 0.0636\n",
      "           actual    predicted\n",
      "0      584.999999   613.630956\n",
      "1      742.000000   743.193749\n",
      "2     1669.999979  1724.669130\n",
      "3      973.999995   915.393640\n",
      "4      515.999993   553.357699\n",
      "...           ...          ...\n",
      "4091   678.117648   635.639761\n",
      "4092   643.999999   668.920832\n",
      "4093   608.000008   603.487175\n",
      "4094   547.999999   581.258560\n",
      "4095  1370.000005  1352.790262\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0662\n",
      "Epoch 3/25, Validation Loss: 0.0491\n",
      "           actual    predicted\n",
      "0      584.999999   622.962694\n",
      "1      742.000000   767.865994\n",
      "2     1669.999979  1772.817517\n",
      "3      973.999995   950.921267\n",
      "4      515.999993   555.088101\n",
      "...           ...          ...\n",
      "4091   678.117648   656.463038\n",
      "4092   643.999999   677.440562\n",
      "4093   608.000008   600.179337\n",
      "4094   547.999999   576.148933\n",
      "4095  1370.000005  1374.309449\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0563\n",
      "Epoch 4/25, Validation Loss: 0.0484\n",
      "           actual    predicted\n",
      "0      584.999999   618.101191\n",
      "1      742.000000   763.626276\n",
      "2     1669.999979  1684.498924\n",
      "3      973.999995   921.698884\n",
      "4      515.999993   556.357061\n",
      "...           ...          ...\n",
      "4091   678.117648   633.926482\n",
      "4092   643.999999   660.624631\n",
      "4093   608.000008   596.018788\n",
      "4094   547.999999   578.103408\n",
      "4095  1370.000005  1352.455241\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0512\n",
      "Epoch 5/25, Validation Loss: 0.0405\n",
      "           actual    predicted\n",
      "0      584.999999   611.271056\n",
      "1      742.000000   757.451525\n",
      "2     1669.999979  1715.634518\n",
      "3      973.999995   917.822733\n",
      "4      515.999993   556.441570\n",
      "...           ...          ...\n",
      "4091   678.117648   621.259502\n",
      "4092   643.999999   648.893949\n",
      "4093   608.000008   592.005978\n",
      "4094   547.999999   571.592942\n",
      "4095  1370.000005  1355.438708\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0477\n",
      "Epoch 6/25, Validation Loss: 0.0402\n",
      "           actual    predicted\n",
      "0      584.999999   617.116287\n",
      "1      742.000000   759.763503\n",
      "2     1669.999979  1678.604271\n",
      "3      973.999995   911.512006\n",
      "4      515.999993   559.821076\n",
      "...           ...          ...\n",
      "4091   678.117648   626.873924\n",
      "4092   643.999999   655.322984\n",
      "4093   608.000008   594.427194\n",
      "4094   547.999999   576.490708\n",
      "4095  1370.000005  1345.359735\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0455\n",
      "Epoch 7/25, Validation Loss: 0.0386\n",
      "           actual    predicted\n",
      "0      584.999999   625.081310\n",
      "1      742.000000   761.898247\n",
      "2     1669.999979  1693.662590\n",
      "3      973.999995   922.923369\n",
      "4      515.999993   561.734533\n",
      "...           ...          ...\n",
      "4091   678.117648   620.258001\n",
      "4092   643.999999   654.908550\n",
      "4093   608.000008   602.117987\n",
      "4094   547.999999   572.009648\n",
      "4095  1370.000005  1331.534411\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0441\n",
      "Epoch 8/25, Validation Loss: 0.0382\n",
      "           actual    predicted\n",
      "0      584.999999   615.089110\n",
      "1      742.000000   771.304417\n",
      "2     1669.999979  1679.911020\n",
      "3      973.999995   934.388522\n",
      "4      515.999993   553.190539\n",
      "...           ...          ...\n",
      "4091   678.117648   626.395446\n",
      "4092   643.999999   654.127578\n",
      "4093   608.000008   600.324654\n",
      "4094   547.999999   571.364010\n",
      "4095  1370.000005  1345.611632\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0428\n",
      "Epoch 9/25, Validation Loss: 0.0374\n",
      "           actual    predicted\n",
      "0      584.999999   621.579856\n",
      "1      742.000000   762.473186\n",
      "2     1669.999979  1741.348306\n",
      "3      973.999995   944.876894\n",
      "4      515.999993   553.510595\n",
      "...           ...          ...\n",
      "4091   678.117648   618.050893\n",
      "4092   643.999999   660.478252\n",
      "4093   608.000008   598.056720\n",
      "4094   547.999999   573.846613\n",
      "4095  1370.000005  1376.491329\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0415\n",
      "Epoch 10/25, Validation Loss: 0.0397\n",
      "           actual    predicted\n",
      "0      584.999999   616.488123\n",
      "1      742.000000   764.029442\n",
      "2     1669.999979  1692.973811\n",
      "3      973.999995   918.328220\n",
      "4      515.999993   557.271157\n",
      "...           ...          ...\n",
      "4091   678.117648   622.417109\n",
      "4092   643.999999   657.426907\n",
      "4093   608.000008   601.068591\n",
      "4094   547.999999   578.560096\n",
      "4095  1370.000005  1351.139018\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0410\n",
      "Epoch 11/25, Validation Loss: 0.0371\n",
      "           actual    predicted\n",
      "0      584.999999   628.357569\n",
      "1      742.000000   764.984234\n",
      "2     1669.999979  1659.529423\n",
      "3      973.999995   926.322437\n",
      "4      515.999993   558.315097\n",
      "...           ...          ...\n",
      "4091   678.117648   627.150312\n",
      "4092   643.999999   653.652213\n",
      "4093   608.000008   601.995196\n",
      "4094   547.999999   581.262560\n",
      "4095  1370.000005  1325.228386\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0405\n",
      "Epoch 12/25, Validation Loss: 0.0373\n",
      "           actual    predicted\n",
      "0      584.999999   631.291264\n",
      "1      742.000000   772.390010\n",
      "2     1669.999979  1691.827625\n",
      "3      973.999995   943.087175\n",
      "4      515.999993   556.625572\n",
      "...           ...          ...\n",
      "4091   678.117648   625.722878\n",
      "4092   643.999999   660.396628\n",
      "4093   608.000008   602.282830\n",
      "4094   547.999999   576.537340\n",
      "4095  1370.000005  1357.331112\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0403\n",
      "Epoch 13/25, Validation Loss: 0.0368\n",
      "           actual    predicted\n",
      "0      584.999999   610.914456\n",
      "1      742.000000   769.612574\n",
      "2     1669.999979  1711.433846\n",
      "3      973.999995   948.654614\n",
      "4      515.999993   549.266853\n",
      "...           ...          ...\n",
      "4091   678.117648   611.970501\n",
      "4092   643.999999   657.646076\n",
      "4093   608.000008   590.637386\n",
      "4094   547.999999   567.377007\n",
      "4095  1370.000005  1374.917981\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0393\n",
      "Epoch 14/25, Validation Loss: 0.0379\n",
      "           actual    predicted\n",
      "0      584.999999   624.988748\n",
      "1      742.000000   768.112999\n",
      "2     1669.999979  1707.181525\n",
      "3      973.999995   933.249108\n",
      "4      515.999993   550.366846\n",
      "...           ...          ...\n",
      "4091   678.117648   612.122064\n",
      "4092   643.999999   650.088302\n",
      "4093   608.000008   593.992453\n",
      "4094   547.999999   571.375291\n",
      "4095  1370.000005  1360.172718\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0391\n",
      "Epoch 15/25, Validation Loss: 0.0362\n",
      "           actual    predicted\n",
      "0      584.999999   620.966534\n",
      "1      742.000000   766.568502\n",
      "2     1669.999979  1707.037664\n",
      "3      973.999995   935.299101\n",
      "4      515.999993   555.419332\n",
      "...           ...          ...\n",
      "4091   678.117648   614.931793\n",
      "4092   643.999999   655.320229\n",
      "4093   608.000008   598.995308\n",
      "4094   547.999999   568.822460\n",
      "4095  1370.000005  1355.040073\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0390\n",
      "Epoch 16/25, Validation Loss: 0.0367\n",
      "           actual    predicted\n",
      "0      584.999999   620.950499\n",
      "1      742.000000   765.436107\n",
      "2     1669.999979  1721.046289\n",
      "3      973.999995   932.986351\n",
      "4      515.999993   557.128085\n",
      "...           ...          ...\n",
      "4091   678.117648   619.479924\n",
      "4092   643.999999   647.847685\n",
      "4093   608.000008   597.979404\n",
      "4094   547.999999   574.755183\n",
      "4095  1370.000005  1369.857161\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0386\n",
      "Epoch 17/25, Validation Loss: 0.0364\n",
      "           actual    predicted\n",
      "0      584.999999   619.885840\n",
      "1      742.000000   766.660165\n",
      "2     1669.999979  1699.559488\n",
      "3      973.999995   936.360120\n",
      "4      515.999993   555.553299\n",
      "...           ...          ...\n",
      "4091   678.117648   617.494238\n",
      "4092   643.999999   649.283689\n",
      "4093   608.000008   594.645267\n",
      "4094   547.999999   572.774567\n",
      "4095  1370.000005  1355.307760\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0386\n",
      "Epoch 18/25, Validation Loss: 0.0359\n",
      "           actual    predicted\n",
      "0      584.999999   630.856944\n",
      "1      742.000000   768.203693\n",
      "2     1669.999979  1684.304887\n",
      "3      973.999995   935.421558\n",
      "4      515.999993   560.032780\n",
      "...           ...          ...\n",
      "4091   678.117648   621.751208\n",
      "4092   643.999999   656.652049\n",
      "4093   608.000008   596.897727\n",
      "4094   547.999999   579.551983\n",
      "4095  1370.000005  1350.312063\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0384\n",
      "Epoch 19/25, Validation Loss: 0.0360\n",
      "           actual    predicted\n",
      "0      584.999999   633.296143\n",
      "1      742.000000   766.196796\n",
      "2     1669.999979  1699.957878\n",
      "3      973.999995   930.676171\n",
      "4      515.999993   558.354974\n",
      "...           ...          ...\n",
      "4091   678.117648   628.469465\n",
      "4092   643.999999   652.076883\n",
      "4093   608.000008   599.542173\n",
      "4094   547.999999   576.881975\n",
      "4095  1370.000005  1340.826920\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0378\n",
      "Epoch 20/25, Validation Loss: 0.0365\n",
      "           actual    predicted\n",
      "0      584.999999   627.785389\n",
      "1      742.000000   765.574871\n",
      "2     1669.999979  1678.551709\n",
      "3      973.999995   935.442795\n",
      "4      515.999993   558.928980\n",
      "...           ...          ...\n",
      "4091   678.117648   613.099740\n",
      "4092   643.999999   647.915957\n",
      "4093   608.000008   593.421957\n",
      "4094   547.999999   576.332829\n",
      "4095  1370.000005  1346.136936\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0378\n",
      "Epoch 21/25, Validation Loss: 0.0358\n",
      "           actual    predicted\n",
      "0      584.999999   623.359557\n",
      "1      742.000000   768.548369\n",
      "2     1669.999979  1681.622756\n",
      "3      973.999995   923.651087\n",
      "4      515.999993   559.468178\n",
      "...           ...          ...\n",
      "4091   678.117648   615.151742\n",
      "4092   643.999999   652.837583\n",
      "4093   608.000008   599.909071\n",
      "4094   547.999999   573.401538\n",
      "4095  1370.000005  1341.301205\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0378\n",
      "Epoch 22/25, Validation Loss: 0.0361\n",
      "           actual    predicted\n",
      "0      584.999999   621.599312\n",
      "1      742.000000   767.407040\n",
      "2     1669.999979  1681.376508\n",
      "3      973.999995   935.760974\n",
      "4      515.999993   557.792390\n",
      "...           ...          ...\n",
      "4091   678.117648   616.877688\n",
      "4092   643.999999   656.462635\n",
      "4093   608.000008   604.374447\n",
      "4094   547.999999   579.963706\n",
      "4095  1370.000005  1379.264303\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0374\n",
      "Epoch 23/25, Validation Loss: 0.0359\n",
      "           actual    predicted\n",
      "0      584.999999   621.327380\n",
      "1      742.000000   769.539045\n",
      "2     1669.999979  1699.837736\n",
      "3      973.999995   935.632771\n",
      "4      515.999993   553.886125\n",
      "...           ...          ...\n",
      "4091   678.117648   607.502634\n",
      "4092   643.999999   645.276274\n",
      "4093   608.000008   586.747227\n",
      "4094   547.999999   571.035990\n",
      "4095  1370.000005  1370.803486\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0377\n",
      "Epoch 24/25, Validation Loss: 0.0363\n",
      "           actual    predicted\n",
      "0      584.999999   622.399706\n",
      "1      742.000000   764.551750\n",
      "2     1669.999979  1731.409755\n",
      "3      973.999995   934.127274\n",
      "4      515.999993   551.402312\n",
      "...           ...          ...\n",
      "4091   678.117648   616.640721\n",
      "4092   643.999999   657.357942\n",
      "4093   608.000008   598.881763\n",
      "4094   547.999999   572.202843\n",
      "4095  1370.000005  1384.022243\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0373\n",
      "Epoch 25/25, Validation Loss: 0.0373\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[-0.5591]], device='cuda:0')\n",
      "[625.95]\n",
      "            actual    predicted\n",
      "0       584.999999   622.399706\n",
      "1       742.000000   764.551750\n",
      "2      1669.999979  1731.409755\n",
      "3       973.999995   934.127274\n",
      "4       515.999993   551.402312\n",
      "...            ...          ...\n",
      "79080   840.000002   831.978176\n",
      "79081   552.000002   568.021435\n",
      "79082   571.000007   570.414756\n",
      "79083   641.000005   630.441484\n",
      "79084   759.000000   731.200622\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 56.3847\n",
      "Score (MAE): 28.7625\n",
      "Score (ME): -5.3159\n",
      "Score (MAPE): 3.5592%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     41.920000  40.569228\n",
      "1     36.420000  34.653461\n",
      "2     34.990000  35.257856\n",
      "3     36.090000  35.793559\n",
      "4     53.120000  51.121710\n",
      "...         ...        ...\n",
      "4091  60.689999  61.832224\n",
      "4092  33.720000  34.212707\n",
      "4093  25.300000  24.363336\n",
      "4094  35.320000  34.308107\n",
      "4095  41.790000  41.136567\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2489\n",
      "Epoch 1/25, Validation Loss: 0.0255\n",
      "         actual  predicted\n",
      "0     41.920000  41.268766\n",
      "1     36.420000  35.435722\n",
      "2     34.990000  35.235334\n",
      "3     36.090000  36.296458\n",
      "4     53.120000  51.269915\n",
      "...         ...        ...\n",
      "4091  60.689999  60.977300\n",
      "4092  33.720000  34.084506\n",
      "4093  25.300000  24.771080\n",
      "4094  35.320000  34.830520\n",
      "4095  41.790000  41.480482\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0219\n",
      "Epoch 2/25, Validation Loss: 0.0147\n",
      "         actual  predicted\n",
      "0     41.920000  41.712702\n",
      "1     36.420000  35.731184\n",
      "2     34.990000  35.446676\n",
      "3     36.090000  36.218162\n",
      "4     53.120000  52.461447\n",
      "...         ...        ...\n",
      "4091  60.689999  61.544378\n",
      "4092  33.720000  33.921219\n",
      "4093  25.300000  25.034395\n",
      "4094  35.320000  35.402047\n",
      "4095  41.790000  42.113781\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0168\n",
      "Epoch 3/25, Validation Loss: 0.0122\n",
      "         actual  predicted\n",
      "0     41.920000  41.622890\n",
      "1     36.420000  35.871271\n",
      "2     34.990000  35.168235\n",
      "3     36.090000  36.186433\n",
      "4     53.120000  51.952669\n",
      "...         ...        ...\n",
      "4091  60.689999  60.156331\n",
      "4092  33.720000  33.618875\n",
      "4093  25.300000  25.089370\n",
      "4094  35.320000  35.198565\n",
      "4095  41.790000  42.097414\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0142\n",
      "Epoch 4/25, Validation Loss: 0.0096\n",
      "         actual  predicted\n",
      "0     41.920000  41.647124\n",
      "1     36.420000  36.044654\n",
      "2     34.990000  35.160437\n",
      "3     36.090000  36.194871\n",
      "4     53.120000  52.428782\n",
      "...         ...        ...\n",
      "4091  60.689999  60.809955\n",
      "4092  33.720000  33.559347\n",
      "4093  25.300000  25.313889\n",
      "4094  35.320000  35.321650\n",
      "4095  41.790000  42.395386\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0125\n",
      "Epoch 5/25, Validation Loss: 0.0083\n",
      "         actual  predicted\n",
      "0     41.920000  41.956941\n",
      "1     36.420000  36.091481\n",
      "2     34.990000  35.238119\n",
      "3     36.090000  36.121020\n",
      "4     53.120000  52.585159\n",
      "...         ...        ...\n",
      "4091  60.689999  60.670613\n",
      "4092  33.720000  33.401638\n",
      "4093  25.300000  25.285783\n",
      "4094  35.320000  35.284931\n",
      "4095  41.790000  42.367483\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0111\n",
      "Epoch 6/25, Validation Loss: 0.0075\n",
      "         actual  predicted\n",
      "0     41.920000  41.861367\n",
      "1     36.420000  35.961878\n",
      "2     34.990000  35.194436\n",
      "3     36.090000  36.195772\n",
      "4     53.120000  52.403713\n",
      "...         ...        ...\n",
      "4091  60.689999  60.149255\n",
      "4092  33.720000  33.326828\n",
      "4093  25.300000  25.124346\n",
      "4094  35.320000  35.314545\n",
      "4095  41.790000  42.193667\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0100\n",
      "Epoch 7/25, Validation Loss: 0.0065\n",
      "         actual  predicted\n",
      "0     41.920000  41.976644\n",
      "1     36.420000  36.124975\n",
      "2     34.990000  35.347913\n",
      "3     36.090000  36.275898\n",
      "4     53.120000  52.954954\n",
      "...         ...        ...\n",
      "4091  60.689999  60.824637\n",
      "4092  33.720000  33.389948\n",
      "4093  25.300000  25.077811\n",
      "4094  35.320000  35.205463\n",
      "4095  41.790000  42.385424\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0091\n",
      "Epoch 8/25, Validation Loss: 0.0065\n",
      "         actual  predicted\n",
      "0     41.920000  41.757523\n",
      "1     36.420000  35.999648\n",
      "2     34.990000  35.164742\n",
      "3     36.090000  36.176001\n",
      "4     53.120000  52.489637\n",
      "...         ...        ...\n",
      "4091  60.689999  60.342896\n",
      "4092  33.720000  33.335818\n",
      "4093  25.300000  25.409349\n",
      "4094  35.320000  35.175354\n",
      "4095  41.790000  42.256242\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0085\n",
      "Epoch 9/25, Validation Loss: 0.0055\n",
      "         actual  predicted\n",
      "0     41.920000  41.843278\n",
      "1     36.420000  35.947999\n",
      "2     34.990000  35.182769\n",
      "3     36.090000  36.197452\n",
      "4     53.120000  52.518444\n",
      "...         ...        ...\n",
      "4091  60.689999  60.135803\n",
      "4092  33.720000  33.243094\n",
      "4093  25.300000  25.096968\n",
      "4094  35.320000  35.266537\n",
      "4095  41.790000  42.228275\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0081\n",
      "Epoch 10/25, Validation Loss: 0.0051\n",
      "         actual  predicted\n",
      "0     41.920000  42.004377\n",
      "1     36.420000  36.128935\n",
      "2     34.990000  35.195680\n",
      "3     36.090000  36.263017\n",
      "4     53.120000  52.885650\n",
      "...         ...        ...\n",
      "4091  60.689999  60.638880\n",
      "4092  33.720000  33.271245\n",
      "4093  25.300000  25.082998\n",
      "4094  35.320000  35.277340\n",
      "4095  41.790000  42.385604\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0074\n",
      "Epoch 11/25, Validation Loss: 0.0051\n",
      "         actual  predicted\n",
      "0     41.920000  41.874696\n",
      "1     36.420000  36.096868\n",
      "2     34.990000  35.282635\n",
      "3     36.090000  36.194847\n",
      "4     53.120000  52.601861\n",
      "...         ...        ...\n",
      "4091  60.689999  60.340995\n",
      "4092  33.720000  33.284261\n",
      "4093  25.300000  25.455093\n",
      "4094  35.320000  35.266690\n",
      "4095  41.790000  42.165159\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0071\n",
      "Epoch 12/25, Validation Loss: 0.0046\n",
      "         actual  predicted\n",
      "0     41.920000  41.758057\n",
      "1     36.420000  36.080064\n",
      "2     34.990000  35.136979\n",
      "3     36.090000  36.227039\n",
      "4     53.120000  52.724033\n",
      "...         ...        ...\n",
      "4091  60.689999  60.298447\n",
      "4092  33.720000  33.373272\n",
      "4093  25.300000  25.197215\n",
      "4094  35.320000  35.201642\n",
      "4095  41.790000  42.177905\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0067\n",
      "Epoch 13/25, Validation Loss: 0.0043\n",
      "         actual  predicted\n",
      "0     41.920000  41.982660\n",
      "1     36.420000  36.099863\n",
      "2     34.990000  35.145195\n",
      "3     36.090000  36.218042\n",
      "4     53.120000  53.144359\n",
      "...         ...        ...\n",
      "4091  60.689999  60.677910\n",
      "4092  33.720000  33.232433\n",
      "4093  25.300000  25.233379\n",
      "4094  35.320000  35.281743\n",
      "4095  41.790000  42.256020\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0065\n",
      "Epoch 14/25, Validation Loss: 0.0044\n",
      "         actual  predicted\n",
      "0     41.920000  42.053783\n",
      "1     36.420000  36.041172\n",
      "2     34.990000  35.123814\n",
      "3     36.090000  36.202881\n",
      "4     53.120000  53.011529\n",
      "...         ...        ...\n",
      "4091  60.689999  60.650672\n",
      "4092  33.720000  33.166895\n",
      "4093  25.300000  25.092713\n",
      "4094  35.320000  35.226446\n",
      "4095  41.790000  42.268030\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0062\n",
      "Epoch 15/25, Validation Loss: 0.0041\n",
      "         actual  predicted\n",
      "0     41.920000  42.034583\n",
      "1     36.420000  36.077258\n",
      "2     34.990000  35.164566\n",
      "3     36.090000  36.179493\n",
      "4     53.120000  52.508678\n",
      "...         ...        ...\n",
      "4091  60.689999  59.840971\n",
      "4092  33.720000  33.084242\n",
      "4093  25.300000  25.348260\n",
      "4094  35.320000  35.267010\n",
      "4095  41.790000  42.238202\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0059\n",
      "Epoch 16/25, Validation Loss: 0.0038\n",
      "         actual  predicted\n",
      "0     41.920000  41.912754\n",
      "1     36.420000  36.001531\n",
      "2     34.990000  35.017238\n",
      "3     36.090000  36.051800\n",
      "4     53.120000  52.930152\n",
      "...         ...        ...\n",
      "4091  60.689999  60.483532\n",
      "4092  33.720000  33.062488\n",
      "4093  25.300000  25.077889\n",
      "4094  35.320000  35.094226\n",
      "4095  41.790000  42.091052\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0057\n",
      "Epoch 17/25, Validation Loss: 0.0037\n",
      "         actual  predicted\n",
      "0     41.920000  41.825505\n",
      "1     36.420000  36.107115\n",
      "2     34.990000  35.062549\n",
      "3     36.090000  36.277761\n",
      "4     53.120000  52.636955\n",
      "...         ...        ...\n",
      "4091  60.689999  60.036039\n",
      "4092  33.720000  33.309444\n",
      "4093  25.300000  25.339324\n",
      "4094  35.320000  35.148548\n",
      "4095  41.790000  42.079882\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0055\n",
      "Epoch 18/25, Validation Loss: 0.0036\n",
      "         actual  predicted\n",
      "0     41.920000  42.035230\n",
      "1     36.420000  36.078966\n",
      "2     34.990000  35.007484\n",
      "3     36.090000  36.264779\n",
      "4     53.120000  52.978336\n",
      "...         ...        ...\n",
      "4091  60.689999  60.524076\n",
      "4092  33.720000  33.153904\n",
      "4093  25.300000  25.272205\n",
      "4094  35.320000  35.270659\n",
      "4095  41.790000  42.353735\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0052\n",
      "Epoch 19/25, Validation Loss: 0.0034\n",
      "         actual  predicted\n",
      "0     41.920000  42.159310\n",
      "1     36.420000  36.169002\n",
      "2     34.990000  35.174239\n",
      "3     36.090000  36.207682\n",
      "4     53.120000  53.301895\n",
      "...         ...        ...\n",
      "4091  60.689999  60.865696\n",
      "4092  33.720000  33.149855\n",
      "4093  25.300000  25.180744\n",
      "4094  35.320000  35.267093\n",
      "4095  41.790000  42.411864\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0050\n",
      "Epoch 20/25, Validation Loss: 0.0038\n",
      "         actual  predicted\n",
      "0     41.920000  41.911385\n",
      "1     36.420000  36.102321\n",
      "2     34.990000  34.988781\n",
      "3     36.090000  36.129459\n",
      "4     53.120000  52.860159\n",
      "...         ...        ...\n",
      "4091  60.689999  60.263853\n",
      "4092  33.720000  33.103071\n",
      "4093  25.300000  24.989836\n",
      "4094  35.320000  35.156072\n",
      "4095  41.790000  42.168533\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0051\n",
      "Epoch 21/25, Validation Loss: 0.0032\n",
      "         actual  predicted\n",
      "0     41.920000  41.842398\n",
      "1     36.420000  36.041752\n",
      "2     34.990000  34.969588\n",
      "3     36.090000  36.114560\n",
      "4     53.120000  52.742276\n",
      "...         ...        ...\n",
      "4091  60.689999  60.125675\n",
      "4092  33.720000  33.140097\n",
      "4093  25.300000  25.313912\n",
      "4094  35.320000  35.296387\n",
      "4095  41.790000  42.119567\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0047\n",
      "Epoch 22/25, Validation Loss: 0.0031\n",
      "         actual  predicted\n",
      "0     41.920000  42.035515\n",
      "1     36.420000  36.176472\n",
      "2     34.990000  35.018954\n",
      "3     36.090000  36.151136\n",
      "4     53.120000  53.002047\n",
      "...         ...        ...\n",
      "4091  60.689999  60.399217\n",
      "4092  33.720000  33.062356\n",
      "4093  25.300000  25.047136\n",
      "4094  35.320000  35.217405\n",
      "4095  41.790000  42.136057\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0047\n",
      "Epoch 23/25, Validation Loss: 0.0029\n",
      "         actual  predicted\n",
      "0     41.920000  42.227419\n",
      "1     36.420000  36.211528\n",
      "2     34.990000  35.121644\n",
      "3     36.090000  36.107548\n",
      "4     53.120000  53.132496\n",
      "...         ...        ...\n",
      "4091  60.689999  60.447750\n",
      "4092  33.720000  33.057657\n",
      "4093  25.300000  25.141306\n",
      "4094  35.320000  35.124926\n",
      "4095  41.790000  42.407468\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0045\n",
      "Epoch 24/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     41.920000  42.091172\n",
      "1     36.420000  36.236425\n",
      "2     34.990000  35.230764\n",
      "3     36.090000  36.256684\n",
      "4     53.120000  52.794197\n",
      "...         ...        ...\n",
      "4091  60.689999  60.198785\n",
      "4092  33.720000  33.204368\n",
      "4093  25.300000  25.295900\n",
      "4094  35.320000  35.275216\n",
      "4095  41.790000  42.092456\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0044\n",
      "Epoch 25/25, Validation Loss: 0.0028\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[1.1324]], device='cuda:0')\n",
      "[46.4]\n",
      "       actual  predicted\n",
      "0       41.92  42.091172\n",
      "1       36.42  36.236425\n",
      "2       34.99  35.230764\n",
      "3       36.09  36.256684\n",
      "4       53.12  52.794197\n",
      "...       ...        ...\n",
      "79080   21.06  21.167783\n",
      "79081   48.19  48.327168\n",
      "79082   38.37  38.459043\n",
      "79083   44.97  45.182688\n",
      "79084   40.48  40.732440\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 0.5124\n",
      "Score (MAE): 0.2741\n",
      "Score (ME): -0.0247\n",
      "Score (MAPE): 0.7679%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     25.990000  25.733705\n",
      "1     26.870000  27.039992\n",
      "2     21.800000  21.873361\n",
      "3     23.320000  23.267532\n",
      "4     18.590000  18.084455\n",
      "...         ...        ...\n",
      "4091  20.910000  21.361675\n",
      "4092  23.000000  23.063573\n",
      "4093  22.950000  23.181920\n",
      "4094  24.506667  23.799655\n",
      "4095  23.570000  23.694759\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2644\n",
      "Epoch 1/25, Validation Loss: 0.0251\n",
      "         actual  predicted\n",
      "0     25.990000  25.839211\n",
      "1     26.870000  27.031382\n",
      "2     21.800000  21.740520\n",
      "3     23.320000  23.295955\n",
      "4     18.590000  18.266493\n",
      "...         ...        ...\n",
      "4091  20.910000  21.535108\n",
      "4092  23.000000  23.018575\n",
      "4093  22.950000  23.074475\n",
      "4094  24.506667  24.091455\n",
      "4095  23.570000  23.459508\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0204\n",
      "Epoch 2/25, Validation Loss: 0.0132\n",
      "         actual  predicted\n",
      "0     25.990000  25.790690\n",
      "1     26.870000  26.962003\n",
      "2     21.800000  21.760482\n",
      "3     23.320000  23.261409\n",
      "4     18.590000  18.297059\n",
      "...         ...        ...\n",
      "4091  20.910000  21.535215\n",
      "4092  23.000000  22.990805\n",
      "4093  22.950000  22.999147\n",
      "4094  24.506667  24.226379\n",
      "4095  23.570000  23.357469\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0150\n",
      "Epoch 3/25, Validation Loss: 0.0099\n",
      "         actual  predicted\n",
      "0     25.990000  25.816104\n",
      "1     26.870000  27.012725\n",
      "2     21.800000  21.669397\n",
      "3     23.320000  23.247071\n",
      "4     18.590000  18.336071\n",
      "...         ...        ...\n",
      "4091  20.910000  21.511177\n",
      "4092  23.000000  23.019301\n",
      "4093  22.950000  22.935233\n",
      "4094  24.506667  24.304846\n",
      "4095  23.570000  23.289173\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0126\n",
      "Epoch 4/25, Validation Loss: 0.0082\n",
      "         actual  predicted\n",
      "0     25.990000  25.758799\n",
      "1     26.870000  26.891584\n",
      "2     21.800000  21.671054\n",
      "3     23.320000  23.297575\n",
      "4     18.590000  18.408779\n",
      "...         ...        ...\n",
      "4091  20.910000  21.508541\n",
      "4092  23.000000  23.021380\n",
      "4093  22.950000  22.978637\n",
      "4094  24.506667  24.275462\n",
      "4095  23.570000  23.287286\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0107\n",
      "Epoch 5/25, Validation Loss: 0.0068\n",
      "         actual  predicted\n",
      "0     25.990000  25.770758\n",
      "1     26.870000  26.918765\n",
      "2     21.800000  21.703732\n",
      "3     23.320000  23.288850\n",
      "4     18.590000  18.544236\n",
      "...         ...        ...\n",
      "4091  20.910000  21.523739\n",
      "4092  23.000000  22.998230\n",
      "4093  22.950000  22.925575\n",
      "4094  24.506667  24.312181\n",
      "4095  23.570000  23.222973\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0095\n",
      "Epoch 6/25, Validation Loss: 0.0060\n",
      "         actual  predicted\n",
      "0     25.990000  25.826896\n",
      "1     26.870000  26.991277\n",
      "2     21.800000  21.728702\n",
      "3     23.320000  23.380841\n",
      "4     18.590000  18.495500\n",
      "...         ...        ...\n",
      "4091  20.910000  21.386608\n",
      "4092  23.000000  23.051805\n",
      "4093  22.950000  23.049452\n",
      "4094  24.506667  24.340399\n",
      "4095  23.570000  23.205929\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0085\n",
      "Epoch 7/25, Validation Loss: 0.0057\n",
      "         actual  predicted\n",
      "0     25.990000  25.802284\n",
      "1     26.870000  26.888715\n",
      "2     21.800000  21.776698\n",
      "3     23.320000  23.371841\n",
      "4     18.590000  18.632967\n",
      "...         ...        ...\n",
      "4091  20.910000  21.454954\n",
      "4092  23.000000  23.037545\n",
      "4093  22.950000  22.950384\n",
      "4094  24.506667  24.299176\n",
      "4095  23.570000  23.230336\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0078\n",
      "Epoch 8/25, Validation Loss: 0.0049\n",
      "         actual  predicted\n",
      "0     25.990000  25.847167\n",
      "1     26.870000  27.015083\n",
      "2     21.800000  21.680989\n",
      "3     23.320000  23.302801\n",
      "4     18.590000  18.459066\n",
      "...         ...        ...\n",
      "4091  20.910000  21.298430\n",
      "4092  23.000000  22.955801\n",
      "4093  22.950000  22.949149\n",
      "4094  24.506667  24.277262\n",
      "4095  23.570000  23.212028\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0071\n",
      "Epoch 9/25, Validation Loss: 0.0045\n",
      "         actual  predicted\n",
      "0     25.990000  25.832780\n",
      "1     26.870000  26.939120\n",
      "2     21.800000  21.779207\n",
      "3     23.320000  23.335650\n",
      "4     18.590000  18.590482\n",
      "...         ...        ...\n",
      "4091  20.910000  21.347620\n",
      "4092  23.000000  23.015980\n",
      "4093  22.950000  22.961899\n",
      "4094  24.506667  24.367663\n",
      "4095  23.570000  23.209531\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0066\n",
      "Epoch 10/25, Validation Loss: 0.0040\n",
      "         actual  predicted\n",
      "0     25.990000  25.957346\n",
      "1     26.870000  27.064526\n",
      "2     21.800000  21.698873\n",
      "3     23.320000  23.331923\n",
      "4     18.590000  18.468462\n",
      "...         ...        ...\n",
      "4091  20.910000  21.290868\n",
      "4092  23.000000  22.954481\n",
      "4093  22.950000  22.893126\n",
      "4094  24.506667  24.357967\n",
      "4095  23.570000  23.297501\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0062\n",
      "Epoch 11/25, Validation Loss: 0.0049\n",
      "         actual  predicted\n",
      "0     25.990000  25.924635\n",
      "1     26.870000  26.937181\n",
      "2     21.800000  21.752455\n",
      "3     23.320000  23.374149\n",
      "4     18.590000  18.520736\n",
      "...         ...        ...\n",
      "4091  20.910000  21.303300\n",
      "4092  23.000000  23.033977\n",
      "4093  22.950000  22.923863\n",
      "4094  24.506667  24.357911\n",
      "4095  23.570000  23.281976\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0057\n",
      "Epoch 12/25, Validation Loss: 0.0035\n",
      "         actual  predicted\n",
      "0     25.990000  25.851086\n",
      "1     26.870000  26.864742\n",
      "2     21.800000  21.745127\n",
      "3     23.320000  23.368582\n",
      "4     18.590000  18.566973\n",
      "...         ...        ...\n",
      "4091  20.910000  21.329345\n",
      "4092  23.000000  23.036727\n",
      "4093  22.950000  22.927983\n",
      "4094  24.506667  24.345833\n",
      "4095  23.570000  23.254131\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0054\n",
      "Epoch 13/25, Validation Loss: 0.0033\n",
      "         actual  predicted\n",
      "0     25.990000  25.844196\n",
      "1     26.870000  26.867600\n",
      "2     21.800000  21.766845\n",
      "3     23.320000  23.380628\n",
      "4     18.590000  18.575463\n",
      "...         ...        ...\n",
      "4091  20.910000  21.285987\n",
      "4092  23.000000  23.052551\n",
      "4093  22.950000  22.944102\n",
      "4094  24.506667  24.398429\n",
      "4095  23.570000  23.244470\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0053\n",
      "Epoch 14/25, Validation Loss: 0.0032\n",
      "         actual  predicted\n",
      "0     25.990000  25.891919\n",
      "1     26.870000  26.939242\n",
      "2     21.800000  21.699920\n",
      "3     23.320000  23.330158\n",
      "4     18.590000  18.420759\n",
      "...         ...        ...\n",
      "4091  20.910000  21.151537\n",
      "4092  23.000000  22.984496\n",
      "4093  22.950000  22.899051\n",
      "4094  24.506667  24.270444\n",
      "4095  23.570000  23.219509\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0049\n",
      "Epoch 15/25, Validation Loss: 0.0032\n",
      "         actual  predicted\n",
      "0     25.990000  25.924666\n",
      "1     26.870000  27.058815\n",
      "2     21.800000  21.719994\n",
      "3     23.320000  23.351412\n",
      "4     18.590000  18.456185\n",
      "...         ...        ...\n",
      "4091  20.910000  21.166964\n",
      "4092  23.000000  22.970363\n",
      "4093  22.950000  22.923872\n",
      "4094  24.506667  24.366454\n",
      "4095  23.570000  23.249406\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0048\n",
      "Epoch 16/25, Validation Loss: 0.0036\n",
      "         actual  predicted\n",
      "0     25.990000  25.860129\n",
      "1     26.870000  26.851558\n",
      "2     21.800000  21.750525\n",
      "3     23.320000  23.345007\n",
      "4     18.590000  18.544248\n",
      "...         ...        ...\n",
      "4091  20.910000  21.199698\n",
      "4092  23.000000  23.008822\n",
      "4093  22.950000  22.912510\n",
      "4094  24.506667  24.339770\n",
      "4095  23.570000  23.208502\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0046\n",
      "Epoch 17/25, Validation Loss: 0.0027\n",
      "         actual  predicted\n",
      "0     25.990000  25.987111\n",
      "1     26.870000  26.997647\n",
      "2     21.800000  21.724804\n",
      "3     23.320000  23.287330\n",
      "4     18.590000  18.487442\n",
      "...         ...        ...\n",
      "4091  20.910000  21.114333\n",
      "4092  23.000000  22.919268\n",
      "4093  22.950000  22.923277\n",
      "4094  24.506667  24.409268\n",
      "4095  23.570000  23.247555\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0043\n",
      "Epoch 18/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     25.990000  25.976782\n",
      "1     26.870000  26.953802\n",
      "2     21.800000  21.710171\n",
      "3     23.320000  23.317414\n",
      "4     18.590000  18.570064\n",
      "...         ...        ...\n",
      "4091  20.910000  21.191107\n",
      "4092  23.000000  22.964604\n",
      "4093  22.950000  22.885294\n",
      "4094  24.506667  24.349787\n",
      "4095  23.570000  23.298930\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0042\n",
      "Epoch 19/25, Validation Loss: 0.0029\n",
      "         actual  predicted\n",
      "0     25.990000  25.922726\n",
      "1     26.870000  26.881639\n",
      "2     21.800000  21.810617\n",
      "3     23.320000  23.328863\n",
      "4     18.590000  18.697067\n",
      "...         ...        ...\n",
      "4091  20.910000  21.205796\n",
      "4092  23.000000  23.040443\n",
      "4093  22.950000  22.966839\n",
      "4094  24.506667  24.450586\n",
      "4095  23.570000  23.319442\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0040\n",
      "Epoch 20/25, Validation Loss: 0.0023\n",
      "         actual  predicted\n",
      "0     25.990000  26.013506\n",
      "1     26.870000  26.946947\n",
      "2     21.800000  21.791778\n",
      "3     23.320000  23.338369\n",
      "4     18.590000  18.628657\n",
      "...         ...        ...\n",
      "4091  20.910000  21.240239\n",
      "4092  23.000000  22.990971\n",
      "4093  22.950000  22.891630\n",
      "4094  24.506667  24.433067\n",
      "4095  23.570000  23.376311\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0038\n",
      "Epoch 21/25, Validation Loss: 0.0023\n",
      "         actual  predicted\n",
      "0     25.990000  25.912734\n",
      "1     26.870000  26.883907\n",
      "2     21.800000  21.793360\n",
      "3     23.320000  23.364196\n",
      "4     18.590000  18.647914\n",
      "...         ...        ...\n",
      "4091  20.910000  21.234349\n",
      "4092  23.000000  22.992176\n",
      "4093  22.950000  22.938809\n",
      "4094  24.506667  24.356133\n",
      "4095  23.570000  23.303742\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0037\n",
      "Epoch 22/25, Validation Loss: 0.0021\n",
      "         actual  predicted\n",
      "0     25.990000  25.941045\n",
      "1     26.870000  26.908054\n",
      "2     21.800000  21.706729\n",
      "3     23.320000  23.324539\n",
      "4     18.590000  18.552604\n",
      "...         ...        ...\n",
      "4091  20.910000  21.155422\n",
      "4092  23.000000  22.982608\n",
      "4093  22.950000  22.892554\n",
      "4094  24.506667  24.297977\n",
      "4095  23.570000  23.322118\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0037\n",
      "Epoch 23/25, Validation Loss: 0.0021\n",
      "         actual  predicted\n",
      "0     25.990000  25.987356\n",
      "1     26.870000  26.897838\n",
      "2     21.800000  21.788298\n",
      "3     23.320000  23.341550\n",
      "4     18.590000  18.717136\n",
      "...         ...        ...\n",
      "4091  20.910000  21.260257\n",
      "4092  23.000000  23.014217\n",
      "4093  22.950000  22.924418\n",
      "4094  24.506667  24.438883\n",
      "4095  23.570000  23.355911\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0034\n",
      "Epoch 24/25, Validation Loss: 0.0021\n",
      "         actual  predicted\n",
      "0     25.990000  26.026943\n",
      "1     26.870000  26.913672\n",
      "2     21.800000  21.750539\n",
      "3     23.320000  23.318877\n",
      "4     18.590000  18.566325\n",
      "...         ...        ...\n",
      "4091  20.910000  21.094162\n",
      "4092  23.000000  22.949132\n",
      "4093  22.950000  22.916322\n",
      "4094  24.506667  24.392696\n",
      "4095  23.570000  23.322281\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0034\n",
      "Epoch 25/25, Validation Loss: 0.0022\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[0.3879]], device='cuda:0')\n",
      "[25.46]\n",
      "       actual  predicted\n",
      "0       25.99  26.026943\n",
      "1       26.87  26.913672\n",
      "2       21.80  21.750539\n",
      "3       23.32  23.318877\n",
      "4       18.59  18.566325\n",
      "...       ...        ...\n",
      "79080   21.66  21.558969\n",
      "79081   20.83  20.818258\n",
      "79082   22.20  22.867217\n",
      "79083   22.53  22.554394\n",
      "79084   20.76  20.491201\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 0.1668\n",
      "Score (MAE): 0.0951\n",
      "Score (ME): -0.0246\n",
      "Score (MAPE): 0.3836%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      117.999997   251.882963\n",
      "1        8.000005     9.016892\n",
      "2      211.000001   190.374520\n",
      "3     1663.000027  2197.752416\n",
      "4        5.999995    -3.960614\n",
      "...           ...          ...\n",
      "4091   207.000000    73.434123\n",
      "4092     8.000005   -24.912315\n",
      "4093    67.000003    58.574233\n",
      "4094   313.999996   165.764203\n",
      "4095     5.999995   -25.844065\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.7729\n",
      "Epoch 1/25, Validation Loss: 0.5010\n",
      "           actual    predicted\n",
      "0      117.999997   212.260165\n",
      "1        8.000005    31.770153\n",
      "2      211.000001   277.541104\n",
      "3     1663.000027  1788.438850\n",
      "4        5.999995    25.828524\n",
      "...           ...          ...\n",
      "4091   207.000000   156.721054\n",
      "4092     8.000005   -54.600335\n",
      "4093    67.000003    85.285678\n",
      "4094   313.999996   212.989375\n",
      "4095     5.999995    -2.377966\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.4825\n",
      "Epoch 2/25, Validation Loss: 0.4209\n",
      "           actual    predicted\n",
      "0      117.999997   196.556705\n",
      "1        8.000005    32.084480\n",
      "2      211.000001   254.930639\n",
      "3     1663.000027  1839.002006\n",
      "4        5.999995     2.255008\n",
      "...           ...          ...\n",
      "4091   207.000000   178.662665\n",
      "4092     8.000005   -20.787543\n",
      "4093    67.000003    86.303272\n",
      "4094   313.999996   227.860198\n",
      "4095     5.999995   -24.945599\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.4303\n",
      "Epoch 3/25, Validation Loss: 0.3990\n",
      "           actual    predicted\n",
      "0      117.999997   194.287838\n",
      "1        8.000005    35.201318\n",
      "2      211.000001   261.193857\n",
      "3     1663.000027  1860.128398\n",
      "4        5.999995   -10.105041\n",
      "...           ...          ...\n",
      "4091   207.000000   140.764698\n",
      "4092     8.000005   -43.609356\n",
      "4093    67.000003    99.541402\n",
      "4094   313.999996   224.080111\n",
      "4095     5.999995   -18.131996\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.4116\n",
      "Epoch 4/25, Validation Loss: 0.4037\n",
      "           actual    predicted\n",
      "0      117.999997   184.020634\n",
      "1        8.000005    24.250762\n",
      "2      211.000001   250.660465\n",
      "3     1663.000027  1738.637572\n",
      "4        5.999995   -28.517162\n",
      "...           ...          ...\n",
      "4091   207.000000   164.834075\n",
      "4092     8.000005   -45.516630\n",
      "4093    67.000003    60.935256\n",
      "4094   313.999996   221.544555\n",
      "4095     5.999995    -9.086282\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.3951\n",
      "Epoch 5/25, Validation Loss: 0.3885\n",
      "           actual    predicted\n",
      "0      117.999997   209.695784\n",
      "1        8.000005    75.922757\n",
      "2      211.000001   268.987522\n",
      "3     1663.000027  1520.144242\n",
      "4        5.999995    37.885453\n",
      "...           ...          ...\n",
      "4091   207.000000   154.209803\n",
      "4092     8.000005    28.264789\n",
      "4093    67.000003   102.874581\n",
      "4094   313.999996   251.055301\n",
      "4095     5.999995    35.914929\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.3812\n",
      "Epoch 6/25, Validation Loss: 0.3773\n",
      "           actual    predicted\n",
      "0      117.999997   174.529091\n",
      "1        8.000005    45.143022\n",
      "2      211.000001   236.768400\n",
      "3     1663.000027  1951.884808\n",
      "4        5.999995    -0.296753\n",
      "...           ...          ...\n",
      "4091   207.000000   134.729545\n",
      "4092     8.000005   -15.223334\n",
      "4093    67.000003    48.001389\n",
      "4094   313.999996   248.017139\n",
      "4095     5.999995    14.489027\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.3699\n",
      "Epoch 7/25, Validation Loss: 0.3422\n",
      "           actual    predicted\n",
      "0      117.999997   228.204670\n",
      "1        8.000005    32.697477\n",
      "2      211.000001   268.471833\n",
      "3     1663.000027  1941.232317\n",
      "4        5.999995    16.285264\n",
      "...           ...          ...\n",
      "4091   207.000000   149.189831\n",
      "4092     8.000005     1.222678\n",
      "4093    67.000003    79.453101\n",
      "4094   313.999996   244.981113\n",
      "4095     5.999995   -16.395782\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.3665\n",
      "Epoch 8/25, Validation Loss: 0.3177\n",
      "           actual    predicted\n",
      "0      117.999997   192.231764\n",
      "1        8.000005    36.692564\n",
      "2      211.000001   279.736021\n",
      "3     1663.000027  1740.032048\n",
      "4        5.999995   -10.656681\n",
      "...           ...          ...\n",
      "4091   207.000000   181.733629\n",
      "4092     8.000005   -31.251447\n",
      "4093    67.000003    79.564967\n",
      "4094   313.999996   258.222717\n",
      "4095     5.999995   -17.689487\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.3236\n",
      "Epoch 9/25, Validation Loss: 0.3273\n",
      "           actual    predicted\n",
      "0      117.999997   192.607963\n",
      "1        8.000005    -2.896177\n",
      "2      211.000001   262.247267\n",
      "3     1663.000027  2320.271572\n",
      "4        5.999995   -49.058853\n",
      "...           ...          ...\n",
      "4091   207.000000   108.452597\n",
      "4092     8.000005   -63.466966\n",
      "4093    67.000003    22.108999\n",
      "4094   313.999996   258.112129\n",
      "4095     5.999995   -38.886414\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.3038\n",
      "Epoch 10/25, Validation Loss: 0.3348\n",
      "           actual    predicted\n",
      "0      117.999997   242.043167\n",
      "1        8.000005    42.792231\n",
      "2      211.000001   262.559425\n",
      "3     1663.000027  1801.109090\n",
      "4        5.999995     7.864773\n",
      "...           ...          ...\n",
      "4091   207.000000   136.123988\n",
      "4092     8.000005    -8.266397\n",
      "4093    67.000003    73.023733\n",
      "4094   313.999996   265.906954\n",
      "4095     5.999995     4.846105\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.2996\n",
      "Epoch 11/25, Validation Loss: 0.3020\n",
      "           actual    predicted\n",
      "0      117.999997   237.238539\n",
      "1        8.000005    37.293379\n",
      "2      211.000001   259.482432\n",
      "3     1663.000027  1715.056100\n",
      "4        5.999995    -0.857695\n",
      "...           ...          ...\n",
      "4091   207.000000   144.150316\n",
      "4092     8.000005     7.626495\n",
      "4093    67.000003    49.275728\n",
      "4094   313.999996   260.659710\n",
      "4095     5.999995     6.806633\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.3100\n",
      "Epoch 12/25, Validation Loss: 0.2861\n",
      "           actual    predicted\n",
      "0      117.999997   163.816246\n",
      "1        8.000005    32.730133\n",
      "2      211.000001   249.566492\n",
      "3     1663.000027  1589.740103\n",
      "4        5.999995    47.428837\n",
      "...           ...          ...\n",
      "4091   207.000000   143.285632\n",
      "4092     8.000005    29.143867\n",
      "4093    67.000003    75.648080\n",
      "4094   313.999996   234.165037\n",
      "4095     5.999995     7.989056\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.2825\n",
      "Epoch 13/25, Validation Loss: 0.2836\n",
      "           actual    predicted\n",
      "0      117.999997   231.297459\n",
      "1        8.000005    55.985770\n",
      "2      211.000001   263.527401\n",
      "3     1663.000027  1800.410866\n",
      "4        5.999995     9.605705\n",
      "...           ...          ...\n",
      "4091   207.000000   172.990793\n",
      "4092     8.000005   -43.525776\n",
      "4093    67.000003    88.006751\n",
      "4094   313.999996   281.824978\n",
      "4095     5.999995    -3.685881\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.2708\n",
      "Epoch 14/25, Validation Loss: 0.2746\n",
      "           actual    predicted\n",
      "0      117.999997   144.564001\n",
      "1        8.000005     8.047151\n",
      "2      211.000001   244.239087\n",
      "3     1663.000027  1840.736808\n",
      "4        5.999995    -4.397520\n",
      "...           ...          ...\n",
      "4091   207.000000   114.904076\n",
      "4092     8.000005   -30.389089\n",
      "4093    67.000003    64.935768\n",
      "4094   313.999996   232.365740\n",
      "4095     5.999995   -29.217426\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.2703\n",
      "Epoch 15/25, Validation Loss: 0.2710\n",
      "           actual    predicted\n",
      "0      117.999997   200.259554\n",
      "1        8.000005    35.061761\n",
      "2      211.000001   269.726213\n",
      "3     1663.000027  2129.158257\n",
      "4        5.999995     4.301862\n",
      "...           ...          ...\n",
      "4091   207.000000   132.444868\n",
      "4092     8.000005   -28.717695\n",
      "4093    67.000003    56.748601\n",
      "4094   313.999996   284.327474\n",
      "4095     5.999995    -6.126203\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.2473\n",
      "Epoch 16/25, Validation Loss: 0.2829\n",
      "           actual    predicted\n",
      "0      117.999997   194.364511\n",
      "1        8.000005    28.710996\n",
      "2      211.000001   261.752271\n",
      "3     1663.000027  1554.794916\n",
      "4        5.999995    13.866067\n",
      "...           ...          ...\n",
      "4091   207.000000   186.107952\n",
      "4092     8.000005   -26.212094\n",
      "4093    67.000003    74.797444\n",
      "4094   313.999996   260.019828\n",
      "4095     5.999995   -10.282937\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.2448\n",
      "Epoch 17/25, Validation Loss: 0.2721\n",
      "           actual    predicted\n",
      "0      117.999997   162.699882\n",
      "1        8.000005    15.801122\n",
      "2      211.000001   242.736012\n",
      "3     1663.000027  2099.297609\n",
      "4        5.999995   -21.893379\n",
      "...           ...          ...\n",
      "4091   207.000000   116.615383\n",
      "4092     8.000005   -55.053110\n",
      "4093    67.000003    31.201838\n",
      "4094   313.999996   250.091769\n",
      "4095     5.999995   -25.979487\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.2663\n",
      "Epoch 18/25, Validation Loss: 0.2677\n",
      "           actual    predicted\n",
      "0      117.999997   209.080238\n",
      "1        8.000005    40.231426\n",
      "2      211.000001   242.774804\n",
      "3     1663.000027  1783.486013\n",
      "4        5.999995    33.951375\n",
      "...           ...          ...\n",
      "4091   207.000000   158.045093\n",
      "4092     8.000005    -6.447702\n",
      "4093    67.000003    41.436127\n",
      "4094   313.999996   252.040793\n",
      "4095     5.999995    11.342065\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.2499\n",
      "Epoch 19/25, Validation Loss: 0.2658\n",
      "           actual    predicted\n",
      "0      117.999997   190.047141\n",
      "1        8.000005    44.290190\n",
      "2      211.000001   269.438055\n",
      "3     1663.000027  1738.888603\n",
      "4        5.999995    16.606695\n",
      "...           ...          ...\n",
      "4091   207.000000   173.497909\n",
      "4092     8.000005   -11.682859\n",
      "4093    67.000003    70.237257\n",
      "4094   313.999996   280.721529\n",
      "4095     5.999995     6.065151\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.2297\n",
      "Epoch 20/25, Validation Loss: 0.2468\n",
      "           actual    predicted\n",
      "0      117.999997   166.130873\n",
      "1        8.000005    18.904994\n",
      "2      211.000001   258.183034\n",
      "3     1663.000027  1807.822191\n",
      "4        5.999995     7.756023\n",
      "...           ...          ...\n",
      "4091   207.000000   139.469869\n",
      "4092     8.000005   -14.654615\n",
      "4093    67.000003    59.991134\n",
      "4094   313.999996   264.033626\n",
      "4095     5.999995    -3.786933\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.2294\n",
      "Epoch 21/25, Validation Loss: 0.2510\n",
      "           actual    predicted\n",
      "0      117.999997   190.128076\n",
      "1        8.000005    28.136472\n",
      "2      211.000001   275.486966\n",
      "3     1663.000027  1856.907673\n",
      "4        5.999995    24.172741\n",
      "...           ...          ...\n",
      "4091   207.000000   146.750810\n",
      "4092     8.000005    -4.358834\n",
      "4093    67.000003    84.121994\n",
      "4094   313.999996   264.912116\n",
      "4095     5.999995     2.276514\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.2211\n",
      "Epoch 22/25, Validation Loss: 0.2484\n",
      "           actual    predicted\n",
      "0      117.999997   161.650820\n",
      "1        8.000005    33.176598\n",
      "2      211.000001   249.788991\n",
      "3     1663.000027  1865.416361\n",
      "4        5.999995    11.430407\n",
      "...           ...          ...\n",
      "4091   207.000000   135.888596\n",
      "4092     8.000005    -4.111849\n",
      "4093    67.000003    66.198464\n",
      "4094   313.999996   257.691154\n",
      "4095     5.999995     6.837172\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.2141\n",
      "Epoch 23/25, Validation Loss: 0.2689\n",
      "           actual    predicted\n",
      "0      117.999997   197.645444\n",
      "1        8.000005    44.535158\n",
      "2      211.000001   260.219381\n",
      "3     1663.000027  1871.974898\n",
      "4        5.999995    12.137866\n",
      "...           ...          ...\n",
      "4091   207.000000   150.158474\n",
      "4092     8.000005    -3.050908\n",
      "4093    67.000003    75.633287\n",
      "4094   313.999996   306.290874\n",
      "4095     5.999995    14.356933\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.2084\n",
      "Epoch 24/25, Validation Loss: 0.2335\n",
      "           actual    predicted\n",
      "0      117.999997   206.661870\n",
      "1        8.000005    36.268871\n",
      "2      211.000001   295.963815\n",
      "3     1663.000027  2188.194957\n",
      "4        5.999995    29.176311\n",
      "...           ...          ...\n",
      "4091   207.000000   183.731173\n",
      "4092     8.000005     3.796158\n",
      "4093    67.000003    87.309491\n",
      "4094   313.999996   314.130319\n",
      "4095     5.999995     3.212063\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.2356\n",
      "Epoch 25/25, Validation Loss: 0.2748\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[-0.1328]], device='cuda:0')\n",
      "[80.19]\n",
      "            actual    predicted\n",
      "0       117.999997   206.661870\n",
      "1         8.000005    36.268871\n",
      "2       211.000001   295.963815\n",
      "3      1663.000027  2188.194957\n",
      "4         5.999995    29.176311\n",
      "...            ...          ...\n",
      "79080     7.000006    53.876865\n",
      "79081    27.999996   103.956932\n",
      "79082     8.000005    18.848793\n",
      "79083     7.000006    21.787265\n",
      "79084   439.999995   415.311724\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 398.3645\n",
      "Score (MAE): 76.2336\n",
      "Score (ME): -43.7718\n",
      "Score (MAPE): 819937.3254%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     415.000003  402.597802\n",
      "1     451.999999  473.718594\n",
      "2     387.000001  397.388600\n",
      "3     433.499999  477.430712\n",
      "4     484.000000  424.955109\n",
      "...          ...         ...\n",
      "4091  824.499995  578.856397\n",
      "4092  445.999999  423.990638\n",
      "4093  473.000001  530.831493\n",
      "4094  457.000002  451.766391\n",
      "4095  433.499999  422.908597\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.6189\n",
      "Epoch 1/25, Validation Loss: 0.3281\n",
      "          actual   predicted\n",
      "0     415.000003  407.582233\n",
      "1     451.999999  457.793184\n",
      "2     387.000001  403.574088\n",
      "3     433.499999  454.898750\n",
      "4     484.000000  445.191474\n",
      "...          ...         ...\n",
      "4091  824.499995  708.806299\n",
      "4092  445.999999  464.052671\n",
      "4093  473.000001  478.706703\n",
      "4094  457.000002  448.044725\n",
      "4095  433.499999  427.806464\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2252\n",
      "Epoch 2/25, Validation Loss: 0.1328\n",
      "          actual   predicted\n",
      "0     415.000003  402.819550\n",
      "1     451.999999  449.585214\n",
      "2     387.000001  397.404258\n",
      "3     433.499999  446.390470\n",
      "4     484.000000  438.774376\n",
      "...          ...         ...\n",
      "4091  824.499995  726.151513\n",
      "4092  445.999999  452.278894\n",
      "4093  473.000001  482.315753\n",
      "4094  457.000002  459.709114\n",
      "4095  433.499999  429.762772\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1252\n",
      "Epoch 3/25, Validation Loss: 0.0962\n",
      "          actual   predicted\n",
      "0     415.000003  410.965618\n",
      "1     451.999999  455.029208\n",
      "2     387.000001  398.672001\n",
      "3     433.499999  445.785208\n",
      "4     484.000000  450.765178\n",
      "...          ...         ...\n",
      "4091  824.499995  769.725851\n",
      "4092  445.999999  449.501170\n",
      "4093  473.000001  492.293469\n",
      "4094  457.000002  464.725220\n",
      "4095  433.499999  429.430534\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1044\n",
      "Epoch 4/25, Validation Loss: 0.0805\n",
      "          actual   predicted\n",
      "0     415.000003  407.995323\n",
      "1     451.999999  456.165512\n",
      "2     387.000001  396.569332\n",
      "3     433.499999  447.140804\n",
      "4     484.000000  446.778719\n",
      "...          ...         ...\n",
      "4091  824.499995  772.150437\n",
      "4092  445.999999  448.529160\n",
      "4093  473.000001  482.707404\n",
      "4094  457.000002  460.332985\n",
      "4095  433.499999  429.162462\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0937\n",
      "Epoch 5/25, Validation Loss: 0.0750\n",
      "          actual   predicted\n",
      "0     415.000003  405.413314\n",
      "1     451.999999  454.047587\n",
      "2     387.000001  392.557096\n",
      "3     433.499999  446.528020\n",
      "4     484.000000  442.882787\n",
      "...          ...         ...\n",
      "4091  824.499995  792.959749\n",
      "4092  445.999999  448.494749\n",
      "4093  473.000001  479.300780\n",
      "4094  457.000002  457.786938\n",
      "4095  433.499999  431.762795\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0893\n",
      "Epoch 6/25, Validation Loss: 0.0742\n",
      "          actual   predicted\n",
      "0     415.000003  412.140096\n",
      "1     451.999999  456.178752\n",
      "2     387.000001  391.463675\n",
      "3     433.499999  447.198995\n",
      "4     484.000000  452.331617\n",
      "...          ...         ...\n",
      "4091  824.499995  803.682982\n",
      "4092  445.999999  449.966547\n",
      "4093  473.000001  479.447409\n",
      "4094  457.000002  455.727761\n",
      "4095  433.499999  429.044853\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0846\n",
      "Epoch 7/25, Validation Loss: 0.0747\n",
      "          actual   predicted\n",
      "0     415.000003  409.402755\n",
      "1     451.999999  453.442978\n",
      "2     387.000001  394.302509\n",
      "3     433.499999  441.645431\n",
      "4     484.000000  446.243192\n",
      "...          ...         ...\n",
      "4091  824.499995  794.480375\n",
      "4092  445.999999  451.877495\n",
      "4093  473.000001  472.616315\n",
      "4094  457.000002  450.635785\n",
      "4095  433.499999  430.792519\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0825\n",
      "Epoch 8/25, Validation Loss: 0.0706\n",
      "          actual   predicted\n",
      "0     415.000003  408.743294\n",
      "1     451.999999  453.366252\n",
      "2     387.000001  393.790477\n",
      "3     433.499999  439.943469\n",
      "4     484.000000  447.553100\n",
      "...          ...         ...\n",
      "4091  824.499995  789.445353\n",
      "4092  445.999999  449.036289\n",
      "4093  473.000001  474.220653\n",
      "4094  457.000002  450.985674\n",
      "4095  433.499999  431.613893\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0815\n",
      "Epoch 9/25, Validation Loss: 0.0703\n",
      "          actual   predicted\n",
      "0     415.000003  415.290919\n",
      "1     451.999999  451.613454\n",
      "2     387.000001  396.666838\n",
      "3     433.499999  441.186108\n",
      "4     484.000000  457.121912\n",
      "...          ...         ...\n",
      "4091  824.499995  791.107519\n",
      "4092  445.999999  450.204713\n",
      "4093  473.000001  471.036435\n",
      "4094  457.000002  448.269598\n",
      "4095  433.499999  428.804356\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0793\n",
      "Epoch 10/25, Validation Loss: 0.0699\n",
      "          actual   predicted\n",
      "0     415.000003  414.532922\n",
      "1     451.999999  446.626417\n",
      "2     387.000001  392.512934\n",
      "3     433.499999  436.561942\n",
      "4     484.000000  451.242672\n",
      "...          ...         ...\n",
      "4091  824.499995  776.904155\n",
      "4092  445.999999  449.155717\n",
      "4093  473.000001  468.721168\n",
      "4094  457.000002  447.871771\n",
      "4095  433.499999  429.508877\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0785\n",
      "Epoch 11/25, Validation Loss: 0.0723\n",
      "          actual   predicted\n",
      "0     415.000003  420.559279\n",
      "1     451.999999  456.379258\n",
      "2     387.000001  399.062965\n",
      "3     433.499999  445.376743\n",
      "4     484.000000  455.217008\n",
      "...          ...         ...\n",
      "4091  824.499995  780.612404\n",
      "4092  445.999999  453.910161\n",
      "4093  473.000001  479.571967\n",
      "4094  457.000002  459.541434\n",
      "4095  433.499999  435.813808\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0761\n",
      "Epoch 12/25, Validation Loss: 0.0718\n",
      "          actual   predicted\n",
      "0     415.000003  417.032015\n",
      "1     451.999999  451.061336\n",
      "2     387.000001  395.660267\n",
      "3     433.499999  438.955139\n",
      "4     484.000000  455.137585\n",
      "...          ...         ...\n",
      "4091  824.499995  784.926401\n",
      "4092  445.999999  451.715954\n",
      "4093  473.000001  476.296414\n",
      "4094  457.000002  453.854530\n",
      "4095  433.499999  431.032817\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0752\n",
      "Epoch 13/25, Validation Loss: 0.0690\n",
      "          actual   predicted\n",
      "0     415.000003  415.993999\n",
      "1     451.999999  456.673370\n",
      "2     387.000001  390.502484\n",
      "3     433.499999  442.854960\n",
      "4     484.000000  460.531795\n",
      "...          ...         ...\n",
      "4091  824.499995  818.108418\n",
      "4092  445.999999  452.456247\n",
      "4093  473.000001  484.217866\n",
      "4094  457.000002  458.283376\n",
      "4095  433.499999  429.008476\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0740\n",
      "Epoch 14/25, Validation Loss: 0.0705\n",
      "          actual   predicted\n",
      "0     415.000003  413.046966\n",
      "1     451.999999  456.467212\n",
      "2     387.000001  390.490210\n",
      "3     433.499999  444.113520\n",
      "4     484.000000  458.786936\n",
      "...          ...         ...\n",
      "4091  824.499995  800.734053\n",
      "4092  445.999999  450.154760\n",
      "4093  473.000001  483.670532\n",
      "4094  457.000002  456.695082\n",
      "4095  433.499999  431.022649\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0745\n",
      "Epoch 15/25, Validation Loss: 0.0676\n",
      "          actual   predicted\n",
      "0     415.000003  418.003494\n",
      "1     451.999999  450.333001\n",
      "2     387.000001  392.267353\n",
      "3     433.499999  438.501036\n",
      "4     484.000000  457.714309\n",
      "...          ...         ...\n",
      "4091  824.499995  775.133991\n",
      "4092  445.999999  450.533909\n",
      "4093  473.000001  480.274787\n",
      "4094  457.000002  454.705415\n",
      "4095  433.499999  428.685307\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0728\n",
      "Epoch 16/25, Validation Loss: 0.0674\n",
      "          actual   predicted\n",
      "0     415.000003  421.094535\n",
      "1     451.999999  451.006195\n",
      "2     387.000001  394.459218\n",
      "3     433.499999  439.077392\n",
      "4     484.000000  456.936136\n",
      "...          ...         ...\n",
      "4091  824.499995  778.152321\n",
      "4092  445.999999  453.438714\n",
      "4093  473.000001  474.029692\n",
      "4094  457.000002  455.823207\n",
      "4095  433.499999  431.643919\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0722\n",
      "Epoch 17/25, Validation Loss: 0.0667\n",
      "          actual   predicted\n",
      "0     415.000003  416.098094\n",
      "1     451.999999  453.712156\n",
      "2     387.000001  390.957005\n",
      "3     433.499999  442.275012\n",
      "4     484.000000  457.308971\n",
      "...          ...         ...\n",
      "4091  824.499995  774.968415\n",
      "4092  445.999999  451.018450\n",
      "4093  473.000001  476.889759\n",
      "4094  457.000002  451.534347\n",
      "4095  433.499999  433.405291\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0707\n",
      "Epoch 18/25, Validation Loss: 0.0660\n",
      "          actual   predicted\n",
      "0     415.000003  418.853507\n",
      "1     451.999999  454.549492\n",
      "2     387.000001  394.970836\n",
      "3     433.499999  441.591564\n",
      "4     484.000000  460.656778\n",
      "...          ...         ...\n",
      "4091  824.499995  780.770158\n",
      "4092  445.999999  452.460353\n",
      "4093  473.000001  476.650694\n",
      "4094  457.000002  454.445312\n",
      "4095  433.499999  433.638658\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0717\n",
      "Epoch 19/25, Validation Loss: 0.0658\n",
      "          actual   predicted\n",
      "0     415.000003  420.517178\n",
      "1     451.999999  449.594633\n",
      "2     387.000001  391.131990\n",
      "3     433.499999  435.608008\n",
      "4     484.000000  454.851783\n",
      "...          ...         ...\n",
      "4091  824.499995  767.314863\n",
      "4092  445.999999  454.777313\n",
      "4093  473.000001  474.704211\n",
      "4094  457.000002  453.181156\n",
      "4095  433.499999  429.888188\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0695\n",
      "Epoch 20/25, Validation Loss: 0.0684\n",
      "          actual   predicted\n",
      "0     415.000003  419.928667\n",
      "1     451.999999  451.283227\n",
      "2     387.000001  389.252896\n",
      "3     433.499999  438.161043\n",
      "4     484.000000  457.159181\n",
      "...          ...         ...\n",
      "4091  824.499995  787.459345\n",
      "4092  445.999999  451.176099\n",
      "4093  473.000001  482.127940\n",
      "4094  457.000002  457.871499\n",
      "4095  433.499999  432.449210\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0705\n",
      "Epoch 21/25, Validation Loss: 0.0654\n",
      "          actual   predicted\n",
      "0     415.000003  420.079901\n",
      "1     451.999999  454.425487\n",
      "2     387.000001  392.232615\n",
      "3     433.499999  441.597280\n",
      "4     484.000000  457.007579\n",
      "...          ...         ...\n",
      "4091  824.499995  771.837575\n",
      "4092  445.999999  452.647777\n",
      "4093  473.000001  481.676387\n",
      "4094  457.000002  457.583285\n",
      "4095  433.499999  431.418723\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0687\n",
      "Epoch 22/25, Validation Loss: 0.0653\n",
      "          actual   predicted\n",
      "0     415.000003  417.020245\n",
      "1     451.999999  454.194071\n",
      "2     387.000001  389.109950\n",
      "3     433.499999  441.479919\n",
      "4     484.000000  458.004161\n",
      "...          ...         ...\n",
      "4091  824.499995  801.816072\n",
      "4092  445.999999  453.282317\n",
      "4093  473.000001  478.729443\n",
      "4094  457.000002  455.729811\n",
      "4095  433.499999  431.728614\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0684\n",
      "Epoch 23/25, Validation Loss: 0.0661\n",
      "          actual   predicted\n",
      "0     415.000003  422.553010\n",
      "1     451.999999  453.746213\n",
      "2     387.000001  392.433959\n",
      "3     433.499999  439.535148\n",
      "4     484.000000  459.886133\n",
      "...          ...         ...\n",
      "4091  824.499995  789.999238\n",
      "4092  445.999999  453.158681\n",
      "4093  473.000001  475.341533\n",
      "4094  457.000002  455.860404\n",
      "4095  433.499999  433.361667\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0675\n",
      "Epoch 24/25, Validation Loss: 0.0654\n",
      "          actual   predicted\n",
      "0     415.000003  425.692243\n",
      "1     451.999999  455.129301\n",
      "2     387.000001  390.750660\n",
      "3     433.499999  440.620024\n",
      "4     484.000000  462.898546\n",
      "...          ...         ...\n",
      "4091  824.499995  786.285664\n",
      "4092  445.999999  457.808532\n",
      "4093  473.000001  482.498474\n",
      "4094  457.000002  459.723330\n",
      "4095  433.499999  434.548043\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0664\n",
      "Epoch 25/25, Validation Loss: 0.0640\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[0.7651]], device='cuda:0')\n",
      "[585.58]\n",
      "           actual   predicted\n",
      "0      415.000003  425.692243\n",
      "1      451.999999  455.129301\n",
      "2      387.000001  390.750660\n",
      "3      433.499999  440.620024\n",
      "4      484.000000  462.898546\n",
      "...           ...         ...\n",
      "39163  469.000000  475.338375\n",
      "39164  483.000000  477.151157\n",
      "39165  445.714287  441.985743\n",
      "39166  465.499999  467.087051\n",
      "39167  452.499999  463.267502\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 32.0273\n",
      "Score (MAE): 11.3565\n",
      "Score (ME): -2.3584\n",
      "Score (MAPE): 2.0273%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      980.500007  1031.044403\n",
      "1      742.999998   703.200012\n",
      "2      662.500001   739.034570\n",
      "3     1414.999982   909.956649\n",
      "4      690.000002   704.042722\n",
      "...           ...          ...\n",
      "4091   714.000004   704.093091\n",
      "4092   635.500003   704.755535\n",
      "4093   788.000000   597.229512\n",
      "4094  1435.000002  1109.949404\n",
      "4095   696.999998   662.971025\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.5589\n",
      "Epoch 1/25, Validation Loss: 0.2914\n",
      "           actual    predicted\n",
      "0      980.500007  1262.807690\n",
      "1      742.999998   692.572208\n",
      "2      662.500001   687.514693\n",
      "3     1414.999982   938.700260\n",
      "4      690.000002   657.068168\n",
      "...           ...          ...\n",
      "4091   714.000004   760.497126\n",
      "4092   635.500003   672.362761\n",
      "4093   788.000000   617.238256\n",
      "4094  1435.000002  1492.812180\n",
      "4095   696.999998   668.271631\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2072\n",
      "Epoch 2/25, Validation Loss: 0.1625\n",
      "           actual    predicted\n",
      "0      980.500007  1107.440433\n",
      "1      742.999998   703.399457\n",
      "2      662.500001   654.175885\n",
      "3     1414.999982   912.838749\n",
      "4      690.000002   685.005556\n",
      "...           ...          ...\n",
      "4091   714.000004   718.427470\n",
      "4092   635.500003   673.840670\n",
      "4093   788.000000   627.881951\n",
      "4094  1435.000002  1395.278151\n",
      "4095   696.999998   677.324601\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1322\n",
      "Epoch 3/25, Validation Loss: 0.1025\n",
      "           actual    predicted\n",
      "0      980.500007  1098.999129\n",
      "1      742.999998   693.623761\n",
      "2      662.500001   642.191129\n",
      "3     1414.999982   987.958282\n",
      "4      690.000002   670.499748\n",
      "...           ...          ...\n",
      "4091   714.000004   692.300489\n",
      "4092   635.500003   684.883552\n",
      "4093   788.000000   640.552316\n",
      "4094  1435.000002  1183.017302\n",
      "4095   696.999998   672.749684\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1001\n",
      "Epoch 4/25, Validation Loss: 0.0836\n",
      "           actual    predicted\n",
      "0      980.500007  1100.768049\n",
      "1      742.999998   706.222936\n",
      "2      662.500001   638.531048\n",
      "3     1414.999982  1105.752424\n",
      "4      690.000002   687.275702\n",
      "...           ...          ...\n",
      "4091   714.000004   701.006974\n",
      "4092   635.500003   671.592692\n",
      "4093   788.000000   656.697802\n",
      "4094  1435.000002  1174.744288\n",
      "4095   696.999998   672.730313\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0875\n",
      "Epoch 5/25, Validation Loss: 0.0744\n",
      "           actual    predicted\n",
      "0      980.500007  1105.501335\n",
      "1      742.999998   718.943281\n",
      "2      662.500001   645.168676\n",
      "3     1414.999982  1223.599137\n",
      "4      690.000002   697.813608\n",
      "...           ...          ...\n",
      "4091   714.000004   700.395151\n",
      "4092   635.500003   673.660421\n",
      "4093   788.000000   672.926576\n",
      "4094  1435.000002  1162.791858\n",
      "4095   696.999998   678.894522\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0790\n",
      "Epoch 6/25, Validation Loss: 0.0686\n",
      "           actual    predicted\n",
      "0      980.500007  1082.761028\n",
      "1      742.999998   700.408709\n",
      "2      662.500001   645.198721\n",
      "3     1414.999982  1278.169601\n",
      "4      690.000002   677.679224\n",
      "...           ...          ...\n",
      "4091   714.000004   702.933275\n",
      "4092   635.500003   677.757075\n",
      "4093   788.000000   647.957112\n",
      "4094  1435.000002  1140.549587\n",
      "4095   696.999998   674.584060\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0748\n",
      "Epoch 7/25, Validation Loss: 0.0673\n",
      "           actual    predicted\n",
      "0      980.500007  1079.418419\n",
      "1      742.999998   718.658043\n",
      "2      662.500001   657.226258\n",
      "3     1414.999982  1348.219321\n",
      "4      690.000002   699.493549\n",
      "...           ...          ...\n",
      "4091   714.000004   710.182901\n",
      "4092   635.500003   681.197011\n",
      "4093   788.000000   663.133983\n",
      "4094  1435.000002  1140.596336\n",
      "4095   696.999998   682.654491\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0712\n",
      "Epoch 8/25, Validation Loss: 0.0642\n",
      "           actual    predicted\n",
      "0      980.500007  1071.089268\n",
      "1      742.999998   713.857877\n",
      "2      662.500001   656.719376\n",
      "3     1414.999982  1371.849664\n",
      "4      690.000002   692.892049\n",
      "...           ...          ...\n",
      "4091   714.000004   704.468597\n",
      "4092   635.500003   672.526436\n",
      "4093   788.000000   658.522020\n",
      "4094  1435.000002  1124.572767\n",
      "4095   696.999998   687.414403\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0689\n",
      "Epoch 9/25, Validation Loss: 0.0628\n",
      "           actual    predicted\n",
      "0      980.500007  1045.583131\n",
      "1      742.999998   704.106413\n",
      "2      662.500001   643.065361\n",
      "3     1414.999982  1390.158318\n",
      "4      690.000002   689.086328\n",
      "...           ...          ...\n",
      "4091   714.000004   696.921622\n",
      "4092   635.500003   667.533162\n",
      "4093   788.000000   649.727256\n",
      "4094  1435.000002  1084.813363\n",
      "4095   696.999998   675.142272\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0665\n",
      "Epoch 10/25, Validation Loss: 0.0634\n",
      "           actual    predicted\n",
      "0      980.500007  1069.021845\n",
      "1      742.999998   717.184628\n",
      "2      662.500001   660.264899\n",
      "3     1414.999982  1417.295627\n",
      "4      690.000002   691.416099\n",
      "...           ...          ...\n",
      "4091   714.000004   706.262509\n",
      "4092   635.500003   683.835478\n",
      "4093   788.000000   650.845436\n",
      "4094  1435.000002  1120.086725\n",
      "4095   696.999998   687.398773\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0662\n",
      "Epoch 11/25, Validation Loss: 0.0607\n",
      "           actual    predicted\n",
      "0      980.500007  1071.769510\n",
      "1      742.999998   713.001984\n",
      "2      662.500001   657.183293\n",
      "3     1414.999982  1463.313630\n",
      "4      690.000002   705.161069\n",
      "...           ...          ...\n",
      "4091   714.000004   711.347865\n",
      "4092   635.500003   667.714879\n",
      "4093   788.000000   664.660249\n",
      "4094  1435.000002  1154.520383\n",
      "4095   696.999998   681.230272\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0639\n",
      "Epoch 12/25, Validation Loss: 0.0610\n",
      "           actual    predicted\n",
      "0      980.500007  1076.220849\n",
      "1      742.999998   719.327332\n",
      "2      662.500001   658.162860\n",
      "3     1414.999982  1436.099886\n",
      "4      690.000002   698.264108\n",
      "...           ...          ...\n",
      "4091   714.000004   705.971593\n",
      "4092   635.500003   676.490238\n",
      "4093   788.000000   650.640230\n",
      "4094  1435.000002  1157.060893\n",
      "4095   696.999998   687.473355\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0624\n",
      "Epoch 13/25, Validation Loss: 0.0598\n",
      "           actual    predicted\n",
      "0      980.500007  1051.097256\n",
      "1      742.999998   712.719556\n",
      "2      662.500001   647.534375\n",
      "3     1414.999982  1475.072840\n",
      "4      690.000002   700.176248\n",
      "...           ...          ...\n",
      "4091   714.000004   707.136768\n",
      "4092   635.500003   667.951117\n",
      "4093   788.000000   651.418350\n",
      "4094  1435.000002  1119.182859\n",
      "4095   696.999998   677.062751\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0615\n",
      "Epoch 14/25, Validation Loss: 0.0587\n",
      "           actual    predicted\n",
      "0      980.500007  1044.768183\n",
      "1      742.999998   710.858187\n",
      "2      662.500001   649.950862\n",
      "3     1414.999982  1497.973272\n",
      "4      690.000002   696.651223\n",
      "...           ...          ...\n",
      "4091   714.000004   698.175671\n",
      "4092   635.500003   669.700753\n",
      "4093   788.000000   643.355997\n",
      "4094  1435.000002  1107.518127\n",
      "4095   696.999998   680.772404\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0608\n",
      "Epoch 15/25, Validation Loss: 0.0582\n",
      "           actual    predicted\n",
      "0      980.500007  1047.200195\n",
      "1      742.999998   723.526481\n",
      "2      662.500001   654.131426\n",
      "3     1414.999982  1490.829549\n",
      "4      690.000002   700.150181\n",
      "...           ...          ...\n",
      "4091   714.000004   696.604089\n",
      "4092   635.500003   672.828436\n",
      "4093   788.000000   650.841388\n",
      "4094  1435.000002  1114.567373\n",
      "4095   696.999998   682.907975\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0594\n",
      "Epoch 16/25, Validation Loss: 0.0584\n",
      "           actual    predicted\n",
      "0      980.500007  1039.850470\n",
      "1      742.999998   718.080408\n",
      "2      662.500001   654.494126\n",
      "3     1414.999982  1498.656994\n",
      "4      690.000002   693.811423\n",
      "...           ...          ...\n",
      "4091   714.000004   700.050301\n",
      "4092   635.500003   670.979094\n",
      "4093   788.000000   652.254042\n",
      "4094  1435.000002  1114.197120\n",
      "4095   696.999998   688.544978\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0589\n",
      "Epoch 17/25, Validation Loss: 0.0584\n",
      "           actual    predicted\n",
      "0      980.500007  1055.932764\n",
      "1      742.999998   718.692585\n",
      "2      662.500001   655.022643\n",
      "3     1414.999982  1489.680853\n",
      "4      690.000002   698.348926\n",
      "...           ...          ...\n",
      "4091   714.000004   709.561733\n",
      "4092   635.500003   670.788706\n",
      "4093   788.000000   649.480511\n",
      "4094  1435.000002  1147.516068\n",
      "4095   696.999998   680.071611\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0585\n",
      "Epoch 18/25, Validation Loss: 0.0575\n",
      "           actual    predicted\n",
      "0      980.500007  1029.700248\n",
      "1      742.999998   724.632650\n",
      "2      662.500001   661.375859\n",
      "3     1414.999982  1489.935054\n",
      "4      690.000002   706.611896\n",
      "...           ...          ...\n",
      "4091   714.000004   703.833959\n",
      "4092   635.500003   669.802548\n",
      "4093   788.000000   651.104829\n",
      "4094  1435.000002  1115.774777\n",
      "4095   696.999998   685.742784\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0574\n",
      "Epoch 19/25, Validation Loss: 0.0571\n",
      "           actual    predicted\n",
      "0      980.500007  1043.568646\n",
      "1      742.999998   715.993011\n",
      "2      662.500001   660.788871\n",
      "3     1414.999982  1484.811445\n",
      "4      690.000002   699.939791\n",
      "...           ...          ...\n",
      "4091   714.000004   705.545605\n",
      "4092   635.500003   672.876103\n",
      "4093   788.000000   649.951614\n",
      "4094  1435.000002  1147.136025\n",
      "4095   696.999998   685.292713\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0575\n",
      "Epoch 20/25, Validation Loss: 0.0566\n",
      "           actual    predicted\n",
      "0      980.500007  1036.590504\n",
      "1      742.999998   726.001526\n",
      "2      662.500001   660.391327\n",
      "3     1414.999982  1497.903900\n",
      "4      690.000002   706.263033\n",
      "...           ...          ...\n",
      "4091   714.000004   706.287553\n",
      "4092   635.500003   669.785729\n",
      "4093   788.000000   657.302623\n",
      "4094  1435.000002  1131.920027\n",
      "4095   696.999998   687.374638\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0576\n",
      "Epoch 21/25, Validation Loss: 0.0564\n",
      "           actual    predicted\n",
      "0      980.500007  1027.740572\n",
      "1      742.999998   716.746157\n",
      "2      662.500001   658.132694\n",
      "3     1414.999982  1477.913951\n",
      "4      690.000002   697.527256\n",
      "...           ...          ...\n",
      "4091   714.000004   706.030275\n",
      "4092   635.500003   679.415023\n",
      "4093   788.000000   643.606929\n",
      "4094  1435.000002  1151.924697\n",
      "4095   696.999998   678.762744\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0565\n",
      "Epoch 22/25, Validation Loss: 0.0568\n",
      "           actual    predicted\n",
      "0      980.500007  1020.030158\n",
      "1      742.999998   721.363085\n",
      "2      662.500001   647.048394\n",
      "3     1414.999982  1503.601368\n",
      "4      690.000002   701.054003\n",
      "...           ...          ...\n",
      "4091   714.000004   699.812848\n",
      "4092   635.500003   666.069153\n",
      "4093   788.000000   647.214368\n",
      "4094  1435.000002  1117.344322\n",
      "4095   696.999998   673.353814\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0557\n",
      "Epoch 23/25, Validation Loss: 0.0566\n",
      "           actual    predicted\n",
      "0      980.500007  1035.247467\n",
      "1      742.999998   732.138252\n",
      "2      662.500001   660.540474\n",
      "3     1414.999982  1484.484095\n",
      "4      690.000002   703.537099\n",
      "...           ...          ...\n",
      "4091   714.000004   708.818046\n",
      "4092   635.500003   676.078210\n",
      "4093   788.000000   653.577462\n",
      "4094  1435.000002  1146.690875\n",
      "4095   696.999998   683.811221\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0553\n",
      "Epoch 24/25, Validation Loss: 0.0559\n",
      "           actual    predicted\n",
      "0      980.500007  1023.159282\n",
      "1      742.999998   730.770814\n",
      "2      662.500001   659.997700\n",
      "3     1414.999982  1454.404554\n",
      "4      690.000002   700.721915\n",
      "...           ...          ...\n",
      "4091   714.000004   705.427499\n",
      "4092   635.500003   674.595109\n",
      "4093   788.000000   655.556824\n",
      "4094  1435.000002  1134.383088\n",
      "4095   696.999998   680.364668\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0553\n",
      "Epoch 25/25, Validation Loss: 0.0565\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[-0.5109]], device='cuda:0')\n",
      "[640.94]\n",
      "            actual    predicted\n",
      "0       980.500007  1023.159282\n",
      "1       742.999998   730.770814\n",
      "2       662.500001   659.997700\n",
      "3      1414.999982  1454.404554\n",
      "4       690.000002   700.721915\n",
      "...            ...          ...\n",
      "39163   602.000001   607.704637\n",
      "39164   611.500000   598.437021\n",
      "39165   821.999999   799.669212\n",
      "39166   799.500000   835.000230\n",
      "39167   527.499992   543.683442\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 69.9312\n",
      "Score (MAE): 31.9489\n",
      "Score (ME): 2.3212\n",
      "Score (MAPE): 3.8156%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     39.690000  42.694986\n",
      "1     48.625001  51.091025\n",
      "2     50.269999  47.080737\n",
      "3     28.380000  28.269087\n",
      "4     32.580000  29.636617\n",
      "...         ...        ...\n",
      "4091  49.934999  50.013173\n",
      "4092  40.480000  42.725355\n",
      "4093  27.890000  27.060827\n",
      "4094  24.405000  24.337408\n",
      "4095  33.500000  38.132917\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.4694\n",
      "Epoch 1/25, Validation Loss: 0.0999\n",
      "         actual  predicted\n",
      "0     39.690000  40.479635\n",
      "1     48.625001  48.302772\n",
      "2     50.269999  47.474736\n",
      "3     28.380000  27.120215\n",
      "4     32.580000  31.034514\n",
      "...         ...        ...\n",
      "4091  49.934999  47.855684\n",
      "4092  40.480000  41.020564\n",
      "4093  27.890000  27.822814\n",
      "4094  24.405000  24.865351\n",
      "4095  33.500000  36.000672\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0673\n",
      "Epoch 2/25, Validation Loss: 0.0447\n",
      "         actual  predicted\n",
      "0     39.690000  40.077576\n",
      "1     48.625001  45.469222\n",
      "2     50.269999  48.407896\n",
      "3     28.380000  27.840834\n",
      "4     32.580000  31.502038\n",
      "...         ...        ...\n",
      "4091  49.934999  48.710950\n",
      "4092  40.480000  41.129200\n",
      "4093  27.890000  27.659129\n",
      "4094  24.405000  25.054590\n",
      "4095  33.500000  35.832232\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0406\n",
      "Epoch 3/25, Validation Loss: 0.0315\n",
      "         actual  predicted\n",
      "0     39.690000  40.240817\n",
      "1     48.625001  45.031929\n",
      "2     50.269999  48.644128\n",
      "3     28.380000  28.115444\n",
      "4     32.580000  31.787548\n",
      "...         ...        ...\n",
      "4091  49.934999  49.454989\n",
      "4092  40.480000  40.806398\n",
      "4093  27.890000  27.710824\n",
      "4094  24.405000  24.568167\n",
      "4095  33.500000  35.473797\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0324\n",
      "Epoch 4/25, Validation Loss: 0.0249\n",
      "         actual  predicted\n",
      "0     39.690000  40.206053\n",
      "1     48.625001  44.986016\n",
      "2     50.269999  48.887470\n",
      "3     28.380000  28.178088\n",
      "4     32.580000  32.016036\n",
      "...         ...        ...\n",
      "4091  49.934999  49.455305\n",
      "4092  40.480000  40.836106\n",
      "4093  27.890000  27.958766\n",
      "4094  24.405000  24.942623\n",
      "4095  33.500000  35.368668\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0276\n",
      "Epoch 5/25, Validation Loss: 0.0208\n",
      "         actual  predicted\n",
      "0     39.690000  40.156932\n",
      "1     48.625001  44.637933\n",
      "2     50.269999  48.577783\n",
      "3     28.380000  28.700993\n",
      "4     32.580000  32.553381\n",
      "...         ...        ...\n",
      "4091  49.934999  49.194755\n",
      "4092  40.480000  40.533507\n",
      "4093  27.890000  28.135695\n",
      "4094  24.405000  24.849468\n",
      "4095  33.500000  35.155366\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0245\n",
      "Epoch 6/25, Validation Loss: 0.0181\n",
      "         actual  predicted\n",
      "0     39.690000  40.241173\n",
      "1     48.625001  44.954125\n",
      "2     50.269999  49.034284\n",
      "3     28.380000  28.368260\n",
      "4     32.580000  32.332487\n",
      "...         ...        ...\n",
      "4091  49.934999  49.511947\n",
      "4092  40.480000  40.683479\n",
      "4093  27.890000  27.785655\n",
      "4094  24.405000  24.779384\n",
      "4095  33.500000  35.067974\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0220\n",
      "Epoch 7/25, Validation Loss: 0.0154\n",
      "         actual  predicted\n",
      "0     39.690000  39.853870\n",
      "1     48.625001  44.989125\n",
      "2     50.269999  49.163126\n",
      "3     28.380000  28.561358\n",
      "4     32.580000  32.412859\n",
      "...         ...        ...\n",
      "4091  49.934999  49.302379\n",
      "4092  40.480000  40.496046\n",
      "4093  27.890000  27.689356\n",
      "4094  24.405000  24.934575\n",
      "4095  33.500000  34.776039\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0197\n",
      "Epoch 8/25, Validation Loss: 0.0136\n",
      "         actual  predicted\n",
      "0     39.690000  39.779591\n",
      "1     48.625001  44.948776\n",
      "2     50.269999  49.441316\n",
      "3     28.380000  28.458360\n",
      "4     32.580000  32.422204\n",
      "...         ...        ...\n",
      "4091  49.934999  49.295090\n",
      "4092  40.480000  40.618526\n",
      "4093  27.890000  27.974088\n",
      "4094  24.405000  24.938063\n",
      "4095  33.500000  34.595984\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0183\n",
      "Epoch 9/25, Validation Loss: 0.0124\n",
      "         actual  predicted\n",
      "0     39.690000  39.959437\n",
      "1     48.625001  45.498918\n",
      "2     50.269999  49.951973\n",
      "3     28.380000  28.224370\n",
      "4     32.580000  32.388410\n",
      "...         ...        ...\n",
      "4091  49.934999  49.654276\n",
      "4092  40.480000  40.690656\n",
      "4093  27.890000  27.568402\n",
      "4094  24.405000  24.759596\n",
      "4095  33.500000  34.750098\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0171\n",
      "Epoch 10/25, Validation Loss: 0.0119\n",
      "         actual  predicted\n",
      "0     39.690000  39.942871\n",
      "1     48.625001  45.068013\n",
      "2     50.269999  49.781565\n",
      "3     28.380000  28.280746\n",
      "4     32.580000  32.383969\n",
      "...         ...        ...\n",
      "4091  49.934999  49.589036\n",
      "4092  40.480000  40.694685\n",
      "4093  27.890000  27.874158\n",
      "4094  24.405000  24.610879\n",
      "4095  33.500000  34.387538\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0160\n",
      "Epoch 11/25, Validation Loss: 0.0106\n",
      "         actual  predicted\n",
      "0     39.690000  40.025173\n",
      "1     48.625001  45.016287\n",
      "2     50.269999  49.624408\n",
      "3     28.380000  28.360142\n",
      "4     32.580000  32.343457\n",
      "...         ...        ...\n",
      "4091  49.934999  49.426011\n",
      "4092  40.480000  40.482704\n",
      "4093  27.890000  27.907269\n",
      "4094  24.405000  24.931903\n",
      "4095  33.500000  34.479871\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0150\n",
      "Epoch 12/25, Validation Loss: 0.0095\n",
      "         actual  predicted\n",
      "0     39.690000  39.975688\n",
      "1     48.625001  45.425171\n",
      "2     50.269999  50.076797\n",
      "3     28.380000  28.190797\n",
      "4     32.580000  32.193006\n",
      "...         ...        ...\n",
      "4091  49.934999  49.765834\n",
      "4092  40.480000  40.508957\n",
      "4093  27.890000  27.811286\n",
      "4094  24.405000  24.761074\n",
      "4095  33.500000  34.454026\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0141\n",
      "Epoch 13/25, Validation Loss: 0.0094\n",
      "         actual  predicted\n",
      "0     39.690000  39.980864\n",
      "1     48.625001  45.515603\n",
      "2     50.269999  50.294371\n",
      "3     28.380000  28.069958\n",
      "4     32.580000  32.295327\n",
      "...         ...        ...\n",
      "4091  49.934999  49.807132\n",
      "4092  40.480000  40.563513\n",
      "4093  27.890000  27.683818\n",
      "4094  24.405000  24.733644\n",
      "4095  33.500000  34.169801\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0134\n",
      "Epoch 14/25, Validation Loss: 0.0094\n",
      "         actual  predicted\n",
      "0     39.690000  40.063463\n",
      "1     48.625001  45.535098\n",
      "2     50.269999  50.130405\n",
      "3     28.380000  28.061010\n",
      "4     32.580000  32.323341\n",
      "...         ...        ...\n",
      "4091  49.934999  49.867820\n",
      "4092  40.480000  40.565163\n",
      "4093  27.890000  27.556135\n",
      "4094  24.405000  24.638385\n",
      "4095  33.500000  34.303241\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0129\n",
      "Epoch 15/25, Validation Loss: 0.0089\n",
      "         actual  predicted\n",
      "0     39.690000  39.853201\n",
      "1     48.625001  45.204314\n",
      "2     50.269999  49.877748\n",
      "3     28.380000  28.302896\n",
      "4     32.580000  32.330245\n",
      "...         ...        ...\n",
      "4091  49.934999  49.598019\n",
      "4092  40.480000  40.492180\n",
      "4093  27.890000  27.784595\n",
      "4094  24.405000  24.618149\n",
      "4095  33.500000  34.029488\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0125\n",
      "Epoch 16/25, Validation Loss: 0.0077\n",
      "         actual  predicted\n",
      "0     39.690000  39.840907\n",
      "1     48.625001  45.306836\n",
      "2     50.269999  50.047655\n",
      "3     28.380000  28.139782\n",
      "4     32.580000  32.291229\n",
      "...         ...        ...\n",
      "4091  49.934999  49.782580\n",
      "4092  40.480000  40.748503\n",
      "4093  27.890000  27.530684\n",
      "4094  24.405000  24.434741\n",
      "4095  33.500000  33.809875\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0117\n",
      "Epoch 17/25, Validation Loss: 0.0076\n",
      "         actual  predicted\n",
      "0     39.690000  39.845758\n",
      "1     48.625001  45.356400\n",
      "2     50.269999  50.045988\n",
      "3     28.380000  28.176430\n",
      "4     32.580000  32.253157\n",
      "...         ...        ...\n",
      "4091  49.934999  49.562337\n",
      "4092  40.480000  40.341977\n",
      "4093  27.890000  27.465260\n",
      "4094  24.405000  24.686371\n",
      "4095  33.500000  33.951714\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0112\n",
      "Epoch 18/25, Validation Loss: 0.0071\n",
      "         actual  predicted\n",
      "0     39.690000  40.029048\n",
      "1     48.625001  45.653515\n",
      "2     50.269999  50.349832\n",
      "3     28.380000  27.902112\n",
      "4     32.580000  32.277574\n",
      "...         ...        ...\n",
      "4091  49.934999  49.854655\n",
      "4092  40.480000  40.692995\n",
      "4093  27.890000  27.367865\n",
      "4094  24.405000  24.614741\n",
      "4095  33.500000  34.041545\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0109\n",
      "Epoch 19/25, Validation Loss: 0.0075\n",
      "         actual  predicted\n",
      "0     39.690000  39.941519\n",
      "1     48.625001  45.480917\n",
      "2     50.269999  50.145519\n",
      "3     28.380000  28.085861\n",
      "4     32.580000  32.188439\n",
      "...         ...        ...\n",
      "4091  49.934999  49.825624\n",
      "4092  40.480000  40.427448\n",
      "4093  27.890000  27.570813\n",
      "4094  24.405000  24.531392\n",
      "4095  33.500000  33.864580\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0104\n",
      "Epoch 20/25, Validation Loss: 0.0068\n",
      "         actual  predicted\n",
      "0     39.690000  39.854412\n",
      "1     48.625001  45.332755\n",
      "2     50.269999  50.082044\n",
      "3     28.380000  28.120607\n",
      "4     32.580000  32.284894\n",
      "...         ...        ...\n",
      "4091  49.934999  49.671276\n",
      "4092  40.480000  40.516801\n",
      "4093  27.890000  27.986789\n",
      "4094  24.405000  24.872086\n",
      "4095  33.500000  33.892844\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0101\n",
      "Epoch 21/25, Validation Loss: 0.0063\n",
      "         actual  predicted\n",
      "0     39.690000  39.747582\n",
      "1     48.625001  45.536905\n",
      "2     50.269999  50.119130\n",
      "3     28.380000  28.249379\n",
      "4     32.580000  32.383292\n",
      "...         ...        ...\n",
      "4091  49.934999  49.602686\n",
      "4092  40.480000  40.456583\n",
      "4093  27.890000  27.760035\n",
      "4094  24.405000  24.845070\n",
      "4095  33.500000  33.870849\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0097\n",
      "Epoch 22/25, Validation Loss: 0.0060\n",
      "         actual  predicted\n",
      "0     39.690000  39.914038\n",
      "1     48.625001  45.476327\n",
      "2     50.269999  50.245136\n",
      "3     28.380000  28.351872\n",
      "4     32.580000  32.568099\n",
      "...         ...        ...\n",
      "4091  49.934999  49.609660\n",
      "4092  40.480000  40.571154\n",
      "4093  27.890000  28.083785\n",
      "4094  24.405000  24.893993\n",
      "4095  33.500000  33.894859\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0095\n",
      "Epoch 23/25, Validation Loss: 0.0060\n",
      "         actual  predicted\n",
      "0     39.690000  39.854663\n",
      "1     48.625001  45.679766\n",
      "2     50.269999  50.053463\n",
      "3     28.380000  28.278912\n",
      "4     32.580000  32.371316\n",
      "...         ...        ...\n",
      "4091  49.934999  49.683418\n",
      "4092  40.480000  40.455295\n",
      "4093  27.890000  27.684454\n",
      "4094  24.405000  24.875278\n",
      "4095  33.500000  33.790731\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0090\n",
      "Epoch 24/25, Validation Loss: 0.0056\n",
      "         actual  predicted\n",
      "0     39.690000  39.898361\n",
      "1     48.625001  46.197034\n",
      "2     50.269999  50.733912\n",
      "3     28.380000  28.282730\n",
      "4     32.580000  32.394591\n",
      "...         ...        ...\n",
      "4091  49.934999  50.062786\n",
      "4092  40.480000  40.663094\n",
      "4093  27.890000  27.464524\n",
      "4094  24.405000  24.978790\n",
      "4095  33.500000  33.846176\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0086\n",
      "Epoch 25/25, Validation Loss: 0.0061\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[1.1765]], device='cuda:0')\n",
      "[46.81]\n",
      "          actual  predicted\n",
      "0      39.690000  39.898361\n",
      "1      48.625001  46.197034\n",
      "2      50.269999  50.733912\n",
      "3      28.380000  28.282730\n",
      "4      32.580000  32.394591\n",
      "...          ...        ...\n",
      "39163  24.145000  23.725545\n",
      "39164  26.990000  26.930917\n",
      "39165  30.970000  30.984930\n",
      "39166  29.720000  30.064002\n",
      "39167  24.780000  24.638438\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 0.7547\n",
      "Score (MAE): 0.4499\n",
      "Score (ME): -0.1493\n",
      "Score (MAPE): 1.2134%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual  predicted\n",
      "0     28.635  28.806160\n",
      "1     27.205  27.130869\n",
      "2     17.785  20.316888\n",
      "3     19.565  20.127801\n",
      "4     25.185  24.358934\n",
      "...      ...        ...\n",
      "4091  29.535  30.657277\n",
      "4092  22.735  20.144125\n",
      "4093  22.135  20.726244\n",
      "4094  23.505  23.618202\n",
      "4095  25.990  24.807707\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.4898\n",
      "Epoch 1/25, Validation Loss: 0.1321\n",
      "      actual  predicted\n",
      "0     28.635  28.625144\n",
      "1     27.205  27.553353\n",
      "2     17.785  19.295139\n",
      "3     19.565  19.744310\n",
      "4     25.185  25.156330\n",
      "...      ...        ...\n",
      "4091  29.535  29.419904\n",
      "4092  22.735  22.629244\n",
      "4093  22.135  22.416808\n",
      "4094  23.505  24.078798\n",
      "4095  25.990  24.509528\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0730\n",
      "Epoch 2/25, Validation Loss: 0.0457\n",
      "      actual  predicted\n",
      "0     28.635  28.770049\n",
      "1     27.205  27.294618\n",
      "2     17.785  18.915721\n",
      "3     19.565  19.709875\n",
      "4     25.185  25.308254\n",
      "...      ...        ...\n",
      "4091  29.535  29.391769\n",
      "4092  22.735  22.871368\n",
      "4093  22.135  22.174091\n",
      "4094  23.505  23.870641\n",
      "4095  25.990  24.814723\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0405\n",
      "Epoch 3/25, Validation Loss: 0.0314\n",
      "      actual  predicted\n",
      "0     28.635  28.822389\n",
      "1     27.205  27.374530\n",
      "2     17.785  18.617118\n",
      "3     19.565  19.592686\n",
      "4     25.185  25.662958\n",
      "...      ...        ...\n",
      "4091  29.535  29.394285\n",
      "4092  22.735  22.948208\n",
      "4093  22.135  22.068561\n",
      "4094  23.505  23.552338\n",
      "4095  25.990  24.945162\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0317\n",
      "Epoch 4/25, Validation Loss: 0.0247\n",
      "      actual  predicted\n",
      "0     28.635  28.716017\n",
      "1     27.205  27.245627\n",
      "2     17.785  18.426040\n",
      "3     19.565  19.576033\n",
      "4     25.185  25.907201\n",
      "...      ...        ...\n",
      "4091  29.535  29.237680\n",
      "4092  22.735  23.087585\n",
      "4093  22.135  22.078467\n",
      "4094  23.505  23.379185\n",
      "4095  25.990  25.089893\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0262\n",
      "Epoch 5/25, Validation Loss: 0.0193\n",
      "      actual  predicted\n",
      "0     28.635  28.547652\n",
      "1     27.205  27.052958\n",
      "2     17.785  18.421538\n",
      "3     19.565  19.699994\n",
      "4     25.185  26.073629\n",
      "...      ...        ...\n",
      "4091  29.535  29.134939\n",
      "4092  22.735  23.054979\n",
      "4093  22.135  22.117479\n",
      "4094  23.505  23.341698\n",
      "4095  25.990  25.247930\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0222\n",
      "Epoch 6/25, Validation Loss: 0.0164\n",
      "      actual  predicted\n",
      "0     28.635  28.519686\n",
      "1     27.205  27.043502\n",
      "2     17.785  18.322218\n",
      "3     19.565  19.680952\n",
      "4     25.185  26.239104\n",
      "...      ...        ...\n",
      "4091  29.535  29.255040\n",
      "4092  22.735  22.962233\n",
      "4093  22.135  22.051133\n",
      "4094  23.505  23.217154\n",
      "4095  25.990  25.274720\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0198\n",
      "Epoch 7/25, Validation Loss: 0.0141\n",
      "      actual  predicted\n",
      "0     28.635  28.527119\n",
      "1     27.205  27.102438\n",
      "2     17.785  18.374817\n",
      "3     19.565  19.773861\n",
      "4     25.185  26.279393\n",
      "...      ...        ...\n",
      "4091  29.535  29.236241\n",
      "4092  22.735  22.906357\n",
      "4093  22.135  22.198218\n",
      "4094  23.505  23.263141\n",
      "4095  25.990  25.415776\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0179\n",
      "Epoch 8/25, Validation Loss: 0.0125\n",
      "      actual  predicted\n",
      "0     28.635  28.730923\n",
      "1     27.205  27.171833\n",
      "2     17.785  18.137801\n",
      "3     19.565  19.627305\n",
      "4     25.185  26.251159\n",
      "...      ...        ...\n",
      "4091  29.535  29.515654\n",
      "4092  22.735  22.836678\n",
      "4093  22.135  22.071018\n",
      "4094  23.505  23.247180\n",
      "4095  25.990  25.432477\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0164\n",
      "Epoch 9/25, Validation Loss: 0.0115\n",
      "      actual  predicted\n",
      "0     28.635  28.536776\n",
      "1     27.205  27.068597\n",
      "2     17.785  18.329489\n",
      "3     19.565  19.764891\n",
      "4     25.185  26.107339\n",
      "...      ...        ...\n",
      "4091  29.535  29.265283\n",
      "4092  22.735  22.870299\n",
      "4093  22.135  22.178353\n",
      "4094  23.505  23.250310\n",
      "4095  25.990  25.542255\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0152\n",
      "Epoch 10/25, Validation Loss: 0.0103\n",
      "      actual  predicted\n",
      "0     28.635  28.541008\n",
      "1     27.205  27.089551\n",
      "2     17.785  18.346169\n",
      "3     19.565  19.786020\n",
      "4     25.185  26.110061\n",
      "...      ...        ...\n",
      "4091  29.535  29.206647\n",
      "4092  22.735  22.841643\n",
      "4093  22.135  22.157188\n",
      "4094  23.505  23.181557\n",
      "4095  25.990  25.559119\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0142\n",
      "Epoch 11/25, Validation Loss: 0.0096\n",
      "      actual  predicted\n",
      "0     28.635  28.752460\n",
      "1     27.205  27.267395\n",
      "2     17.785  18.123307\n",
      "3     19.565  19.638377\n",
      "4     25.185  26.169530\n",
      "...      ...        ...\n",
      "4091  29.535  29.516607\n",
      "4092  22.735  22.851423\n",
      "4093  22.135  22.080592\n",
      "4094  23.505  23.172155\n",
      "4095  25.990  25.661116\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0133\n",
      "Epoch 12/25, Validation Loss: 0.0097\n",
      "      actual  predicted\n",
      "0     28.635  28.601152\n",
      "1     27.205  27.117398\n",
      "2     17.785  18.120567\n",
      "3     19.565  19.652843\n",
      "4     25.185  25.734514\n",
      "...      ...        ...\n",
      "4091  29.535  29.423005\n",
      "4092  22.735  22.797262\n",
      "4093  22.135  22.064661\n",
      "4094  23.505  23.224308\n",
      "4095  25.990  25.668042\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0125\n",
      "Epoch 13/25, Validation Loss: 0.0084\n",
      "      actual  predicted\n",
      "0     28.635  28.555243\n",
      "1     27.205  27.073079\n",
      "2     17.785  18.199515\n",
      "3     19.565  19.752749\n",
      "4     25.185  25.717047\n",
      "...      ...        ...\n",
      "4091  29.535  29.348265\n",
      "4092  22.735  22.811840\n",
      "4093  22.135  22.200428\n",
      "4094  23.505  23.233958\n",
      "4095  25.990  25.668503\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0121\n",
      "Epoch 14/25, Validation Loss: 0.0079\n",
      "      actual  predicted\n",
      "0     28.635  28.669189\n",
      "1     27.205  27.181792\n",
      "2     17.785  18.127254\n",
      "3     19.565  19.651065\n",
      "4     25.185  25.622167\n",
      "...      ...        ...\n",
      "4091  29.535  29.452794\n",
      "4092  22.735  22.756571\n",
      "4093  22.135  22.073592\n",
      "4094  23.505  23.179188\n",
      "4095  25.990  25.705769\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0113\n",
      "Epoch 15/25, Validation Loss: 0.0074\n",
      "      actual  predicted\n",
      "0     28.635  28.595912\n",
      "1     27.205  27.109144\n",
      "2     17.785  18.138213\n",
      "3     19.565  19.680328\n",
      "4     25.185  25.557191\n",
      "...      ...        ...\n",
      "4091  29.535  29.372449\n",
      "4092  22.735  22.823737\n",
      "4093  22.135  22.132081\n",
      "4094  23.505  23.188150\n",
      "4095  25.990  25.718189\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0108\n",
      "Epoch 16/25, Validation Loss: 0.0071\n",
      "      actual  predicted\n",
      "0     28.635  28.667967\n",
      "1     27.205  27.149992\n",
      "2     17.785  18.104221\n",
      "3     19.565  19.656216\n",
      "4     25.185  25.468880\n",
      "...      ...        ...\n",
      "4091  29.535  29.476918\n",
      "4092  22.735  22.736856\n",
      "4093  22.135  22.063309\n",
      "4094  23.505  23.239365\n",
      "4095  25.990  25.727022\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0103\n",
      "Epoch 17/25, Validation Loss: 0.0069\n",
      "      actual  predicted\n",
      "0     28.635  28.617123\n",
      "1     27.205  27.159509\n",
      "2     17.785  18.130355\n",
      "3     19.565  19.740049\n",
      "4     25.185  25.436304\n",
      "...      ...        ...\n",
      "4091  29.535  29.433867\n",
      "4092  22.735  22.728707\n",
      "4093  22.135  22.142608\n",
      "4094  23.505  23.239384\n",
      "4095  25.990  25.747042\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0099\n",
      "Epoch 18/25, Validation Loss: 0.0065\n",
      "      actual  predicted\n",
      "0     28.635  28.613026\n",
      "1     27.205  27.161557\n",
      "2     17.785  18.134290\n",
      "3     19.565  19.682489\n",
      "4     25.185  25.428100\n",
      "...      ...        ...\n",
      "4091  29.535  29.469308\n",
      "4092  22.735  22.809789\n",
      "4093  22.135  22.103119\n",
      "4094  23.505  23.198948\n",
      "4095  25.990  25.746554\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0096\n",
      "Epoch 19/25, Validation Loss: 0.0063\n",
      "      actual  predicted\n",
      "0     28.635  28.560551\n",
      "1     27.205  27.063951\n",
      "2     17.785  18.019167\n",
      "3     19.565  19.658760\n",
      "4     25.185  25.205285\n",
      "...      ...        ...\n",
      "4091  29.535  29.382343\n",
      "4092  22.735  22.727101\n",
      "4093  22.135  22.117961\n",
      "4094  23.505  23.287482\n",
      "4095  25.990  25.754131\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0093\n",
      "Epoch 20/25, Validation Loss: 0.0061\n",
      "      actual  predicted\n",
      "0     28.635  28.511627\n",
      "1     27.205  27.040409\n",
      "2     17.785  18.030894\n",
      "3     19.565  19.655878\n",
      "4     25.185  25.073340\n",
      "...      ...        ...\n",
      "4091  29.535  29.405148\n",
      "4092  22.735  22.751958\n",
      "4093  22.135  22.076010\n",
      "4094  23.505  23.237357\n",
      "4095  25.990  25.734549\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0091\n",
      "Epoch 21/25, Validation Loss: 0.0061\n",
      "      actual  predicted\n",
      "0     28.635  28.579879\n",
      "1     27.205  27.069471\n",
      "2     17.785  17.988175\n",
      "3     19.565  19.640797\n",
      "4     25.185  25.282194\n",
      "...      ...        ...\n",
      "4091  29.535  29.441115\n",
      "4092  22.735  22.682651\n",
      "4093  22.135  22.067264\n",
      "4094  23.505  23.319237\n",
      "4095  25.990  25.752824\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0087\n",
      "Epoch 22/25, Validation Loss: 0.0058\n",
      "      actual  predicted\n",
      "0     28.635  28.662905\n",
      "1     27.205  27.132539\n",
      "2     17.785  18.133338\n",
      "3     19.565  19.753901\n",
      "4     25.185  25.075074\n",
      "...      ...        ...\n",
      "4091  29.535  29.575808\n",
      "4092  22.735  22.792160\n",
      "4093  22.135  22.155915\n",
      "4094  23.505  23.265635\n",
      "4095  25.990  25.795774\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0083\n",
      "Epoch 23/25, Validation Loss: 0.0057\n",
      "      actual  predicted\n",
      "0     28.635  28.612872\n",
      "1     27.205  27.157902\n",
      "2     17.785  17.989239\n",
      "3     19.565  19.665657\n",
      "4     25.185  25.000865\n",
      "...      ...        ...\n",
      "4091  29.535  29.509864\n",
      "4092  22.735  22.793452\n",
      "4093  22.135  22.141541\n",
      "4094  23.505  23.281342\n",
      "4095  25.990  25.824785\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0082\n",
      "Epoch 24/25, Validation Loss: 0.0053\n",
      "      actual  predicted\n",
      "0     28.635  28.601511\n",
      "1     27.205  27.075741\n",
      "2     17.785  17.950370\n",
      "3     19.565  19.663633\n",
      "4     25.185  25.026620\n",
      "...      ...        ...\n",
      "4091  29.535  29.478496\n",
      "4092  22.735  22.701382\n",
      "4093  22.135  22.107765\n",
      "4094  23.505  23.296312\n",
      "4095  25.990  25.779427\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0079\n",
      "Epoch 25/25, Validation Loss: 0.0051\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[0.3937]], device='cuda:0')\n",
      "[25.48]\n",
      "       actual  predicted\n",
      "0      28.635  28.601511\n",
      "1      27.205  27.075741\n",
      "2      17.785  17.950370\n",
      "3      19.565  19.663633\n",
      "4      25.185  25.026620\n",
      "...       ...        ...\n",
      "39163  20.200  20.535541\n",
      "39164  18.485  18.829708\n",
      "39165  22.085  22.034350\n",
      "39166  23.170  23.230224\n",
      "39167  26.155  26.080709\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 0.2580\n",
      "Score (MAE): 0.1384\n",
      "Score (ME): 0.0164\n",
      "Score (MAPE): 0.5655%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     256.071426  806.632236\n",
      "1       4.000003  -18.132357\n",
      "2      60.499998   28.490975\n",
      "3      18.000005  986.543657\n",
      "4      18.000005   99.014785\n",
      "...          ...         ...\n",
      "4091   16.999997  169.996086\n",
      "4092    5.999997  -21.972877\n",
      "4093    9.999996   20.037094\n",
      "4094    8.999999  -26.835996\n",
      "4095  278.000003  393.229669\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8198\n",
      "Epoch 1/25, Validation Loss: 1.0104\n",
      "          actual   predicted\n",
      "0     256.071426  378.834452\n",
      "1       4.000003   14.341283\n",
      "2      60.499998   64.214304\n",
      "3      18.000005 -158.404901\n",
      "4      18.000005   78.244237\n",
      "...          ...         ...\n",
      "4091   16.999997   85.012957\n",
      "4092    5.999997   -7.681615\n",
      "4093    9.999996    1.512660\n",
      "4094    8.999999   35.285454\n",
      "4095  278.000003  290.672220\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.6623\n",
      "Epoch 2/25, Validation Loss: 0.8475\n",
      "          actual   predicted\n",
      "0     256.071426  322.829668\n",
      "1       4.000003   -2.029707\n",
      "2      60.499998  171.360473\n",
      "3      18.000005  921.759603\n",
      "4      18.000005  188.897976\n",
      "...          ...         ...\n",
      "4091   16.999997  160.376616\n",
      "4092    5.999997  -10.424732\n",
      "4093    9.999996   40.209062\n",
      "4094    8.999999   37.092539\n",
      "4095  278.000003  281.717045\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.5315\n",
      "Epoch 3/25, Validation Loss: 0.7627\n",
      "          actual   predicted\n",
      "0     256.071426  198.666278\n",
      "1       4.000003   34.070502\n",
      "2      60.499998  141.681048\n",
      "3      18.000005  404.783619\n",
      "4      18.000005  150.095474\n",
      "...          ...         ...\n",
      "4091   16.999997   81.486607\n",
      "4092    5.999997    7.592670\n",
      "4093    9.999996   43.440486\n",
      "4094    8.999999   32.888684\n",
      "4095  278.000003  208.637365\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.4927\n",
      "Epoch 4/25, Validation Loss: 0.7281\n",
      "          actual   predicted\n",
      "0     256.071426  256.406951\n",
      "1       4.000003  -13.944344\n",
      "2      60.499998  157.656642\n",
      "3      18.000005  579.282522\n",
      "4      18.000005  172.634823\n",
      "...          ...         ...\n",
      "4091   16.999997   79.439270\n",
      "4092    5.999997  -39.595276\n",
      "4093    9.999996    1.537804\n",
      "4094    8.999999  -17.720935\n",
      "4095  278.000003  295.188317\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.4761\n",
      "Epoch 5/25, Validation Loss: 0.7086\n",
      "          actual   predicted\n",
      "0     256.071426  267.493488\n",
      "1       4.000003   10.619801\n",
      "2      60.499998  165.420500\n",
      "3      18.000005  134.712592\n",
      "4      18.000005  254.205502\n",
      "...          ...         ...\n",
      "4091   16.999997  107.408309\n",
      "4092    5.999997  -23.773449\n",
      "4093    9.999996   10.618135\n",
      "4094    8.999999   15.008798\n",
      "4095  278.000003  293.383916\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.4723\n",
      "Epoch 6/25, Validation Loss: 0.7353\n",
      "          actual   predicted\n",
      "0     256.071426  198.936293\n",
      "1       4.000003    1.893490\n",
      "2      60.499998  121.429529\n",
      "3      18.000005  -26.646054\n",
      "4      18.000005  242.464164\n",
      "...          ...         ...\n",
      "4091   16.999997   97.143642\n",
      "4092    5.999997  -36.437923\n",
      "4093    9.999996    1.364837\n",
      "4094    8.999999    0.920055\n",
      "4095  278.000003  263.766764\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.4478\n",
      "Epoch 7/25, Validation Loss: 0.6979\n",
      "          actual   predicted\n",
      "0     256.071426  251.047316\n",
      "1       4.000003   25.280947\n",
      "2      60.499998  150.115434\n",
      "3      18.000005 -139.674366\n",
      "4      18.000005  210.610040\n",
      "...          ...         ...\n",
      "4091   16.999997  127.217031\n",
      "4092    5.999997   -3.328826\n",
      "4093    9.999996   10.133291\n",
      "4094    8.999999   17.662808\n",
      "4095  278.000003  294.412177\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.4393\n",
      "Epoch 8/25, Validation Loss: 0.7372\n",
      "          actual   predicted\n",
      "0     256.071426  277.463465\n",
      "1       4.000003   -4.091525\n",
      "2      60.499998  135.102713\n",
      "3      18.000005  -59.434846\n",
      "4      18.000005  219.708628\n",
      "...          ...         ...\n",
      "4091   16.999997   99.051845\n",
      "4092    5.999997  -33.313787\n",
      "4093    9.999996   -9.840862\n",
      "4094    8.999999   -6.941399\n",
      "4095  278.000003  263.027169\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.4432\n",
      "Epoch 9/25, Validation Loss: 0.6702\n",
      "          actual   predicted\n",
      "0     256.071426  205.758683\n",
      "1       4.000003   28.698294\n",
      "2      60.499998  129.713039\n",
      "3      18.000005 -150.988678\n",
      "4      18.000005  196.138538\n",
      "...          ...         ...\n",
      "4091   16.999997  114.120359\n",
      "4092    5.999997   -1.652625\n",
      "4093    9.999996   -3.590971\n",
      "4094    8.999999   10.653007\n",
      "4095  278.000003  274.340511\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.4198\n",
      "Epoch 10/25, Validation Loss: 0.6950\n",
      "          actual   predicted\n",
      "0     256.071426  209.181010\n",
      "1       4.000003  -14.170127\n",
      "2      60.499998  108.991714\n",
      "3      18.000005 -199.380512\n",
      "4      18.000005  165.779762\n",
      "...          ...         ...\n",
      "4091   16.999997   35.733002\n",
      "4092    5.999997  -42.875711\n",
      "4093    9.999996  -21.712056\n",
      "4094    8.999999  -33.420603\n",
      "4095  278.000003  264.473939\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.4288\n",
      "Epoch 11/25, Validation Loss: 0.6506\n",
      "          actual   predicted\n",
      "0     256.071426  259.759093\n",
      "1       4.000003   26.657483\n",
      "2      60.499998  121.629188\n",
      "3      18.000005 -162.073116\n",
      "4      18.000005  189.834672\n",
      "...          ...         ...\n",
      "4091   16.999997   73.838466\n",
      "4092    5.999997   -3.912967\n",
      "4093    9.999996    5.328864\n",
      "4094    8.999999   15.760870\n",
      "4095  278.000003  316.385166\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.4362\n",
      "Epoch 12/25, Validation Loss: 0.6780\n",
      "          actual   predicted\n",
      "0     256.071426  268.953788\n",
      "1       4.000003   -4.652580\n",
      "2      60.499998  128.746999\n",
      "3      18.000005 -334.202001\n",
      "4      18.000005  223.325880\n",
      "...          ...         ...\n",
      "4091   16.999997   58.908531\n",
      "4092    5.999997  -25.553711\n",
      "4093    9.999996   -9.768588\n",
      "4094    8.999999  -22.577221\n",
      "4095  278.000003  294.702670\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.4044\n",
      "Epoch 13/25, Validation Loss: 0.6414\n",
      "          actual   predicted\n",
      "0     256.071426  252.208854\n",
      "1       4.000003   14.806539\n",
      "2      60.499998  118.741960\n",
      "3      18.000005 -420.710693\n",
      "4      18.000005  195.621866\n",
      "...          ...         ...\n",
      "4091   16.999997   96.735819\n",
      "4092    5.999997    1.433009\n",
      "4093    9.999996   -4.809163\n",
      "4094    8.999999    7.915328\n",
      "4095  278.000003  292.229796\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.3822\n",
      "Epoch 14/25, Validation Loss: 0.6812\n",
      "          actual   predicted\n",
      "0     256.071426  289.127659\n",
      "1       4.000003   -0.139899\n",
      "2      60.499998  119.514376\n",
      "3      18.000005 -342.433575\n",
      "4      18.000005  174.282307\n",
      "...          ...         ...\n",
      "4091   16.999997   74.717225\n",
      "4092    5.999997  -28.622902\n",
      "4093    9.999996  -11.175799\n",
      "4094    8.999999  -18.472357\n",
      "4095  278.000003  304.875604\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.3946\n",
      "Epoch 15/25, Validation Loss: 0.6717\n",
      "          actual   predicted\n",
      "0     256.071426  363.059554\n",
      "1       4.000003  -14.250393\n",
      "2      60.499998  124.700289\n",
      "3      18.000005 -189.836863\n",
      "4      18.000005  204.052291\n",
      "...          ...         ...\n",
      "4091   16.999997   47.544416\n",
      "4092    5.999997  -25.562600\n",
      "4093    9.999996  -18.698897\n",
      "4094    8.999999  -16.435056\n",
      "4095  278.000003  303.092942\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.3696\n",
      "Epoch 16/25, Validation Loss: 0.7256\n",
      "          actual   predicted\n",
      "0     256.071426  328.026521\n",
      "1       4.000003  -17.199481\n",
      "2      60.499998  141.573059\n",
      "3      18.000005 -199.076236\n",
      "4      18.000005  127.060673\n",
      "...          ...         ...\n",
      "4091   16.999997   27.731066\n",
      "4092    5.999997  -15.981857\n",
      "4093    9.999996  -13.090409\n",
      "4094    8.999999  -16.389828\n",
      "4095  278.000003  240.710203\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.4156\n",
      "Epoch 17/25, Validation Loss: 0.6519\n",
      "          actual   predicted\n",
      "0     256.071426  254.026477\n",
      "1       4.000003   19.274217\n",
      "2      60.499998  132.104637\n",
      "3      18.000005 -340.619788\n",
      "4      18.000005  158.076005\n",
      "...          ...         ...\n",
      "4091   16.999997   55.259596\n",
      "4092    5.999997   23.132492\n",
      "4093    9.999996    5.409910\n",
      "4094    8.999999   14.401488\n",
      "4095  278.000003  260.418275\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.3571\n",
      "Epoch 18/25, Validation Loss: 0.6203\n",
      "          actual   predicted\n",
      "0     256.071426  324.536119\n",
      "1       4.000003  -24.677410\n",
      "2      60.499998  130.775546\n",
      "3      18.000005 -182.568712\n",
      "4      18.000005  156.919488\n",
      "...          ...         ...\n",
      "4091   16.999997   38.181726\n",
      "4092    5.999997  -17.680412\n",
      "4093    9.999996  -23.981359\n",
      "4094    8.999999  -21.701393\n",
      "4095  278.000003  277.180243\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.3536\n",
      "Epoch 19/25, Validation Loss: 0.6005\n",
      "          actual   predicted\n",
      "0     256.071426  269.397884\n",
      "1       4.000003    2.543522\n",
      "2      60.499998  133.398224\n",
      "3      18.000005 -269.183747\n",
      "4      18.000005  148.887854\n",
      "...          ...         ...\n",
      "4091   16.999997   38.676358\n",
      "4092    5.999997    5.182424\n",
      "4093    9.999996   -5.302778\n",
      "4094    8.999999    0.370550\n",
      "4095  278.000003  247.554804\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.3940\n",
      "Epoch 20/25, Validation Loss: 0.6046\n",
      "          actual   predicted\n",
      "0     256.071426  261.683839\n",
      "1       4.000003   -3.891890\n",
      "2      60.499998  126.322497\n",
      "3      18.000005 -198.317886\n",
      "4      18.000005  151.888323\n",
      "...          ...         ...\n",
      "4091   16.999997   50.380980\n",
      "4092    5.999997    5.511300\n",
      "4093    9.999996   -4.468879\n",
      "4094    8.999999    1.431260\n",
      "4095  278.000003  254.893072\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.3355\n",
      "Epoch 21/25, Validation Loss: 0.6050\n",
      "          actual   predicted\n",
      "0     256.071426  288.397787\n",
      "1       4.000003   -4.350254\n",
      "2      60.499998  142.286009\n",
      "3      18.000005 -127.407390\n",
      "4      18.000005  163.670908\n",
      "...          ...         ...\n",
      "4091   16.999997   50.611078\n",
      "4092    5.999997   -4.206923\n",
      "4093    9.999996   -0.792094\n",
      "4094    8.999999   -3.408134\n",
      "4095  278.000003  289.389308\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.3278\n",
      "Epoch 22/25, Validation Loss: 0.6496\n",
      "          actual   predicted\n",
      "0     256.071426  329.065374\n",
      "1       4.000003  -10.648328\n",
      "2      60.499998  147.002933\n",
      "3      18.000005 -228.475176\n",
      "4      18.000005  164.089791\n",
      "...          ...         ...\n",
      "4091   16.999997   31.185851\n",
      "4092    5.999997   -1.009556\n",
      "4093    9.999996    0.122507\n",
      "4094    8.999999   -9.055100\n",
      "4095  278.000003  260.754149\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.3511\n",
      "Epoch 23/25, Validation Loss: 0.5942\n",
      "          actual   predicted\n",
      "0     256.071426  296.762159\n",
      "1       4.000003  -10.141344\n",
      "2      60.499998  134.471718\n",
      "3      18.000005  -68.441649\n",
      "4      18.000005  147.386399\n",
      "...          ...         ...\n",
      "4091   16.999997   30.559236\n",
      "4092    5.999997   -0.736558\n",
      "4093    9.999996   -5.669883\n",
      "4094    8.999999  -13.790716\n",
      "4095  278.000003  266.019416\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.3366\n",
      "Epoch 24/25, Validation Loss: 0.6314\n",
      "          actual   predicted\n",
      "0     256.071426  303.233826\n",
      "1       4.000003  -13.396245\n",
      "2      60.499998  129.243924\n",
      "3      18.000005 -230.763794\n",
      "4      18.000005  148.026489\n",
      "...          ...         ...\n",
      "4091   16.999997   23.479383\n",
      "4092    5.999997   -0.628147\n",
      "4093    9.999996   -0.190647\n",
      "4094    8.999999   -6.628411\n",
      "4095  278.000003  250.596913\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.3388\n",
      "Epoch 25/25, Validation Loss: 0.6834\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[-0.1748]], device='cuda:0')\n",
      "[47.39]\n",
      "           actual   predicted\n",
      "0      256.071426  303.233826\n",
      "1        4.000003  -13.396245\n",
      "2       60.499998  129.243924\n",
      "3       18.000005 -230.763794\n",
      "4       18.000005  148.026489\n",
      "...           ...         ...\n",
      "39163    4.000003   23.943481\n",
      "39164   15.000003    0.437126\n",
      "39165   30.500000  -30.236510\n",
      "39166    5.999997   -0.618122\n",
      "39167  251.000001  234.471064\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 632.1242\n",
      "Score (MAE): 78.9470\n",
      "Score (ME): 13.9571\n",
      "Score (MAPE): 154017.7305%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     496.250000  458.301253\n",
      "1     423.000003  498.280723\n",
      "2     455.750000  550.751129\n",
      "3     428.000001  453.293966\n",
      "4     367.499997  430.485918\n",
      "...          ...         ...\n",
      "4091  411.928568  458.741899\n",
      "4092  485.750000  470.116552\n",
      "4093  412.250001  439.552326\n",
      "4094  471.000000  492.433020\n",
      "4095  421.499998  475.579879\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8025\n",
      "Epoch 1/25, Validation Loss: 0.6739\n",
      "          actual   predicted\n",
      "0     496.250000  484.475034\n",
      "1     423.000003  494.687780\n",
      "2     455.750000  522.104663\n",
      "3     428.000001  460.966463\n",
      "4     367.499997  416.704328\n",
      "...          ...         ...\n",
      "4091  411.928568  451.413250\n",
      "4092  485.750000  512.653706\n",
      "4093  412.250001  405.187254\n",
      "4094  471.000000  445.533884\n",
      "4095  421.499998  404.535103\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.5839\n",
      "Epoch 2/25, Validation Loss: 0.5460\n",
      "          actual   predicted\n",
      "0     496.250000  456.988802\n",
      "1     423.000003  519.417943\n",
      "2     455.750000  520.974615\n",
      "3     428.000001  450.551702\n",
      "4     367.499997  420.201167\n",
      "...          ...         ...\n",
      "4091  411.928568  429.293117\n",
      "4092  485.750000  505.929922\n",
      "4093  412.250001  426.445518\n",
      "4094  471.000000  453.310960\n",
      "4095  421.499998  423.311111\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.4532\n",
      "Epoch 3/25, Validation Loss: 0.3882\n",
      "          actual   predicted\n",
      "0     496.250000  443.793729\n",
      "1     423.000003  487.231903\n",
      "2     455.750000  450.497739\n",
      "3     428.000001  443.469217\n",
      "4     367.499997  394.658478\n",
      "...          ...         ...\n",
      "4091  411.928568  415.282634\n",
      "4092  485.750000  475.226461\n",
      "4093  412.250001  406.324902\n",
      "4094  471.000000  469.931363\n",
      "4095  421.499998  402.953114\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.3163\n",
      "Epoch 4/25, Validation Loss: 0.2404\n",
      "          actual   predicted\n",
      "0     496.250000  447.704629\n",
      "1     423.000003  458.580511\n",
      "2     455.750000  442.882208\n",
      "3     428.000001  442.174022\n",
      "4     367.499997  391.012475\n",
      "...          ...         ...\n",
      "4091  411.928568  418.509984\n",
      "4092  485.750000  465.065236\n",
      "4093  412.250001  401.819930\n",
      "4094  471.000000  468.915961\n",
      "4095  421.499998  403.757968\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.2209\n",
      "Epoch 5/25, Validation Loss: 0.1745\n",
      "          actual   predicted\n",
      "0     496.250000  437.243164\n",
      "1     423.000003  453.688173\n",
      "2     455.750000  454.693104\n",
      "3     428.000001  442.880614\n",
      "4     367.499997  387.594858\n",
      "...          ...         ...\n",
      "4091  411.928568  422.347411\n",
      "4092  485.750000  470.448150\n",
      "4093  412.250001  419.391021\n",
      "4094  471.000000  468.373364\n",
      "4095  421.499998  412.622100\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1841\n",
      "Epoch 6/25, Validation Loss: 0.1522\n",
      "          actual   predicted\n",
      "0     496.250000  441.372479\n",
      "1     423.000003  448.693136\n",
      "2     455.750000  446.205860\n",
      "3     428.000001  443.414187\n",
      "4     367.499997  381.723017\n",
      "...          ...         ...\n",
      "4091  411.928568  426.911023\n",
      "4092  485.750000  471.740130\n",
      "4093  412.250001  415.156441\n",
      "4094  471.000000  468.999163\n",
      "4095  421.499998  412.392207\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.1656\n",
      "Epoch 7/25, Validation Loss: 0.1460\n",
      "          actual   predicted\n",
      "0     496.250000  435.868650\n",
      "1     423.000003  449.529444\n",
      "2     455.750000  453.313334\n",
      "3     428.000001  443.848143\n",
      "4     367.499997  381.479500\n",
      "...          ...         ...\n",
      "4091  411.928568  418.359857\n",
      "4092  485.750000  476.640706\n",
      "4093  412.250001  405.464701\n",
      "4094  471.000000  476.528516\n",
      "4095  421.499998  420.788587\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1607\n",
      "Epoch 8/25, Validation Loss: 0.1427\n",
      "          actual   predicted\n",
      "0     496.250000  436.773784\n",
      "1     423.000003  449.656014\n",
      "2     455.750000  461.330919\n",
      "3     428.000001  449.962479\n",
      "4     367.499997  390.399352\n",
      "...          ...         ...\n",
      "4091  411.928568  421.197826\n",
      "4092  485.750000  480.839399\n",
      "4093  412.250001  413.457225\n",
      "4094  471.000000  474.944680\n",
      "4095  421.499998  424.545725\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1534\n",
      "Epoch 9/25, Validation Loss: 0.1370\n",
      "          actual   predicted\n",
      "0     496.250000  434.731002\n",
      "1     423.000003  450.826961\n",
      "2     455.750000  450.884313\n",
      "3     428.000001  445.511126\n",
      "4     367.499997  385.568073\n",
      "...          ...         ...\n",
      "4091  411.928568  420.863740\n",
      "4092  485.750000  476.797392\n",
      "4093  412.250001  414.695557\n",
      "4094  471.000000  477.683080\n",
      "4095  421.499998  424.617444\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.1489\n",
      "Epoch 10/25, Validation Loss: 0.1340\n",
      "          actual   predicted\n",
      "0     496.250000  433.230329\n",
      "1     423.000003  443.132242\n",
      "2     455.750000  452.808066\n",
      "3     428.000001  442.702504\n",
      "4     367.499997  390.467981\n",
      "...          ...         ...\n",
      "4091  411.928568  422.499598\n",
      "4092  485.750000  479.023924\n",
      "4093  412.250001  414.325242\n",
      "4094  471.000000  471.408028\n",
      "4095  421.499998  417.081825\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.1463\n",
      "Epoch 11/25, Validation Loss: 0.1350\n",
      "          actual   predicted\n",
      "0     496.250000  433.939608\n",
      "1     423.000003  450.740349\n",
      "2     455.750000  446.599354\n",
      "3     428.000001  440.656877\n",
      "4     367.499997  384.181100\n",
      "...          ...         ...\n",
      "4091  411.928568  426.259700\n",
      "4092  485.750000  479.571199\n",
      "4093  412.250001  416.721931\n",
      "4094  471.000000  470.935082\n",
      "4095  421.499998  414.823654\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1438\n",
      "Epoch 12/25, Validation Loss: 0.1307\n",
      "          actual   predicted\n",
      "0     496.250000  427.756349\n",
      "1     423.000003  438.444901\n",
      "2     455.750000  459.935434\n",
      "3     428.000001  441.750599\n",
      "4     367.499997  385.034217\n",
      "...          ...         ...\n",
      "4091  411.928568  414.280779\n",
      "4092  485.750000  477.515739\n",
      "4093  412.250001  408.417590\n",
      "4094  471.000000  474.134253\n",
      "4095  421.499998  416.676848\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1432\n",
      "Epoch 13/25, Validation Loss: 0.1291\n",
      "          actual   predicted\n",
      "0     496.250000  431.764242\n",
      "1     423.000003  451.344477\n",
      "2     455.750000  462.703290\n",
      "3     428.000001  447.127358\n",
      "4     367.499997  383.477043\n",
      "...          ...         ...\n",
      "4091  411.928568  420.044053\n",
      "4092  485.750000  482.126495\n",
      "4093  412.250001  416.384031\n",
      "4094  471.000000  477.036058\n",
      "4095  421.499998  427.754173\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.1398\n",
      "Epoch 14/25, Validation Loss: 0.1308\n",
      "          actual   predicted\n",
      "0     496.250000  428.966653\n",
      "1     423.000003  445.063266\n",
      "2     455.750000  455.401644\n",
      "3     428.000001  441.524080\n",
      "4     367.499997  386.769206\n",
      "...          ...         ...\n",
      "4091  411.928568  421.429033\n",
      "4092  485.750000  476.405940\n",
      "4093  412.250001  413.691784\n",
      "4094  471.000000  468.797261\n",
      "4095  421.499998  423.416647\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.1372\n",
      "Epoch 15/25, Validation Loss: 0.1278\n",
      "          actual   predicted\n",
      "0     496.250000  427.734342\n",
      "1     423.000003  443.225562\n",
      "2     455.750000  451.771412\n",
      "3     428.000001  440.202100\n",
      "4     367.499997  385.569096\n",
      "...          ...         ...\n",
      "4091  411.928568  421.171735\n",
      "4092  485.750000  472.831685\n",
      "4093  412.250001  415.101616\n",
      "4094  471.000000  468.135489\n",
      "4095  421.499998  419.301131\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.1375\n",
      "Epoch 16/25, Validation Loss: 0.1279\n",
      "          actual   predicted\n",
      "0     496.250000  425.931310\n",
      "1     423.000003  440.784346\n",
      "2     455.750000  462.970058\n",
      "3     428.000001  440.768251\n",
      "4     367.499997  384.627650\n",
      "...          ...         ...\n",
      "4091  411.928568  417.074673\n",
      "4092  485.750000  477.490259\n",
      "4093  412.250001  412.349222\n",
      "4094  471.000000  471.255296\n",
      "4095  421.499998  412.891357\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.1346\n",
      "Epoch 17/25, Validation Loss: 0.1258\n",
      "          actual   predicted\n",
      "0     496.250000  429.365971\n",
      "1     423.000003  445.417855\n",
      "2     455.750000  461.675709\n",
      "3     428.000001  439.104003\n",
      "4     367.499997  382.954862\n",
      "...          ...         ...\n",
      "4091  411.928568  424.741039\n",
      "4092  485.750000  477.117236\n",
      "4093  412.250001  416.947405\n",
      "4094  471.000000  472.597056\n",
      "4095  421.499998  420.510334\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.1312\n",
      "Epoch 18/25, Validation Loss: 0.1247\n",
      "          actual   predicted\n",
      "0     496.250000  425.943344\n",
      "1     423.000003  442.415307\n",
      "2     455.750000  466.385847\n",
      "3     428.000001  439.951050\n",
      "4     367.499997  380.269864\n",
      "...          ...         ...\n",
      "4091  411.928568  415.300842\n",
      "4092  485.750000  476.677919\n",
      "4093  412.250001  410.397336\n",
      "4094  471.000000  469.574569\n",
      "4095  421.499998  413.122094\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.1309\n",
      "Epoch 19/25, Validation Loss: 0.1247\n",
      "          actual   predicted\n",
      "0     496.250000  422.308039\n",
      "1     423.000003  441.264214\n",
      "2     455.750000  466.338373\n",
      "3     428.000001  439.557152\n",
      "4     367.499997  381.980740\n",
      "...          ...         ...\n",
      "4091  411.928568  416.496017\n",
      "4092  485.750000  472.712864\n",
      "4093  412.250001  412.143583\n",
      "4094  471.000000  470.777399\n",
      "4095  421.499998  422.122079\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.1292\n",
      "Epoch 20/25, Validation Loss: 0.1252\n",
      "          actual   predicted\n",
      "0     496.250000  423.343973\n",
      "1     423.000003  439.180339\n",
      "2     455.750000  463.810988\n",
      "3     428.000001  441.760024\n",
      "4     367.499997  386.070785\n",
      "...          ...         ...\n",
      "4091  411.928568  414.493368\n",
      "4092  485.750000  477.332568\n",
      "4093  412.250001  414.979992\n",
      "4094  471.000000  468.958022\n",
      "4095  421.499998  426.061049\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.1295\n",
      "Epoch 21/25, Validation Loss: 0.1259\n",
      "          actual   predicted\n",
      "0     496.250000  423.745940\n",
      "1     423.000003  441.865392\n",
      "2     455.750000  467.585692\n",
      "3     428.000001  439.187121\n",
      "4     367.499997  382.713517\n",
      "...          ...         ...\n",
      "4091  411.928568  411.619185\n",
      "4092  485.750000  477.884632\n",
      "4093  412.250001  412.102076\n",
      "4094  471.000000  465.685289\n",
      "4095  421.499998  416.358917\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.1314\n",
      "Epoch 22/25, Validation Loss: 0.1231\n",
      "          actual   predicted\n",
      "0     496.250000  420.351771\n",
      "1     423.000003  440.567760\n",
      "2     455.750000  467.917400\n",
      "3     428.000001  436.058469\n",
      "4     367.499997  378.635387\n",
      "...          ...         ...\n",
      "4091  411.928568  415.127849\n",
      "4092  485.750000  477.849079\n",
      "4093  412.250001  414.620121\n",
      "4094  471.000000  466.783219\n",
      "4095  421.499998  417.766375\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.1266\n",
      "Epoch 23/25, Validation Loss: 0.1218\n",
      "          actual   predicted\n",
      "0     496.250000  422.910569\n",
      "1     423.000003  439.486639\n",
      "2     455.750000  467.289714\n",
      "3     428.000001  438.117369\n",
      "4     367.499997  383.359629\n",
      "...          ...         ...\n",
      "4091  411.928568  417.027052\n",
      "4092  485.750000  478.797186\n",
      "4093  412.250001  409.893102\n",
      "4094  471.000000  469.103423\n",
      "4095  421.499998  420.822091\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.1261\n",
      "Epoch 24/25, Validation Loss: 0.1220\n",
      "          actual   predicted\n",
      "0     496.250000  422.856767\n",
      "1     423.000003  440.446393\n",
      "2     455.750000  462.925020\n",
      "3     428.000001  437.112704\n",
      "4     367.499997  378.630340\n",
      "...          ...         ...\n",
      "4091  411.928568  415.517730\n",
      "4092  485.750000  477.447341\n",
      "4093  412.250001  414.766246\n",
      "4094  471.000000  467.051740\n",
      "4095  421.499998  413.604814\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.1260\n",
      "Epoch 25/25, Validation Loss: 0.1210\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.6661]], device='cuda:0')\n",
      "[572.79]\n",
      "           actual   predicted\n",
      "0      496.250000  422.856767\n",
      "1      423.000003  440.446393\n",
      "2      455.750000  462.925020\n",
      "3      428.000001  437.112704\n",
      "4      367.499997  378.630340\n",
      "...           ...         ...\n",
      "19155  451.500001  449.102390\n",
      "19156  567.714289  828.383661\n",
      "19157  439.000000  440.331189\n",
      "19158  483.333333  462.037493\n",
      "19159  585.666668  575.904874\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 43.4488\n",
      "Score (MAE): 16.4013\n",
      "Score (ME): 1.4362\n",
      "Score (MAPE): 2.8605%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual   predicted\n",
      "0      916.499996  683.714683\n",
      "1      587.041658  650.883760\n",
      "2      619.250003  672.572882\n",
      "3      698.499999  692.880140\n",
      "4      615.999997  658.666311\n",
      "...           ...         ...\n",
      "4091   654.250002  665.667678\n",
      "4092   762.250001  706.483361\n",
      "4093   975.000000  899.701152\n",
      "4094  1049.750006  925.396422\n",
      "4095  1081.361119  938.231958\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8152\n",
      "Epoch 1/25, Validation Loss: 0.5172\n",
      "           actual    predicted\n",
      "0      916.499996   877.232366\n",
      "1      587.041658   648.359591\n",
      "2      619.250003   587.738015\n",
      "3      698.499999   686.186336\n",
      "4      615.999997   616.880949\n",
      "...           ...          ...\n",
      "4091   654.250002   730.325582\n",
      "4092   762.250001   745.748972\n",
      "4093   975.000000  1090.612524\n",
      "4094  1049.750006  1061.622898\n",
      "4095  1081.361119  1000.070834\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.4715\n",
      "Epoch 2/25, Validation Loss: 0.4036\n",
      "           actual    predicted\n",
      "0      916.499996   862.801313\n",
      "1      587.041658   642.812269\n",
      "2      619.250003   575.442757\n",
      "3      698.499999   662.232061\n",
      "4      615.999997   603.268025\n",
      "...           ...          ...\n",
      "4091   654.250002   748.041837\n",
      "4092   762.250001   780.216861\n",
      "4093   975.000000  1070.429233\n",
      "4094  1049.750006  1053.648195\n",
      "4095  1081.361119   897.032353\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.3682\n",
      "Epoch 3/25, Validation Loss: 0.3078\n",
      "           actual    predicted\n",
      "0      916.499996   805.513247\n",
      "1      587.041658   655.861394\n",
      "2      619.250003   579.358545\n",
      "3      698.499999   638.080207\n",
      "4      615.999997   613.402050\n",
      "...           ...          ...\n",
      "4091   654.250002   732.851957\n",
      "4092   762.250001   775.908174\n",
      "4093   975.000000  1168.690588\n",
      "4094  1049.750006  1012.750690\n",
      "4095  1081.361119   913.800848\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.2706\n",
      "Epoch 4/25, Validation Loss: 0.2260\n",
      "           actual    predicted\n",
      "0      916.499996   758.850772\n",
      "1      587.041658   648.046079\n",
      "2      619.250003   568.338079\n",
      "3      698.499999   618.008339\n",
      "4      615.999997   603.133114\n",
      "...           ...          ...\n",
      "4091   654.250002   756.561223\n",
      "4092   762.250001   786.914670\n",
      "4093   975.000000  1135.043320\n",
      "4094  1049.750006   966.900549\n",
      "4095  1081.361119   917.172162\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.2041\n",
      "Epoch 5/25, Validation Loss: 0.1650\n",
      "           actual    predicted\n",
      "0      916.499996   764.598576\n",
      "1      587.041658   668.631593\n",
      "2      619.250003   602.498116\n",
      "3      698.499999   625.685877\n",
      "4      615.999997   629.231276\n",
      "...           ...          ...\n",
      "4091   654.250002   755.271278\n",
      "4092   762.250001   787.746289\n",
      "4093   975.000000  1095.548083\n",
      "4094  1049.750006   971.570160\n",
      "4095  1081.361119   928.442811\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1644\n",
      "Epoch 6/25, Validation Loss: 0.1312\n",
      "           actual    predicted\n",
      "0      916.499996   763.965557\n",
      "1      587.041658   637.918300\n",
      "2      619.250003   585.331541\n",
      "3      698.499999   617.284275\n",
      "4      615.999997   607.373046\n",
      "...           ...          ...\n",
      "4091   654.250002   740.823585\n",
      "4092   762.250001   763.547327\n",
      "4093   975.000000  1053.077890\n",
      "4094  1049.750006   964.465238\n",
      "4095  1081.361119   901.939352\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.1427\n",
      "Epoch 7/25, Validation Loss: 0.1211\n",
      "           actual    predicted\n",
      "0      916.499996   767.438027\n",
      "1      587.041658   627.235702\n",
      "2      619.250003   599.365336\n",
      "3      698.499999   635.752480\n",
      "4      615.999997   619.699660\n",
      "...           ...          ...\n",
      "4091   654.250002   734.562743\n",
      "4092   762.250001   763.917669\n",
      "4093   975.000000  1058.529466\n",
      "4094  1049.750006   991.174355\n",
      "4095  1081.361119   924.302125\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1328\n",
      "Epoch 8/25, Validation Loss: 0.1127\n",
      "           actual    predicted\n",
      "0      916.499996   759.234865\n",
      "1      587.041658   635.097261\n",
      "2      619.250003   602.915061\n",
      "3      698.499999   629.402425\n",
      "4      615.999997   612.830094\n",
      "...           ...          ...\n",
      "4091   654.250002   740.196299\n",
      "4092   762.250001   751.323477\n",
      "4093   975.000000  1039.577606\n",
      "4094  1049.750006   991.990075\n",
      "4095  1081.361119   922.681301\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1254\n",
      "Epoch 9/25, Validation Loss: 0.1074\n",
      "           actual    predicted\n",
      "0      916.499996   771.695087\n",
      "1      587.041658   629.744427\n",
      "2      619.250003   597.818290\n",
      "3      698.499999   637.564762\n",
      "4      615.999997   608.559397\n",
      "...           ...          ...\n",
      "4091   654.250002   736.070858\n",
      "4092   762.250001   753.825900\n",
      "4093   975.000000  1064.661591\n",
      "4094  1049.750006  1002.950955\n",
      "4095  1081.361119   924.973953\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.1184\n",
      "Epoch 10/25, Validation Loss: 0.1041\n",
      "           actual    predicted\n",
      "0      916.499996   774.015620\n",
      "1      587.041658   625.588732\n",
      "2      619.250003   604.594985\n",
      "3      698.499999   640.894401\n",
      "4      615.999997   610.321456\n",
      "...           ...          ...\n",
      "4091   654.250002   738.975382\n",
      "4092   762.250001   751.223605\n",
      "4093   975.000000  1064.631224\n",
      "4094  1049.750006  1024.105792\n",
      "4095  1081.361119   926.953301\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.1154\n",
      "Epoch 11/25, Validation Loss: 0.1015\n",
      "           actual    predicted\n",
      "0      916.499996   785.406945\n",
      "1      587.041658   627.635553\n",
      "2      619.250003   609.576493\n",
      "3      698.499999   642.776344\n",
      "4      615.999997   611.607922\n",
      "...           ...          ...\n",
      "4091   654.250002   748.138064\n",
      "4092   762.250001   755.514667\n",
      "4093   975.000000  1086.343043\n",
      "4094  1049.750006  1039.320640\n",
      "4095  1081.361119   956.195994\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1131\n",
      "Epoch 12/25, Validation Loss: 0.1001\n",
      "           actual    predicted\n",
      "0      916.499996   785.510811\n",
      "1      587.041658   621.169209\n",
      "2      619.250003   600.362802\n",
      "3      698.499999   646.976337\n",
      "4      615.999997   606.288877\n",
      "...           ...          ...\n",
      "4091   654.250002   731.672463\n",
      "4092   762.250001   752.964648\n",
      "4093   975.000000  1064.194127\n",
      "4094  1049.750006  1021.795212\n",
      "4095  1081.361119   928.144733\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1093\n",
      "Epoch 13/25, Validation Loss: 0.0990\n",
      "           actual    predicted\n",
      "0      916.499996   795.027215\n",
      "1      587.041658   622.556329\n",
      "2      619.250003   604.270251\n",
      "3      698.499999   653.621622\n",
      "4      615.999997   611.869249\n",
      "...           ...          ...\n",
      "4091   654.250002   746.709344\n",
      "4092   762.250001   757.695141\n",
      "4093   975.000000  1061.355334\n",
      "4094  1049.750006  1038.393392\n",
      "4095  1081.361119   933.988951\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.1061\n",
      "Epoch 14/25, Validation Loss: 0.0967\n",
      "           actual    predicted\n",
      "0      916.499996   808.819699\n",
      "1      587.041658   619.617681\n",
      "2      619.250003   603.223344\n",
      "3      698.499999   650.714209\n",
      "4      615.999997   610.878229\n",
      "...           ...          ...\n",
      "4091   654.250002   739.823292\n",
      "4092   762.250001   760.461977\n",
      "4093   975.000000  1056.609009\n",
      "4094  1049.750006  1042.724770\n",
      "4095  1081.361119   934.985487\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.1041\n",
      "Epoch 15/25, Validation Loss: 0.0951\n",
      "           actual    predicted\n",
      "0      916.499996   800.796190\n",
      "1      587.041658   619.271249\n",
      "2      619.250003   610.490468\n",
      "3      698.499999   654.272629\n",
      "4      615.999997   612.679983\n",
      "...           ...          ...\n",
      "4091   654.250002   726.295917\n",
      "4092   762.250001   746.236999\n",
      "4093   975.000000  1046.373654\n",
      "4094  1049.750006  1019.771861\n",
      "4095  1081.361119   921.790352\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.1028\n",
      "Epoch 16/25, Validation Loss: 0.0946\n",
      "           actual    predicted\n",
      "0      916.499996   809.112016\n",
      "1      587.041658   615.444702\n",
      "2      619.250003   609.064800\n",
      "3      698.499999   650.093048\n",
      "4      615.999997   609.715886\n",
      "...           ...          ...\n",
      "4091   654.250002   734.378630\n",
      "4092   762.250001   756.134584\n",
      "4093   975.000000  1037.997258\n",
      "4094  1049.750006  1030.160437\n",
      "4095  1081.361119   930.910442\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.1005\n",
      "Epoch 17/25, Validation Loss: 0.0935\n",
      "           actual    predicted\n",
      "0      916.499996   815.169426\n",
      "1      587.041658   619.757422\n",
      "2      619.250003   609.752124\n",
      "3      698.499999   652.157770\n",
      "4      615.999997   606.842852\n",
      "...           ...          ...\n",
      "4091   654.250002   729.922578\n",
      "4092   762.250001   750.758092\n",
      "4093   975.000000  1060.009144\n",
      "4094  1049.750006  1042.354242\n",
      "4095  1081.361119   948.550091\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0989\n",
      "Epoch 18/25, Validation Loss: 0.0940\n",
      "           actual    predicted\n",
      "0      916.499996   810.076224\n",
      "1      587.041658   621.565847\n",
      "2      619.250003   608.153604\n",
      "3      698.499999   653.611902\n",
      "4      615.999997   605.775430\n",
      "...           ...          ...\n",
      "4091   654.250002   721.323395\n",
      "4092   762.250001   746.344480\n",
      "4093   975.000000  1038.838984\n",
      "4094  1049.750006  1021.398749\n",
      "4095  1081.361119   925.351872\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0975\n",
      "Epoch 19/25, Validation Loss: 0.0929\n",
      "           actual    predicted\n",
      "0      916.499996   816.025653\n",
      "1      587.041658   615.640901\n",
      "2      619.250003   610.494985\n",
      "3      698.499999   654.808954\n",
      "4      615.999997   607.827028\n",
      "...           ...          ...\n",
      "4091   654.250002   720.173495\n",
      "4092   762.250001   746.845627\n",
      "4093   975.000000  1019.434931\n",
      "4094  1049.750006  1022.785677\n",
      "4095  1081.361119   924.598057\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0965\n",
      "Epoch 20/25, Validation Loss: 0.0923\n",
      "           actual    predicted\n",
      "0      916.499996   840.801892\n",
      "1      587.041658   614.843057\n",
      "2      619.250003   599.340512\n",
      "3      698.499999   655.322306\n",
      "4      615.999997   598.214563\n",
      "...           ...          ...\n",
      "4091   654.250002   735.620406\n",
      "4092   762.250001   758.991668\n",
      "4093   975.000000  1069.227716\n",
      "4094  1049.750006  1058.316816\n",
      "4095  1081.361119   953.046346\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0959\n",
      "Epoch 21/25, Validation Loss: 0.0922\n",
      "           actual    predicted\n",
      "0      916.499996   831.861072\n",
      "1      587.041658   618.542841\n",
      "2      619.250003   608.453028\n",
      "3      698.499999   655.203672\n",
      "4      615.999997   606.325115\n",
      "...           ...          ...\n",
      "4091   654.250002   725.525278\n",
      "4092   762.250001   751.483994\n",
      "4093   975.000000  1044.481061\n",
      "4094  1049.750006  1031.289774\n",
      "4095  1081.361119   926.355800\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0952\n",
      "Epoch 22/25, Validation Loss: 0.0914\n",
      "           actual    predicted\n",
      "0      916.499996   839.563004\n",
      "1      587.041658   620.737202\n",
      "2      619.250003   615.372173\n",
      "3      698.499999   663.996909\n",
      "4      615.999997   614.431619\n",
      "...           ...          ...\n",
      "4091   654.250002   727.416094\n",
      "4092   762.250001   753.482677\n",
      "4093   975.000000  1043.230712\n",
      "4094  1049.750006  1039.722784\n",
      "4095  1081.361119   942.922067\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0937\n",
      "Epoch 23/25, Validation Loss: 0.0910\n",
      "           actual    predicted\n",
      "0      916.499996   824.897582\n",
      "1      587.041658   609.179473\n",
      "2      619.250003   610.593119\n",
      "3      698.499999   656.424064\n",
      "4      615.999997   604.526331\n",
      "...           ...          ...\n",
      "4091   654.250002   712.635932\n",
      "4092   762.250001   743.086852\n",
      "4093   975.000000  1031.324014\n",
      "4094  1049.750006  1047.421411\n",
      "4095  1081.361119   932.271929\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0932\n",
      "Epoch 24/25, Validation Loss: 0.0911\n",
      "           actual    predicted\n",
      "0      916.499996   841.697510\n",
      "1      587.041658   616.840003\n",
      "2      619.250003   607.565528\n",
      "3      698.499999   654.245624\n",
      "4      615.999997   604.249317\n",
      "...           ...          ...\n",
      "4091   654.250002   724.358096\n",
      "4092   762.250001   747.269599\n",
      "4093   975.000000  1009.069859\n",
      "4094  1049.750006  1021.756334\n",
      "4095  1081.361119   929.657739\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0923\n",
      "Epoch 25/25, Validation Loss: 0.0916\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.4302]], device='cuda:0')\n",
      "[665.67]\n",
      "            actual    predicted\n",
      "0       916.499996   841.697510\n",
      "1       587.041658   616.840003\n",
      "2       619.250003   607.565528\n",
      "3       698.499999   654.245624\n",
      "4       615.999997   604.249317\n",
      "...            ...          ...\n",
      "19155  1763.499987  1655.849189\n",
      "19156   540.750008   564.363418\n",
      "19157   588.250002   600.674959\n",
      "19158   842.000000   798.352271\n",
      "19159   609.000003   661.069962\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 87.8695\n",
      "Score (MAE): 41.3346\n",
      "Score (ME): 8.2683\n",
      "Score (MAPE): 4.7821%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       actual  predicted\n",
      "0     44.1400  41.331316\n",
      "1     34.9300  31.946779\n",
      "2     44.3925  39.095021\n",
      "3     41.5500  44.812517\n",
      "4     27.9475  30.061359\n",
      "...       ...        ...\n",
      "4091  41.8425  34.933179\n",
      "4092  26.0300  31.344147\n",
      "4093  27.8700  31.530480\n",
      "4094  43.0450  35.907463\n",
      "4095  39.5425  30.886384\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.7650\n",
      "Epoch 1/25, Validation Loss: 0.3183\n",
      "       actual  predicted\n",
      "0     44.1400  44.365680\n",
      "1     34.9300  33.738823\n",
      "2     44.3925  47.412698\n",
      "3     41.5500  37.874104\n",
      "4     27.9475  25.000972\n",
      "...       ...        ...\n",
      "4091  41.8425  42.998129\n",
      "4092  26.0300  26.715606\n",
      "4093  27.8700  28.104729\n",
      "4094  43.0450  42.628159\n",
      "4095  39.5425  31.148584\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2155\n",
      "Epoch 2/25, Validation Loss: 0.1434\n",
      "       actual  predicted\n",
      "0     44.1400  44.261567\n",
      "1     34.9300  34.335671\n",
      "2     44.3925  45.066007\n",
      "3     41.5500  40.044749\n",
      "4     27.9475  27.660601\n",
      "...       ...        ...\n",
      "4091  41.8425  42.258629\n",
      "4092  26.0300  26.821610\n",
      "4093  27.8700  29.230217\n",
      "4094  43.0450  42.427348\n",
      "4095  39.5425  31.781316\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1218\n",
      "Epoch 3/25, Validation Loss: 0.1007\n",
      "       actual  predicted\n",
      "0     44.1400  44.968914\n",
      "1     34.9300  34.154144\n",
      "2     44.3925  45.343045\n",
      "3     41.5500  39.712737\n",
      "4     27.9475  27.894258\n",
      "...       ...        ...\n",
      "4091  41.8425  41.893450\n",
      "4092  26.0300  26.697207\n",
      "4093  27.8700  28.489755\n",
      "4094  43.0450  42.787546\n",
      "4095  39.5425  32.228692\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0968\n",
      "Epoch 4/25, Validation Loss: 0.0853\n",
      "       actual  predicted\n",
      "0     44.1400  44.093493\n",
      "1     34.9300  34.055104\n",
      "2     44.3925  45.523237\n",
      "3     41.5500  39.654927\n",
      "4     27.9475  29.035139\n",
      "...       ...        ...\n",
      "4091  41.8425  40.252492\n",
      "4092  26.0300  26.832705\n",
      "4093  27.8700  27.975137\n",
      "4094  43.0450  43.224703\n",
      "4095  39.5425  33.885636\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0829\n",
      "Epoch 5/25, Validation Loss: 0.0720\n",
      "       actual  predicted\n",
      "0     44.1400  43.573561\n",
      "1     34.9300  34.255082\n",
      "2     44.3925  46.135885\n",
      "3     41.5500  39.498416\n",
      "4     27.9475  28.610619\n",
      "...       ...        ...\n",
      "4091  41.8425  39.668227\n",
      "4092  26.0300  26.069809\n",
      "4093  27.8700  27.399661\n",
      "4094  43.0450  43.609317\n",
      "4095  39.5425  34.809874\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0709\n",
      "Epoch 6/25, Validation Loss: 0.0605\n",
      "       actual  predicted\n",
      "0     44.1400  43.152981\n",
      "1     34.9300  34.892257\n",
      "2     44.3925  46.889598\n",
      "3     41.5500  39.363132\n",
      "4     27.9475  28.883698\n",
      "...       ...        ...\n",
      "4091  41.8425  39.808005\n",
      "4092  26.0300  26.171823\n",
      "4093  27.8700  27.348971\n",
      "4094  43.0450  43.837061\n",
      "4095  39.5425  35.638034\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0618\n",
      "Epoch 7/25, Validation Loss: 0.0518\n",
      "       actual  predicted\n",
      "0     44.1400  42.360244\n",
      "1     34.9300  34.863545\n",
      "2     44.3925  45.997257\n",
      "3     41.5500  39.355030\n",
      "4     27.9475  28.802089\n",
      "...       ...        ...\n",
      "4091  41.8425  39.722932\n",
      "4092  26.0300  26.090941\n",
      "4093  27.8700  27.193340\n",
      "4094  43.0450  43.285115\n",
      "4095  39.5425  35.862456\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0537\n",
      "Epoch 8/25, Validation Loss: 0.0452\n",
      "       actual  predicted\n",
      "0     44.1400  42.673859\n",
      "1     34.9300  35.199622\n",
      "2     44.3925  45.726499\n",
      "3     41.5500  39.961060\n",
      "4     27.9475  28.583223\n",
      "...       ...        ...\n",
      "4091  41.8425  40.167300\n",
      "4092  26.0300  26.418556\n",
      "4093  27.8700  27.394858\n",
      "4094  43.0450  43.481436\n",
      "4095  39.5425  35.916147\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0483\n",
      "Epoch 9/25, Validation Loss: 0.0385\n",
      "       actual  predicted\n",
      "0     44.1400  42.660338\n",
      "1     34.9300  35.255301\n",
      "2     44.3925  45.011565\n",
      "3     41.5500  40.322812\n",
      "4     27.9475  28.162999\n",
      "...       ...        ...\n",
      "4091  41.8425  40.078275\n",
      "4092  26.0300  26.011503\n",
      "4093  27.8700  27.208372\n",
      "4094  43.0450  43.268962\n",
      "4095  39.5425  35.991300\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0434\n",
      "Epoch 10/25, Validation Loss: 0.0336\n",
      "       actual  predicted\n",
      "0     44.1400  43.254348\n",
      "1     34.9300  35.445065\n",
      "2     44.3925  45.043064\n",
      "3     41.5500  40.793170\n",
      "4     27.9475  27.875338\n",
      "...       ...        ...\n",
      "4091  41.8425  40.468601\n",
      "4092  26.0300  25.783319\n",
      "4093  27.8700  27.080257\n",
      "4094  43.0450  43.322373\n",
      "4095  39.5425  36.507357\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0399\n",
      "Epoch 11/25, Validation Loss: 0.0304\n",
      "       actual  predicted\n",
      "0     44.1400  43.368386\n",
      "1     34.9300  35.383886\n",
      "2     44.3925  44.768134\n",
      "3     41.5500  40.920922\n",
      "4     27.9475  27.807765\n",
      "...       ...        ...\n",
      "4091  41.8425  40.607309\n",
      "4092  26.0300  25.945776\n",
      "4093  27.8700  27.150655\n",
      "4094  43.0450  43.208411\n",
      "4095  39.5425  36.824032\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0367\n",
      "Epoch 12/25, Validation Loss: 0.0268\n",
      "       actual  predicted\n",
      "0     44.1400  43.198208\n",
      "1     34.9300  35.207617\n",
      "2     44.3925  44.324409\n",
      "3     41.5500  40.778512\n",
      "4     27.9475  27.734883\n",
      "...       ...        ...\n",
      "4091  41.8425  40.572652\n",
      "4092  26.0300  25.915714\n",
      "4093  27.8700  27.180996\n",
      "4094  43.0450  43.192723\n",
      "4095  39.5425  37.051871\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0338\n",
      "Epoch 13/25, Validation Loss: 0.0244\n",
      "       actual  predicted\n",
      "0     44.1400  43.577715\n",
      "1     34.9300  35.166955\n",
      "2     44.3925  44.942479\n",
      "3     41.5500  41.057426\n",
      "4     27.9475  27.962488\n",
      "...       ...        ...\n",
      "4091  41.8425  40.682417\n",
      "4092  26.0300  26.180815\n",
      "4093  27.8700  27.095293\n",
      "4094  43.0450  43.536131\n",
      "4095  39.5425  37.162015\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0316\n",
      "Epoch 14/25, Validation Loss: 0.0227\n",
      "       actual  predicted\n",
      "0     44.1400  43.398152\n",
      "1     34.9300  35.301660\n",
      "2     44.3925  44.942019\n",
      "3     41.5500  41.073672\n",
      "4     27.9475  27.813818\n",
      "...       ...        ...\n",
      "4091  41.8425  40.751675\n",
      "4092  26.0300  25.987584\n",
      "4093  27.8700  27.049638\n",
      "4094  43.0450  43.366086\n",
      "4095  39.5425  37.715592\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0297\n",
      "Epoch 15/25, Validation Loss: 0.0208\n",
      "       actual  predicted\n",
      "0     44.1400  43.378499\n",
      "1     34.9300  35.125303\n",
      "2     44.3925  44.524177\n",
      "3     41.5500  40.799025\n",
      "4     27.9475  27.702435\n",
      "...       ...        ...\n",
      "4091  41.8425  40.848219\n",
      "4092  26.0300  25.816505\n",
      "4093  27.8700  27.129579\n",
      "4094  43.0450  43.294666\n",
      "4095  39.5425  37.703575\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0282\n",
      "Epoch 16/25, Validation Loss: 0.0193\n",
      "       actual  predicted\n",
      "0     44.1400  43.600213\n",
      "1     34.9300  35.097980\n",
      "2     44.3925  44.655898\n",
      "3     41.5500  40.714103\n",
      "4     27.9475  28.009874\n",
      "...       ...        ...\n",
      "4091  41.8425  40.964143\n",
      "4092  26.0300  25.964018\n",
      "4093  27.8700  27.337723\n",
      "4094  43.0450  43.595227\n",
      "4095  39.5425  37.501477\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0269\n",
      "Epoch 17/25, Validation Loss: 0.0182\n",
      "       actual  predicted\n",
      "0     44.1400  43.564979\n",
      "1     34.9300  34.876881\n",
      "2     44.3925  44.564695\n",
      "3     41.5500  40.944383\n",
      "4     27.9475  27.821002\n",
      "...       ...        ...\n",
      "4091  41.8425  40.850776\n",
      "4092  26.0300  25.959776\n",
      "4093  27.8700  27.236452\n",
      "4094  43.0450  43.355903\n",
      "4095  39.5425  37.746458\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0257\n",
      "Epoch 18/25, Validation Loss: 0.0172\n",
      "       actual  predicted\n",
      "0     44.1400  43.768012\n",
      "1     34.9300  35.276137\n",
      "2     44.3925  44.563848\n",
      "3     41.5500  40.968135\n",
      "4     27.9475  28.247476\n",
      "...       ...        ...\n",
      "4091  41.8425  41.222085\n",
      "4092  26.0300  26.183715\n",
      "4093  27.8700  27.695231\n",
      "4094  43.0450  43.359368\n",
      "4095  39.5425  38.180957\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0246\n",
      "Epoch 19/25, Validation Loss: 0.0166\n",
      "       actual  predicted\n",
      "0     44.1400  43.395016\n",
      "1     34.9300  35.091552\n",
      "2     44.3925  44.579885\n",
      "3     41.5500  40.994381\n",
      "4     27.9475  27.820724\n",
      "...       ...        ...\n",
      "4091  41.8425  40.853809\n",
      "4092  26.0300  26.031502\n",
      "4093  27.8700  27.324830\n",
      "4094  43.0450  43.198920\n",
      "4095  39.5425  38.288391\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0238\n",
      "Epoch 20/25, Validation Loss: 0.0156\n",
      "       actual  predicted\n",
      "0     44.1400  43.683198\n",
      "1     34.9300  35.097289\n",
      "2     44.3925  44.901150\n",
      "3     41.5500  40.997288\n",
      "4     27.9475  27.821936\n",
      "...       ...        ...\n",
      "4091  41.8425  41.096329\n",
      "4092  26.0300  26.068190\n",
      "4093  27.8700  27.198115\n",
      "4094  43.0450  43.372468\n",
      "4095  39.5425  38.626889\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0229\n",
      "Epoch 21/25, Validation Loss: 0.0158\n",
      "       actual  predicted\n",
      "0     44.1400  43.552425\n",
      "1     34.9300  35.047852\n",
      "2     44.3925  44.393268\n",
      "3     41.5500  40.859271\n",
      "4     27.9475  28.162081\n",
      "...       ...        ...\n",
      "4091  41.8425  40.988357\n",
      "4092  26.0300  26.009093\n",
      "4093  27.8700  27.467812\n",
      "4094  43.0450  43.176296\n",
      "4095  39.5425  38.624005\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0220\n",
      "Epoch 22/25, Validation Loss: 0.0146\n",
      "       actual  predicted\n",
      "0     44.1400  43.449198\n",
      "1     34.9300  34.796995\n",
      "2     44.3925  44.392836\n",
      "3     41.5500  40.603536\n",
      "4     27.9475  27.891266\n",
      "...       ...        ...\n",
      "4091  41.8425  40.774683\n",
      "4092  26.0300  26.061148\n",
      "4093  27.8700  27.341892\n",
      "4094  43.0450  43.112446\n",
      "4095  39.5425  38.820985\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0213\n",
      "Epoch 23/25, Validation Loss: 0.0140\n",
      "       actual  predicted\n",
      "0     44.1400  43.723121\n",
      "1     34.9300  35.122580\n",
      "2     44.3925  44.491176\n",
      "3     41.5500  40.781423\n",
      "4     27.9475  27.826028\n",
      "...       ...        ...\n",
      "4091  41.8425  41.053031\n",
      "4092  26.0300  26.003388\n",
      "4093  27.8700  27.383691\n",
      "4094  43.0450  43.115637\n",
      "4095  39.5425  38.883554\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0207\n",
      "Epoch 24/25, Validation Loss: 0.0137\n",
      "       actual  predicted\n",
      "0     44.1400  43.682939\n",
      "1     34.9300  35.170497\n",
      "2     44.3925  44.438832\n",
      "3     41.5500  40.757268\n",
      "4     27.9475  28.238269\n",
      "...       ...        ...\n",
      "4091  41.8425  41.180905\n",
      "4092  26.0300  26.077190\n",
      "4093  27.8700  27.678134\n",
      "4094  43.0450  43.111419\n",
      "4095  39.5425  39.095605\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0205\n",
      "Epoch 25/25, Validation Loss: 0.0134\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[1.1012]], device='cuda:0')\n",
      "[46.09]\n",
      "        actual  predicted\n",
      "0      44.1400  43.682939\n",
      "1      34.9300  35.170497\n",
      "2      44.3925  44.438832\n",
      "3      41.5500  40.757268\n",
      "4      27.9475  28.238269\n",
      "...        ...        ...\n",
      "19155  56.2900  55.548213\n",
      "19156  33.1950  33.287117\n",
      "19157  42.1700  42.112119\n",
      "19158  32.5500  31.604358\n",
      "19159  42.5050  41.625133\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 1.1101\n",
      "Score (MAE): 0.7057\n",
      "Score (ME): -0.0674\n",
      "Score (MAPE): 1.9561%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        actual  predicted\n",
      "0     19.07000  22.189606\n",
      "1     23.25500  25.741082\n",
      "2     24.08750  26.317904\n",
      "3     26.62000  25.889522\n",
      "4     24.74375  22.988254\n",
      "...        ...        ...\n",
      "4091  27.60250  26.256883\n",
      "4092  19.52750  23.037590\n",
      "4093  29.56875  27.378903\n",
      "4094  30.61250  27.295585\n",
      "4095  16.22250  22.166928\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8100\n",
      "Epoch 1/25, Validation Loss: 0.3699\n",
      "        actual  predicted\n",
      "0     19.07000  19.377250\n",
      "1     23.25500  23.959262\n",
      "2     24.08750  25.667661\n",
      "3     26.62000  29.088463\n",
      "4     24.74375  23.564086\n",
      "...        ...        ...\n",
      "4091  27.60250  28.156037\n",
      "4092  19.52750  20.727120\n",
      "4093  29.56875  30.444346\n",
      "4094  30.61250  31.527872\n",
      "4095  16.22250  18.836116\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2351\n",
      "Epoch 2/25, Validation Loss: 0.1531\n",
      "        actual  predicted\n",
      "0     19.07000  19.642599\n",
      "1     23.25500  23.780533\n",
      "2     24.08750  24.628258\n",
      "3     26.62000  28.002794\n",
      "4     24.74375  24.090299\n",
      "...        ...        ...\n",
      "4091  27.60250  27.559399\n",
      "4092  19.52750  20.080399\n",
      "4093  29.56875  29.074824\n",
      "4094  30.61250  31.350178\n",
      "4095  16.22250  15.293549\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1188\n",
      "Epoch 3/25, Validation Loss: 0.0978\n",
      "        actual  predicted\n",
      "0     19.07000  19.736840\n",
      "1     23.25500  23.680041\n",
      "2     24.08750  24.618624\n",
      "3     26.62000  28.173602\n",
      "4     24.74375  24.614882\n",
      "...        ...        ...\n",
      "4091  27.60250  27.934817\n",
      "4092  19.52750  19.620515\n",
      "4093  29.56875  29.310017\n",
      "4094  30.61250  31.295082\n",
      "4095  16.22250  15.189758\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0932\n",
      "Epoch 4/25, Validation Loss: 0.0881\n",
      "        actual  predicted\n",
      "0     19.07000  19.469925\n",
      "1     23.25500  23.585620\n",
      "2     24.08750  24.611772\n",
      "3     26.62000  28.276247\n",
      "4     24.74375  24.830400\n",
      "...        ...        ...\n",
      "4091  27.60250  27.990743\n",
      "4092  19.52750  19.520478\n",
      "4093  29.56875  29.461207\n",
      "4094  30.61250  31.532000\n",
      "4095  16.22250  15.454004\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0865\n",
      "Epoch 5/25, Validation Loss: 0.0824\n",
      "        actual  predicted\n",
      "0     19.07000  19.379852\n",
      "1     23.25500  23.606208\n",
      "2     24.08750  24.693524\n",
      "3     26.62000  28.168908\n",
      "4     24.74375  24.838660\n",
      "...        ...        ...\n",
      "4091  27.60250  27.913167\n",
      "4092  19.52750  19.698435\n",
      "4093  29.56875  29.402734\n",
      "4094  30.61250  31.520086\n",
      "4095  16.22250  15.603852\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0809\n",
      "Epoch 6/25, Validation Loss: 0.0766\n",
      "        actual  predicted\n",
      "0     19.07000  19.250462\n",
      "1     23.25500  23.478786\n",
      "2     24.08750  24.750227\n",
      "3     26.62000  28.095217\n",
      "4     24.74375  24.874306\n",
      "...        ...        ...\n",
      "4091  27.60250  28.002228\n",
      "4092  19.52750  19.633260\n",
      "4093  29.56875  29.502035\n",
      "4094  30.61250  31.834106\n",
      "4095  16.22250  15.648948\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0755\n",
      "Epoch 7/25, Validation Loss: 0.0720\n",
      "        actual  predicted\n",
      "0     19.07000  19.329573\n",
      "1     23.25500  23.356796\n",
      "2     24.08750  24.786805\n",
      "3     26.62000  27.760347\n",
      "4     24.74375  24.821516\n",
      "...        ...        ...\n",
      "4091  27.60250  27.744091\n",
      "4092  19.52750  19.659212\n",
      "4093  29.56875  29.221254\n",
      "4094  30.61250  31.394806\n",
      "4095  16.22250  15.839966\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0703\n",
      "Epoch 8/25, Validation Loss: 0.0664\n",
      "        actual  predicted\n",
      "0     19.07000  19.423859\n",
      "1     23.25500  23.402568\n",
      "2     24.08750  24.734516\n",
      "3     26.62000  27.591573\n",
      "4     24.74375  24.933974\n",
      "...        ...        ...\n",
      "4091  27.60250  27.824774\n",
      "4092  19.52750  19.605778\n",
      "4093  29.56875  29.249950\n",
      "4094  30.61250  31.362796\n",
      "4095  16.22250  15.973707\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0658\n",
      "Epoch 9/25, Validation Loss: 0.0618\n",
      "        actual  predicted\n",
      "0     19.07000  19.279449\n",
      "1     23.25500  23.468840\n",
      "2     24.08750  24.791227\n",
      "3     26.62000  27.387847\n",
      "4     24.74375  24.838914\n",
      "...        ...        ...\n",
      "4091  27.60250  27.773314\n",
      "4092  19.52750  19.453371\n",
      "4093  29.56875  29.361237\n",
      "4094  30.61250  30.991896\n",
      "4095  16.22250  16.066141\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0612\n",
      "Epoch 10/25, Validation Loss: 0.0565\n",
      "        actual  predicted\n",
      "0     19.07000  19.228942\n",
      "1     23.25500  23.361868\n",
      "2     24.08750  24.955587\n",
      "3     26.62000  27.340555\n",
      "4     24.74375  24.729068\n",
      "...        ...        ...\n",
      "4091  27.60250  27.852777\n",
      "4092  19.52750  19.447420\n",
      "4093  29.56875  29.372240\n",
      "4094  30.61250  31.066982\n",
      "4095  16.22250  15.884388\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0562\n",
      "Epoch 11/25, Validation Loss: 0.0510\n",
      "        actual  predicted\n",
      "0     19.07000  19.293504\n",
      "1     23.25500  23.276802\n",
      "2     24.08750  24.814815\n",
      "3     26.62000  27.233425\n",
      "4     24.74375  24.666634\n",
      "...        ...        ...\n",
      "4091  27.60250  27.866482\n",
      "4092  19.52750  19.287920\n",
      "4093  29.56875  29.382498\n",
      "4094  30.61250  31.124739\n",
      "4095  16.22250  16.040064\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0519\n",
      "Epoch 12/25, Validation Loss: 0.0456\n",
      "        actual  predicted\n",
      "0     19.07000  19.391333\n",
      "1     23.25500  23.274469\n",
      "2     24.08750  24.839495\n",
      "3     26.62000  27.176839\n",
      "4     24.74375  24.620031\n",
      "...        ...        ...\n",
      "4091  27.60250  27.749111\n",
      "4092  19.52750  19.489282\n",
      "4093  29.56875  29.438982\n",
      "4094  30.61250  30.887631\n",
      "4095  16.22250  16.194970\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0457\n",
      "Epoch 13/25, Validation Loss: 0.0395\n",
      "        actual  predicted\n",
      "0     19.07000  19.360666\n",
      "1     23.25500  23.124106\n",
      "2     24.08750  24.587560\n",
      "3     26.62000  27.042310\n",
      "4     24.74375  24.551529\n",
      "...        ...        ...\n",
      "4091  27.60250  27.623072\n",
      "4092  19.52750  19.246455\n",
      "4093  29.56875  29.198448\n",
      "4094  30.61250  30.677381\n",
      "4095  16.22250  16.147100\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0411\n",
      "Epoch 14/25, Validation Loss: 0.0343\n",
      "        actual  predicted\n",
      "0     19.07000  19.217151\n",
      "1     23.25500  23.213528\n",
      "2     24.08750  24.524167\n",
      "3     26.62000  26.926353\n",
      "4     24.74375  24.629345\n",
      "...        ...        ...\n",
      "4091  27.60250  27.633204\n",
      "4092  19.52750  19.515375\n",
      "4093  29.56875  29.274396\n",
      "4094  30.61250  30.740276\n",
      "4095  16.22250  16.121213\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0371\n",
      "Epoch 15/25, Validation Loss: 0.0300\n",
      "        actual  predicted\n",
      "0     19.07000  19.220145\n",
      "1     23.25500  23.164040\n",
      "2     24.08750  24.322487\n",
      "3     26.62000  26.923502\n",
      "4     24.74375  24.597045\n",
      "...        ...        ...\n",
      "4091  27.60250  27.520776\n",
      "4092  19.52750  19.612822\n",
      "4093  29.56875  29.191280\n",
      "4094  30.61250  30.675173\n",
      "4095  16.22250  16.142225\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0335\n",
      "Epoch 16/25, Validation Loss: 0.0272\n",
      "        actual  predicted\n",
      "0     19.07000  19.197818\n",
      "1     23.25500  23.198379\n",
      "2     24.08750  24.360412\n",
      "3     26.62000  27.115147\n",
      "4     24.74375  24.644831\n",
      "...        ...        ...\n",
      "4091  27.60250  27.872005\n",
      "4092  19.52750  19.652438\n",
      "4093  29.56875  29.536607\n",
      "4094  30.61250  31.143121\n",
      "4095  16.22250  16.238331\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0314\n",
      "Epoch 17/25, Validation Loss: 0.0278\n",
      "        actual  predicted\n",
      "0     19.07000  19.351287\n",
      "1     23.25500  23.169285\n",
      "2     24.08750  24.232447\n",
      "3     26.62000  26.777981\n",
      "4     24.74375  24.538998\n",
      "...        ...        ...\n",
      "4091  27.60250  27.508178\n",
      "4092  19.52750  19.702487\n",
      "4093  29.56875  29.168639\n",
      "4094  30.61250  30.612130\n",
      "4095  16.22250  16.356673\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0303\n",
      "Epoch 18/25, Validation Loss: 0.0231\n",
      "        actual  predicted\n",
      "0     19.07000  19.272956\n",
      "1     23.25500  23.184943\n",
      "2     24.08750  24.215758\n",
      "3     26.62000  26.779650\n",
      "4     24.74375  24.625294\n",
      "...        ...        ...\n",
      "4091  27.60250  27.534336\n",
      "4092  19.52750  19.690204\n",
      "4093  29.56875  29.162633\n",
      "4094  30.61250  30.593558\n",
      "4095  16.22250  16.141482\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0279\n",
      "Epoch 19/25, Validation Loss: 0.0218\n",
      "        actual  predicted\n",
      "0     19.07000  19.249232\n",
      "1     23.25500  23.242108\n",
      "2     24.08750  24.195963\n",
      "3     26.62000  26.848569\n",
      "4     24.74375  24.676148\n",
      "...        ...        ...\n",
      "4091  27.60250  27.698550\n",
      "4092  19.52750  19.671307\n",
      "4093  29.56875  29.278448\n",
      "4094  30.61250  30.801209\n",
      "4095  16.22250  16.276817\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0264\n",
      "Epoch 20/25, Validation Loss: 0.0211\n",
      "        actual  predicted\n",
      "0     19.07000  19.250897\n",
      "1     23.25500  23.198158\n",
      "2     24.08750  24.262037\n",
      "3     26.62000  26.757887\n",
      "4     24.74375  24.601813\n",
      "...        ...        ...\n",
      "4091  27.60250  27.564200\n",
      "4092  19.52750  19.762807\n",
      "4093  29.56875  29.105679\n",
      "4094  30.61250  30.469321\n",
      "4095  16.22250  16.306142\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0257\n",
      "Epoch 21/25, Validation Loss: 0.0198\n",
      "        actual  predicted\n",
      "0     19.07000  19.243466\n",
      "1     23.25500  23.187226\n",
      "2     24.08750  24.083880\n",
      "3     26.62000  26.802806\n",
      "4     24.74375  24.650488\n",
      "...        ...        ...\n",
      "4091  27.60250  27.726006\n",
      "4092  19.52750  19.647095\n",
      "4093  29.56875  29.402530\n",
      "4094  30.61250  30.985998\n",
      "4095  16.22250  16.225403\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0247\n",
      "Epoch 22/25, Validation Loss: 0.0196\n",
      "        actual  predicted\n",
      "0     19.07000  19.016029\n",
      "1     23.25500  23.177740\n",
      "2     24.08750  23.940666\n",
      "3     26.62000  26.665073\n",
      "4     24.74375  24.629083\n",
      "...        ...        ...\n",
      "4091  27.60250  27.531573\n",
      "4092  19.52750  19.577358\n",
      "4093  29.56875  29.178860\n",
      "4094  30.61250  30.551570\n",
      "4095  16.22250  16.111397\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0239\n",
      "Epoch 23/25, Validation Loss: 0.0183\n",
      "        actual  predicted\n",
      "0     19.07000  18.968276\n",
      "1     23.25500  23.201735\n",
      "2     24.08750  24.035585\n",
      "3     26.62000  26.724541\n",
      "4     24.74375  24.565530\n",
      "...        ...        ...\n",
      "4091  27.60250  27.598963\n",
      "4092  19.52750  19.574642\n",
      "4093  29.56875  29.326504\n",
      "4094  30.61250  30.799934\n",
      "4095  16.22250  16.045715\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0230\n",
      "Epoch 24/25, Validation Loss: 0.0179\n",
      "        actual  predicted\n",
      "0     19.07000  19.128012\n",
      "1     23.25500  23.248892\n",
      "2     24.08750  23.948955\n",
      "3     26.62000  26.536504\n",
      "4     24.74375  24.578075\n",
      "...        ...        ...\n",
      "4091  27.60250  27.533618\n",
      "4092  19.52750  19.713481\n",
      "4093  29.56875  29.112576\n",
      "4094  30.61250  30.648334\n",
      "4095  16.22250  16.196171\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0223\n",
      "Epoch 25/25, Validation Loss: 0.0171\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.2940]], device='cuda:0')\n",
      "[25.11]\n",
      "          actual  predicted\n",
      "0      19.070000  19.128012\n",
      "1      23.255000  23.248892\n",
      "2      24.087500  23.948955\n",
      "3      26.620000  26.536504\n",
      "4      24.743750  24.578075\n",
      "...          ...        ...\n",
      "19155  24.007500  24.095702\n",
      "19156  22.568750  22.261335\n",
      "19157  27.387500  27.162133\n",
      "19158  23.703333  24.055520\n",
      "19159  21.860000  21.794996\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 0.4695\n",
      "Score (MAE): 0.2802\n",
      "Score (ME): 0.0344\n",
      "Score (MAPE): 1.1409%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0       7.500007  206.393850\n",
      "1     203.461538  192.169627\n",
      "2       4.999994  456.866401\n",
      "3      11.249995   45.806929\n",
      "4      24.499993  116.804401\n",
      "...          ...         ...\n",
      "4091  266.384616  214.550772\n",
      "4092  910.500029  522.002813\n",
      "4093    8.499999  340.977483\n",
      "4094    4.000001  145.782121\n",
      "4095  379.043483  297.319932\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.9308\n",
      "Epoch 1/25, Validation Loss: 0.7272\n",
      "          actual   predicted\n",
      "0       7.500007  195.064551\n",
      "1     203.461538  286.057588\n",
      "2       4.999994  363.406123\n",
      "3      11.249995   35.270104\n",
      "4      24.499993  139.370906\n",
      "...          ...         ...\n",
      "4091  266.384616  229.226148\n",
      "4092  910.500029  976.970197\n",
      "4093    8.499999   93.114324\n",
      "4094    4.000001   -4.840214\n",
      "4095  379.043483  152.999933\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.8885\n",
      "Epoch 2/25, Validation Loss: 0.6999\n",
      "          actual   predicted\n",
      "0       7.500007  124.795465\n",
      "1     203.461538  411.274653\n",
      "2       4.999994  479.344101\n",
      "3      11.249995   59.541050\n",
      "4      24.499993  114.882829\n",
      "...          ...         ...\n",
      "4091  266.384616  170.150859\n",
      "4092  910.500029  520.690876\n",
      "4093    8.499999   36.115153\n",
      "4094    4.000001   55.161275\n",
      "4095  379.043483  272.799329\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.8461\n",
      "Epoch 3/25, Validation Loss: 0.6735\n",
      "          actual   predicted\n",
      "0       7.500007   93.886034\n",
      "1     203.461538  313.007151\n",
      "2       4.999994  479.499204\n",
      "3      11.249995   27.039011\n",
      "4      24.499993   82.474769\n",
      "...          ...         ...\n",
      "4091  266.384616  227.515711\n",
      "4092  910.500029  364.228166\n",
      "4093    8.499999   18.813863\n",
      "4094    4.000001   54.620740\n",
      "4095  379.043483  411.587768\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.8105\n",
      "Epoch 4/25, Validation Loss: 0.6998\n",
      "          actual   predicted\n",
      "0       7.500007  138.490746\n",
      "1     203.461538  293.955234\n",
      "2       4.999994  324.781153\n",
      "3      11.249995   53.271791\n",
      "4      24.499993  111.224424\n",
      "...          ...         ...\n",
      "4091  266.384616  218.737680\n",
      "4092  910.500029  636.204323\n",
      "4093    8.499999   92.657380\n",
      "4094    4.000001   49.513359\n",
      "4095  379.043483  235.091891\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.8128\n",
      "Epoch 5/25, Validation Loss: 0.6484\n",
      "          actual   predicted\n",
      "0       7.500007   57.415559\n",
      "1     203.461538  246.495346\n",
      "2       4.999994  410.336733\n",
      "3      11.249995   20.045118\n",
      "4      24.499993   80.414711\n",
      "...          ...         ...\n",
      "4091  266.384616  263.487949\n",
      "4092  910.500029  592.726492\n",
      "4093    8.499999   15.480296\n",
      "4094    4.000001   34.945338\n",
      "4095  379.043483  357.865166\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.7616\n",
      "Epoch 6/25, Validation Loss: 0.6444\n",
      "          actual   predicted\n",
      "0       7.500007   19.901778\n",
      "1     203.461538  212.049976\n",
      "2       4.999994  476.847756\n",
      "3      11.249995   -0.657772\n",
      "4      24.499993   73.361623\n",
      "...          ...         ...\n",
      "4091  266.384616  245.606925\n",
      "4092  910.500029  384.798216\n",
      "4093    8.499999  -39.452233\n",
      "4094    4.000001    5.534220\n",
      "4095  379.043483  386.021645\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.7329\n",
      "Epoch 7/25, Validation Loss: 0.6158\n",
      "          actual   predicted\n",
      "0       7.500007   75.628958\n",
      "1     203.461538  230.907900\n",
      "2       4.999994  393.527281\n",
      "3      11.249995  -81.026792\n",
      "4      24.499993   -3.325932\n",
      "...          ...         ...\n",
      "4091  266.384616  200.919575\n",
      "4092  910.500029  423.738229\n",
      "4093    8.499999 -164.936119\n",
      "4094    4.000001  -89.993682\n",
      "4095  379.043483  413.439575\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.8043\n",
      "Epoch 8/25, Validation Loss: 0.6432\n",
      "          actual   predicted\n",
      "0       7.500007   52.316953\n",
      "1     203.461538  180.281595\n",
      "2       4.999994  344.538012\n",
      "3      11.249995    8.141999\n",
      "4      24.499993   85.291177\n",
      "...          ...         ...\n",
      "4091  266.384616  224.002234\n",
      "4092  910.500029  281.508971\n",
      "4093    8.499999  -76.366062\n",
      "4094    4.000001  -22.801962\n",
      "4095  379.043483  292.839128\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.7646\n",
      "Epoch 9/25, Validation Loss: 0.6424\n",
      "          actual   predicted\n",
      "0       7.500007   28.628077\n",
      "1     203.461538  196.117063\n",
      "2       4.999994  439.983461\n",
      "3      11.249995   10.013775\n",
      "4      24.499993   87.917415\n",
      "...          ...         ...\n",
      "4091  266.384616  225.749890\n",
      "4092  910.500029  345.994538\n",
      "4093    8.499999 -108.860385\n",
      "4094    4.000001  -14.994136\n",
      "4095  379.043483  352.542887\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.7327\n",
      "Epoch 10/25, Validation Loss: 0.6535\n",
      "          actual   predicted\n",
      "0       7.500007   59.708832\n",
      "1     203.461538  296.422029\n",
      "2       4.999994  433.193794\n",
      "3      11.249995   49.199676\n",
      "4      24.499993  129.960033\n",
      "...          ...         ...\n",
      "4091  266.384616  296.246895\n",
      "4092  910.500029  464.002416\n",
      "4093    8.499999  -74.278928\n",
      "4094    4.000001    8.888214\n",
      "4095  379.043483  384.663778\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.7275\n",
      "Epoch 11/25, Validation Loss: 0.6066\n",
      "          actual   predicted\n",
      "0       7.500007   72.910639\n",
      "1     203.461538  235.565614\n",
      "2       4.999994  396.453534\n",
      "3      11.249995   61.426471\n",
      "4      24.499993  131.911883\n",
      "...          ...         ...\n",
      "4091  266.384616  287.163599\n",
      "4092  910.500029  476.915932\n",
      "4093    8.499999   -6.252770\n",
      "4094    4.000001   33.285218\n",
      "4095  379.043483  340.920090\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.6856\n",
      "Epoch 12/25, Validation Loss: 0.6395\n",
      "          actual   predicted\n",
      "0       7.500007  136.529971\n",
      "1     203.461538  292.783538\n",
      "2       4.999994  364.770188\n",
      "3      11.249995  101.863248\n",
      "4      24.499993  168.808660\n",
      "...          ...         ...\n",
      "4091  266.384616  348.752644\n",
      "4092  910.500029  419.686779\n",
      "4093    8.499999  -24.360277\n",
      "4094    4.000001   70.752911\n",
      "4095  379.043483  365.172442\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.6951\n",
      "Epoch 13/25, Validation Loss: 0.6319\n",
      "          actual   predicted\n",
      "0       7.500007   47.731087\n",
      "1     203.461538  290.436074\n",
      "2       4.999994  352.799288\n",
      "3      11.249995   48.541383\n",
      "4      24.499993  121.553663\n",
      "...          ...         ...\n",
      "4091  266.384616  325.490135\n",
      "4092  910.500029  458.983733\n",
      "4093    8.499999  -40.656673\n",
      "4094    4.000001   24.435786\n",
      "4095  379.043483  360.536567\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.6928\n",
      "Epoch 14/25, Validation Loss: 0.5651\n",
      "          actual   predicted\n",
      "0       7.500007    4.955281\n",
      "1     203.461538  176.691484\n",
      "2       4.999994  284.444600\n",
      "3      11.249995   35.893550\n",
      "4      24.499993  113.045219\n",
      "...          ...         ...\n",
      "4091  266.384616  254.509960\n",
      "4092  910.500029  314.882351\n",
      "4093    8.499999   -2.489105\n",
      "4094    4.000001    4.687670\n",
      "4095  379.043483  246.279341\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.6160\n",
      "Epoch 15/25, Validation Loss: 0.5269\n",
      "          actual   predicted\n",
      "0       7.500007   17.054753\n",
      "1     203.461538  211.254553\n",
      "2       4.999994  202.524009\n",
      "3      11.249995   54.045011\n",
      "4      24.499993  124.349533\n",
      "...          ...         ...\n",
      "4091  266.384616  295.434329\n",
      "4092  910.500029  411.851331\n",
      "4093    8.499999   46.172772\n",
      "4094    4.000001   10.174522\n",
      "4095  379.043483  257.579611\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.5898\n",
      "Epoch 16/25, Validation Loss: 0.5393\n",
      "          actual   predicted\n",
      "0       7.500007   16.425789\n",
      "1     203.461538  121.981752\n",
      "2       4.999994  208.514495\n",
      "3      11.249995   44.505364\n",
      "4      24.499993  119.804656\n",
      "...          ...         ...\n",
      "4091  266.384616  285.025634\n",
      "4092  910.500029  415.855223\n",
      "4093    8.499999   32.429593\n",
      "4094    4.000001   -7.268510\n",
      "4095  379.043483  236.414504\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.6515\n",
      "Epoch 17/25, Validation Loss: 0.5955\n",
      "          actual   predicted\n",
      "0       7.500007   -8.341516\n",
      "1     203.461538  194.996403\n",
      "2       4.999994  279.203477\n",
      "3      11.249995   59.467419\n",
      "4      24.499993  158.096384\n",
      "...          ...         ...\n",
      "4091  266.384616  344.836265\n",
      "4092  910.500029  554.900846\n",
      "4093    8.499999   26.038781\n",
      "4094    4.000001   -0.398493\n",
      "4095  379.043483  388.378397\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.5917\n",
      "Epoch 18/25, Validation Loss: 0.6005\n",
      "          actual   predicted\n",
      "0       7.500007   62.580175\n",
      "1     203.461538  185.748345\n",
      "2       4.999994  237.372702\n",
      "3      11.249995  119.191582\n",
      "4      24.499993  211.111902\n",
      "...          ...         ...\n",
      "4091  266.384616  366.579449\n",
      "4092  910.500029  508.002157\n",
      "4093    8.499999  109.837773\n",
      "4094    4.000001   39.343879\n",
      "4095  379.043483  308.045774\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.5910\n",
      "Epoch 19/25, Validation Loss: 0.5129\n",
      "          actual   predicted\n",
      "0       7.500007    4.920909\n",
      "1     203.461538  157.408637\n",
      "2       4.999994  195.087346\n",
      "3      11.249995   53.767977\n",
      "4      24.499993  166.427575\n",
      "...          ...         ...\n",
      "4091  266.384616  296.693158\n",
      "4092  910.500029  493.270226\n",
      "4093    8.499999   74.552932\n",
      "4094    4.000001  -26.016159\n",
      "4095  379.043483  309.648026\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.5824\n",
      "Epoch 20/25, Validation Loss: 0.5460\n",
      "          actual   predicted\n",
      "0       7.500007   -6.330511\n",
      "1     203.461538  179.025264\n",
      "2       4.999994  258.601332\n",
      "3      11.249995   37.151747\n",
      "4      24.499993  188.956051\n",
      "...          ...         ...\n",
      "4091  266.384616  405.474417\n",
      "4092  910.500029  480.145575\n",
      "4093    8.499999    6.978840\n",
      "4094    4.000001  -50.224890\n",
      "4095  379.043483  350.583045\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.5425\n",
      "Epoch 21/25, Validation Loss: 0.4766\n",
      "          actual   predicted\n",
      "0       7.500007   66.872247\n",
      "1     203.461538  181.480454\n",
      "2       4.999994  230.080394\n",
      "3      11.249995   96.332473\n",
      "4      24.499993  240.984706\n",
      "...          ...         ...\n",
      "4091  266.384616  396.463795\n",
      "4092  910.500029  528.862680\n",
      "4093    8.499999  121.932636\n",
      "4094    4.000001    7.980793\n",
      "4095  379.043483  333.021970\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.4894\n",
      "Epoch 22/25, Validation Loss: 0.4563\n",
      "          actual   predicted\n",
      "0       7.500007  -26.725663\n",
      "1     203.461538  140.416595\n",
      "2       4.999994  128.719948\n",
      "3      11.249995  -14.087601\n",
      "4      24.499993  149.301048\n",
      "...          ...         ...\n",
      "4091  266.384616  335.941566\n",
      "4092  910.500029  559.862705\n",
      "4093    8.499999    7.330026\n",
      "4094    4.000001  -95.496203\n",
      "4095  379.043483  274.604092\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.4568\n",
      "Epoch 23/25, Validation Loss: 0.4979\n",
      "          actual   predicted\n",
      "0       7.500007    4.704808\n",
      "1     203.461538  140.028262\n",
      "2       4.999994  168.651337\n",
      "3      11.249995    7.023157\n",
      "4      24.499993  182.112407\n",
      "...          ...         ...\n",
      "4091  266.384616  359.306213\n",
      "4092  910.500029  614.467700\n",
      "4093    8.499999   61.305583\n",
      "4094    4.000001  -59.560152\n",
      "4095  379.043483  341.607538\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.5080\n",
      "Epoch 24/25, Validation Loss: 0.5383\n",
      "          actual   predicted\n",
      "0       7.500007   74.210418\n",
      "1     203.461538  236.733674\n",
      "2       4.999994  293.905454\n",
      "3      11.249995   70.214621\n",
      "4      24.499993  242.132623\n",
      "...          ...         ...\n",
      "4091  266.384616  471.785277\n",
      "4092  910.500029  642.787218\n",
      "4093    8.499999  128.371797\n",
      "4094    4.000001   15.700791\n",
      "4095  379.043483  436.917284\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.4420\n",
      "Epoch 25/25, Validation Loss: 0.5216\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.1913]], device='cuda:0')\n",
      "[1.32]\n",
      "           actual   predicted\n",
      "0        7.500007   74.210418\n",
      "1      203.461538  236.733674\n",
      "2        4.999994  293.905454\n",
      "3       11.249995   70.214621\n",
      "4       24.499993  242.132623\n",
      "...           ...         ...\n",
      "19155   35.666667  102.776440\n",
      "19156  703.650009  889.671634\n",
      "19157    6.000002   33.206829\n",
      "19158    4.000001   17.642846\n",
      "19159    8.999996  188.179020\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 769.8993\n",
      "Score (MAE): 164.8195\n",
      "Score (ME): -31.9840\n",
      "Score (MAPE): 571.3633%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    }
   ],
   "source": [
    "# Reload the utils module if you have made changes\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Read existing model performance records\n",
    "performance_df = pd.read_csv('model_performances.csv')\n",
    "\n",
    "# Define LSTM-specific hyperparameters\n",
    "hidden_dim = 256\n",
    "num_layers = 5\n",
    "dropout = 0.3\n",
    "batch_size = 4096\n",
    "learning_rate = 0.00031\n",
    "epochs = 25\n",
    "aggregation_level = 'quarter_hour'\n",
    "y_feature = 'CO2'\n",
    "freq = 15 if aggregation_level == 'quarter_hour' else 30 if aggregation_level == 'half_hour' else 60\n",
    "window_size = 5 * 60 // freq\n",
    "\n",
    "# Loop over each feature to create and evaluate the LSTM model\n",
    "for aggregation_level in ['quarter_hour', 'half_hour', 'hour']:\n",
    "    for y_feature in ['CO2', 'VOC', 'hum', 'tmp', 'vis']:\n",
    "        model, scaler, rmse, mae, me, mape, train_loss, val_loss, n_features = utils.create_multivariate_lstm_model_for_feature(\n",
    "            df, \n",
    "            hidden_dim=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout, \n",
    "            batch_size=batch_size, \n",
    "            learning_rate=learning_rate, \n",
    "            epochs=epochs, \n",
    "            y_feature=y_feature, \n",
    "            aggregation_level=aggregation_level, \n",
    "            window_size=window_size\n",
    "        )\n",
    "        performance_df = performance_df.append({\n",
    "            'model_name': 'multivariate_lstm',\n",
    "            'aggregation_level': aggregation_level,\n",
    "            'y_feature': y_feature,\n",
    "            'n_features': n_features,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'me': me,\n",
    "            'mape': mape,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'num_layers': num_layers,\n",
    "            'dropout': dropout,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epochs': epochs,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'note': 'LSTM model with device_id embedding'\n",
    "        }, ignore_index=True)\n",
    "        performance_df.to_csv('model_performances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'please_test_this.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main abb8b35] Updating models with 25 features instead of 26. Adding back snr feature\n",
      " 76 files changed, 958052 insertions(+), 949959 deletions(-)\n",
      " delete mode 100644 data/half_hour_23f_20ws_dataframe_v1.parquet\n",
      " create mode 100644 data/half_hour_25f_20ws_dataframe_v1.parquet\n",
      " delete mode 100644 data/half_hour_26f_20ws_dataframe_v1.parquet\n",
      " delete mode 100644 data/hour_23f_20ws_dataframe_v1.parquet\n",
      " create mode 100644 data/hour_25f_20ws_dataframe_v1.parquet\n",
      " delete mode 100644 data/hour_26f_20ws_dataframe_v1.parquet\n",
      " create mode 100644 data/lstm_multivariate_quarter_hour_25f_20ws_CO2_predictions.csv\n",
      " create mode 100644 data/lstm_multivariate_quarter_hour_25f_20ws_VOC_predictions.csv\n",
      " create mode 100644 data/lstm_multivariate_quarter_hour_25f_20ws_hum_predictions.csv\n",
      " create mode 100644 data/lstm_multivariate_quarter_hour_25f_20ws_tmp_predictions.csv\n",
      " create mode 100644 data/lstm_multivariate_quarter_hour_25f_20ws_vis_predictions.csv\n",
      " delete mode 100644 data/lstm_multivariate_quarter_hour_26f_20ws_CO2_predictions.csv\n",
      " delete mode 100644 data/lstm_multivariate_quarter_hour_26f_20ws_VOC_predictions.csv\n",
      " delete mode 100644 data/lstm_multivariate_quarter_hour_26f_20ws_hum_predictions.csv\n",
      " delete mode 100644 data/lstm_multivariate_quarter_hour_26f_20ws_tmp_predictions.csv\n",
      " delete mode 100644 data/lstm_multivariate_quarter_hour_26f_20ws_vis_predictions.csv\n",
      " delete mode 100644 data/quarter_hour_23f_20ws_dataframe_v1.parquet\n",
      " rename data/{quarter_hour_26f_20ws_dataframe_v1.parquet => quarter_hour_25f_20ws_dataframe_v1.parquet} (56%)\n",
      " create mode 100644 data/transformer_multivariate_quarter_hour_25f_20ws_CO2_predictions.csv\n",
      " create mode 100644 data/transformer_multivariate_quarter_hour_25f_20ws_VOC_predictions.csv\n",
      " create mode 100644 data/transformer_multivariate_quarter_hour_25f_20ws_hum_predictions.csv\n",
      " create mode 100644 data/transformer_multivariate_quarter_hour_25f_20ws_tmp_predictions.csv\n",
      " create mode 100644 data/transformer_multivariate_quarter_hour_25f_20ws_vis_predictions.csv\n",
      " delete mode 100644 data/transformer_multivariate_quarter_hour_26f_20ws_CO2_predictions.csv\n",
      " delete mode 100644 data/transformer_multivariate_quarter_hour_26f_20ws_VOC_predictions.csv\n",
      " delete mode 100644 data/transformer_multivariate_quarter_hour_26f_20ws_hum_predictions.csv\n",
      " delete mode 100644 data/transformer_multivariate_quarter_hour_26f_20ws_tmp_predictions.csv\n",
      " delete mode 100644 data/transformer_multivariate_quarter_hour_26f_20ws_vis_predictions.csv\n",
      " create mode 100644 models/half_hour_25f_20ws_scaler_v1.pth\n",
      " delete mode 100644 models/half_hour_26f_20ws_scaler_v1.pth\n",
      " create mode 100644 models/hour_25f_20ws_scaler_v1.pth\n",
      " delete mode 100644 models/hour_26f_20ws_scaler_v1.pth\n",
      " rename models/{lstm_multivariate_quarter_hour_26f_20ws_CO2_model_v1.pth.gz => lstm_multivariate_half_hour_25f_20ws_CO2_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_hour_26f_20ws_hum_model_v1.pth.gz => lstm_multivariate_half_hour_25f_20ws_VOC_model_v1.pth.gz} (53%)\n",
      " create mode 100644 models/lstm_multivariate_half_hour_25f_20ws_hum_model_v1.pth.gz\n",
      " rename models/{lstm_multivariate_half_hour_26f_20ws_hum_model_v1.pth.gz => lstm_multivariate_half_hour_25f_20ws_tmp_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_hour_26f_20ws_VOC_model_v1.pth.gz => lstm_multivariate_half_hour_25f_20ws_vis_model_v1.pth.gz} (53%)\n",
      " delete mode 100644 models/lstm_multivariate_half_hour_26f_20ws_vis_model_v1.pth.gz\n",
      " rename models/{lstm_multivariate_quarter_hour_26f_20ws_vis_model_v1.pth.gz => lstm_multivariate_hour_25f_20ws_CO2_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_half_hour_26f_20ws_tmp_model_v1.pth.gz => lstm_multivariate_hour_25f_20ws_VOC_model_v1.pth.gz} (53%)\n",
      " create mode 100644 models/lstm_multivariate_hour_25f_20ws_hum_model_v1.pth.gz\n",
      " create mode 100644 models/lstm_multivariate_hour_25f_20ws_tmp_model_v1.pth.gz\n",
      " rename models/{lstm_multivariate_hour_26f_20ws_tmp_model_v1.pth.gz => lstm_multivariate_hour_25f_20ws_vis_model_v1.pth.gz} (53%)\n",
      " delete mode 100644 models/lstm_multivariate_hour_26f_20ws_vis_model_v1.pth.gz\n",
      " rename models/{lstm_multivariate_half_hour_26f_20ws_VOC_model_v1.pth.gz => lstm_multivariate_quarter_hour_25f_20ws_CO2_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_half_hour_26f_20ws_CO2_model_v1.pth.gz => lstm_multivariate_quarter_hour_25f_20ws_VOC_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_quarter_hour_26f_20ws_VOC_model_v1.pth.gz => lstm_multivariate_quarter_hour_25f_20ws_hum_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_quarter_hour_26f_20ws_tmp_model_v1.pth.gz => lstm_multivariate_quarter_hour_25f_20ws_tmp_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_hour_26f_20ws_CO2_model_v1.pth.gz => lstm_multivariate_quarter_hour_25f_20ws_vis_model_v1.pth.gz} (53%)\n",
      " delete mode 100644 models/lstm_multivariate_quarter_hour_26f_20ws_hum_model_v1.pth.gz\n",
      " create mode 100644 models/quarter_hour_25f_20ws_scaler_v1.pth\n",
      " delete mode 100644 models/quarter_hour_26f_20ws_scaler_v1.pth\n",
      " delete mode 100644 models/quarter_hour_26f_40ws_scaler_v1.pth\n",
      " rename models/{transformer_multivariate_half_hour_26f_20ws_tmp_model_v1.pth.gz => transformer_multivariate_half_hour_25f_20ws_CO2_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_quarter_hour_26f_20ws_vis_model_v1.pth.gz => transformer_multivariate_half_hour_25f_20ws_VOC_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_quarter_hour_26f_20ws_CO2_model_v2.pth.gz => transformer_multivariate_half_hour_25f_20ws_hum_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_hour_26f_20ws_vis_model_v1.pth.gz => transformer_multivariate_half_hour_25f_20ws_tmp_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_half_hour_26f_20ws_vis_model_v1.pth.gz => transformer_multivariate_half_hour_25f_20ws_vis_model_v1.pth.gz} (73%)\n",
      " delete mode 100644 models/transformer_multivariate_half_hour_26f_20ws_CO2_model_v1.pth.gz\n",
      " delete mode 100644 models/transformer_multivariate_half_hour_26f_20ws_hum_model_v1.pth.gz\n",
      " rename models/{transformer_multivariate_hour_26f_20ws_VOC_model_v1.pth.gz => transformer_multivariate_hour_25f_20ws_CO2_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_hour_26f_20ws_hum_model_v1.pth.gz => transformer_multivariate_hour_25f_20ws_VOC_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_hour_26f_20ws_tmp_model_v1.pth.gz => transformer_multivariate_hour_25f_20ws_hum_model_v1.pth.gz} (73%)\n",
      " create mode 100644 models/transformer_multivariate_hour_25f_20ws_tmp_model_v1.pth.gz\n",
      " create mode 100644 models/transformer_multivariate_hour_25f_20ws_vis_model_v1.pth.gz\n",
      " delete mode 100644 models/transformer_multivariate_hour_26f_20ws_CO2_model_v1.pth.gz\n",
      " rename models/{transformer_multivariate_half_hour_26f_20ws_VOC_model_v1.pth.gz => transformer_multivariate_quarter_hour_25f_20ws_CO2_model_v1.pth.gz} (73%)\n",
      " create mode 100644 models/transformer_multivariate_quarter_hour_25f_20ws_VOC_model_v1.pth.gz\n",
      " rename models/{transformer_multivariate_quarter_hour_26f_20ws_VOC_model_v1.pth.gz => transformer_multivariate_quarter_hour_25f_20ws_hum_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_quarter_hour_26f_20ws_hum_model_v1.pth.gz => transformer_multivariate_quarter_hour_25f_20ws_tmp_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_quarter_hour_26f_40ws_CO2_model_v1.pth.gz => transformer_multivariate_quarter_hour_25f_20ws_vis_model_v1.pth.gz} (73%)\n",
      " delete mode 100644 models/transformer_multivariate_quarter_hour_26f_20ws_tmp_model_v1.pth.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: RPC failed; curl 16 Error in the HTTP2 framing layer\n",
      "send-pack: unexpected disconnect while reading sideband packet\n",
      "fatal: the remote end hung up unexpectedly\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!git add -A\n",
    "!git commit -m \"Updating models with 25 features instead of 26. Adding back snr feature\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device_ids = torch.tensor([0])\n",
    "\n",
    "device = utils.get_device()\n",
    "\n",
    "model = utils.load_transformer_model(y_feature=\"CO2\", model_name = 'transformer_multivariate_quarter_hour_26f', device=device)\n",
    "scaler = utils.load_scaler(y_feature=\"CO2\", model_name = 'transformer_multivariate_quarter_hour_26f')\n",
    "\n",
    "real_data = utils.load_dataframe(model_name='transformer_multivariate_quarter_hour_26f')\n",
    "pd.set_option('display.max_columns', None)\n",
    "real_data.iloc[7:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = real_data.iloc[7:27].drop(columns=['device_id', 'date_time_rounded'])\n",
    "\n",
    "data_df_scaled = scaler.transform(data_df)\n",
    "\n",
    "input_data = torch.tensor(data_df_scaled, dtype=torch.float32).view(-1, 20, data_df_scaled.shape[1])\n",
    "print(input_data.shape)\n",
    "device_ids = torch.tensor([0])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_data.to(device), device_ids.to(device))\n",
    "    print(output)\n",
    "\n",
    "    prediction = output.cpu().numpy().reshape(-1, 1)\n",
    "    print(prediction)\n",
    "    print(prediction.shape)\n",
    "    zeroes_for_scaler = np.zeros((prediction.shape[0], 25))\n",
    "\n",
    "    zeroes_for_scaler[:, 2] = prediction  # Insert predicted values into the correct column\n",
    "    print(zeroes_for_scaler)\n",
    "    inverse_transformed = scaler.inverse_transform(zeroes_for_scaler)\n",
    "    predicted_unscaled = inverse_transformed[:, 2].round(2)\n",
    "    print(predicted_unscaled)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
