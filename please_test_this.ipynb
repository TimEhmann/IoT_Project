{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings = ['am', 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(f'data/hka-aqm-{buildings[1]}-combined-RAW.parquet') #switch between buildings for training\n",
    "# drop channel_rssi and channel_index\n",
    "df = df.drop(columns=['channel_rssi', 'channel_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1293: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_cpy['date_time_rounded'] = df_cpy['date_time'].dt.round('15T')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1336: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1321: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1324: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (553093, 26) to (564981, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexe\\Desktop\\IoT_Project\\utils.py:1352: FutureWarning: 'T' is deprecated and will be removed in a future version. Please use 'min' instead of 'T'.\n",
      "  df_cpy['consecutive_data_point'] = (df_cpy['date_time_rounded'] - df_cpy['date_time_rounded'].shift(1)).dt.total_seconds() == pd.to_timedelta(freq).total_seconds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data cutoff:  2023-06-26 22:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "performance_df = pd.read_csv('model_performances.csv')\n",
    "d_model=256\n",
    "nhead=8\n",
    "num_layers=5 \n",
    "dropout_pe=0.15\n",
    "dropout_encoder=0.15\n",
    "batch_size=1024\n",
    "learning_rate=0.00031\n",
    "epochs=25\n",
    "y_feature = 'CO2'\n",
    "aggregation_level = 'quarter_hour'\n",
    "freq = 15 if aggregation_level == 'quarter_hour' else 30 if aggregation_level == 'half_hour' else 60\n",
    "window_size = 5 * 60 // freq\n",
    "for aggregation_level in ['quarter_hour', 'half_hour', 'hour']:\n",
    "    for y_feature in ['CO2', 'VOC', 'hum', 'tmp', 'vis']:\n",
    "        \n",
    "        model, scaler, rmse, mae, me, mape, train_loss, val_loss, n_features = utils.create_multivariate_transformer_model_for_feature(df, d_model=d_model, nhead=nhead, num_layers=num_layers, dropout_pe=dropout_pe, dropout_encoder=dropout_encoder, batch_size=batch_size, learning_rate=learning_rate, epochs=epochs, y_feature=y_feature, aggregation_level=aggregation_level, window_size=window_size, selected_building='f')\n",
    "        performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n",
    "        performance_df.to_csv('model_performances.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     475.000000  475.310204\n",
      "1     415.000002  415.023051\n",
      "2     473.000000  477.090989\n",
      "3     512.000001  529.613180\n",
      "4     554.000002  574.161563\n",
      "...          ...         ...\n",
      "4091  420.000001  414.515299\n",
      "4092  429.999999  430.388544\n",
      "4093  425.000000  416.999382\n",
      "4094  415.000002  413.225235\n",
      "4095  412.999998  416.412842\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.3546\n",
      "Epoch 1/25, Validation Loss: 0.0907\n",
      "          actual   predicted\n",
      "0     475.000000  474.009144\n",
      "1     415.000002  418.210591\n",
      "2     473.000000  470.695207\n",
      "3     512.000001  521.129846\n",
      "4     554.000002  574.822406\n",
      "...          ...         ...\n",
      "4091  420.000001  417.722072\n",
      "4092  429.999999  431.246147\n",
      "4093  425.000000  420.243562\n",
      "4094  415.000002  413.374141\n",
      "4095  412.999998  416.235154\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0706\n",
      "Epoch 2/25, Validation Loss: 0.0477\n",
      "          actual   predicted\n",
      "0     475.000000  473.969152\n",
      "1     415.000002  423.640817\n",
      "2     473.000000  470.905437\n",
      "3     512.000001  516.851436\n",
      "4     554.000002  566.636655\n",
      "...          ...         ...\n",
      "4091  420.000001  420.692721\n",
      "4092  429.999999  431.221576\n",
      "4093  425.000000  420.671918\n",
      "4094  415.000002  417.884756\n",
      "4095  412.999998  418.703701\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0544\n",
      "Epoch 3/25, Validation Loss: 0.0439\n",
      "          actual   predicted\n",
      "0     475.000000  476.187944\n",
      "1     415.000002  421.480360\n",
      "2     473.000000  476.091446\n",
      "3     512.000001  516.066173\n",
      "4     554.000002  578.300823\n",
      "...          ...         ...\n",
      "4091  420.000001  418.605380\n",
      "4092  429.999999  428.015712\n",
      "4093  425.000000  414.079558\n",
      "4094  415.000002  413.501747\n",
      "4095  412.999998  415.241046\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0480\n",
      "Epoch 4/25, Validation Loss: 0.0396\n",
      "          actual   predicted\n",
      "0     475.000000  476.852557\n",
      "1     415.000002  419.384025\n",
      "2     473.000000  478.255592\n",
      "3     512.000001  520.934221\n",
      "4     554.000002  581.129712\n",
      "...          ...         ...\n",
      "4091  420.000001  416.732441\n",
      "4092  429.999999  428.817317\n",
      "4093  425.000000  413.247991\n",
      "4094  415.000002  412.525475\n",
      "4095  412.999998  414.370007\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0455\n",
      "Epoch 5/25, Validation Loss: 0.0368\n",
      "          actual   predicted\n",
      "0     475.000000  471.456732\n",
      "1     415.000002  419.698035\n",
      "2     473.000000  472.298347\n",
      "3     512.000001  513.731031\n",
      "4     554.000002  566.642951\n",
      "...          ...         ...\n",
      "4091  420.000001  417.422953\n",
      "4092  429.999999  428.687584\n",
      "4093  425.000000  417.215356\n",
      "4094  415.000002  413.262897\n",
      "4095  412.999998  414.341676\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0426\n",
      "Epoch 6/25, Validation Loss: 0.0356\n",
      "          actual   predicted\n",
      "0     475.000000  471.908365\n",
      "1     415.000002  423.449343\n",
      "2     473.000000  480.877326\n",
      "3     512.000001  516.198466\n",
      "4     554.000002  566.835665\n",
      "...          ...         ...\n",
      "4091  420.000001  422.416611\n",
      "4092  429.999999  431.292915\n",
      "4093  425.000000  420.565821\n",
      "4094  415.000002  416.677334\n",
      "4095  412.999998  419.314906\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0406\n",
      "Epoch 7/25, Validation Loss: 0.0340\n",
      "          actual   predicted\n",
      "0     475.000000  471.660982\n",
      "1     415.000002  421.486332\n",
      "2     473.000000  476.205254\n",
      "3     512.000001  516.324334\n",
      "4     554.000002  565.693815\n",
      "...          ...         ...\n",
      "4091  420.000001  416.476446\n",
      "4092  429.999999  427.816350\n",
      "4093  425.000000  415.131921\n",
      "4094  415.000002  412.643653\n",
      "4095  412.999998  413.604907\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0404\n",
      "Epoch 8/25, Validation Loss: 0.0340\n",
      "          actual   predicted\n",
      "0     475.000000  472.897267\n",
      "1     415.000002  421.066872\n",
      "2     473.000000  477.083962\n",
      "3     512.000001  516.759566\n",
      "4     554.000002  565.416236\n",
      "...          ...         ...\n",
      "4091  420.000001  420.009467\n",
      "4092  429.999999  429.693998\n",
      "4093  425.000000  418.527456\n",
      "4094  415.000002  416.560478\n",
      "4095  412.999998  417.070267\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0389\n",
      "Epoch 9/25, Validation Loss: 0.0330\n",
      "          actual   predicted\n",
      "0     475.000000  470.934735\n",
      "1     415.000002  421.661744\n",
      "2     473.000000  477.706277\n",
      "3     512.000001  516.896910\n",
      "4     554.000002  565.757345\n",
      "...          ...         ...\n",
      "4091  420.000001  422.756998\n",
      "4092  429.999999  429.581218\n",
      "4093  425.000000  421.032778\n",
      "4094  415.000002  417.616185\n",
      "4095  412.999998  419.918576\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0368\n",
      "Epoch 10/25, Validation Loss: 0.0336\n",
      "          actual   predicted\n",
      "0     475.000000  473.848163\n",
      "1     415.000002  417.272875\n",
      "2     473.000000  481.258723\n",
      "3     512.000001  522.342347\n",
      "4     554.000002  575.472716\n",
      "...          ...         ...\n",
      "4091  420.000001  418.826214\n",
      "4092  429.999999  430.122287\n",
      "4093  425.000000  418.104794\n",
      "4094  415.000002  412.696326\n",
      "4095  412.999998  415.465306\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0363\n",
      "Epoch 11/25, Validation Loss: 0.0368\n",
      "          actual   predicted\n",
      "0     475.000000  471.368899\n",
      "1     415.000002  420.542275\n",
      "2     473.000000  475.799395\n",
      "3     512.000001  516.216257\n",
      "4     554.000002  568.303733\n",
      "...          ...         ...\n",
      "4091  420.000001  421.431909\n",
      "4092  429.999999  431.021342\n",
      "4093  425.000000  420.336662\n",
      "4094  415.000002  416.580958\n",
      "4095  412.999998  418.377776\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0357\n",
      "Epoch 12/25, Validation Loss: 0.0336\n",
      "          actual   predicted\n",
      "0     475.000000  473.494772\n",
      "1     415.000002  423.722558\n",
      "2     473.000000  480.851534\n",
      "3     512.000001  516.068662\n",
      "4     554.000002  566.753804\n",
      "...          ...         ...\n",
      "4091  420.000001  420.745357\n",
      "4092  429.999999  430.807862\n",
      "4093  425.000000  420.054680\n",
      "4094  415.000002  415.980962\n",
      "4095  412.999998  416.674081\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0358\n",
      "Epoch 13/25, Validation Loss: 0.0342\n",
      "          actual   predicted\n",
      "0     475.000000  473.171225\n",
      "1     415.000002  418.903860\n",
      "2     473.000000  478.170028\n",
      "3     512.000001  519.961161\n",
      "4     554.000002  569.752222\n",
      "...          ...         ...\n",
      "4091  420.000001  419.593034\n",
      "4092  429.999999  429.475343\n",
      "4093  425.000000  418.076531\n",
      "4094  415.000002  415.885398\n",
      "4095  412.999998  416.599914\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0341\n",
      "Epoch 14/25, Validation Loss: 0.0315\n",
      "          actual   predicted\n",
      "0     475.000000  473.142002\n",
      "1     415.000002  421.399754\n",
      "2     473.000000  480.965882\n",
      "3     512.000001  519.515778\n",
      "4     554.000002  568.237927\n",
      "...          ...         ...\n",
      "4091  420.000001  421.422157\n",
      "4092  429.999999  430.250671\n",
      "4093  425.000000  420.737265\n",
      "4094  415.000002  418.237285\n",
      "4095  412.999998  418.444934\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0338\n",
      "Epoch 15/25, Validation Loss: 0.0316\n",
      "          actual   predicted\n",
      "0     475.000000  472.524751\n",
      "1     415.000002  421.814474\n",
      "2     473.000000  478.087492\n",
      "3     512.000001  516.642100\n",
      "4     554.000002  566.025721\n",
      "...          ...         ...\n",
      "4091  420.000001  419.435571\n",
      "4092  429.999999  428.063092\n",
      "4093  425.000000  417.220578\n",
      "4094  415.000002  417.975828\n",
      "4095  412.999998  416.299149\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0338\n",
      "Epoch 16/25, Validation Loss: 0.0316\n",
      "          actual   predicted\n",
      "0     475.000000  471.655259\n",
      "1     415.000002  421.312056\n",
      "2     473.000000  478.362434\n",
      "3     512.000001  517.196650\n",
      "4     554.000002  567.982721\n",
      "...          ...         ...\n",
      "4091  420.000001  420.426268\n",
      "4092  429.999999  429.583480\n",
      "4093  425.000000  420.011684\n",
      "4094  415.000002  416.829410\n",
      "4095  412.999998  417.560432\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0337\n",
      "Epoch 17/25, Validation Loss: 0.0339\n",
      "          actual   predicted\n",
      "0     475.000000  471.678904\n",
      "1     415.000002  422.764834\n",
      "2     473.000000  477.785784\n",
      "3     512.000001  516.260179\n",
      "4     554.000002  557.798025\n",
      "...          ...         ...\n",
      "4091  420.000001  421.576735\n",
      "4092  429.999999  429.584167\n",
      "4093  425.000000  422.888429\n",
      "4094  415.000002  418.433206\n",
      "4095  412.999998  418.763451\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0334\n",
      "Epoch 18/25, Validation Loss: 0.0321\n",
      "          actual   predicted\n",
      "0     475.000000  471.599179\n",
      "1     415.000002  421.966896\n",
      "2     473.000000  478.942017\n",
      "3     512.000001  519.341828\n",
      "4     554.000002  562.012662\n",
      "...          ...         ...\n",
      "4091  420.000001  420.811493\n",
      "4092  429.999999  429.502750\n",
      "4093  425.000000  421.480097\n",
      "4094  415.000002  417.906859\n",
      "4095  412.999998  418.044308\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0327\n",
      "Epoch 19/25, Validation Loss: 0.0311\n",
      "          actual   predicted\n",
      "0     475.000000  472.710890\n",
      "1     415.000002  420.219964\n",
      "2     473.000000  477.868394\n",
      "3     512.000001  518.386451\n",
      "4     554.000002  563.711166\n",
      "...          ...         ...\n",
      "4091  420.000001  420.134189\n",
      "4092  429.999999  428.704289\n",
      "4093  425.000000  419.586498\n",
      "4094  415.000002  416.896380\n",
      "4095  412.999998  416.839095\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0329\n",
      "Epoch 20/25, Validation Loss: 0.0312\n",
      "          actual   predicted\n",
      "0     475.000000  473.471369\n",
      "1     415.000002  419.720679\n",
      "2     473.000000  479.366135\n",
      "3     512.000001  521.296160\n",
      "4     554.000002  568.849884\n",
      "...          ...         ...\n",
      "4091  420.000001  418.922004\n",
      "4092  429.999999  428.812839\n",
      "4093  425.000000  418.996494\n",
      "4094  415.000002  416.433014\n",
      "4095  412.999998  415.979377\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0326\n",
      "Epoch 21/25, Validation Loss: 0.0310\n",
      "          actual   predicted\n",
      "0     475.000000  473.357333\n",
      "1     415.000002  420.567045\n",
      "2     473.000000  482.003689\n",
      "3     512.000001  520.304219\n",
      "4     554.000002  569.750614\n",
      "...          ...         ...\n",
      "4091  420.000001  420.168223\n",
      "4092  429.999999  429.720594\n",
      "4093  425.000000  419.234864\n",
      "4094  415.000002  415.935081\n",
      "4095  412.999998  416.847997\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0328\n",
      "Epoch 22/25, Validation Loss: 0.0314\n",
      "          actual   predicted\n",
      "0     475.000000  472.058481\n",
      "1     415.000002  421.525467\n",
      "2     473.000000  476.513219\n",
      "3     512.000001  518.545520\n",
      "4     554.000002  560.646853\n",
      "...          ...         ...\n",
      "4091  420.000001  420.451805\n",
      "4092  429.999999  429.969189\n",
      "4093  425.000000  421.396005\n",
      "4094  415.000002  417.294490\n",
      "4095  412.999998  417.076953\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0323\n",
      "Epoch 23/25, Validation Loss: 0.0318\n",
      "          actual   predicted\n",
      "0     475.000000  471.328136\n",
      "1     415.000002  420.604873\n",
      "2     473.000000  478.664001\n",
      "3     512.000001  518.066660\n",
      "4     554.000002  563.440559\n",
      "...          ...         ...\n",
      "4091  420.000001  420.849884\n",
      "4092  429.999999  429.707210\n",
      "4093  425.000000  420.754222\n",
      "4094  415.000002  416.924366\n",
      "4095  412.999998  417.289839\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0322\n",
      "Epoch 24/25, Validation Loss: 0.0311\n",
      "          actual   predicted\n",
      "0     475.000000  470.934805\n",
      "1     415.000002  420.203676\n",
      "2     473.000000  477.926399\n",
      "3     512.000001  516.481958\n",
      "4     554.000002  562.597648\n",
      "...          ...         ...\n",
      "4091  420.000001  419.390426\n",
      "4092  429.999999  428.817584\n",
      "4093  425.000000  419.877383\n",
      "4094  415.000002  415.686922\n",
      "4095  412.999998  416.053237\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0323\n",
      "Epoch 25/25, Validation Loss: 0.0319\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[0.7085]], device='cuda:0')\n",
      "[578.1]\n",
      "           actual   predicted\n",
      "0      475.000000  470.934805\n",
      "1      415.000002  420.203676\n",
      "2      473.000000  477.926399\n",
      "3      512.000001  516.481958\n",
      "4      554.000002  562.597648\n",
      "...           ...         ...\n",
      "79080  460.000000  455.841745\n",
      "79081  587.999997  588.255098\n",
      "79082  490.000000  480.200046\n",
      "79083  447.000001  449.638561\n",
      "79084  438.000001  427.009009\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 22.5269\n",
      "Score (MAE): 7.6593\n",
      "Score (ME): 0.5431\n",
      "Score (MAPE): 1.3813%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      584.999999   632.771541\n",
      "1      742.000000   744.016520\n",
      "2     1669.999979  1930.202522\n",
      "3      973.999995   936.825870\n",
      "4      515.999993   560.142307\n",
      "...           ...          ...\n",
      "4091   678.117648   684.709631\n",
      "4092   643.999999   693.356222\n",
      "4093   608.000008   618.960392\n",
      "4094   547.999999   588.418523\n",
      "4095  1370.000005  1203.730845\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.3455\n",
      "Epoch 1/25, Validation Loss: 0.1252\n",
      "           actual    predicted\n",
      "0      584.999999   616.638002\n",
      "1      742.000000   749.331039\n",
      "2     1669.999979  1756.890840\n",
      "3      973.999995   919.297423\n",
      "4      515.999993   561.492285\n",
      "...           ...          ...\n",
      "4091   678.117648   622.930307\n",
      "4092   643.999999   678.846742\n",
      "4093   608.000008   611.195038\n",
      "4094   547.999999   584.928437\n",
      "4095  1370.000005  1297.190260\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0968\n",
      "Epoch 2/25, Validation Loss: 0.0636\n",
      "           actual    predicted\n",
      "0      584.999999   613.630956\n",
      "1      742.000000   743.193749\n",
      "2     1669.999979  1724.669130\n",
      "3      973.999995   915.393640\n",
      "4      515.999993   553.357699\n",
      "...           ...          ...\n",
      "4091   678.117648   635.639761\n",
      "4092   643.999999   668.920832\n",
      "4093   608.000008   603.487175\n",
      "4094   547.999999   581.258560\n",
      "4095  1370.000005  1352.790262\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0662\n",
      "Epoch 3/25, Validation Loss: 0.0491\n",
      "           actual    predicted\n",
      "0      584.999999   622.962694\n",
      "1      742.000000   767.865994\n",
      "2     1669.999979  1772.817517\n",
      "3      973.999995   950.921267\n",
      "4      515.999993   555.088101\n",
      "...           ...          ...\n",
      "4091   678.117648   656.463038\n",
      "4092   643.999999   677.440562\n",
      "4093   608.000008   600.179337\n",
      "4094   547.999999   576.148933\n",
      "4095  1370.000005  1374.309449\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0563\n",
      "Epoch 4/25, Validation Loss: 0.0484\n",
      "           actual    predicted\n",
      "0      584.999999   618.101191\n",
      "1      742.000000   763.626276\n",
      "2     1669.999979  1684.498924\n",
      "3      973.999995   921.698884\n",
      "4      515.999993   556.357061\n",
      "...           ...          ...\n",
      "4091   678.117648   633.926482\n",
      "4092   643.999999   660.624631\n",
      "4093   608.000008   596.018788\n",
      "4094   547.999999   578.103408\n",
      "4095  1370.000005  1352.455241\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0512\n",
      "Epoch 5/25, Validation Loss: 0.0405\n",
      "           actual    predicted\n",
      "0      584.999999   611.271056\n",
      "1      742.000000   757.451525\n",
      "2     1669.999979  1715.634518\n",
      "3      973.999995   917.822733\n",
      "4      515.999993   556.441570\n",
      "...           ...          ...\n",
      "4091   678.117648   621.259502\n",
      "4092   643.999999   648.893949\n",
      "4093   608.000008   592.005978\n",
      "4094   547.999999   571.592942\n",
      "4095  1370.000005  1355.438708\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0477\n",
      "Epoch 6/25, Validation Loss: 0.0402\n",
      "           actual    predicted\n",
      "0      584.999999   617.116287\n",
      "1      742.000000   759.763503\n",
      "2     1669.999979  1678.604271\n",
      "3      973.999995   911.512006\n",
      "4      515.999993   559.821076\n",
      "...           ...          ...\n",
      "4091   678.117648   626.873924\n",
      "4092   643.999999   655.322984\n",
      "4093   608.000008   594.427194\n",
      "4094   547.999999   576.490708\n",
      "4095  1370.000005  1345.359735\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0455\n",
      "Epoch 7/25, Validation Loss: 0.0386\n",
      "           actual    predicted\n",
      "0      584.999999   625.081310\n",
      "1      742.000000   761.898247\n",
      "2     1669.999979  1693.662590\n",
      "3      973.999995   922.923369\n",
      "4      515.999993   561.734533\n",
      "...           ...          ...\n",
      "4091   678.117648   620.258001\n",
      "4092   643.999999   654.908550\n",
      "4093   608.000008   602.117987\n",
      "4094   547.999999   572.009648\n",
      "4095  1370.000005  1331.534411\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0441\n",
      "Epoch 8/25, Validation Loss: 0.0382\n",
      "           actual    predicted\n",
      "0      584.999999   615.089110\n",
      "1      742.000000   771.304417\n",
      "2     1669.999979  1679.911020\n",
      "3      973.999995   934.388522\n",
      "4      515.999993   553.190539\n",
      "...           ...          ...\n",
      "4091   678.117648   626.395446\n",
      "4092   643.999999   654.127578\n",
      "4093   608.000008   600.324654\n",
      "4094   547.999999   571.364010\n",
      "4095  1370.000005  1345.611632\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0428\n",
      "Epoch 9/25, Validation Loss: 0.0374\n",
      "           actual    predicted\n",
      "0      584.999999   621.579856\n",
      "1      742.000000   762.473186\n",
      "2     1669.999979  1741.348306\n",
      "3      973.999995   944.876894\n",
      "4      515.999993   553.510595\n",
      "...           ...          ...\n",
      "4091   678.117648   618.050893\n",
      "4092   643.999999   660.478252\n",
      "4093   608.000008   598.056720\n",
      "4094   547.999999   573.846613\n",
      "4095  1370.000005  1376.491329\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0415\n",
      "Epoch 10/25, Validation Loss: 0.0397\n",
      "           actual    predicted\n",
      "0      584.999999   616.488123\n",
      "1      742.000000   764.029442\n",
      "2     1669.999979  1692.973811\n",
      "3      973.999995   918.328220\n",
      "4      515.999993   557.271157\n",
      "...           ...          ...\n",
      "4091   678.117648   622.417109\n",
      "4092   643.999999   657.426907\n",
      "4093   608.000008   601.068591\n",
      "4094   547.999999   578.560096\n",
      "4095  1370.000005  1351.139018\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0410\n",
      "Epoch 11/25, Validation Loss: 0.0371\n",
      "           actual    predicted\n",
      "0      584.999999   628.357569\n",
      "1      742.000000   764.984234\n",
      "2     1669.999979  1659.529423\n",
      "3      973.999995   926.322437\n",
      "4      515.999993   558.315097\n",
      "...           ...          ...\n",
      "4091   678.117648   627.150312\n",
      "4092   643.999999   653.652213\n",
      "4093   608.000008   601.995196\n",
      "4094   547.999999   581.262560\n",
      "4095  1370.000005  1325.228386\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0405\n",
      "Epoch 12/25, Validation Loss: 0.0373\n",
      "           actual    predicted\n",
      "0      584.999999   631.291264\n",
      "1      742.000000   772.390010\n",
      "2     1669.999979  1691.827625\n",
      "3      973.999995   943.087175\n",
      "4      515.999993   556.625572\n",
      "...           ...          ...\n",
      "4091   678.117648   625.722878\n",
      "4092   643.999999   660.396628\n",
      "4093   608.000008   602.282830\n",
      "4094   547.999999   576.537340\n",
      "4095  1370.000005  1357.331112\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0403\n",
      "Epoch 13/25, Validation Loss: 0.0368\n",
      "           actual    predicted\n",
      "0      584.999999   610.914456\n",
      "1      742.000000   769.612574\n",
      "2     1669.999979  1711.433846\n",
      "3      973.999995   948.654614\n",
      "4      515.999993   549.266853\n",
      "...           ...          ...\n",
      "4091   678.117648   611.970501\n",
      "4092   643.999999   657.646076\n",
      "4093   608.000008   590.637386\n",
      "4094   547.999999   567.377007\n",
      "4095  1370.000005  1374.917981\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0393\n",
      "Epoch 14/25, Validation Loss: 0.0379\n",
      "           actual    predicted\n",
      "0      584.999999   624.988748\n",
      "1      742.000000   768.112999\n",
      "2     1669.999979  1707.181525\n",
      "3      973.999995   933.249108\n",
      "4      515.999993   550.366846\n",
      "...           ...          ...\n",
      "4091   678.117648   612.122064\n",
      "4092   643.999999   650.088302\n",
      "4093   608.000008   593.992453\n",
      "4094   547.999999   571.375291\n",
      "4095  1370.000005  1360.172718\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0391\n",
      "Epoch 15/25, Validation Loss: 0.0362\n",
      "           actual    predicted\n",
      "0      584.999999   620.966534\n",
      "1      742.000000   766.568502\n",
      "2     1669.999979  1707.037664\n",
      "3      973.999995   935.299101\n",
      "4      515.999993   555.419332\n",
      "...           ...          ...\n",
      "4091   678.117648   614.931793\n",
      "4092   643.999999   655.320229\n",
      "4093   608.000008   598.995308\n",
      "4094   547.999999   568.822460\n",
      "4095  1370.000005  1355.040073\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0390\n",
      "Epoch 16/25, Validation Loss: 0.0367\n",
      "           actual    predicted\n",
      "0      584.999999   620.950499\n",
      "1      742.000000   765.436107\n",
      "2     1669.999979  1721.046289\n",
      "3      973.999995   932.986351\n",
      "4      515.999993   557.128085\n",
      "...           ...          ...\n",
      "4091   678.117648   619.479924\n",
      "4092   643.999999   647.847685\n",
      "4093   608.000008   597.979404\n",
      "4094   547.999999   574.755183\n",
      "4095  1370.000005  1369.857161\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0386\n",
      "Epoch 17/25, Validation Loss: 0.0364\n",
      "           actual    predicted\n",
      "0      584.999999   619.885840\n",
      "1      742.000000   766.660165\n",
      "2     1669.999979  1699.559488\n",
      "3      973.999995   936.360120\n",
      "4      515.999993   555.553299\n",
      "...           ...          ...\n",
      "4091   678.117648   617.494238\n",
      "4092   643.999999   649.283689\n",
      "4093   608.000008   594.645267\n",
      "4094   547.999999   572.774567\n",
      "4095  1370.000005  1355.307760\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0386\n",
      "Epoch 18/25, Validation Loss: 0.0359\n",
      "           actual    predicted\n",
      "0      584.999999   630.856944\n",
      "1      742.000000   768.203693\n",
      "2     1669.999979  1684.304887\n",
      "3      973.999995   935.421558\n",
      "4      515.999993   560.032780\n",
      "...           ...          ...\n",
      "4091   678.117648   621.751208\n",
      "4092   643.999999   656.652049\n",
      "4093   608.000008   596.897727\n",
      "4094   547.999999   579.551983\n",
      "4095  1370.000005  1350.312063\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0384\n",
      "Epoch 19/25, Validation Loss: 0.0360\n",
      "           actual    predicted\n",
      "0      584.999999   633.296143\n",
      "1      742.000000   766.196796\n",
      "2     1669.999979  1699.957878\n",
      "3      973.999995   930.676171\n",
      "4      515.999993   558.354974\n",
      "...           ...          ...\n",
      "4091   678.117648   628.469465\n",
      "4092   643.999999   652.076883\n",
      "4093   608.000008   599.542173\n",
      "4094   547.999999   576.881975\n",
      "4095  1370.000005  1340.826920\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0378\n",
      "Epoch 20/25, Validation Loss: 0.0365\n",
      "           actual    predicted\n",
      "0      584.999999   627.785389\n",
      "1      742.000000   765.574871\n",
      "2     1669.999979  1678.551709\n",
      "3      973.999995   935.442795\n",
      "4      515.999993   558.928980\n",
      "...           ...          ...\n",
      "4091   678.117648   613.099740\n",
      "4092   643.999999   647.915957\n",
      "4093   608.000008   593.421957\n",
      "4094   547.999999   576.332829\n",
      "4095  1370.000005  1346.136936\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0378\n",
      "Epoch 21/25, Validation Loss: 0.0358\n",
      "           actual    predicted\n",
      "0      584.999999   623.359557\n",
      "1      742.000000   768.548369\n",
      "2     1669.999979  1681.622756\n",
      "3      973.999995   923.651087\n",
      "4      515.999993   559.468178\n",
      "...           ...          ...\n",
      "4091   678.117648   615.151742\n",
      "4092   643.999999   652.837583\n",
      "4093   608.000008   599.909071\n",
      "4094   547.999999   573.401538\n",
      "4095  1370.000005  1341.301205\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0378\n",
      "Epoch 22/25, Validation Loss: 0.0361\n",
      "           actual    predicted\n",
      "0      584.999999   621.599312\n",
      "1      742.000000   767.407040\n",
      "2     1669.999979  1681.376508\n",
      "3      973.999995   935.760974\n",
      "4      515.999993   557.792390\n",
      "...           ...          ...\n",
      "4091   678.117648   616.877688\n",
      "4092   643.999999   656.462635\n",
      "4093   608.000008   604.374447\n",
      "4094   547.999999   579.963706\n",
      "4095  1370.000005  1379.264303\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0374\n",
      "Epoch 23/25, Validation Loss: 0.0359\n",
      "           actual    predicted\n",
      "0      584.999999   621.327380\n",
      "1      742.000000   769.539045\n",
      "2     1669.999979  1699.837736\n",
      "3      973.999995   935.632771\n",
      "4      515.999993   553.886125\n",
      "...           ...          ...\n",
      "4091   678.117648   607.502634\n",
      "4092   643.999999   645.276274\n",
      "4093   608.000008   586.747227\n",
      "4094   547.999999   571.035990\n",
      "4095  1370.000005  1370.803486\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0377\n",
      "Epoch 24/25, Validation Loss: 0.0363\n",
      "           actual    predicted\n",
      "0      584.999999   622.399706\n",
      "1      742.000000   764.551750\n",
      "2     1669.999979  1731.409755\n",
      "3      973.999995   934.127274\n",
      "4      515.999993   551.402312\n",
      "...           ...          ...\n",
      "4091   678.117648   616.640721\n",
      "4092   643.999999   657.357942\n",
      "4093   608.000008   598.881763\n",
      "4094   547.999999   572.202843\n",
      "4095  1370.000005  1384.022243\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0373\n",
      "Epoch 25/25, Validation Loss: 0.0373\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[-0.5591]], device='cuda:0')\n",
      "[625.95]\n",
      "            actual    predicted\n",
      "0       584.999999   622.399706\n",
      "1       742.000000   764.551750\n",
      "2      1669.999979  1731.409755\n",
      "3       973.999995   934.127274\n",
      "4       515.999993   551.402312\n",
      "...            ...          ...\n",
      "79080   840.000002   831.978176\n",
      "79081   552.000002   568.021435\n",
      "79082   571.000007   570.414756\n",
      "79083   641.000005   630.441484\n",
      "79084   759.000000   731.200622\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 56.3847\n",
      "Score (MAE): 28.7625\n",
      "Score (ME): -5.3159\n",
      "Score (MAPE): 3.5592%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     41.920000  40.569228\n",
      "1     36.420000  34.653461\n",
      "2     34.990000  35.257856\n",
      "3     36.090000  35.793559\n",
      "4     53.120000  51.121710\n",
      "...         ...        ...\n",
      "4091  60.689999  61.832224\n",
      "4092  33.720000  34.212707\n",
      "4093  25.300000  24.363336\n",
      "4094  35.320000  34.308107\n",
      "4095  41.790000  41.136567\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2489\n",
      "Epoch 1/25, Validation Loss: 0.0255\n",
      "         actual  predicted\n",
      "0     41.920000  41.268766\n",
      "1     36.420000  35.435722\n",
      "2     34.990000  35.235334\n",
      "3     36.090000  36.296458\n",
      "4     53.120000  51.269915\n",
      "...         ...        ...\n",
      "4091  60.689999  60.977300\n",
      "4092  33.720000  34.084506\n",
      "4093  25.300000  24.771080\n",
      "4094  35.320000  34.830520\n",
      "4095  41.790000  41.480482\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0219\n",
      "Epoch 2/25, Validation Loss: 0.0147\n",
      "         actual  predicted\n",
      "0     41.920000  41.712702\n",
      "1     36.420000  35.731184\n",
      "2     34.990000  35.446676\n",
      "3     36.090000  36.218162\n",
      "4     53.120000  52.461447\n",
      "...         ...        ...\n",
      "4091  60.689999  61.544378\n",
      "4092  33.720000  33.921219\n",
      "4093  25.300000  25.034395\n",
      "4094  35.320000  35.402047\n",
      "4095  41.790000  42.113781\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0168\n",
      "Epoch 3/25, Validation Loss: 0.0122\n",
      "         actual  predicted\n",
      "0     41.920000  41.622890\n",
      "1     36.420000  35.871271\n",
      "2     34.990000  35.168235\n",
      "3     36.090000  36.186433\n",
      "4     53.120000  51.952669\n",
      "...         ...        ...\n",
      "4091  60.689999  60.156331\n",
      "4092  33.720000  33.618875\n",
      "4093  25.300000  25.089370\n",
      "4094  35.320000  35.198565\n",
      "4095  41.790000  42.097414\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0142\n",
      "Epoch 4/25, Validation Loss: 0.0096\n",
      "         actual  predicted\n",
      "0     41.920000  41.647124\n",
      "1     36.420000  36.044654\n",
      "2     34.990000  35.160437\n",
      "3     36.090000  36.194871\n",
      "4     53.120000  52.428782\n",
      "...         ...        ...\n",
      "4091  60.689999  60.809955\n",
      "4092  33.720000  33.559347\n",
      "4093  25.300000  25.313889\n",
      "4094  35.320000  35.321650\n",
      "4095  41.790000  42.395386\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0125\n",
      "Epoch 5/25, Validation Loss: 0.0083\n",
      "         actual  predicted\n",
      "0     41.920000  41.956941\n",
      "1     36.420000  36.091481\n",
      "2     34.990000  35.238119\n",
      "3     36.090000  36.121020\n",
      "4     53.120000  52.585159\n",
      "...         ...        ...\n",
      "4091  60.689999  60.670613\n",
      "4092  33.720000  33.401638\n",
      "4093  25.300000  25.285783\n",
      "4094  35.320000  35.284931\n",
      "4095  41.790000  42.367483\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0111\n",
      "Epoch 6/25, Validation Loss: 0.0075\n",
      "         actual  predicted\n",
      "0     41.920000  41.861367\n",
      "1     36.420000  35.961878\n",
      "2     34.990000  35.194436\n",
      "3     36.090000  36.195772\n",
      "4     53.120000  52.403713\n",
      "...         ...        ...\n",
      "4091  60.689999  60.149255\n",
      "4092  33.720000  33.326828\n",
      "4093  25.300000  25.124346\n",
      "4094  35.320000  35.314545\n",
      "4095  41.790000  42.193667\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0100\n",
      "Epoch 7/25, Validation Loss: 0.0065\n",
      "         actual  predicted\n",
      "0     41.920000  41.976644\n",
      "1     36.420000  36.124975\n",
      "2     34.990000  35.347913\n",
      "3     36.090000  36.275898\n",
      "4     53.120000  52.954954\n",
      "...         ...        ...\n",
      "4091  60.689999  60.824637\n",
      "4092  33.720000  33.389948\n",
      "4093  25.300000  25.077811\n",
      "4094  35.320000  35.205463\n",
      "4095  41.790000  42.385424\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0091\n",
      "Epoch 8/25, Validation Loss: 0.0065\n",
      "         actual  predicted\n",
      "0     41.920000  41.757523\n",
      "1     36.420000  35.999648\n",
      "2     34.990000  35.164742\n",
      "3     36.090000  36.176001\n",
      "4     53.120000  52.489637\n",
      "...         ...        ...\n",
      "4091  60.689999  60.342896\n",
      "4092  33.720000  33.335818\n",
      "4093  25.300000  25.409349\n",
      "4094  35.320000  35.175354\n",
      "4095  41.790000  42.256242\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0085\n",
      "Epoch 9/25, Validation Loss: 0.0055\n",
      "         actual  predicted\n",
      "0     41.920000  41.843278\n",
      "1     36.420000  35.947999\n",
      "2     34.990000  35.182769\n",
      "3     36.090000  36.197452\n",
      "4     53.120000  52.518444\n",
      "...         ...        ...\n",
      "4091  60.689999  60.135803\n",
      "4092  33.720000  33.243094\n",
      "4093  25.300000  25.096968\n",
      "4094  35.320000  35.266537\n",
      "4095  41.790000  42.228275\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0081\n",
      "Epoch 10/25, Validation Loss: 0.0051\n",
      "         actual  predicted\n",
      "0     41.920000  42.004377\n",
      "1     36.420000  36.128935\n",
      "2     34.990000  35.195680\n",
      "3     36.090000  36.263017\n",
      "4     53.120000  52.885650\n",
      "...         ...        ...\n",
      "4091  60.689999  60.638880\n",
      "4092  33.720000  33.271245\n",
      "4093  25.300000  25.082998\n",
      "4094  35.320000  35.277340\n",
      "4095  41.790000  42.385604\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0074\n",
      "Epoch 11/25, Validation Loss: 0.0051\n",
      "         actual  predicted\n",
      "0     41.920000  41.874696\n",
      "1     36.420000  36.096868\n",
      "2     34.990000  35.282635\n",
      "3     36.090000  36.194847\n",
      "4     53.120000  52.601861\n",
      "...         ...        ...\n",
      "4091  60.689999  60.340995\n",
      "4092  33.720000  33.284261\n",
      "4093  25.300000  25.455093\n",
      "4094  35.320000  35.266690\n",
      "4095  41.790000  42.165159\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0071\n",
      "Epoch 12/25, Validation Loss: 0.0046\n",
      "         actual  predicted\n",
      "0     41.920000  41.758057\n",
      "1     36.420000  36.080064\n",
      "2     34.990000  35.136979\n",
      "3     36.090000  36.227039\n",
      "4     53.120000  52.724033\n",
      "...         ...        ...\n",
      "4091  60.689999  60.298447\n",
      "4092  33.720000  33.373272\n",
      "4093  25.300000  25.197215\n",
      "4094  35.320000  35.201642\n",
      "4095  41.790000  42.177905\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0067\n",
      "Epoch 13/25, Validation Loss: 0.0043\n",
      "         actual  predicted\n",
      "0     41.920000  41.982660\n",
      "1     36.420000  36.099863\n",
      "2     34.990000  35.145195\n",
      "3     36.090000  36.218042\n",
      "4     53.120000  53.144359\n",
      "...         ...        ...\n",
      "4091  60.689999  60.677910\n",
      "4092  33.720000  33.232433\n",
      "4093  25.300000  25.233379\n",
      "4094  35.320000  35.281743\n",
      "4095  41.790000  42.256020\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0065\n",
      "Epoch 14/25, Validation Loss: 0.0044\n",
      "         actual  predicted\n",
      "0     41.920000  42.053783\n",
      "1     36.420000  36.041172\n",
      "2     34.990000  35.123814\n",
      "3     36.090000  36.202881\n",
      "4     53.120000  53.011529\n",
      "...         ...        ...\n",
      "4091  60.689999  60.650672\n",
      "4092  33.720000  33.166895\n",
      "4093  25.300000  25.092713\n",
      "4094  35.320000  35.226446\n",
      "4095  41.790000  42.268030\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0062\n",
      "Epoch 15/25, Validation Loss: 0.0041\n",
      "         actual  predicted\n",
      "0     41.920000  42.034583\n",
      "1     36.420000  36.077258\n",
      "2     34.990000  35.164566\n",
      "3     36.090000  36.179493\n",
      "4     53.120000  52.508678\n",
      "...         ...        ...\n",
      "4091  60.689999  59.840971\n",
      "4092  33.720000  33.084242\n",
      "4093  25.300000  25.348260\n",
      "4094  35.320000  35.267010\n",
      "4095  41.790000  42.238202\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0059\n",
      "Epoch 16/25, Validation Loss: 0.0038\n",
      "         actual  predicted\n",
      "0     41.920000  41.912754\n",
      "1     36.420000  36.001531\n",
      "2     34.990000  35.017238\n",
      "3     36.090000  36.051800\n",
      "4     53.120000  52.930152\n",
      "...         ...        ...\n",
      "4091  60.689999  60.483532\n",
      "4092  33.720000  33.062488\n",
      "4093  25.300000  25.077889\n",
      "4094  35.320000  35.094226\n",
      "4095  41.790000  42.091052\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0057\n",
      "Epoch 17/25, Validation Loss: 0.0037\n",
      "         actual  predicted\n",
      "0     41.920000  41.825505\n",
      "1     36.420000  36.107115\n",
      "2     34.990000  35.062549\n",
      "3     36.090000  36.277761\n",
      "4     53.120000  52.636955\n",
      "...         ...        ...\n",
      "4091  60.689999  60.036039\n",
      "4092  33.720000  33.309444\n",
      "4093  25.300000  25.339324\n",
      "4094  35.320000  35.148548\n",
      "4095  41.790000  42.079882\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0055\n",
      "Epoch 18/25, Validation Loss: 0.0036\n",
      "         actual  predicted\n",
      "0     41.920000  42.035230\n",
      "1     36.420000  36.078966\n",
      "2     34.990000  35.007484\n",
      "3     36.090000  36.264779\n",
      "4     53.120000  52.978336\n",
      "...         ...        ...\n",
      "4091  60.689999  60.524076\n",
      "4092  33.720000  33.153904\n",
      "4093  25.300000  25.272205\n",
      "4094  35.320000  35.270659\n",
      "4095  41.790000  42.353735\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0052\n",
      "Epoch 19/25, Validation Loss: 0.0034\n",
      "         actual  predicted\n",
      "0     41.920000  42.159310\n",
      "1     36.420000  36.169002\n",
      "2     34.990000  35.174239\n",
      "3     36.090000  36.207682\n",
      "4     53.120000  53.301895\n",
      "...         ...        ...\n",
      "4091  60.689999  60.865696\n",
      "4092  33.720000  33.149855\n",
      "4093  25.300000  25.180744\n",
      "4094  35.320000  35.267093\n",
      "4095  41.790000  42.411864\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0050\n",
      "Epoch 20/25, Validation Loss: 0.0038\n",
      "         actual  predicted\n",
      "0     41.920000  41.911385\n",
      "1     36.420000  36.102321\n",
      "2     34.990000  34.988781\n",
      "3     36.090000  36.129459\n",
      "4     53.120000  52.860159\n",
      "...         ...        ...\n",
      "4091  60.689999  60.263853\n",
      "4092  33.720000  33.103071\n",
      "4093  25.300000  24.989836\n",
      "4094  35.320000  35.156072\n",
      "4095  41.790000  42.168533\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0051\n",
      "Epoch 21/25, Validation Loss: 0.0032\n",
      "         actual  predicted\n",
      "0     41.920000  41.842398\n",
      "1     36.420000  36.041752\n",
      "2     34.990000  34.969588\n",
      "3     36.090000  36.114560\n",
      "4     53.120000  52.742276\n",
      "...         ...        ...\n",
      "4091  60.689999  60.125675\n",
      "4092  33.720000  33.140097\n",
      "4093  25.300000  25.313912\n",
      "4094  35.320000  35.296387\n",
      "4095  41.790000  42.119567\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0047\n",
      "Epoch 22/25, Validation Loss: 0.0031\n",
      "         actual  predicted\n",
      "0     41.920000  42.035515\n",
      "1     36.420000  36.176472\n",
      "2     34.990000  35.018954\n",
      "3     36.090000  36.151136\n",
      "4     53.120000  53.002047\n",
      "...         ...        ...\n",
      "4091  60.689999  60.399217\n",
      "4092  33.720000  33.062356\n",
      "4093  25.300000  25.047136\n",
      "4094  35.320000  35.217405\n",
      "4095  41.790000  42.136057\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0047\n",
      "Epoch 23/25, Validation Loss: 0.0029\n",
      "         actual  predicted\n",
      "0     41.920000  42.227419\n",
      "1     36.420000  36.211528\n",
      "2     34.990000  35.121644\n",
      "3     36.090000  36.107548\n",
      "4     53.120000  53.132496\n",
      "...         ...        ...\n",
      "4091  60.689999  60.447750\n",
      "4092  33.720000  33.057657\n",
      "4093  25.300000  25.141306\n",
      "4094  35.320000  35.124926\n",
      "4095  41.790000  42.407468\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0045\n",
      "Epoch 24/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     41.920000  42.091172\n",
      "1     36.420000  36.236425\n",
      "2     34.990000  35.230764\n",
      "3     36.090000  36.256684\n",
      "4     53.120000  52.794197\n",
      "...         ...        ...\n",
      "4091  60.689999  60.198785\n",
      "4092  33.720000  33.204368\n",
      "4093  25.300000  25.295900\n",
      "4094  35.320000  35.275216\n",
      "4095  41.790000  42.092456\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0044\n",
      "Epoch 25/25, Validation Loss: 0.0028\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[1.1324]], device='cuda:0')\n",
      "[46.4]\n",
      "       actual  predicted\n",
      "0       41.92  42.091172\n",
      "1       36.42  36.236425\n",
      "2       34.99  35.230764\n",
      "3       36.09  36.256684\n",
      "4       53.12  52.794197\n",
      "...       ...        ...\n",
      "79080   21.06  21.167783\n",
      "79081   48.19  48.327168\n",
      "79082   38.37  38.459043\n",
      "79083   44.97  45.182688\n",
      "79084   40.48  40.732440\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 0.5124\n",
      "Score (MAE): 0.2741\n",
      "Score (ME): -0.0247\n",
      "Score (MAPE): 0.7679%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     25.990000  25.733705\n",
      "1     26.870000  27.039992\n",
      "2     21.800000  21.873361\n",
      "3     23.320000  23.267532\n",
      "4     18.590000  18.084455\n",
      "...         ...        ...\n",
      "4091  20.910000  21.361675\n",
      "4092  23.000000  23.063573\n",
      "4093  22.950000  23.181920\n",
      "4094  24.506667  23.799655\n",
      "4095  23.570000  23.694759\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2644\n",
      "Epoch 1/25, Validation Loss: 0.0251\n",
      "         actual  predicted\n",
      "0     25.990000  25.839211\n",
      "1     26.870000  27.031382\n",
      "2     21.800000  21.740520\n",
      "3     23.320000  23.295955\n",
      "4     18.590000  18.266493\n",
      "...         ...        ...\n",
      "4091  20.910000  21.535108\n",
      "4092  23.000000  23.018575\n",
      "4093  22.950000  23.074475\n",
      "4094  24.506667  24.091455\n",
      "4095  23.570000  23.459508\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0204\n",
      "Epoch 2/25, Validation Loss: 0.0132\n",
      "         actual  predicted\n",
      "0     25.990000  25.790690\n",
      "1     26.870000  26.962003\n",
      "2     21.800000  21.760482\n",
      "3     23.320000  23.261409\n",
      "4     18.590000  18.297059\n",
      "...         ...        ...\n",
      "4091  20.910000  21.535215\n",
      "4092  23.000000  22.990805\n",
      "4093  22.950000  22.999147\n",
      "4094  24.506667  24.226379\n",
      "4095  23.570000  23.357469\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0150\n",
      "Epoch 3/25, Validation Loss: 0.0099\n",
      "         actual  predicted\n",
      "0     25.990000  25.816104\n",
      "1     26.870000  27.012725\n",
      "2     21.800000  21.669397\n",
      "3     23.320000  23.247071\n",
      "4     18.590000  18.336071\n",
      "...         ...        ...\n",
      "4091  20.910000  21.511177\n",
      "4092  23.000000  23.019301\n",
      "4093  22.950000  22.935233\n",
      "4094  24.506667  24.304846\n",
      "4095  23.570000  23.289173\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0126\n",
      "Epoch 4/25, Validation Loss: 0.0082\n",
      "         actual  predicted\n",
      "0     25.990000  25.758799\n",
      "1     26.870000  26.891584\n",
      "2     21.800000  21.671054\n",
      "3     23.320000  23.297575\n",
      "4     18.590000  18.408779\n",
      "...         ...        ...\n",
      "4091  20.910000  21.508541\n",
      "4092  23.000000  23.021380\n",
      "4093  22.950000  22.978637\n",
      "4094  24.506667  24.275462\n",
      "4095  23.570000  23.287286\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0107\n",
      "Epoch 5/25, Validation Loss: 0.0068\n",
      "         actual  predicted\n",
      "0     25.990000  25.770758\n",
      "1     26.870000  26.918765\n",
      "2     21.800000  21.703732\n",
      "3     23.320000  23.288850\n",
      "4     18.590000  18.544236\n",
      "...         ...        ...\n",
      "4091  20.910000  21.523739\n",
      "4092  23.000000  22.998230\n",
      "4093  22.950000  22.925575\n",
      "4094  24.506667  24.312181\n",
      "4095  23.570000  23.222973\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0095\n",
      "Epoch 6/25, Validation Loss: 0.0060\n",
      "         actual  predicted\n",
      "0     25.990000  25.826896\n",
      "1     26.870000  26.991277\n",
      "2     21.800000  21.728702\n",
      "3     23.320000  23.380841\n",
      "4     18.590000  18.495500\n",
      "...         ...        ...\n",
      "4091  20.910000  21.386608\n",
      "4092  23.000000  23.051805\n",
      "4093  22.950000  23.049452\n",
      "4094  24.506667  24.340399\n",
      "4095  23.570000  23.205929\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0085\n",
      "Epoch 7/25, Validation Loss: 0.0057\n",
      "         actual  predicted\n",
      "0     25.990000  25.802284\n",
      "1     26.870000  26.888715\n",
      "2     21.800000  21.776698\n",
      "3     23.320000  23.371841\n",
      "4     18.590000  18.632967\n",
      "...         ...        ...\n",
      "4091  20.910000  21.454954\n",
      "4092  23.000000  23.037545\n",
      "4093  22.950000  22.950384\n",
      "4094  24.506667  24.299176\n",
      "4095  23.570000  23.230336\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0078\n",
      "Epoch 8/25, Validation Loss: 0.0049\n",
      "         actual  predicted\n",
      "0     25.990000  25.847167\n",
      "1     26.870000  27.015083\n",
      "2     21.800000  21.680989\n",
      "3     23.320000  23.302801\n",
      "4     18.590000  18.459066\n",
      "...         ...        ...\n",
      "4091  20.910000  21.298430\n",
      "4092  23.000000  22.955801\n",
      "4093  22.950000  22.949149\n",
      "4094  24.506667  24.277262\n",
      "4095  23.570000  23.212028\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0071\n",
      "Epoch 9/25, Validation Loss: 0.0045\n",
      "         actual  predicted\n",
      "0     25.990000  25.832780\n",
      "1     26.870000  26.939120\n",
      "2     21.800000  21.779207\n",
      "3     23.320000  23.335650\n",
      "4     18.590000  18.590482\n",
      "...         ...        ...\n",
      "4091  20.910000  21.347620\n",
      "4092  23.000000  23.015980\n",
      "4093  22.950000  22.961899\n",
      "4094  24.506667  24.367663\n",
      "4095  23.570000  23.209531\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0066\n",
      "Epoch 10/25, Validation Loss: 0.0040\n",
      "         actual  predicted\n",
      "0     25.990000  25.957346\n",
      "1     26.870000  27.064526\n",
      "2     21.800000  21.698873\n",
      "3     23.320000  23.331923\n",
      "4     18.590000  18.468462\n",
      "...         ...        ...\n",
      "4091  20.910000  21.290868\n",
      "4092  23.000000  22.954481\n",
      "4093  22.950000  22.893126\n",
      "4094  24.506667  24.357967\n",
      "4095  23.570000  23.297501\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0062\n",
      "Epoch 11/25, Validation Loss: 0.0049\n",
      "         actual  predicted\n",
      "0     25.990000  25.924635\n",
      "1     26.870000  26.937181\n",
      "2     21.800000  21.752455\n",
      "3     23.320000  23.374149\n",
      "4     18.590000  18.520736\n",
      "...         ...        ...\n",
      "4091  20.910000  21.303300\n",
      "4092  23.000000  23.033977\n",
      "4093  22.950000  22.923863\n",
      "4094  24.506667  24.357911\n",
      "4095  23.570000  23.281976\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0057\n",
      "Epoch 12/25, Validation Loss: 0.0035\n",
      "         actual  predicted\n",
      "0     25.990000  25.851086\n",
      "1     26.870000  26.864742\n",
      "2     21.800000  21.745127\n",
      "3     23.320000  23.368582\n",
      "4     18.590000  18.566973\n",
      "...         ...        ...\n",
      "4091  20.910000  21.329345\n",
      "4092  23.000000  23.036727\n",
      "4093  22.950000  22.927983\n",
      "4094  24.506667  24.345833\n",
      "4095  23.570000  23.254131\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0054\n",
      "Epoch 13/25, Validation Loss: 0.0033\n",
      "         actual  predicted\n",
      "0     25.990000  25.844196\n",
      "1     26.870000  26.867600\n",
      "2     21.800000  21.766845\n",
      "3     23.320000  23.380628\n",
      "4     18.590000  18.575463\n",
      "...         ...        ...\n",
      "4091  20.910000  21.285987\n",
      "4092  23.000000  23.052551\n",
      "4093  22.950000  22.944102\n",
      "4094  24.506667  24.398429\n",
      "4095  23.570000  23.244470\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0053\n",
      "Epoch 14/25, Validation Loss: 0.0032\n",
      "         actual  predicted\n",
      "0     25.990000  25.891919\n",
      "1     26.870000  26.939242\n",
      "2     21.800000  21.699920\n",
      "3     23.320000  23.330158\n",
      "4     18.590000  18.420759\n",
      "...         ...        ...\n",
      "4091  20.910000  21.151537\n",
      "4092  23.000000  22.984496\n",
      "4093  22.950000  22.899051\n",
      "4094  24.506667  24.270444\n",
      "4095  23.570000  23.219509\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0049\n",
      "Epoch 15/25, Validation Loss: 0.0032\n",
      "         actual  predicted\n",
      "0     25.990000  25.924666\n",
      "1     26.870000  27.058815\n",
      "2     21.800000  21.719994\n",
      "3     23.320000  23.351412\n",
      "4     18.590000  18.456185\n",
      "...         ...        ...\n",
      "4091  20.910000  21.166964\n",
      "4092  23.000000  22.970363\n",
      "4093  22.950000  22.923872\n",
      "4094  24.506667  24.366454\n",
      "4095  23.570000  23.249406\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0048\n",
      "Epoch 16/25, Validation Loss: 0.0036\n",
      "         actual  predicted\n",
      "0     25.990000  25.860129\n",
      "1     26.870000  26.851558\n",
      "2     21.800000  21.750525\n",
      "3     23.320000  23.345007\n",
      "4     18.590000  18.544248\n",
      "...         ...        ...\n",
      "4091  20.910000  21.199698\n",
      "4092  23.000000  23.008822\n",
      "4093  22.950000  22.912510\n",
      "4094  24.506667  24.339770\n",
      "4095  23.570000  23.208502\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0046\n",
      "Epoch 17/25, Validation Loss: 0.0027\n",
      "         actual  predicted\n",
      "0     25.990000  25.987111\n",
      "1     26.870000  26.997647\n",
      "2     21.800000  21.724804\n",
      "3     23.320000  23.287330\n",
      "4     18.590000  18.487442\n",
      "...         ...        ...\n",
      "4091  20.910000  21.114333\n",
      "4092  23.000000  22.919268\n",
      "4093  22.950000  22.923277\n",
      "4094  24.506667  24.409268\n",
      "4095  23.570000  23.247555\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0043\n",
      "Epoch 18/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     25.990000  25.976782\n",
      "1     26.870000  26.953802\n",
      "2     21.800000  21.710171\n",
      "3     23.320000  23.317414\n",
      "4     18.590000  18.570064\n",
      "...         ...        ...\n",
      "4091  20.910000  21.191107\n",
      "4092  23.000000  22.964604\n",
      "4093  22.950000  22.885294\n",
      "4094  24.506667  24.349787\n",
      "4095  23.570000  23.298930\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0042\n",
      "Epoch 19/25, Validation Loss: 0.0029\n",
      "         actual  predicted\n",
      "0     25.990000  25.922726\n",
      "1     26.870000  26.881639\n",
      "2     21.800000  21.810617\n",
      "3     23.320000  23.328863\n",
      "4     18.590000  18.697067\n",
      "...         ...        ...\n",
      "4091  20.910000  21.205796\n",
      "4092  23.000000  23.040443\n",
      "4093  22.950000  22.966839\n",
      "4094  24.506667  24.450586\n",
      "4095  23.570000  23.319442\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0040\n",
      "Epoch 20/25, Validation Loss: 0.0023\n",
      "         actual  predicted\n",
      "0     25.990000  26.013506\n",
      "1     26.870000  26.946947\n",
      "2     21.800000  21.791778\n",
      "3     23.320000  23.338369\n",
      "4     18.590000  18.628657\n",
      "...         ...        ...\n",
      "4091  20.910000  21.240239\n",
      "4092  23.000000  22.990971\n",
      "4093  22.950000  22.891630\n",
      "4094  24.506667  24.433067\n",
      "4095  23.570000  23.376311\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0038\n",
      "Epoch 21/25, Validation Loss: 0.0023\n",
      "         actual  predicted\n",
      "0     25.990000  25.912734\n",
      "1     26.870000  26.883907\n",
      "2     21.800000  21.793360\n",
      "3     23.320000  23.364196\n",
      "4     18.590000  18.647914\n",
      "...         ...        ...\n",
      "4091  20.910000  21.234349\n",
      "4092  23.000000  22.992176\n",
      "4093  22.950000  22.938809\n",
      "4094  24.506667  24.356133\n",
      "4095  23.570000  23.303742\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0037\n",
      "Epoch 22/25, Validation Loss: 0.0021\n",
      "         actual  predicted\n",
      "0     25.990000  25.941045\n",
      "1     26.870000  26.908054\n",
      "2     21.800000  21.706729\n",
      "3     23.320000  23.324539\n",
      "4     18.590000  18.552604\n",
      "...         ...        ...\n",
      "4091  20.910000  21.155422\n",
      "4092  23.000000  22.982608\n",
      "4093  22.950000  22.892554\n",
      "4094  24.506667  24.297977\n",
      "4095  23.570000  23.322118\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0037\n",
      "Epoch 23/25, Validation Loss: 0.0021\n",
      "         actual  predicted\n",
      "0     25.990000  25.987356\n",
      "1     26.870000  26.897838\n",
      "2     21.800000  21.788298\n",
      "3     23.320000  23.341550\n",
      "4     18.590000  18.717136\n",
      "...         ...        ...\n",
      "4091  20.910000  21.260257\n",
      "4092  23.000000  23.014217\n",
      "4093  22.950000  22.924418\n",
      "4094  24.506667  24.438883\n",
      "4095  23.570000  23.355911\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0034\n",
      "Epoch 24/25, Validation Loss: 0.0021\n",
      "         actual  predicted\n",
      "0     25.990000  26.026943\n",
      "1     26.870000  26.913672\n",
      "2     21.800000  21.750539\n",
      "3     23.320000  23.318877\n",
      "4     18.590000  18.566325\n",
      "...         ...        ...\n",
      "4091  20.910000  21.094162\n",
      "4092  23.000000  22.949132\n",
      "4093  22.950000  22.916322\n",
      "4094  24.506667  24.392696\n",
      "4095  23.570000  23.322281\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0034\n",
      "Epoch 25/25, Validation Loss: 0.0022\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[0.3879]], device='cuda:0')\n",
      "[25.46]\n",
      "       actual  predicted\n",
      "0       25.99  26.026943\n",
      "1       26.87  26.913672\n",
      "2       21.80  21.750539\n",
      "3       23.32  23.318877\n",
      "4       18.59  18.566325\n",
      "...       ...        ...\n",
      "79080   21.66  21.558969\n",
      "79081   20.83  20.818258\n",
      "79082   22.20  22.867217\n",
      "79083   22.53  22.554394\n",
      "79084   20.76  20.491201\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 0.1668\n",
      "Score (MAE): 0.0951\n",
      "Score (ME): -0.0246\n",
      "Score (MAPE): 0.3836%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395288, 26) to (401657, 26)\n",
      "training data cutoff:  2023-07-14 02:15:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316121, 20, 24]) torch.Size([316121]) torch.Size([316121, 1])\n",
      "Testing data shape: torch.Size([79304, 20, 24]) torch.Size([79304]) torch.Size([79304, 1])\n",
      "Shuffled Training data shape: torch.Size([316340, 20, 24]) torch.Size([316340]) torch.Size([316340, 1])\n",
      "Shuffled Testing data shape: torch.Size([79085, 20, 24]) torch.Size([79085]) torch.Size([79085, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      117.999997   251.882963\n",
      "1        8.000005     9.016892\n",
      "2      211.000001   190.374520\n",
      "3     1663.000027  2197.752416\n",
      "4        5.999995    -3.960614\n",
      "...           ...          ...\n",
      "4091   207.000000    73.434123\n",
      "4092     8.000005   -24.912315\n",
      "4093    67.000003    58.574233\n",
      "4094   313.999996   165.764203\n",
      "4095     5.999995   -25.844065\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.7729\n",
      "Epoch 1/25, Validation Loss: 0.5010\n",
      "           actual    predicted\n",
      "0      117.999997   212.260165\n",
      "1        8.000005    31.770153\n",
      "2      211.000001   277.541104\n",
      "3     1663.000027  1788.438850\n",
      "4        5.999995    25.828524\n",
      "...           ...          ...\n",
      "4091   207.000000   156.721054\n",
      "4092     8.000005   -54.600335\n",
      "4093    67.000003    85.285678\n",
      "4094   313.999996   212.989375\n",
      "4095     5.999995    -2.377966\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.4825\n",
      "Epoch 2/25, Validation Loss: 0.4209\n",
      "           actual    predicted\n",
      "0      117.999997   196.556705\n",
      "1        8.000005    32.084480\n",
      "2      211.000001   254.930639\n",
      "3     1663.000027  1839.002006\n",
      "4        5.999995     2.255008\n",
      "...           ...          ...\n",
      "4091   207.000000   178.662665\n",
      "4092     8.000005   -20.787543\n",
      "4093    67.000003    86.303272\n",
      "4094   313.999996   227.860198\n",
      "4095     5.999995   -24.945599\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.4303\n",
      "Epoch 3/25, Validation Loss: 0.3990\n",
      "           actual    predicted\n",
      "0      117.999997   194.287838\n",
      "1        8.000005    35.201318\n",
      "2      211.000001   261.193857\n",
      "3     1663.000027  1860.128398\n",
      "4        5.999995   -10.105041\n",
      "...           ...          ...\n",
      "4091   207.000000   140.764698\n",
      "4092     8.000005   -43.609356\n",
      "4093    67.000003    99.541402\n",
      "4094   313.999996   224.080111\n",
      "4095     5.999995   -18.131996\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.4116\n",
      "Epoch 4/25, Validation Loss: 0.4037\n",
      "           actual    predicted\n",
      "0      117.999997   184.020634\n",
      "1        8.000005    24.250762\n",
      "2      211.000001   250.660465\n",
      "3     1663.000027  1738.637572\n",
      "4        5.999995   -28.517162\n",
      "...           ...          ...\n",
      "4091   207.000000   164.834075\n",
      "4092     8.000005   -45.516630\n",
      "4093    67.000003    60.935256\n",
      "4094   313.999996   221.544555\n",
      "4095     5.999995    -9.086282\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.3951\n",
      "Epoch 5/25, Validation Loss: 0.3885\n",
      "           actual    predicted\n",
      "0      117.999997   209.695784\n",
      "1        8.000005    75.922757\n",
      "2      211.000001   268.987522\n",
      "3     1663.000027  1520.144242\n",
      "4        5.999995    37.885453\n",
      "...           ...          ...\n",
      "4091   207.000000   154.209803\n",
      "4092     8.000005    28.264789\n",
      "4093    67.000003   102.874581\n",
      "4094   313.999996   251.055301\n",
      "4095     5.999995    35.914929\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.3812\n",
      "Epoch 6/25, Validation Loss: 0.3773\n",
      "           actual    predicted\n",
      "0      117.999997   174.529091\n",
      "1        8.000005    45.143022\n",
      "2      211.000001   236.768400\n",
      "3     1663.000027  1951.884808\n",
      "4        5.999995    -0.296753\n",
      "...           ...          ...\n",
      "4091   207.000000   134.729545\n",
      "4092     8.000005   -15.223334\n",
      "4093    67.000003    48.001389\n",
      "4094   313.999996   248.017139\n",
      "4095     5.999995    14.489027\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.3699\n",
      "Epoch 7/25, Validation Loss: 0.3422\n",
      "           actual    predicted\n",
      "0      117.999997   228.204670\n",
      "1        8.000005    32.697477\n",
      "2      211.000001   268.471833\n",
      "3     1663.000027  1941.232317\n",
      "4        5.999995    16.285264\n",
      "...           ...          ...\n",
      "4091   207.000000   149.189831\n",
      "4092     8.000005     1.222678\n",
      "4093    67.000003    79.453101\n",
      "4094   313.999996   244.981113\n",
      "4095     5.999995   -16.395782\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.3665\n",
      "Epoch 8/25, Validation Loss: 0.3177\n",
      "           actual    predicted\n",
      "0      117.999997   192.231764\n",
      "1        8.000005    36.692564\n",
      "2      211.000001   279.736021\n",
      "3     1663.000027  1740.032048\n",
      "4        5.999995   -10.656681\n",
      "...           ...          ...\n",
      "4091   207.000000   181.733629\n",
      "4092     8.000005   -31.251447\n",
      "4093    67.000003    79.564967\n",
      "4094   313.999996   258.222717\n",
      "4095     5.999995   -17.689487\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.3236\n",
      "Epoch 9/25, Validation Loss: 0.3273\n",
      "           actual    predicted\n",
      "0      117.999997   192.607963\n",
      "1        8.000005    -2.896177\n",
      "2      211.000001   262.247267\n",
      "3     1663.000027  2320.271572\n",
      "4        5.999995   -49.058853\n",
      "...           ...          ...\n",
      "4091   207.000000   108.452597\n",
      "4092     8.000005   -63.466966\n",
      "4093    67.000003    22.108999\n",
      "4094   313.999996   258.112129\n",
      "4095     5.999995   -38.886414\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.3038\n",
      "Epoch 10/25, Validation Loss: 0.3348\n",
      "           actual    predicted\n",
      "0      117.999997   242.043167\n",
      "1        8.000005    42.792231\n",
      "2      211.000001   262.559425\n",
      "3     1663.000027  1801.109090\n",
      "4        5.999995     7.864773\n",
      "...           ...          ...\n",
      "4091   207.000000   136.123988\n",
      "4092     8.000005    -8.266397\n",
      "4093    67.000003    73.023733\n",
      "4094   313.999996   265.906954\n",
      "4095     5.999995     4.846105\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.2996\n",
      "Epoch 11/25, Validation Loss: 0.3020\n",
      "           actual    predicted\n",
      "0      117.999997   237.238539\n",
      "1        8.000005    37.293379\n",
      "2      211.000001   259.482432\n",
      "3     1663.000027  1715.056100\n",
      "4        5.999995    -0.857695\n",
      "...           ...          ...\n",
      "4091   207.000000   144.150316\n",
      "4092     8.000005     7.626495\n",
      "4093    67.000003    49.275728\n",
      "4094   313.999996   260.659710\n",
      "4095     5.999995     6.806633\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.3100\n",
      "Epoch 12/25, Validation Loss: 0.2861\n",
      "           actual    predicted\n",
      "0      117.999997   163.816246\n",
      "1        8.000005    32.730133\n",
      "2      211.000001   249.566492\n",
      "3     1663.000027  1589.740103\n",
      "4        5.999995    47.428837\n",
      "...           ...          ...\n",
      "4091   207.000000   143.285632\n",
      "4092     8.000005    29.143867\n",
      "4093    67.000003    75.648080\n",
      "4094   313.999996   234.165037\n",
      "4095     5.999995     7.989056\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.2825\n",
      "Epoch 13/25, Validation Loss: 0.2836\n",
      "           actual    predicted\n",
      "0      117.999997   231.297459\n",
      "1        8.000005    55.985770\n",
      "2      211.000001   263.527401\n",
      "3     1663.000027  1800.410866\n",
      "4        5.999995     9.605705\n",
      "...           ...          ...\n",
      "4091   207.000000   172.990793\n",
      "4092     8.000005   -43.525776\n",
      "4093    67.000003    88.006751\n",
      "4094   313.999996   281.824978\n",
      "4095     5.999995    -3.685881\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.2708\n",
      "Epoch 14/25, Validation Loss: 0.2746\n",
      "           actual    predicted\n",
      "0      117.999997   144.564001\n",
      "1        8.000005     8.047151\n",
      "2      211.000001   244.239087\n",
      "3     1663.000027  1840.736808\n",
      "4        5.999995    -4.397520\n",
      "...           ...          ...\n",
      "4091   207.000000   114.904076\n",
      "4092     8.000005   -30.389089\n",
      "4093    67.000003    64.935768\n",
      "4094   313.999996   232.365740\n",
      "4095     5.999995   -29.217426\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.2703\n",
      "Epoch 15/25, Validation Loss: 0.2710\n",
      "           actual    predicted\n",
      "0      117.999997   200.259554\n",
      "1        8.000005    35.061761\n",
      "2      211.000001   269.726213\n",
      "3     1663.000027  2129.158257\n",
      "4        5.999995     4.301862\n",
      "...           ...          ...\n",
      "4091   207.000000   132.444868\n",
      "4092     8.000005   -28.717695\n",
      "4093    67.000003    56.748601\n",
      "4094   313.999996   284.327474\n",
      "4095     5.999995    -6.126203\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.2473\n",
      "Epoch 16/25, Validation Loss: 0.2829\n",
      "           actual    predicted\n",
      "0      117.999997   194.364511\n",
      "1        8.000005    28.710996\n",
      "2      211.000001   261.752271\n",
      "3     1663.000027  1554.794916\n",
      "4        5.999995    13.866067\n",
      "...           ...          ...\n",
      "4091   207.000000   186.107952\n",
      "4092     8.000005   -26.212094\n",
      "4093    67.000003    74.797444\n",
      "4094   313.999996   260.019828\n",
      "4095     5.999995   -10.282937\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.2448\n",
      "Epoch 17/25, Validation Loss: 0.2721\n",
      "           actual    predicted\n",
      "0      117.999997   162.699882\n",
      "1        8.000005    15.801122\n",
      "2      211.000001   242.736012\n",
      "3     1663.000027  2099.297609\n",
      "4        5.999995   -21.893379\n",
      "...           ...          ...\n",
      "4091   207.000000   116.615383\n",
      "4092     8.000005   -55.053110\n",
      "4093    67.000003    31.201838\n",
      "4094   313.999996   250.091769\n",
      "4095     5.999995   -25.979487\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.2663\n",
      "Epoch 18/25, Validation Loss: 0.2677\n",
      "           actual    predicted\n",
      "0      117.999997   209.080238\n",
      "1        8.000005    40.231426\n",
      "2      211.000001   242.774804\n",
      "3     1663.000027  1783.486013\n",
      "4        5.999995    33.951375\n",
      "...           ...          ...\n",
      "4091   207.000000   158.045093\n",
      "4092     8.000005    -6.447702\n",
      "4093    67.000003    41.436127\n",
      "4094   313.999996   252.040793\n",
      "4095     5.999995    11.342065\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.2499\n",
      "Epoch 19/25, Validation Loss: 0.2658\n",
      "           actual    predicted\n",
      "0      117.999997   190.047141\n",
      "1        8.000005    44.290190\n",
      "2      211.000001   269.438055\n",
      "3     1663.000027  1738.888603\n",
      "4        5.999995    16.606695\n",
      "...           ...          ...\n",
      "4091   207.000000   173.497909\n",
      "4092     8.000005   -11.682859\n",
      "4093    67.000003    70.237257\n",
      "4094   313.999996   280.721529\n",
      "4095     5.999995     6.065151\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.2297\n",
      "Epoch 20/25, Validation Loss: 0.2468\n",
      "           actual    predicted\n",
      "0      117.999997   166.130873\n",
      "1        8.000005    18.904994\n",
      "2      211.000001   258.183034\n",
      "3     1663.000027  1807.822191\n",
      "4        5.999995     7.756023\n",
      "...           ...          ...\n",
      "4091   207.000000   139.469869\n",
      "4092     8.000005   -14.654615\n",
      "4093    67.000003    59.991134\n",
      "4094   313.999996   264.033626\n",
      "4095     5.999995    -3.786933\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.2294\n",
      "Epoch 21/25, Validation Loss: 0.2510\n",
      "           actual    predicted\n",
      "0      117.999997   190.128076\n",
      "1        8.000005    28.136472\n",
      "2      211.000001   275.486966\n",
      "3     1663.000027  1856.907673\n",
      "4        5.999995    24.172741\n",
      "...           ...          ...\n",
      "4091   207.000000   146.750810\n",
      "4092     8.000005    -4.358834\n",
      "4093    67.000003    84.121994\n",
      "4094   313.999996   264.912116\n",
      "4095     5.999995     2.276514\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.2211\n",
      "Epoch 22/25, Validation Loss: 0.2484\n",
      "           actual    predicted\n",
      "0      117.999997   161.650820\n",
      "1        8.000005    33.176598\n",
      "2      211.000001   249.788991\n",
      "3     1663.000027  1865.416361\n",
      "4        5.999995    11.430407\n",
      "...           ...          ...\n",
      "4091   207.000000   135.888596\n",
      "4092     8.000005    -4.111849\n",
      "4093    67.000003    66.198464\n",
      "4094   313.999996   257.691154\n",
      "4095     5.999995     6.837172\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.2141\n",
      "Epoch 23/25, Validation Loss: 0.2689\n",
      "           actual    predicted\n",
      "0      117.999997   197.645444\n",
      "1        8.000005    44.535158\n",
      "2      211.000001   260.219381\n",
      "3     1663.000027  1871.974898\n",
      "4        5.999995    12.137866\n",
      "...           ...          ...\n",
      "4091   207.000000   150.158474\n",
      "4092     8.000005    -3.050908\n",
      "4093    67.000003    75.633287\n",
      "4094   313.999996   306.290874\n",
      "4095     5.999995    14.356933\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.2084\n",
      "Epoch 24/25, Validation Loss: 0.2335\n",
      "           actual    predicted\n",
      "0      117.999997   206.661870\n",
      "1        8.000005    36.268871\n",
      "2      211.000001   295.963815\n",
      "3     1663.000027  2188.194957\n",
      "4        5.999995    29.176311\n",
      "...           ...          ...\n",
      "4091   207.000000   183.731173\n",
      "4092     8.000005     3.796158\n",
      "4093    67.000003    87.309491\n",
      "4094   313.999996   314.130319\n",
      "4095     5.999995     3.212063\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.2356\n",
      "Epoch 25/25, Validation Loss: 0.2748\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4287,  1.6480,  1.1758, -0.6819, -0.1094, -0.1020, -1.5047,\n",
      "          -0.5238, -1.8374, -3.7636,  4.1518,  0.0000, -0.9097,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9383, -1.0388,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4359,  1.6197,  1.1362, -0.6452, -0.1336, -0.1092, -1.3924,\n",
      "          -0.5238, -1.8719, -3.7992,  4.1518,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -0.9977, -0.9732,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4432,  1.5913,  1.0965, -0.6085, -0.1578, -0.1164, -1.2800,\n",
      "          -0.5238, -1.9063, -3.8347,  4.1518,  0.0000, -0.9094,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.0572, -0.9076,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4504,  1.5630,  1.0568, -0.5718, -0.1820, -0.1235, -1.1677,\n",
      "          -0.5238, -1.9408, -3.8702,  4.1518,  0.0000, -0.9093,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1167, -0.8421,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4577,  1.5347,  1.0172, -0.5351, -0.2062, -0.1307, -1.0553,\n",
      "          -0.5238, -1.9752, -3.9057,  4.1518,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.9712,  0.1438, -1.1761, -0.7765,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.6768,  2.1811,  1.2155, -1.1568,  0.4121,  0.0340, -0.4936,\n",
      "          -0.3662,  1.4263,  0.7102, -0.8020,  0.0000, -0.9109,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.7742, -1.1808,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [-0.1994,  1.6997,  3.4948, -1.1302,  0.0967, -0.0537, -1.0834,\n",
      "          -0.4332,  0.8149,  0.0791, -0.8020,  0.0000, -0.9095,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.8446, -1.1307,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.0851,  1.2585,  2.1517, -1.0289,  0.0358, -0.0662, -0.6809,\n",
      "          -0.3136,  1.2111,  0.5741, -0.8020,  0.0000, -0.9078,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -0.9133, -1.0751,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.1928,  1.1184,  1.7867, -1.0243,  0.0185, -0.0806, -0.4936,\n",
      "          -0.4450,  1.4263,  0.7635, -0.8020,  0.0000, -0.9075,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1960, -0.7453,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2291,  0.9362,  0.9140, -1.0277,  0.0172, -0.0806, -1.6170,\n",
      "          -0.5238, -1.5447, -2.4321, -0.8020,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0320, -0.9613,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.2793,  0.8383,  0.8664, -0.9632, -0.0147, -0.0877, -1.6170,\n",
      "          -0.4450,  1.6416,  0.4794, -0.8020,  0.0000, -0.9073,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.0925, -0.8912,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3266,  0.7603,  0.8347, -0.9598, -0.0227, -0.0949, -1.0553,\n",
      "          -0.2873,  1.4694,  0.7102, -0.8020,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.1484, -0.8174,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3462,  0.8113,  0.7712, -0.9122, -0.0280, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.8877, -0.8020,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3493, -0.4024,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3434,  0.8175,  0.7236, -0.7661, -0.0293, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3402,  0.7635, -0.8020,  0.0000, -0.9069,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.2450, -0.6595,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3601,  0.9171,  0.5940, -0.8001, -0.0865, -0.1176, -1.0553,\n",
      "          -0.3662,  1.4263,  0.7694, -0.8020,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3012, -0.5382,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3838,  1.0252,  1.1322, -0.5742, -0.0420, -0.1056, -1.3362,\n",
      "          -0.5238,  1.5125,  0.5149, -0.8020,  0.0000, -0.9063,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3364, -0.4437,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4186,  0.9799,  0.6760, -0.7186, -0.0307, -0.1056, -1.6170,\n",
      "          -0.5238,  1.5986,  0.3551, -0.8020,  0.0000, -0.9061,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3640, -0.3488,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4075,  1.0007,  0.6840, -0.6982, -0.0333, -0.1056, -1.6170,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9059,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3838, -0.2583,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.4103,  1.1048,  0.7474, -0.6404, -0.0307, -0.1020, -0.4936,\n",
      "          -0.5238,  1.4263,  0.8345, -0.8020,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.3977, -0.1667,\n",
      "          -1.1622,  1.1622,  0.0000],\n",
      "         [ 0.3935,  1.1517,  0.7712, -0.5793, -0.0307, -0.0985, -1.6170,\n",
      "          -0.5238,  1.3833,  0.7102, -0.8020,  0.0000, -0.9057,  0.0000,\n",
      "          -0.4479, -0.0402,  1.4126, -1.7654,  0.8611, -1.4056, -0.0745,\n",
      "          -1.1622,  1.1622,  0.0000]]])\n",
      "predicted: tensor([[-0.1328]], device='cuda:0')\n",
      "[80.19]\n",
      "            actual    predicted\n",
      "0       117.999997   206.661870\n",
      "1         8.000005    36.268871\n",
      "2       211.000001   295.963815\n",
      "3      1663.000027  2188.194957\n",
      "4         5.999995    29.176311\n",
      "...            ...          ...\n",
      "79080     7.000006    53.876865\n",
      "79081    27.999996   103.956932\n",
      "79082     8.000005    18.848793\n",
      "79083     7.000006    21.787265\n",
      "79084   439.999995   415.311724\n",
      "\n",
      "[79085 rows x 2 columns]\n",
      "Score (RMSE): 398.3645\n",
      "Score (MAE): 76.2336\n",
      "Score (ME): -43.7718\n",
      "Score (MAPE): 819937.3254%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     415.000003  402.597802\n",
      "1     451.999999  473.718594\n",
      "2     387.000001  397.388600\n",
      "3     433.499999  477.430712\n",
      "4     484.000000  424.955109\n",
      "...          ...         ...\n",
      "4091  824.499995  578.856397\n",
      "4092  445.999999  423.990638\n",
      "4093  473.000001  530.831493\n",
      "4094  457.000002  451.766391\n",
      "4095  433.499999  422.908597\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.6189\n",
      "Epoch 1/25, Validation Loss: 0.3281\n",
      "          actual   predicted\n",
      "0     415.000003  407.582233\n",
      "1     451.999999  457.793184\n",
      "2     387.000001  403.574088\n",
      "3     433.499999  454.898750\n",
      "4     484.000000  445.191474\n",
      "...          ...         ...\n",
      "4091  824.499995  708.806299\n",
      "4092  445.999999  464.052671\n",
      "4093  473.000001  478.706703\n",
      "4094  457.000002  448.044725\n",
      "4095  433.499999  427.806464\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2252\n",
      "Epoch 2/25, Validation Loss: 0.1328\n",
      "          actual   predicted\n",
      "0     415.000003  402.819550\n",
      "1     451.999999  449.585214\n",
      "2     387.000001  397.404258\n",
      "3     433.499999  446.390470\n",
      "4     484.000000  438.774376\n",
      "...          ...         ...\n",
      "4091  824.499995  726.151513\n",
      "4092  445.999999  452.278894\n",
      "4093  473.000001  482.315753\n",
      "4094  457.000002  459.709114\n",
      "4095  433.499999  429.762772\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1252\n",
      "Epoch 3/25, Validation Loss: 0.0962\n",
      "          actual   predicted\n",
      "0     415.000003  410.965618\n",
      "1     451.999999  455.029208\n",
      "2     387.000001  398.672001\n",
      "3     433.499999  445.785208\n",
      "4     484.000000  450.765178\n",
      "...          ...         ...\n",
      "4091  824.499995  769.725851\n",
      "4092  445.999999  449.501170\n",
      "4093  473.000001  492.293469\n",
      "4094  457.000002  464.725220\n",
      "4095  433.499999  429.430534\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1044\n",
      "Epoch 4/25, Validation Loss: 0.0805\n",
      "          actual   predicted\n",
      "0     415.000003  407.995323\n",
      "1     451.999999  456.165512\n",
      "2     387.000001  396.569332\n",
      "3     433.499999  447.140804\n",
      "4     484.000000  446.778719\n",
      "...          ...         ...\n",
      "4091  824.499995  772.150437\n",
      "4092  445.999999  448.529160\n",
      "4093  473.000001  482.707404\n",
      "4094  457.000002  460.332985\n",
      "4095  433.499999  429.162462\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0937\n",
      "Epoch 5/25, Validation Loss: 0.0750\n",
      "          actual   predicted\n",
      "0     415.000003  405.413314\n",
      "1     451.999999  454.047587\n",
      "2     387.000001  392.557096\n",
      "3     433.499999  446.528020\n",
      "4     484.000000  442.882787\n",
      "...          ...         ...\n",
      "4091  824.499995  792.959749\n",
      "4092  445.999999  448.494749\n",
      "4093  473.000001  479.300780\n",
      "4094  457.000002  457.786938\n",
      "4095  433.499999  431.762795\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0893\n",
      "Epoch 6/25, Validation Loss: 0.0742\n",
      "          actual   predicted\n",
      "0     415.000003  412.140096\n",
      "1     451.999999  456.178752\n",
      "2     387.000001  391.463675\n",
      "3     433.499999  447.198995\n",
      "4     484.000000  452.331617\n",
      "...          ...         ...\n",
      "4091  824.499995  803.682982\n",
      "4092  445.999999  449.966547\n",
      "4093  473.000001  479.447409\n",
      "4094  457.000002  455.727761\n",
      "4095  433.499999  429.044853\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0846\n",
      "Epoch 7/25, Validation Loss: 0.0747\n",
      "          actual   predicted\n",
      "0     415.000003  409.402755\n",
      "1     451.999999  453.442978\n",
      "2     387.000001  394.302509\n",
      "3     433.499999  441.645431\n",
      "4     484.000000  446.243192\n",
      "...          ...         ...\n",
      "4091  824.499995  794.480375\n",
      "4092  445.999999  451.877495\n",
      "4093  473.000001  472.616315\n",
      "4094  457.000002  450.635785\n",
      "4095  433.499999  430.792519\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0825\n",
      "Epoch 8/25, Validation Loss: 0.0706\n",
      "          actual   predicted\n",
      "0     415.000003  408.743294\n",
      "1     451.999999  453.366252\n",
      "2     387.000001  393.790477\n",
      "3     433.499999  439.943469\n",
      "4     484.000000  447.553100\n",
      "...          ...         ...\n",
      "4091  824.499995  789.445353\n",
      "4092  445.999999  449.036289\n",
      "4093  473.000001  474.220653\n",
      "4094  457.000002  450.985674\n",
      "4095  433.499999  431.613893\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0815\n",
      "Epoch 9/25, Validation Loss: 0.0703\n",
      "          actual   predicted\n",
      "0     415.000003  415.290919\n",
      "1     451.999999  451.613454\n",
      "2     387.000001  396.666838\n",
      "3     433.499999  441.186108\n",
      "4     484.000000  457.121912\n",
      "...          ...         ...\n",
      "4091  824.499995  791.107519\n",
      "4092  445.999999  450.204713\n",
      "4093  473.000001  471.036435\n",
      "4094  457.000002  448.269598\n",
      "4095  433.499999  428.804356\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0793\n",
      "Epoch 10/25, Validation Loss: 0.0699\n",
      "          actual   predicted\n",
      "0     415.000003  414.532922\n",
      "1     451.999999  446.626417\n",
      "2     387.000001  392.512934\n",
      "3     433.499999  436.561942\n",
      "4     484.000000  451.242672\n",
      "...          ...         ...\n",
      "4091  824.499995  776.904155\n",
      "4092  445.999999  449.155717\n",
      "4093  473.000001  468.721168\n",
      "4094  457.000002  447.871771\n",
      "4095  433.499999  429.508877\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0785\n",
      "Epoch 11/25, Validation Loss: 0.0723\n",
      "          actual   predicted\n",
      "0     415.000003  420.559279\n",
      "1     451.999999  456.379258\n",
      "2     387.000001  399.062965\n",
      "3     433.499999  445.376743\n",
      "4     484.000000  455.217008\n",
      "...          ...         ...\n",
      "4091  824.499995  780.612404\n",
      "4092  445.999999  453.910161\n",
      "4093  473.000001  479.571967\n",
      "4094  457.000002  459.541434\n",
      "4095  433.499999  435.813808\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0761\n",
      "Epoch 12/25, Validation Loss: 0.0718\n",
      "          actual   predicted\n",
      "0     415.000003  417.032015\n",
      "1     451.999999  451.061336\n",
      "2     387.000001  395.660267\n",
      "3     433.499999  438.955139\n",
      "4     484.000000  455.137585\n",
      "...          ...         ...\n",
      "4091  824.499995  784.926401\n",
      "4092  445.999999  451.715954\n",
      "4093  473.000001  476.296414\n",
      "4094  457.000002  453.854530\n",
      "4095  433.499999  431.032817\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0752\n",
      "Epoch 13/25, Validation Loss: 0.0690\n",
      "          actual   predicted\n",
      "0     415.000003  415.993999\n",
      "1     451.999999  456.673370\n",
      "2     387.000001  390.502484\n",
      "3     433.499999  442.854960\n",
      "4     484.000000  460.531795\n",
      "...          ...         ...\n",
      "4091  824.499995  818.108418\n",
      "4092  445.999999  452.456247\n",
      "4093  473.000001  484.217866\n",
      "4094  457.000002  458.283376\n",
      "4095  433.499999  429.008476\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0740\n",
      "Epoch 14/25, Validation Loss: 0.0705\n",
      "          actual   predicted\n",
      "0     415.000003  413.046966\n",
      "1     451.999999  456.467212\n",
      "2     387.000001  390.490210\n",
      "3     433.499999  444.113520\n",
      "4     484.000000  458.786936\n",
      "...          ...         ...\n",
      "4091  824.499995  800.734053\n",
      "4092  445.999999  450.154760\n",
      "4093  473.000001  483.670532\n",
      "4094  457.000002  456.695082\n",
      "4095  433.499999  431.022649\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0745\n",
      "Epoch 15/25, Validation Loss: 0.0676\n",
      "          actual   predicted\n",
      "0     415.000003  418.003494\n",
      "1     451.999999  450.333001\n",
      "2     387.000001  392.267353\n",
      "3     433.499999  438.501036\n",
      "4     484.000000  457.714309\n",
      "...          ...         ...\n",
      "4091  824.499995  775.133991\n",
      "4092  445.999999  450.533909\n",
      "4093  473.000001  480.274787\n",
      "4094  457.000002  454.705415\n",
      "4095  433.499999  428.685307\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0728\n",
      "Epoch 16/25, Validation Loss: 0.0674\n",
      "          actual   predicted\n",
      "0     415.000003  421.094535\n",
      "1     451.999999  451.006195\n",
      "2     387.000001  394.459218\n",
      "3     433.499999  439.077392\n",
      "4     484.000000  456.936136\n",
      "...          ...         ...\n",
      "4091  824.499995  778.152321\n",
      "4092  445.999999  453.438714\n",
      "4093  473.000001  474.029692\n",
      "4094  457.000002  455.823207\n",
      "4095  433.499999  431.643919\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0722\n",
      "Epoch 17/25, Validation Loss: 0.0667\n",
      "          actual   predicted\n",
      "0     415.000003  416.098094\n",
      "1     451.999999  453.712156\n",
      "2     387.000001  390.957005\n",
      "3     433.499999  442.275012\n",
      "4     484.000000  457.308971\n",
      "...          ...         ...\n",
      "4091  824.499995  774.968415\n",
      "4092  445.999999  451.018450\n",
      "4093  473.000001  476.889759\n",
      "4094  457.000002  451.534347\n",
      "4095  433.499999  433.405291\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0707\n",
      "Epoch 18/25, Validation Loss: 0.0660\n",
      "          actual   predicted\n",
      "0     415.000003  418.853507\n",
      "1     451.999999  454.549492\n",
      "2     387.000001  394.970836\n",
      "3     433.499999  441.591564\n",
      "4     484.000000  460.656778\n",
      "...          ...         ...\n",
      "4091  824.499995  780.770158\n",
      "4092  445.999999  452.460353\n",
      "4093  473.000001  476.650694\n",
      "4094  457.000002  454.445312\n",
      "4095  433.499999  433.638658\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0717\n",
      "Epoch 19/25, Validation Loss: 0.0658\n",
      "          actual   predicted\n",
      "0     415.000003  420.517178\n",
      "1     451.999999  449.594633\n",
      "2     387.000001  391.131990\n",
      "3     433.499999  435.608008\n",
      "4     484.000000  454.851783\n",
      "...          ...         ...\n",
      "4091  824.499995  767.314863\n",
      "4092  445.999999  454.777313\n",
      "4093  473.000001  474.704211\n",
      "4094  457.000002  453.181156\n",
      "4095  433.499999  429.888188\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0695\n",
      "Epoch 20/25, Validation Loss: 0.0684\n",
      "          actual   predicted\n",
      "0     415.000003  419.928667\n",
      "1     451.999999  451.283227\n",
      "2     387.000001  389.252896\n",
      "3     433.499999  438.161043\n",
      "4     484.000000  457.159181\n",
      "...          ...         ...\n",
      "4091  824.499995  787.459345\n",
      "4092  445.999999  451.176099\n",
      "4093  473.000001  482.127940\n",
      "4094  457.000002  457.871499\n",
      "4095  433.499999  432.449210\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0705\n",
      "Epoch 21/25, Validation Loss: 0.0654\n",
      "          actual   predicted\n",
      "0     415.000003  420.079901\n",
      "1     451.999999  454.425487\n",
      "2     387.000001  392.232615\n",
      "3     433.499999  441.597280\n",
      "4     484.000000  457.007579\n",
      "...          ...         ...\n",
      "4091  824.499995  771.837575\n",
      "4092  445.999999  452.647777\n",
      "4093  473.000001  481.676387\n",
      "4094  457.000002  457.583285\n",
      "4095  433.499999  431.418723\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0687\n",
      "Epoch 22/25, Validation Loss: 0.0653\n",
      "          actual   predicted\n",
      "0     415.000003  417.020245\n",
      "1     451.999999  454.194071\n",
      "2     387.000001  389.109950\n",
      "3     433.499999  441.479919\n",
      "4     484.000000  458.004161\n",
      "...          ...         ...\n",
      "4091  824.499995  801.816072\n",
      "4092  445.999999  453.282317\n",
      "4093  473.000001  478.729443\n",
      "4094  457.000002  455.729811\n",
      "4095  433.499999  431.728614\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0684\n",
      "Epoch 23/25, Validation Loss: 0.0661\n",
      "          actual   predicted\n",
      "0     415.000003  422.553010\n",
      "1     451.999999  453.746213\n",
      "2     387.000001  392.433959\n",
      "3     433.499999  439.535148\n",
      "4     484.000000  459.886133\n",
      "...          ...         ...\n",
      "4091  824.499995  789.999238\n",
      "4092  445.999999  453.158681\n",
      "4093  473.000001  475.341533\n",
      "4094  457.000002  455.860404\n",
      "4095  433.499999  433.361667\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0675\n",
      "Epoch 24/25, Validation Loss: 0.0654\n",
      "          actual   predicted\n",
      "0     415.000003  425.692243\n",
      "1     451.999999  455.129301\n",
      "2     387.000001  390.750660\n",
      "3     433.499999  440.620024\n",
      "4     484.000000  462.898546\n",
      "...          ...         ...\n",
      "4091  824.499995  786.285664\n",
      "4092  445.999999  457.808532\n",
      "4093  473.000001  482.498474\n",
      "4094  457.000002  459.723330\n",
      "4095  433.499999  434.548043\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0664\n",
      "Epoch 25/25, Validation Loss: 0.0640\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[0.7651]], device='cuda:0')\n",
      "[585.58]\n",
      "           actual   predicted\n",
      "0      415.000003  425.692243\n",
      "1      451.999999  455.129301\n",
      "2      387.000001  390.750660\n",
      "3      433.499999  440.620024\n",
      "4      484.000000  462.898546\n",
      "...           ...         ...\n",
      "39163  469.000000  475.338375\n",
      "39164  483.000000  477.151157\n",
      "39165  445.714287  441.985743\n",
      "39166  465.499999  467.087051\n",
      "39167  452.499999  463.267502\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 32.0273\n",
      "Score (MAE): 11.3565\n",
      "Score (ME): -2.3584\n",
      "Score (MAPE): 2.0273%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      980.500007  1031.044403\n",
      "1      742.999998   703.200012\n",
      "2      662.500001   739.034570\n",
      "3     1414.999982   909.956649\n",
      "4      690.000002   704.042722\n",
      "...           ...          ...\n",
      "4091   714.000004   704.093091\n",
      "4092   635.500003   704.755535\n",
      "4093   788.000000   597.229512\n",
      "4094  1435.000002  1109.949404\n",
      "4095   696.999998   662.971025\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.5589\n",
      "Epoch 1/25, Validation Loss: 0.2914\n",
      "           actual    predicted\n",
      "0      980.500007  1262.807690\n",
      "1      742.999998   692.572208\n",
      "2      662.500001   687.514693\n",
      "3     1414.999982   938.700260\n",
      "4      690.000002   657.068168\n",
      "...           ...          ...\n",
      "4091   714.000004   760.497126\n",
      "4092   635.500003   672.362761\n",
      "4093   788.000000   617.238256\n",
      "4094  1435.000002  1492.812180\n",
      "4095   696.999998   668.271631\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2072\n",
      "Epoch 2/25, Validation Loss: 0.1625\n",
      "           actual    predicted\n",
      "0      980.500007  1107.440433\n",
      "1      742.999998   703.399457\n",
      "2      662.500001   654.175885\n",
      "3     1414.999982   912.838749\n",
      "4      690.000002   685.005556\n",
      "...           ...          ...\n",
      "4091   714.000004   718.427470\n",
      "4092   635.500003   673.840670\n",
      "4093   788.000000   627.881951\n",
      "4094  1435.000002  1395.278151\n",
      "4095   696.999998   677.324601\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1322\n",
      "Epoch 3/25, Validation Loss: 0.1025\n",
      "           actual    predicted\n",
      "0      980.500007  1098.999129\n",
      "1      742.999998   693.623761\n",
      "2      662.500001   642.191129\n",
      "3     1414.999982   987.958282\n",
      "4      690.000002   670.499748\n",
      "...           ...          ...\n",
      "4091   714.000004   692.300489\n",
      "4092   635.500003   684.883552\n",
      "4093   788.000000   640.552316\n",
      "4094  1435.000002  1183.017302\n",
      "4095   696.999998   672.749684\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1001\n",
      "Epoch 4/25, Validation Loss: 0.0836\n",
      "           actual    predicted\n",
      "0      980.500007  1100.768049\n",
      "1      742.999998   706.222936\n",
      "2      662.500001   638.531048\n",
      "3     1414.999982  1105.752424\n",
      "4      690.000002   687.275702\n",
      "...           ...          ...\n",
      "4091   714.000004   701.006974\n",
      "4092   635.500003   671.592692\n",
      "4093   788.000000   656.697802\n",
      "4094  1435.000002  1174.744288\n",
      "4095   696.999998   672.730313\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0875\n",
      "Epoch 5/25, Validation Loss: 0.0744\n",
      "           actual    predicted\n",
      "0      980.500007  1105.501335\n",
      "1      742.999998   718.943281\n",
      "2      662.500001   645.168676\n",
      "3     1414.999982  1223.599137\n",
      "4      690.000002   697.813608\n",
      "...           ...          ...\n",
      "4091   714.000004   700.395151\n",
      "4092   635.500003   673.660421\n",
      "4093   788.000000   672.926576\n",
      "4094  1435.000002  1162.791858\n",
      "4095   696.999998   678.894522\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0790\n",
      "Epoch 6/25, Validation Loss: 0.0686\n",
      "           actual    predicted\n",
      "0      980.500007  1082.761028\n",
      "1      742.999998   700.408709\n",
      "2      662.500001   645.198721\n",
      "3     1414.999982  1278.169601\n",
      "4      690.000002   677.679224\n",
      "...           ...          ...\n",
      "4091   714.000004   702.933275\n",
      "4092   635.500003   677.757075\n",
      "4093   788.000000   647.957112\n",
      "4094  1435.000002  1140.549587\n",
      "4095   696.999998   674.584060\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0748\n",
      "Epoch 7/25, Validation Loss: 0.0673\n",
      "           actual    predicted\n",
      "0      980.500007  1079.418419\n",
      "1      742.999998   718.658043\n",
      "2      662.500001   657.226258\n",
      "3     1414.999982  1348.219321\n",
      "4      690.000002   699.493549\n",
      "...           ...          ...\n",
      "4091   714.000004   710.182901\n",
      "4092   635.500003   681.197011\n",
      "4093   788.000000   663.133983\n",
      "4094  1435.000002  1140.596336\n",
      "4095   696.999998   682.654491\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0712\n",
      "Epoch 8/25, Validation Loss: 0.0642\n",
      "           actual    predicted\n",
      "0      980.500007  1071.089268\n",
      "1      742.999998   713.857877\n",
      "2      662.500001   656.719376\n",
      "3     1414.999982  1371.849664\n",
      "4      690.000002   692.892049\n",
      "...           ...          ...\n",
      "4091   714.000004   704.468597\n",
      "4092   635.500003   672.526436\n",
      "4093   788.000000   658.522020\n",
      "4094  1435.000002  1124.572767\n",
      "4095   696.999998   687.414403\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0689\n",
      "Epoch 9/25, Validation Loss: 0.0628\n",
      "           actual    predicted\n",
      "0      980.500007  1045.583131\n",
      "1      742.999998   704.106413\n",
      "2      662.500001   643.065361\n",
      "3     1414.999982  1390.158318\n",
      "4      690.000002   689.086328\n",
      "...           ...          ...\n",
      "4091   714.000004   696.921622\n",
      "4092   635.500003   667.533162\n",
      "4093   788.000000   649.727256\n",
      "4094  1435.000002  1084.813363\n",
      "4095   696.999998   675.142272\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0665\n",
      "Epoch 10/25, Validation Loss: 0.0634\n",
      "           actual    predicted\n",
      "0      980.500007  1069.021845\n",
      "1      742.999998   717.184628\n",
      "2      662.500001   660.264899\n",
      "3     1414.999982  1417.295627\n",
      "4      690.000002   691.416099\n",
      "...           ...          ...\n",
      "4091   714.000004   706.262509\n",
      "4092   635.500003   683.835478\n",
      "4093   788.000000   650.845436\n",
      "4094  1435.000002  1120.086725\n",
      "4095   696.999998   687.398773\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0662\n",
      "Epoch 11/25, Validation Loss: 0.0607\n",
      "           actual    predicted\n",
      "0      980.500007  1071.769510\n",
      "1      742.999998   713.001984\n",
      "2      662.500001   657.183293\n",
      "3     1414.999982  1463.313630\n",
      "4      690.000002   705.161069\n",
      "...           ...          ...\n",
      "4091   714.000004   711.347865\n",
      "4092   635.500003   667.714879\n",
      "4093   788.000000   664.660249\n",
      "4094  1435.000002  1154.520383\n",
      "4095   696.999998   681.230272\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0639\n",
      "Epoch 12/25, Validation Loss: 0.0610\n",
      "           actual    predicted\n",
      "0      980.500007  1076.220849\n",
      "1      742.999998   719.327332\n",
      "2      662.500001   658.162860\n",
      "3     1414.999982  1436.099886\n",
      "4      690.000002   698.264108\n",
      "...           ...          ...\n",
      "4091   714.000004   705.971593\n",
      "4092   635.500003   676.490238\n",
      "4093   788.000000   650.640230\n",
      "4094  1435.000002  1157.060893\n",
      "4095   696.999998   687.473355\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0624\n",
      "Epoch 13/25, Validation Loss: 0.0598\n",
      "           actual    predicted\n",
      "0      980.500007  1051.097256\n",
      "1      742.999998   712.719556\n",
      "2      662.500001   647.534375\n",
      "3     1414.999982  1475.072840\n",
      "4      690.000002   700.176248\n",
      "...           ...          ...\n",
      "4091   714.000004   707.136768\n",
      "4092   635.500003   667.951117\n",
      "4093   788.000000   651.418350\n",
      "4094  1435.000002  1119.182859\n",
      "4095   696.999998   677.062751\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0615\n",
      "Epoch 14/25, Validation Loss: 0.0587\n",
      "           actual    predicted\n",
      "0      980.500007  1044.768183\n",
      "1      742.999998   710.858187\n",
      "2      662.500001   649.950862\n",
      "3     1414.999982  1497.973272\n",
      "4      690.000002   696.651223\n",
      "...           ...          ...\n",
      "4091   714.000004   698.175671\n",
      "4092   635.500003   669.700753\n",
      "4093   788.000000   643.355997\n",
      "4094  1435.000002  1107.518127\n",
      "4095   696.999998   680.772404\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0608\n",
      "Epoch 15/25, Validation Loss: 0.0582\n",
      "           actual    predicted\n",
      "0      980.500007  1047.200195\n",
      "1      742.999998   723.526481\n",
      "2      662.500001   654.131426\n",
      "3     1414.999982  1490.829549\n",
      "4      690.000002   700.150181\n",
      "...           ...          ...\n",
      "4091   714.000004   696.604089\n",
      "4092   635.500003   672.828436\n",
      "4093   788.000000   650.841388\n",
      "4094  1435.000002  1114.567373\n",
      "4095   696.999998   682.907975\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0594\n",
      "Epoch 16/25, Validation Loss: 0.0584\n",
      "           actual    predicted\n",
      "0      980.500007  1039.850470\n",
      "1      742.999998   718.080408\n",
      "2      662.500001   654.494126\n",
      "3     1414.999982  1498.656994\n",
      "4      690.000002   693.811423\n",
      "...           ...          ...\n",
      "4091   714.000004   700.050301\n",
      "4092   635.500003   670.979094\n",
      "4093   788.000000   652.254042\n",
      "4094  1435.000002  1114.197120\n",
      "4095   696.999998   688.544978\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0589\n",
      "Epoch 17/25, Validation Loss: 0.0584\n",
      "           actual    predicted\n",
      "0      980.500007  1055.932764\n",
      "1      742.999998   718.692585\n",
      "2      662.500001   655.022643\n",
      "3     1414.999982  1489.680853\n",
      "4      690.000002   698.348926\n",
      "...           ...          ...\n",
      "4091   714.000004   709.561733\n",
      "4092   635.500003   670.788706\n",
      "4093   788.000000   649.480511\n",
      "4094  1435.000002  1147.516068\n",
      "4095   696.999998   680.071611\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0585\n",
      "Epoch 18/25, Validation Loss: 0.0575\n",
      "           actual    predicted\n",
      "0      980.500007  1029.700248\n",
      "1      742.999998   724.632650\n",
      "2      662.500001   661.375859\n",
      "3     1414.999982  1489.935054\n",
      "4      690.000002   706.611896\n",
      "...           ...          ...\n",
      "4091   714.000004   703.833959\n",
      "4092   635.500003   669.802548\n",
      "4093   788.000000   651.104829\n",
      "4094  1435.000002  1115.774777\n",
      "4095   696.999998   685.742784\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0574\n",
      "Epoch 19/25, Validation Loss: 0.0571\n",
      "           actual    predicted\n",
      "0      980.500007  1043.568646\n",
      "1      742.999998   715.993011\n",
      "2      662.500001   660.788871\n",
      "3     1414.999982  1484.811445\n",
      "4      690.000002   699.939791\n",
      "...           ...          ...\n",
      "4091   714.000004   705.545605\n",
      "4092   635.500003   672.876103\n",
      "4093   788.000000   649.951614\n",
      "4094  1435.000002  1147.136025\n",
      "4095   696.999998   685.292713\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0575\n",
      "Epoch 20/25, Validation Loss: 0.0566\n",
      "           actual    predicted\n",
      "0      980.500007  1036.590504\n",
      "1      742.999998   726.001526\n",
      "2      662.500001   660.391327\n",
      "3     1414.999982  1497.903900\n",
      "4      690.000002   706.263033\n",
      "...           ...          ...\n",
      "4091   714.000004   706.287553\n",
      "4092   635.500003   669.785729\n",
      "4093   788.000000   657.302623\n",
      "4094  1435.000002  1131.920027\n",
      "4095   696.999998   687.374638\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0576\n",
      "Epoch 21/25, Validation Loss: 0.0564\n",
      "           actual    predicted\n",
      "0      980.500007  1027.740572\n",
      "1      742.999998   716.746157\n",
      "2      662.500001   658.132694\n",
      "3     1414.999982  1477.913951\n",
      "4      690.000002   697.527256\n",
      "...           ...          ...\n",
      "4091   714.000004   706.030275\n",
      "4092   635.500003   679.415023\n",
      "4093   788.000000   643.606929\n",
      "4094  1435.000002  1151.924697\n",
      "4095   696.999998   678.762744\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0565\n",
      "Epoch 22/25, Validation Loss: 0.0568\n",
      "           actual    predicted\n",
      "0      980.500007  1020.030158\n",
      "1      742.999998   721.363085\n",
      "2      662.500001   647.048394\n",
      "3     1414.999982  1503.601368\n",
      "4      690.000002   701.054003\n",
      "...           ...          ...\n",
      "4091   714.000004   699.812848\n",
      "4092   635.500003   666.069153\n",
      "4093   788.000000   647.214368\n",
      "4094  1435.000002  1117.344322\n",
      "4095   696.999998   673.353814\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0557\n",
      "Epoch 23/25, Validation Loss: 0.0566\n",
      "           actual    predicted\n",
      "0      980.500007  1035.247467\n",
      "1      742.999998   732.138252\n",
      "2      662.500001   660.540474\n",
      "3     1414.999982  1484.484095\n",
      "4      690.000002   703.537099\n",
      "...           ...          ...\n",
      "4091   714.000004   708.818046\n",
      "4092   635.500003   676.078210\n",
      "4093   788.000000   653.577462\n",
      "4094  1435.000002  1146.690875\n",
      "4095   696.999998   683.811221\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0553\n",
      "Epoch 24/25, Validation Loss: 0.0559\n",
      "           actual    predicted\n",
      "0      980.500007  1023.159282\n",
      "1      742.999998   730.770814\n",
      "2      662.500001   659.997700\n",
      "3     1414.999982  1454.404554\n",
      "4      690.000002   700.721915\n",
      "...           ...          ...\n",
      "4091   714.000004   705.427499\n",
      "4092   635.500003   674.595109\n",
      "4093   788.000000   655.556824\n",
      "4094  1435.000002  1134.383088\n",
      "4095   696.999998   680.364668\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0553\n",
      "Epoch 25/25, Validation Loss: 0.0565\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[-0.5109]], device='cuda:0')\n",
      "[640.94]\n",
      "            actual    predicted\n",
      "0       980.500007  1023.159282\n",
      "1       742.999998   730.770814\n",
      "2       662.500001   659.997700\n",
      "3      1414.999982  1454.404554\n",
      "4       690.000002   700.721915\n",
      "...            ...          ...\n",
      "39163   602.000001   607.704637\n",
      "39164   611.500000   598.437021\n",
      "39165   821.999999   799.669212\n",
      "39166   799.500000   835.000230\n",
      "39167   527.499992   543.683442\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 69.9312\n",
      "Score (MAE): 31.9489\n",
      "Score (ME): 2.3212\n",
      "Score (MAPE): 3.8156%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     39.690000  42.694986\n",
      "1     48.625001  51.091025\n",
      "2     50.269999  47.080737\n",
      "3     28.380000  28.269087\n",
      "4     32.580000  29.636617\n",
      "...         ...        ...\n",
      "4091  49.934999  50.013173\n",
      "4092  40.480000  42.725355\n",
      "4093  27.890000  27.060827\n",
      "4094  24.405000  24.337408\n",
      "4095  33.500000  38.132917\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.4694\n",
      "Epoch 1/25, Validation Loss: 0.0999\n",
      "         actual  predicted\n",
      "0     39.690000  40.479635\n",
      "1     48.625001  48.302772\n",
      "2     50.269999  47.474736\n",
      "3     28.380000  27.120215\n",
      "4     32.580000  31.034514\n",
      "...         ...        ...\n",
      "4091  49.934999  47.855684\n",
      "4092  40.480000  41.020564\n",
      "4093  27.890000  27.822814\n",
      "4094  24.405000  24.865351\n",
      "4095  33.500000  36.000672\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0673\n",
      "Epoch 2/25, Validation Loss: 0.0447\n",
      "         actual  predicted\n",
      "0     39.690000  40.077576\n",
      "1     48.625001  45.469222\n",
      "2     50.269999  48.407896\n",
      "3     28.380000  27.840834\n",
      "4     32.580000  31.502038\n",
      "...         ...        ...\n",
      "4091  49.934999  48.710950\n",
      "4092  40.480000  41.129200\n",
      "4093  27.890000  27.659129\n",
      "4094  24.405000  25.054590\n",
      "4095  33.500000  35.832232\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0406\n",
      "Epoch 3/25, Validation Loss: 0.0315\n",
      "         actual  predicted\n",
      "0     39.690000  40.240817\n",
      "1     48.625001  45.031929\n",
      "2     50.269999  48.644128\n",
      "3     28.380000  28.115444\n",
      "4     32.580000  31.787548\n",
      "...         ...        ...\n",
      "4091  49.934999  49.454989\n",
      "4092  40.480000  40.806398\n",
      "4093  27.890000  27.710824\n",
      "4094  24.405000  24.568167\n",
      "4095  33.500000  35.473797\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0324\n",
      "Epoch 4/25, Validation Loss: 0.0249\n",
      "         actual  predicted\n",
      "0     39.690000  40.206053\n",
      "1     48.625001  44.986016\n",
      "2     50.269999  48.887470\n",
      "3     28.380000  28.178088\n",
      "4     32.580000  32.016036\n",
      "...         ...        ...\n",
      "4091  49.934999  49.455305\n",
      "4092  40.480000  40.836106\n",
      "4093  27.890000  27.958766\n",
      "4094  24.405000  24.942623\n",
      "4095  33.500000  35.368668\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0276\n",
      "Epoch 5/25, Validation Loss: 0.0208\n",
      "         actual  predicted\n",
      "0     39.690000  40.156932\n",
      "1     48.625001  44.637933\n",
      "2     50.269999  48.577783\n",
      "3     28.380000  28.700993\n",
      "4     32.580000  32.553381\n",
      "...         ...        ...\n",
      "4091  49.934999  49.194755\n",
      "4092  40.480000  40.533507\n",
      "4093  27.890000  28.135695\n",
      "4094  24.405000  24.849468\n",
      "4095  33.500000  35.155366\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0245\n",
      "Epoch 6/25, Validation Loss: 0.0181\n",
      "         actual  predicted\n",
      "0     39.690000  40.241173\n",
      "1     48.625001  44.954125\n",
      "2     50.269999  49.034284\n",
      "3     28.380000  28.368260\n",
      "4     32.580000  32.332487\n",
      "...         ...        ...\n",
      "4091  49.934999  49.511947\n",
      "4092  40.480000  40.683479\n",
      "4093  27.890000  27.785655\n",
      "4094  24.405000  24.779384\n",
      "4095  33.500000  35.067974\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0220\n",
      "Epoch 7/25, Validation Loss: 0.0154\n",
      "         actual  predicted\n",
      "0     39.690000  39.853870\n",
      "1     48.625001  44.989125\n",
      "2     50.269999  49.163126\n",
      "3     28.380000  28.561358\n",
      "4     32.580000  32.412859\n",
      "...         ...        ...\n",
      "4091  49.934999  49.302379\n",
      "4092  40.480000  40.496046\n",
      "4093  27.890000  27.689356\n",
      "4094  24.405000  24.934575\n",
      "4095  33.500000  34.776039\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0197\n",
      "Epoch 8/25, Validation Loss: 0.0136\n",
      "         actual  predicted\n",
      "0     39.690000  39.779591\n",
      "1     48.625001  44.948776\n",
      "2     50.269999  49.441316\n",
      "3     28.380000  28.458360\n",
      "4     32.580000  32.422204\n",
      "...         ...        ...\n",
      "4091  49.934999  49.295090\n",
      "4092  40.480000  40.618526\n",
      "4093  27.890000  27.974088\n",
      "4094  24.405000  24.938063\n",
      "4095  33.500000  34.595984\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0183\n",
      "Epoch 9/25, Validation Loss: 0.0124\n",
      "         actual  predicted\n",
      "0     39.690000  39.959437\n",
      "1     48.625001  45.498918\n",
      "2     50.269999  49.951973\n",
      "3     28.380000  28.224370\n",
      "4     32.580000  32.388410\n",
      "...         ...        ...\n",
      "4091  49.934999  49.654276\n",
      "4092  40.480000  40.690656\n",
      "4093  27.890000  27.568402\n",
      "4094  24.405000  24.759596\n",
      "4095  33.500000  34.750098\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0171\n",
      "Epoch 10/25, Validation Loss: 0.0119\n",
      "         actual  predicted\n",
      "0     39.690000  39.942871\n",
      "1     48.625001  45.068013\n",
      "2     50.269999  49.781565\n",
      "3     28.380000  28.280746\n",
      "4     32.580000  32.383969\n",
      "...         ...        ...\n",
      "4091  49.934999  49.589036\n",
      "4092  40.480000  40.694685\n",
      "4093  27.890000  27.874158\n",
      "4094  24.405000  24.610879\n",
      "4095  33.500000  34.387538\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0160\n",
      "Epoch 11/25, Validation Loss: 0.0106\n",
      "         actual  predicted\n",
      "0     39.690000  40.025173\n",
      "1     48.625001  45.016287\n",
      "2     50.269999  49.624408\n",
      "3     28.380000  28.360142\n",
      "4     32.580000  32.343457\n",
      "...         ...        ...\n",
      "4091  49.934999  49.426011\n",
      "4092  40.480000  40.482704\n",
      "4093  27.890000  27.907269\n",
      "4094  24.405000  24.931903\n",
      "4095  33.500000  34.479871\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0150\n",
      "Epoch 12/25, Validation Loss: 0.0095\n",
      "         actual  predicted\n",
      "0     39.690000  39.975688\n",
      "1     48.625001  45.425171\n",
      "2     50.269999  50.076797\n",
      "3     28.380000  28.190797\n",
      "4     32.580000  32.193006\n",
      "...         ...        ...\n",
      "4091  49.934999  49.765834\n",
      "4092  40.480000  40.508957\n",
      "4093  27.890000  27.811286\n",
      "4094  24.405000  24.761074\n",
      "4095  33.500000  34.454026\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0141\n",
      "Epoch 13/25, Validation Loss: 0.0094\n",
      "         actual  predicted\n",
      "0     39.690000  39.980864\n",
      "1     48.625001  45.515603\n",
      "2     50.269999  50.294371\n",
      "3     28.380000  28.069958\n",
      "4     32.580000  32.295327\n",
      "...         ...        ...\n",
      "4091  49.934999  49.807132\n",
      "4092  40.480000  40.563513\n",
      "4093  27.890000  27.683818\n",
      "4094  24.405000  24.733644\n",
      "4095  33.500000  34.169801\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0134\n",
      "Epoch 14/25, Validation Loss: 0.0094\n",
      "         actual  predicted\n",
      "0     39.690000  40.063463\n",
      "1     48.625001  45.535098\n",
      "2     50.269999  50.130405\n",
      "3     28.380000  28.061010\n",
      "4     32.580000  32.323341\n",
      "...         ...        ...\n",
      "4091  49.934999  49.867820\n",
      "4092  40.480000  40.565163\n",
      "4093  27.890000  27.556135\n",
      "4094  24.405000  24.638385\n",
      "4095  33.500000  34.303241\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0129\n",
      "Epoch 15/25, Validation Loss: 0.0089\n",
      "         actual  predicted\n",
      "0     39.690000  39.853201\n",
      "1     48.625001  45.204314\n",
      "2     50.269999  49.877748\n",
      "3     28.380000  28.302896\n",
      "4     32.580000  32.330245\n",
      "...         ...        ...\n",
      "4091  49.934999  49.598019\n",
      "4092  40.480000  40.492180\n",
      "4093  27.890000  27.784595\n",
      "4094  24.405000  24.618149\n",
      "4095  33.500000  34.029488\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0125\n",
      "Epoch 16/25, Validation Loss: 0.0077\n",
      "         actual  predicted\n",
      "0     39.690000  39.840907\n",
      "1     48.625001  45.306836\n",
      "2     50.269999  50.047655\n",
      "3     28.380000  28.139782\n",
      "4     32.580000  32.291229\n",
      "...         ...        ...\n",
      "4091  49.934999  49.782580\n",
      "4092  40.480000  40.748503\n",
      "4093  27.890000  27.530684\n",
      "4094  24.405000  24.434741\n",
      "4095  33.500000  33.809875\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0117\n",
      "Epoch 17/25, Validation Loss: 0.0076\n",
      "         actual  predicted\n",
      "0     39.690000  39.845758\n",
      "1     48.625001  45.356400\n",
      "2     50.269999  50.045988\n",
      "3     28.380000  28.176430\n",
      "4     32.580000  32.253157\n",
      "...         ...        ...\n",
      "4091  49.934999  49.562337\n",
      "4092  40.480000  40.341977\n",
      "4093  27.890000  27.465260\n",
      "4094  24.405000  24.686371\n",
      "4095  33.500000  33.951714\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0112\n",
      "Epoch 18/25, Validation Loss: 0.0071\n",
      "         actual  predicted\n",
      "0     39.690000  40.029048\n",
      "1     48.625001  45.653515\n",
      "2     50.269999  50.349832\n",
      "3     28.380000  27.902112\n",
      "4     32.580000  32.277574\n",
      "...         ...        ...\n",
      "4091  49.934999  49.854655\n",
      "4092  40.480000  40.692995\n",
      "4093  27.890000  27.367865\n",
      "4094  24.405000  24.614741\n",
      "4095  33.500000  34.041545\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0109\n",
      "Epoch 19/25, Validation Loss: 0.0075\n",
      "         actual  predicted\n",
      "0     39.690000  39.941519\n",
      "1     48.625001  45.480917\n",
      "2     50.269999  50.145519\n",
      "3     28.380000  28.085861\n",
      "4     32.580000  32.188439\n",
      "...         ...        ...\n",
      "4091  49.934999  49.825624\n",
      "4092  40.480000  40.427448\n",
      "4093  27.890000  27.570813\n",
      "4094  24.405000  24.531392\n",
      "4095  33.500000  33.864580\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0104\n",
      "Epoch 20/25, Validation Loss: 0.0068\n",
      "         actual  predicted\n",
      "0     39.690000  39.854412\n",
      "1     48.625001  45.332755\n",
      "2     50.269999  50.082044\n",
      "3     28.380000  28.120607\n",
      "4     32.580000  32.284894\n",
      "...         ...        ...\n",
      "4091  49.934999  49.671276\n",
      "4092  40.480000  40.516801\n",
      "4093  27.890000  27.986789\n",
      "4094  24.405000  24.872086\n",
      "4095  33.500000  33.892844\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0101\n",
      "Epoch 21/25, Validation Loss: 0.0063\n",
      "         actual  predicted\n",
      "0     39.690000  39.747582\n",
      "1     48.625001  45.536905\n",
      "2     50.269999  50.119130\n",
      "3     28.380000  28.249379\n",
      "4     32.580000  32.383292\n",
      "...         ...        ...\n",
      "4091  49.934999  49.602686\n",
      "4092  40.480000  40.456583\n",
      "4093  27.890000  27.760035\n",
      "4094  24.405000  24.845070\n",
      "4095  33.500000  33.870849\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0097\n",
      "Epoch 22/25, Validation Loss: 0.0060\n",
      "         actual  predicted\n",
      "0     39.690000  39.914038\n",
      "1     48.625001  45.476327\n",
      "2     50.269999  50.245136\n",
      "3     28.380000  28.351872\n",
      "4     32.580000  32.568099\n",
      "...         ...        ...\n",
      "4091  49.934999  49.609660\n",
      "4092  40.480000  40.571154\n",
      "4093  27.890000  28.083785\n",
      "4094  24.405000  24.893993\n",
      "4095  33.500000  33.894859\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0095\n",
      "Epoch 23/25, Validation Loss: 0.0060\n",
      "         actual  predicted\n",
      "0     39.690000  39.854663\n",
      "1     48.625001  45.679766\n",
      "2     50.269999  50.053463\n",
      "3     28.380000  28.278912\n",
      "4     32.580000  32.371316\n",
      "...         ...        ...\n",
      "4091  49.934999  49.683418\n",
      "4092  40.480000  40.455295\n",
      "4093  27.890000  27.684454\n",
      "4094  24.405000  24.875278\n",
      "4095  33.500000  33.790731\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0090\n",
      "Epoch 24/25, Validation Loss: 0.0056\n",
      "         actual  predicted\n",
      "0     39.690000  39.898361\n",
      "1     48.625001  46.197034\n",
      "2     50.269999  50.733912\n",
      "3     28.380000  28.282730\n",
      "4     32.580000  32.394591\n",
      "...         ...        ...\n",
      "4091  49.934999  50.062786\n",
      "4092  40.480000  40.663094\n",
      "4093  27.890000  27.464524\n",
      "4094  24.405000  24.978790\n",
      "4095  33.500000  33.846176\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0086\n",
      "Epoch 25/25, Validation Loss: 0.0061\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[1.1765]], device='cuda:0')\n",
      "[46.81]\n",
      "          actual  predicted\n",
      "0      39.690000  39.898361\n",
      "1      48.625001  46.197034\n",
      "2      50.269999  50.733912\n",
      "3      28.380000  28.282730\n",
      "4      32.580000  32.394591\n",
      "...          ...        ...\n",
      "39163  24.145000  23.725545\n",
      "39164  26.990000  26.930917\n",
      "39165  30.970000  30.984930\n",
      "39166  29.720000  30.064002\n",
      "39167  24.780000  24.638438\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 0.7547\n",
      "Score (MAE): 0.4499\n",
      "Score (ME): -0.1493\n",
      "Score (MAPE): 1.2134%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual  predicted\n",
      "0     28.635  28.806160\n",
      "1     27.205  27.130869\n",
      "2     17.785  20.316888\n",
      "3     19.565  20.127801\n",
      "4     25.185  24.358934\n",
      "...      ...        ...\n",
      "4091  29.535  30.657277\n",
      "4092  22.735  20.144125\n",
      "4093  22.135  20.726244\n",
      "4094  23.505  23.618202\n",
      "4095  25.990  24.807707\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.4898\n",
      "Epoch 1/25, Validation Loss: 0.1321\n",
      "      actual  predicted\n",
      "0     28.635  28.625144\n",
      "1     27.205  27.553353\n",
      "2     17.785  19.295139\n",
      "3     19.565  19.744310\n",
      "4     25.185  25.156330\n",
      "...      ...        ...\n",
      "4091  29.535  29.419904\n",
      "4092  22.735  22.629244\n",
      "4093  22.135  22.416808\n",
      "4094  23.505  24.078798\n",
      "4095  25.990  24.509528\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0730\n",
      "Epoch 2/25, Validation Loss: 0.0457\n",
      "      actual  predicted\n",
      "0     28.635  28.770049\n",
      "1     27.205  27.294618\n",
      "2     17.785  18.915721\n",
      "3     19.565  19.709875\n",
      "4     25.185  25.308254\n",
      "...      ...        ...\n",
      "4091  29.535  29.391769\n",
      "4092  22.735  22.871368\n",
      "4093  22.135  22.174091\n",
      "4094  23.505  23.870641\n",
      "4095  25.990  24.814723\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0405\n",
      "Epoch 3/25, Validation Loss: 0.0314\n",
      "      actual  predicted\n",
      "0     28.635  28.822389\n",
      "1     27.205  27.374530\n",
      "2     17.785  18.617118\n",
      "3     19.565  19.592686\n",
      "4     25.185  25.662958\n",
      "...      ...        ...\n",
      "4091  29.535  29.394285\n",
      "4092  22.735  22.948208\n",
      "4093  22.135  22.068561\n",
      "4094  23.505  23.552338\n",
      "4095  25.990  24.945162\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0317\n",
      "Epoch 4/25, Validation Loss: 0.0247\n",
      "      actual  predicted\n",
      "0     28.635  28.716017\n",
      "1     27.205  27.245627\n",
      "2     17.785  18.426040\n",
      "3     19.565  19.576033\n",
      "4     25.185  25.907201\n",
      "...      ...        ...\n",
      "4091  29.535  29.237680\n",
      "4092  22.735  23.087585\n",
      "4093  22.135  22.078467\n",
      "4094  23.505  23.379185\n",
      "4095  25.990  25.089893\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0262\n",
      "Epoch 5/25, Validation Loss: 0.0193\n",
      "      actual  predicted\n",
      "0     28.635  28.547652\n",
      "1     27.205  27.052958\n",
      "2     17.785  18.421538\n",
      "3     19.565  19.699994\n",
      "4     25.185  26.073629\n",
      "...      ...        ...\n",
      "4091  29.535  29.134939\n",
      "4092  22.735  23.054979\n",
      "4093  22.135  22.117479\n",
      "4094  23.505  23.341698\n",
      "4095  25.990  25.247930\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0222\n",
      "Epoch 6/25, Validation Loss: 0.0164\n",
      "      actual  predicted\n",
      "0     28.635  28.519686\n",
      "1     27.205  27.043502\n",
      "2     17.785  18.322218\n",
      "3     19.565  19.680952\n",
      "4     25.185  26.239104\n",
      "...      ...        ...\n",
      "4091  29.535  29.255040\n",
      "4092  22.735  22.962233\n",
      "4093  22.135  22.051133\n",
      "4094  23.505  23.217154\n",
      "4095  25.990  25.274720\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0198\n",
      "Epoch 7/25, Validation Loss: 0.0141\n",
      "      actual  predicted\n",
      "0     28.635  28.527119\n",
      "1     27.205  27.102438\n",
      "2     17.785  18.374817\n",
      "3     19.565  19.773861\n",
      "4     25.185  26.279393\n",
      "...      ...        ...\n",
      "4091  29.535  29.236241\n",
      "4092  22.735  22.906357\n",
      "4093  22.135  22.198218\n",
      "4094  23.505  23.263141\n",
      "4095  25.990  25.415776\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0179\n",
      "Epoch 8/25, Validation Loss: 0.0125\n",
      "      actual  predicted\n",
      "0     28.635  28.730923\n",
      "1     27.205  27.171833\n",
      "2     17.785  18.137801\n",
      "3     19.565  19.627305\n",
      "4     25.185  26.251159\n",
      "...      ...        ...\n",
      "4091  29.535  29.515654\n",
      "4092  22.735  22.836678\n",
      "4093  22.135  22.071018\n",
      "4094  23.505  23.247180\n",
      "4095  25.990  25.432477\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0164\n",
      "Epoch 9/25, Validation Loss: 0.0115\n",
      "      actual  predicted\n",
      "0     28.635  28.536776\n",
      "1     27.205  27.068597\n",
      "2     17.785  18.329489\n",
      "3     19.565  19.764891\n",
      "4     25.185  26.107339\n",
      "...      ...        ...\n",
      "4091  29.535  29.265283\n",
      "4092  22.735  22.870299\n",
      "4093  22.135  22.178353\n",
      "4094  23.505  23.250310\n",
      "4095  25.990  25.542255\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0152\n",
      "Epoch 10/25, Validation Loss: 0.0103\n",
      "      actual  predicted\n",
      "0     28.635  28.541008\n",
      "1     27.205  27.089551\n",
      "2     17.785  18.346169\n",
      "3     19.565  19.786020\n",
      "4     25.185  26.110061\n",
      "...      ...        ...\n",
      "4091  29.535  29.206647\n",
      "4092  22.735  22.841643\n",
      "4093  22.135  22.157188\n",
      "4094  23.505  23.181557\n",
      "4095  25.990  25.559119\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0142\n",
      "Epoch 11/25, Validation Loss: 0.0096\n",
      "      actual  predicted\n",
      "0     28.635  28.752460\n",
      "1     27.205  27.267395\n",
      "2     17.785  18.123307\n",
      "3     19.565  19.638377\n",
      "4     25.185  26.169530\n",
      "...      ...        ...\n",
      "4091  29.535  29.516607\n",
      "4092  22.735  22.851423\n",
      "4093  22.135  22.080592\n",
      "4094  23.505  23.172155\n",
      "4095  25.990  25.661116\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0133\n",
      "Epoch 12/25, Validation Loss: 0.0097\n",
      "      actual  predicted\n",
      "0     28.635  28.601152\n",
      "1     27.205  27.117398\n",
      "2     17.785  18.120567\n",
      "3     19.565  19.652843\n",
      "4     25.185  25.734514\n",
      "...      ...        ...\n",
      "4091  29.535  29.423005\n",
      "4092  22.735  22.797262\n",
      "4093  22.135  22.064661\n",
      "4094  23.505  23.224308\n",
      "4095  25.990  25.668042\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0125\n",
      "Epoch 13/25, Validation Loss: 0.0084\n",
      "      actual  predicted\n",
      "0     28.635  28.555243\n",
      "1     27.205  27.073079\n",
      "2     17.785  18.199515\n",
      "3     19.565  19.752749\n",
      "4     25.185  25.717047\n",
      "...      ...        ...\n",
      "4091  29.535  29.348265\n",
      "4092  22.735  22.811840\n",
      "4093  22.135  22.200428\n",
      "4094  23.505  23.233958\n",
      "4095  25.990  25.668503\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0121\n",
      "Epoch 14/25, Validation Loss: 0.0079\n",
      "      actual  predicted\n",
      "0     28.635  28.669189\n",
      "1     27.205  27.181792\n",
      "2     17.785  18.127254\n",
      "3     19.565  19.651065\n",
      "4     25.185  25.622167\n",
      "...      ...        ...\n",
      "4091  29.535  29.452794\n",
      "4092  22.735  22.756571\n",
      "4093  22.135  22.073592\n",
      "4094  23.505  23.179188\n",
      "4095  25.990  25.705769\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0113\n",
      "Epoch 15/25, Validation Loss: 0.0074\n",
      "      actual  predicted\n",
      "0     28.635  28.595912\n",
      "1     27.205  27.109144\n",
      "2     17.785  18.138213\n",
      "3     19.565  19.680328\n",
      "4     25.185  25.557191\n",
      "...      ...        ...\n",
      "4091  29.535  29.372449\n",
      "4092  22.735  22.823737\n",
      "4093  22.135  22.132081\n",
      "4094  23.505  23.188150\n",
      "4095  25.990  25.718189\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0108\n",
      "Epoch 16/25, Validation Loss: 0.0071\n",
      "      actual  predicted\n",
      "0     28.635  28.667967\n",
      "1     27.205  27.149992\n",
      "2     17.785  18.104221\n",
      "3     19.565  19.656216\n",
      "4     25.185  25.468880\n",
      "...      ...        ...\n",
      "4091  29.535  29.476918\n",
      "4092  22.735  22.736856\n",
      "4093  22.135  22.063309\n",
      "4094  23.505  23.239365\n",
      "4095  25.990  25.727022\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0103\n",
      "Epoch 17/25, Validation Loss: 0.0069\n",
      "      actual  predicted\n",
      "0     28.635  28.617123\n",
      "1     27.205  27.159509\n",
      "2     17.785  18.130355\n",
      "3     19.565  19.740049\n",
      "4     25.185  25.436304\n",
      "...      ...        ...\n",
      "4091  29.535  29.433867\n",
      "4092  22.735  22.728707\n",
      "4093  22.135  22.142608\n",
      "4094  23.505  23.239384\n",
      "4095  25.990  25.747042\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0099\n",
      "Epoch 18/25, Validation Loss: 0.0065\n",
      "      actual  predicted\n",
      "0     28.635  28.613026\n",
      "1     27.205  27.161557\n",
      "2     17.785  18.134290\n",
      "3     19.565  19.682489\n",
      "4     25.185  25.428100\n",
      "...      ...        ...\n",
      "4091  29.535  29.469308\n",
      "4092  22.735  22.809789\n",
      "4093  22.135  22.103119\n",
      "4094  23.505  23.198948\n",
      "4095  25.990  25.746554\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0096\n",
      "Epoch 19/25, Validation Loss: 0.0063\n",
      "      actual  predicted\n",
      "0     28.635  28.560551\n",
      "1     27.205  27.063951\n",
      "2     17.785  18.019167\n",
      "3     19.565  19.658760\n",
      "4     25.185  25.205285\n",
      "...      ...        ...\n",
      "4091  29.535  29.382343\n",
      "4092  22.735  22.727101\n",
      "4093  22.135  22.117961\n",
      "4094  23.505  23.287482\n",
      "4095  25.990  25.754131\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0093\n",
      "Epoch 20/25, Validation Loss: 0.0061\n",
      "      actual  predicted\n",
      "0     28.635  28.511627\n",
      "1     27.205  27.040409\n",
      "2     17.785  18.030894\n",
      "3     19.565  19.655878\n",
      "4     25.185  25.073340\n",
      "...      ...        ...\n",
      "4091  29.535  29.405148\n",
      "4092  22.735  22.751958\n",
      "4093  22.135  22.076010\n",
      "4094  23.505  23.237357\n",
      "4095  25.990  25.734549\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0091\n",
      "Epoch 21/25, Validation Loss: 0.0061\n",
      "      actual  predicted\n",
      "0     28.635  28.579879\n",
      "1     27.205  27.069471\n",
      "2     17.785  17.988175\n",
      "3     19.565  19.640797\n",
      "4     25.185  25.282194\n",
      "...      ...        ...\n",
      "4091  29.535  29.441115\n",
      "4092  22.735  22.682651\n",
      "4093  22.135  22.067264\n",
      "4094  23.505  23.319237\n",
      "4095  25.990  25.752824\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0087\n",
      "Epoch 22/25, Validation Loss: 0.0058\n",
      "      actual  predicted\n",
      "0     28.635  28.662905\n",
      "1     27.205  27.132539\n",
      "2     17.785  18.133338\n",
      "3     19.565  19.753901\n",
      "4     25.185  25.075074\n",
      "...      ...        ...\n",
      "4091  29.535  29.575808\n",
      "4092  22.735  22.792160\n",
      "4093  22.135  22.155915\n",
      "4094  23.505  23.265635\n",
      "4095  25.990  25.795774\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0083\n",
      "Epoch 23/25, Validation Loss: 0.0057\n",
      "      actual  predicted\n",
      "0     28.635  28.612872\n",
      "1     27.205  27.157902\n",
      "2     17.785  17.989239\n",
      "3     19.565  19.665657\n",
      "4     25.185  25.000865\n",
      "...      ...        ...\n",
      "4091  29.535  29.509864\n",
      "4092  22.735  22.793452\n",
      "4093  22.135  22.141541\n",
      "4094  23.505  23.281342\n",
      "4095  25.990  25.824785\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0082\n",
      "Epoch 24/25, Validation Loss: 0.0053\n",
      "      actual  predicted\n",
      "0     28.635  28.601511\n",
      "1     27.205  27.075741\n",
      "2     17.785  17.950370\n",
      "3     19.565  19.663633\n",
      "4     25.185  25.026620\n",
      "...      ...        ...\n",
      "4091  29.535  29.478496\n",
      "4092  22.735  22.701382\n",
      "4093  22.135  22.107765\n",
      "4094  23.505  23.296312\n",
      "4095  25.990  25.779427\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0079\n",
      "Epoch 25/25, Validation Loss: 0.0051\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[0.3937]], device='cuda:0')\n",
      "[25.48]\n",
      "       actual  predicted\n",
      "0      28.635  28.601511\n",
      "1      27.205  27.075741\n",
      "2      17.785  17.950370\n",
      "3      19.565  19.663633\n",
      "4      25.185  25.026620\n",
      "...       ...        ...\n",
      "39163  20.200  20.535541\n",
      "39164  18.485  18.829708\n",
      "39165  22.085  22.034350\n",
      "39166  23.170  23.230224\n",
      "39167  26.155  26.080709\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 0.2580\n",
      "Score (MAE): 0.1384\n",
      "Score (ME): 0.0164\n",
      "Score (MAPE): 0.5655%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (200568, 26) to (201198, 26)\n",
      "training data cutoff:  2023-07-14 01:42:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([156527, 20, 24]) torch.Size([156527]) torch.Size([156527, 1])\n",
      "Testing data shape: torch.Size([39309, 20, 24]) torch.Size([39309]) torch.Size([39309, 1])\n",
      "Shuffled Training data shape: torch.Size([156668, 20, 24]) torch.Size([156668]) torch.Size([156668, 1])\n",
      "Shuffled Testing data shape: torch.Size([39168, 20, 24]) torch.Size([39168]) torch.Size([39168, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     256.071426  806.632236\n",
      "1       4.000003  -18.132357\n",
      "2      60.499998   28.490975\n",
      "3      18.000005  986.543657\n",
      "4      18.000005   99.014785\n",
      "...          ...         ...\n",
      "4091   16.999997  169.996086\n",
      "4092    5.999997  -21.972877\n",
      "4093    9.999996   20.037094\n",
      "4094    8.999999  -26.835996\n",
      "4095  278.000003  393.229669\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8198\n",
      "Epoch 1/25, Validation Loss: 1.0104\n",
      "          actual   predicted\n",
      "0     256.071426  378.834452\n",
      "1       4.000003   14.341283\n",
      "2      60.499998   64.214304\n",
      "3      18.000005 -158.404901\n",
      "4      18.000005   78.244237\n",
      "...          ...         ...\n",
      "4091   16.999997   85.012957\n",
      "4092    5.999997   -7.681615\n",
      "4093    9.999996    1.512660\n",
      "4094    8.999999   35.285454\n",
      "4095  278.000003  290.672220\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.6623\n",
      "Epoch 2/25, Validation Loss: 0.8475\n",
      "          actual   predicted\n",
      "0     256.071426  322.829668\n",
      "1       4.000003   -2.029707\n",
      "2      60.499998  171.360473\n",
      "3      18.000005  921.759603\n",
      "4      18.000005  188.897976\n",
      "...          ...         ...\n",
      "4091   16.999997  160.376616\n",
      "4092    5.999997  -10.424732\n",
      "4093    9.999996   40.209062\n",
      "4094    8.999999   37.092539\n",
      "4095  278.000003  281.717045\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.5315\n",
      "Epoch 3/25, Validation Loss: 0.7627\n",
      "          actual   predicted\n",
      "0     256.071426  198.666278\n",
      "1       4.000003   34.070502\n",
      "2      60.499998  141.681048\n",
      "3      18.000005  404.783619\n",
      "4      18.000005  150.095474\n",
      "...          ...         ...\n",
      "4091   16.999997   81.486607\n",
      "4092    5.999997    7.592670\n",
      "4093    9.999996   43.440486\n",
      "4094    8.999999   32.888684\n",
      "4095  278.000003  208.637365\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.4927\n",
      "Epoch 4/25, Validation Loss: 0.7281\n",
      "          actual   predicted\n",
      "0     256.071426  256.406951\n",
      "1       4.000003  -13.944344\n",
      "2      60.499998  157.656642\n",
      "3      18.000005  579.282522\n",
      "4      18.000005  172.634823\n",
      "...          ...         ...\n",
      "4091   16.999997   79.439270\n",
      "4092    5.999997  -39.595276\n",
      "4093    9.999996    1.537804\n",
      "4094    8.999999  -17.720935\n",
      "4095  278.000003  295.188317\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.4761\n",
      "Epoch 5/25, Validation Loss: 0.7086\n",
      "          actual   predicted\n",
      "0     256.071426  267.493488\n",
      "1       4.000003   10.619801\n",
      "2      60.499998  165.420500\n",
      "3      18.000005  134.712592\n",
      "4      18.000005  254.205502\n",
      "...          ...         ...\n",
      "4091   16.999997  107.408309\n",
      "4092    5.999997  -23.773449\n",
      "4093    9.999996   10.618135\n",
      "4094    8.999999   15.008798\n",
      "4095  278.000003  293.383916\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.4723\n",
      "Epoch 6/25, Validation Loss: 0.7353\n",
      "          actual   predicted\n",
      "0     256.071426  198.936293\n",
      "1       4.000003    1.893490\n",
      "2      60.499998  121.429529\n",
      "3      18.000005  -26.646054\n",
      "4      18.000005  242.464164\n",
      "...          ...         ...\n",
      "4091   16.999997   97.143642\n",
      "4092    5.999997  -36.437923\n",
      "4093    9.999996    1.364837\n",
      "4094    8.999999    0.920055\n",
      "4095  278.000003  263.766764\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.4478\n",
      "Epoch 7/25, Validation Loss: 0.6979\n",
      "          actual   predicted\n",
      "0     256.071426  251.047316\n",
      "1       4.000003   25.280947\n",
      "2      60.499998  150.115434\n",
      "3      18.000005 -139.674366\n",
      "4      18.000005  210.610040\n",
      "...          ...         ...\n",
      "4091   16.999997  127.217031\n",
      "4092    5.999997   -3.328826\n",
      "4093    9.999996   10.133291\n",
      "4094    8.999999   17.662808\n",
      "4095  278.000003  294.412177\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.4393\n",
      "Epoch 8/25, Validation Loss: 0.7372\n",
      "          actual   predicted\n",
      "0     256.071426  277.463465\n",
      "1       4.000003   -4.091525\n",
      "2      60.499998  135.102713\n",
      "3      18.000005  -59.434846\n",
      "4      18.000005  219.708628\n",
      "...          ...         ...\n",
      "4091   16.999997   99.051845\n",
      "4092    5.999997  -33.313787\n",
      "4093    9.999996   -9.840862\n",
      "4094    8.999999   -6.941399\n",
      "4095  278.000003  263.027169\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.4432\n",
      "Epoch 9/25, Validation Loss: 0.6702\n",
      "          actual   predicted\n",
      "0     256.071426  205.758683\n",
      "1       4.000003   28.698294\n",
      "2      60.499998  129.713039\n",
      "3      18.000005 -150.988678\n",
      "4      18.000005  196.138538\n",
      "...          ...         ...\n",
      "4091   16.999997  114.120359\n",
      "4092    5.999997   -1.652625\n",
      "4093    9.999996   -3.590971\n",
      "4094    8.999999   10.653007\n",
      "4095  278.000003  274.340511\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.4198\n",
      "Epoch 10/25, Validation Loss: 0.6950\n",
      "          actual   predicted\n",
      "0     256.071426  209.181010\n",
      "1       4.000003  -14.170127\n",
      "2      60.499998  108.991714\n",
      "3      18.000005 -199.380512\n",
      "4      18.000005  165.779762\n",
      "...          ...         ...\n",
      "4091   16.999997   35.733002\n",
      "4092    5.999997  -42.875711\n",
      "4093    9.999996  -21.712056\n",
      "4094    8.999999  -33.420603\n",
      "4095  278.000003  264.473939\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.4288\n",
      "Epoch 11/25, Validation Loss: 0.6506\n",
      "          actual   predicted\n",
      "0     256.071426  259.759093\n",
      "1       4.000003   26.657483\n",
      "2      60.499998  121.629188\n",
      "3      18.000005 -162.073116\n",
      "4      18.000005  189.834672\n",
      "...          ...         ...\n",
      "4091   16.999997   73.838466\n",
      "4092    5.999997   -3.912967\n",
      "4093    9.999996    5.328864\n",
      "4094    8.999999   15.760870\n",
      "4095  278.000003  316.385166\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.4362\n",
      "Epoch 12/25, Validation Loss: 0.6780\n",
      "          actual   predicted\n",
      "0     256.071426  268.953788\n",
      "1       4.000003   -4.652580\n",
      "2      60.499998  128.746999\n",
      "3      18.000005 -334.202001\n",
      "4      18.000005  223.325880\n",
      "...          ...         ...\n",
      "4091   16.999997   58.908531\n",
      "4092    5.999997  -25.553711\n",
      "4093    9.999996   -9.768588\n",
      "4094    8.999999  -22.577221\n",
      "4095  278.000003  294.702670\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.4044\n",
      "Epoch 13/25, Validation Loss: 0.6414\n",
      "          actual   predicted\n",
      "0     256.071426  252.208854\n",
      "1       4.000003   14.806539\n",
      "2      60.499998  118.741960\n",
      "3      18.000005 -420.710693\n",
      "4      18.000005  195.621866\n",
      "...          ...         ...\n",
      "4091   16.999997   96.735819\n",
      "4092    5.999997    1.433009\n",
      "4093    9.999996   -4.809163\n",
      "4094    8.999999    7.915328\n",
      "4095  278.000003  292.229796\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.3822\n",
      "Epoch 14/25, Validation Loss: 0.6812\n",
      "          actual   predicted\n",
      "0     256.071426  289.127659\n",
      "1       4.000003   -0.139899\n",
      "2      60.499998  119.514376\n",
      "3      18.000005 -342.433575\n",
      "4      18.000005  174.282307\n",
      "...          ...         ...\n",
      "4091   16.999997   74.717225\n",
      "4092    5.999997  -28.622902\n",
      "4093    9.999996  -11.175799\n",
      "4094    8.999999  -18.472357\n",
      "4095  278.000003  304.875604\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.3946\n",
      "Epoch 15/25, Validation Loss: 0.6717\n",
      "          actual   predicted\n",
      "0     256.071426  363.059554\n",
      "1       4.000003  -14.250393\n",
      "2      60.499998  124.700289\n",
      "3      18.000005 -189.836863\n",
      "4      18.000005  204.052291\n",
      "...          ...         ...\n",
      "4091   16.999997   47.544416\n",
      "4092    5.999997  -25.562600\n",
      "4093    9.999996  -18.698897\n",
      "4094    8.999999  -16.435056\n",
      "4095  278.000003  303.092942\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.3696\n",
      "Epoch 16/25, Validation Loss: 0.7256\n",
      "          actual   predicted\n",
      "0     256.071426  328.026521\n",
      "1       4.000003  -17.199481\n",
      "2      60.499998  141.573059\n",
      "3      18.000005 -199.076236\n",
      "4      18.000005  127.060673\n",
      "...          ...         ...\n",
      "4091   16.999997   27.731066\n",
      "4092    5.999997  -15.981857\n",
      "4093    9.999996  -13.090409\n",
      "4094    8.999999  -16.389828\n",
      "4095  278.000003  240.710203\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.4156\n",
      "Epoch 17/25, Validation Loss: 0.6519\n",
      "          actual   predicted\n",
      "0     256.071426  254.026477\n",
      "1       4.000003   19.274217\n",
      "2      60.499998  132.104637\n",
      "3      18.000005 -340.619788\n",
      "4      18.000005  158.076005\n",
      "...          ...         ...\n",
      "4091   16.999997   55.259596\n",
      "4092    5.999997   23.132492\n",
      "4093    9.999996    5.409910\n",
      "4094    8.999999   14.401488\n",
      "4095  278.000003  260.418275\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.3571\n",
      "Epoch 18/25, Validation Loss: 0.6203\n",
      "          actual   predicted\n",
      "0     256.071426  324.536119\n",
      "1       4.000003  -24.677410\n",
      "2      60.499998  130.775546\n",
      "3      18.000005 -182.568712\n",
      "4      18.000005  156.919488\n",
      "...          ...         ...\n",
      "4091   16.999997   38.181726\n",
      "4092    5.999997  -17.680412\n",
      "4093    9.999996  -23.981359\n",
      "4094    8.999999  -21.701393\n",
      "4095  278.000003  277.180243\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.3536\n",
      "Epoch 19/25, Validation Loss: 0.6005\n",
      "          actual   predicted\n",
      "0     256.071426  269.397884\n",
      "1       4.000003    2.543522\n",
      "2      60.499998  133.398224\n",
      "3      18.000005 -269.183747\n",
      "4      18.000005  148.887854\n",
      "...          ...         ...\n",
      "4091   16.999997   38.676358\n",
      "4092    5.999997    5.182424\n",
      "4093    9.999996   -5.302778\n",
      "4094    8.999999    0.370550\n",
      "4095  278.000003  247.554804\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.3940\n",
      "Epoch 20/25, Validation Loss: 0.6046\n",
      "          actual   predicted\n",
      "0     256.071426  261.683839\n",
      "1       4.000003   -3.891890\n",
      "2      60.499998  126.322497\n",
      "3      18.000005 -198.317886\n",
      "4      18.000005  151.888323\n",
      "...          ...         ...\n",
      "4091   16.999997   50.380980\n",
      "4092    5.999997    5.511300\n",
      "4093    9.999996   -4.468879\n",
      "4094    8.999999    1.431260\n",
      "4095  278.000003  254.893072\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.3355\n",
      "Epoch 21/25, Validation Loss: 0.6050\n",
      "          actual   predicted\n",
      "0     256.071426  288.397787\n",
      "1       4.000003   -4.350254\n",
      "2      60.499998  142.286009\n",
      "3      18.000005 -127.407390\n",
      "4      18.000005  163.670908\n",
      "...          ...         ...\n",
      "4091   16.999997   50.611078\n",
      "4092    5.999997   -4.206923\n",
      "4093    9.999996   -0.792094\n",
      "4094    8.999999   -3.408134\n",
      "4095  278.000003  289.389308\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.3278\n",
      "Epoch 22/25, Validation Loss: 0.6496\n",
      "          actual   predicted\n",
      "0     256.071426  329.065374\n",
      "1       4.000003  -10.648328\n",
      "2      60.499998  147.002933\n",
      "3      18.000005 -228.475176\n",
      "4      18.000005  164.089791\n",
      "...          ...         ...\n",
      "4091   16.999997   31.185851\n",
      "4092    5.999997   -1.009556\n",
      "4093    9.999996    0.122507\n",
      "4094    8.999999   -9.055100\n",
      "4095  278.000003  260.754149\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.3511\n",
      "Epoch 23/25, Validation Loss: 0.5942\n",
      "          actual   predicted\n",
      "0     256.071426  296.762159\n",
      "1       4.000003  -10.141344\n",
      "2      60.499998  134.471718\n",
      "3      18.000005  -68.441649\n",
      "4      18.000005  147.386399\n",
      "...          ...         ...\n",
      "4091   16.999997   30.559236\n",
      "4092    5.999997   -0.736558\n",
      "4093    9.999996   -5.669883\n",
      "4094    8.999999  -13.790716\n",
      "4095  278.000003  266.019416\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.3366\n",
      "Epoch 24/25, Validation Loss: 0.6314\n",
      "          actual   predicted\n",
      "0     256.071426  303.233826\n",
      "1       4.000003  -13.396245\n",
      "2      60.499998  129.243924\n",
      "3      18.000005 -230.763794\n",
      "4      18.000005  148.026489\n",
      "...          ...         ...\n",
      "4091   16.999997   23.479383\n",
      "4092    5.999997   -0.628147\n",
      "4093    9.999996   -0.190647\n",
      "4094    8.999999   -6.628411\n",
      "4095  278.000003  250.596913\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.3388\n",
      "Epoch 25/25, Validation Loss: 0.6834\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4290,  1.6500,  1.1726, -0.6852, -0.1112, -0.1014, -1.7541,\n",
      "          -0.5687, -1.9713, -4.3015,  4.1521,  0.0000, -0.9092,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9399, -1.0404,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4363,  1.6217,  1.1330, -0.6484, -0.1342, -0.1081, -1.6229,\n",
      "          -0.5687, -2.0083, -4.3421,  4.1521,  0.0000, -0.9091,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -0.9995, -0.9746,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4435,  1.5933,  1.0934, -0.6116, -0.1571, -0.1148, -1.4917,\n",
      "          -0.5687, -2.0453, -4.3827,  4.1521,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.0591, -0.9089,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4508,  1.5650,  1.0537, -0.5748, -0.1800, -0.1215, -1.3605,\n",
      "          -0.5687, -2.0823, -4.4233,  4.1521,  0.0000, -0.9088,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1187, -0.8432,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4580,  1.5367,  1.0141, -0.5379, -0.2030, -0.1281, -1.2293,\n",
      "          -0.5687, -2.1193, -4.4639,  4.1521,  0.0000, -0.9087,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.9729,  0.1423, -1.1783, -0.7774,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.6762,  2.1835,  1.2122, -1.1618,  0.3832,  0.0253, -0.5732,\n",
      "          -0.3973,  1.5337,  0.8155, -0.8016,  0.0000, -0.9104,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.7755, -1.1827,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [-0.1988,  1.7018,  3.4892, -1.1351,  0.0841, -0.0564, -1.2621,\n",
      "          -0.4702,  0.8771,  0.0936, -0.8016,  0.0000, -0.9090,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.8461, -1.1326,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.0855,  1.2603,  2.1474, -1.0334,  0.0264, -0.0681, -0.7919,\n",
      "          -0.3402,  1.3025,  0.6598, -0.8016,  0.0000, -0.9074,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -0.9149, -1.0767,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.1933,  1.1200,  1.7829, -1.0289,  0.0100, -0.0814, -0.5732,\n",
      "          -0.4830,  1.5337,  0.8764, -0.8016,  0.0000, -0.9071,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1983, -0.7461,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2295,  0.9378,  0.9111, -1.0323,  0.0088, -0.0814, -1.8854,\n",
      "          -0.5687, -1.6569, -2.7786, -0.8016,  0.0000, -0.9070,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0339, -0.9626,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.2797,  0.8399,  0.8635, -0.9675, -0.0215, -0.0881, -1.8854,\n",
      "          -0.4830,  1.7649,  0.5515, -0.8016,  0.0000, -0.9068,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.0946, -0.8924,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3270,  0.7617,  0.8318, -0.9641, -0.0290, -0.0948, -1.2293,\n",
      "          -0.3117,  1.5800,  0.8155, -0.8016,  0.0000, -0.9067,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.1505, -0.8184,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3466,  0.8128,  0.7684, -0.9164, -0.0341, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  1.0185, -0.8016,  0.0000, -0.9066,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3519, -0.4024,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3438,  0.8190,  0.7209, -0.7698, -0.0353, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4413,  0.8764, -0.8016,  0.0000, -0.9064,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.2473, -0.6601,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3605,  0.9187,  0.5914, -0.8039, -0.0895, -0.1159, -1.2293,\n",
      "          -0.3973,  1.5337,  0.8832, -0.8016,  0.0000, -0.9062,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3037, -0.5385,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3842,  1.0268,  1.1290, -0.5771, -0.0473, -0.1048, -1.5573,\n",
      "          -0.5687,  1.6262,  0.5921, -0.8016,  0.0000, -0.9058,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3390, -0.4438,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4190,  0.9815,  0.6733, -0.7220, -0.0366, -0.1048, -1.8854,\n",
      "          -0.5687,  1.7187,  0.4094, -0.8016,  0.0000, -0.9056,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3667, -0.3487,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4079,  1.0024,  0.6812, -0.7016, -0.0391, -0.1048, -1.8854,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9055,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.3866, -0.2580,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.4106,  1.1065,  0.7446, -0.6436, -0.0366, -0.1014, -0.5732,\n",
      "          -0.5687,  1.5337,  0.9576, -0.8016,  0.0000, -0.9054,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4005, -0.1661,\n",
      "          -1.1609,  1.1609,  0.0000],\n",
      "         [ 0.3939,  1.1534,  0.7684, -0.5823, -0.0366, -0.0981, -1.8854,\n",
      "          -0.5687,  1.4875,  0.8155, -0.8016,  0.0000, -0.9052,  0.0000,\n",
      "          -0.4480, -0.0399,  1.4129, -1.7670,  0.8592, -1.4083, -0.0736,\n",
      "          -1.1609,  1.1609,  0.0000]]])\n",
      "predicted: tensor([[-0.1748]], device='cuda:0')\n",
      "[47.39]\n",
      "           actual   predicted\n",
      "0      256.071426  303.233826\n",
      "1        4.000003  -13.396245\n",
      "2       60.499998  129.243924\n",
      "3       18.000005 -230.763794\n",
      "4       18.000005  148.026489\n",
      "...           ...         ...\n",
      "39163    4.000003   23.943481\n",
      "39164   15.000003    0.437126\n",
      "39165   30.500000  -30.236510\n",
      "39166    5.999997   -0.618122\n",
      "39167  251.000001  234.471064\n",
      "\n",
      "[39168 rows x 2 columns]\n",
      "Score (RMSE): 632.1242\n",
      "Score (MAE): 78.9470\n",
      "Score (ME): 13.9571\n",
      "Score (MAPE): 154017.7305%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     496.250000  458.301253\n",
      "1     423.000003  498.280723\n",
      "2     455.750000  550.751129\n",
      "3     428.000001  453.293966\n",
      "4     367.499997  430.485918\n",
      "...          ...         ...\n",
      "4091  411.928568  458.741899\n",
      "4092  485.750000  470.116552\n",
      "4093  412.250001  439.552326\n",
      "4094  471.000000  492.433020\n",
      "4095  421.499998  475.579879\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8025\n",
      "Epoch 1/25, Validation Loss: 0.6739\n",
      "          actual   predicted\n",
      "0     496.250000  484.475034\n",
      "1     423.000003  494.687780\n",
      "2     455.750000  522.104663\n",
      "3     428.000001  460.966463\n",
      "4     367.499997  416.704328\n",
      "...          ...         ...\n",
      "4091  411.928568  451.413250\n",
      "4092  485.750000  512.653706\n",
      "4093  412.250001  405.187254\n",
      "4094  471.000000  445.533884\n",
      "4095  421.499998  404.535103\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.5839\n",
      "Epoch 2/25, Validation Loss: 0.5460\n",
      "          actual   predicted\n",
      "0     496.250000  456.988802\n",
      "1     423.000003  519.417943\n",
      "2     455.750000  520.974615\n",
      "3     428.000001  450.551702\n",
      "4     367.499997  420.201167\n",
      "...          ...         ...\n",
      "4091  411.928568  429.293117\n",
      "4092  485.750000  505.929922\n",
      "4093  412.250001  426.445518\n",
      "4094  471.000000  453.310960\n",
      "4095  421.499998  423.311111\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.4532\n",
      "Epoch 3/25, Validation Loss: 0.3882\n",
      "          actual   predicted\n",
      "0     496.250000  443.793729\n",
      "1     423.000003  487.231903\n",
      "2     455.750000  450.497739\n",
      "3     428.000001  443.469217\n",
      "4     367.499997  394.658478\n",
      "...          ...         ...\n",
      "4091  411.928568  415.282634\n",
      "4092  485.750000  475.226461\n",
      "4093  412.250001  406.324902\n",
      "4094  471.000000  469.931363\n",
      "4095  421.499998  402.953114\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.3163\n",
      "Epoch 4/25, Validation Loss: 0.2404\n",
      "          actual   predicted\n",
      "0     496.250000  447.704629\n",
      "1     423.000003  458.580511\n",
      "2     455.750000  442.882208\n",
      "3     428.000001  442.174022\n",
      "4     367.499997  391.012475\n",
      "...          ...         ...\n",
      "4091  411.928568  418.509984\n",
      "4092  485.750000  465.065236\n",
      "4093  412.250001  401.819930\n",
      "4094  471.000000  468.915961\n",
      "4095  421.499998  403.757968\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.2209\n",
      "Epoch 5/25, Validation Loss: 0.1745\n",
      "          actual   predicted\n",
      "0     496.250000  437.243164\n",
      "1     423.000003  453.688173\n",
      "2     455.750000  454.693104\n",
      "3     428.000001  442.880614\n",
      "4     367.499997  387.594858\n",
      "...          ...         ...\n",
      "4091  411.928568  422.347411\n",
      "4092  485.750000  470.448150\n",
      "4093  412.250001  419.391021\n",
      "4094  471.000000  468.373364\n",
      "4095  421.499998  412.622100\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1841\n",
      "Epoch 6/25, Validation Loss: 0.1522\n",
      "          actual   predicted\n",
      "0     496.250000  441.372479\n",
      "1     423.000003  448.693136\n",
      "2     455.750000  446.205860\n",
      "3     428.000001  443.414187\n",
      "4     367.499997  381.723017\n",
      "...          ...         ...\n",
      "4091  411.928568  426.911023\n",
      "4092  485.750000  471.740130\n",
      "4093  412.250001  415.156441\n",
      "4094  471.000000  468.999163\n",
      "4095  421.499998  412.392207\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.1656\n",
      "Epoch 7/25, Validation Loss: 0.1460\n",
      "          actual   predicted\n",
      "0     496.250000  435.868650\n",
      "1     423.000003  449.529444\n",
      "2     455.750000  453.313334\n",
      "3     428.000001  443.848143\n",
      "4     367.499997  381.479500\n",
      "...          ...         ...\n",
      "4091  411.928568  418.359857\n",
      "4092  485.750000  476.640706\n",
      "4093  412.250001  405.464701\n",
      "4094  471.000000  476.528516\n",
      "4095  421.499998  420.788587\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1607\n",
      "Epoch 8/25, Validation Loss: 0.1427\n",
      "          actual   predicted\n",
      "0     496.250000  436.773784\n",
      "1     423.000003  449.656014\n",
      "2     455.750000  461.330919\n",
      "3     428.000001  449.962479\n",
      "4     367.499997  390.399352\n",
      "...          ...         ...\n",
      "4091  411.928568  421.197826\n",
      "4092  485.750000  480.839399\n",
      "4093  412.250001  413.457225\n",
      "4094  471.000000  474.944680\n",
      "4095  421.499998  424.545725\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1534\n",
      "Epoch 9/25, Validation Loss: 0.1370\n",
      "          actual   predicted\n",
      "0     496.250000  434.731002\n",
      "1     423.000003  450.826961\n",
      "2     455.750000  450.884313\n",
      "3     428.000001  445.511126\n",
      "4     367.499997  385.568073\n",
      "...          ...         ...\n",
      "4091  411.928568  420.863740\n",
      "4092  485.750000  476.797392\n",
      "4093  412.250001  414.695557\n",
      "4094  471.000000  477.683080\n",
      "4095  421.499998  424.617444\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.1489\n",
      "Epoch 10/25, Validation Loss: 0.1340\n",
      "          actual   predicted\n",
      "0     496.250000  433.230329\n",
      "1     423.000003  443.132242\n",
      "2     455.750000  452.808066\n",
      "3     428.000001  442.702504\n",
      "4     367.499997  390.467981\n",
      "...          ...         ...\n",
      "4091  411.928568  422.499598\n",
      "4092  485.750000  479.023924\n",
      "4093  412.250001  414.325242\n",
      "4094  471.000000  471.408028\n",
      "4095  421.499998  417.081825\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.1463\n",
      "Epoch 11/25, Validation Loss: 0.1350\n",
      "          actual   predicted\n",
      "0     496.250000  433.939608\n",
      "1     423.000003  450.740349\n",
      "2     455.750000  446.599354\n",
      "3     428.000001  440.656877\n",
      "4     367.499997  384.181100\n",
      "...          ...         ...\n",
      "4091  411.928568  426.259700\n",
      "4092  485.750000  479.571199\n",
      "4093  412.250001  416.721931\n",
      "4094  471.000000  470.935082\n",
      "4095  421.499998  414.823654\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1438\n",
      "Epoch 12/25, Validation Loss: 0.1307\n",
      "          actual   predicted\n",
      "0     496.250000  427.756349\n",
      "1     423.000003  438.444901\n",
      "2     455.750000  459.935434\n",
      "3     428.000001  441.750599\n",
      "4     367.499997  385.034217\n",
      "...          ...         ...\n",
      "4091  411.928568  414.280779\n",
      "4092  485.750000  477.515739\n",
      "4093  412.250001  408.417590\n",
      "4094  471.000000  474.134253\n",
      "4095  421.499998  416.676848\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1432\n",
      "Epoch 13/25, Validation Loss: 0.1291\n",
      "          actual   predicted\n",
      "0     496.250000  431.764242\n",
      "1     423.000003  451.344477\n",
      "2     455.750000  462.703290\n",
      "3     428.000001  447.127358\n",
      "4     367.499997  383.477043\n",
      "...          ...         ...\n",
      "4091  411.928568  420.044053\n",
      "4092  485.750000  482.126495\n",
      "4093  412.250001  416.384031\n",
      "4094  471.000000  477.036058\n",
      "4095  421.499998  427.754173\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.1398\n",
      "Epoch 14/25, Validation Loss: 0.1308\n",
      "          actual   predicted\n",
      "0     496.250000  428.966653\n",
      "1     423.000003  445.063266\n",
      "2     455.750000  455.401644\n",
      "3     428.000001  441.524080\n",
      "4     367.499997  386.769206\n",
      "...          ...         ...\n",
      "4091  411.928568  421.429033\n",
      "4092  485.750000  476.405940\n",
      "4093  412.250001  413.691784\n",
      "4094  471.000000  468.797261\n",
      "4095  421.499998  423.416647\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.1372\n",
      "Epoch 15/25, Validation Loss: 0.1278\n",
      "          actual   predicted\n",
      "0     496.250000  427.734342\n",
      "1     423.000003  443.225562\n",
      "2     455.750000  451.771412\n",
      "3     428.000001  440.202100\n",
      "4     367.499997  385.569096\n",
      "...          ...         ...\n",
      "4091  411.928568  421.171735\n",
      "4092  485.750000  472.831685\n",
      "4093  412.250001  415.101616\n",
      "4094  471.000000  468.135489\n",
      "4095  421.499998  419.301131\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.1375\n",
      "Epoch 16/25, Validation Loss: 0.1279\n",
      "          actual   predicted\n",
      "0     496.250000  425.931310\n",
      "1     423.000003  440.784346\n",
      "2     455.750000  462.970058\n",
      "3     428.000001  440.768251\n",
      "4     367.499997  384.627650\n",
      "...          ...         ...\n",
      "4091  411.928568  417.074673\n",
      "4092  485.750000  477.490259\n",
      "4093  412.250001  412.349222\n",
      "4094  471.000000  471.255296\n",
      "4095  421.499998  412.891357\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.1346\n",
      "Epoch 17/25, Validation Loss: 0.1258\n",
      "          actual   predicted\n",
      "0     496.250000  429.365971\n",
      "1     423.000003  445.417855\n",
      "2     455.750000  461.675709\n",
      "3     428.000001  439.104003\n",
      "4     367.499997  382.954862\n",
      "...          ...         ...\n",
      "4091  411.928568  424.741039\n",
      "4092  485.750000  477.117236\n",
      "4093  412.250001  416.947405\n",
      "4094  471.000000  472.597056\n",
      "4095  421.499998  420.510334\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.1312\n",
      "Epoch 18/25, Validation Loss: 0.1247\n",
      "          actual   predicted\n",
      "0     496.250000  425.943344\n",
      "1     423.000003  442.415307\n",
      "2     455.750000  466.385847\n",
      "3     428.000001  439.951050\n",
      "4     367.499997  380.269864\n",
      "...          ...         ...\n",
      "4091  411.928568  415.300842\n",
      "4092  485.750000  476.677919\n",
      "4093  412.250001  410.397336\n",
      "4094  471.000000  469.574569\n",
      "4095  421.499998  413.122094\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.1309\n",
      "Epoch 19/25, Validation Loss: 0.1247\n",
      "          actual   predicted\n",
      "0     496.250000  422.308039\n",
      "1     423.000003  441.264214\n",
      "2     455.750000  466.338373\n",
      "3     428.000001  439.557152\n",
      "4     367.499997  381.980740\n",
      "...          ...         ...\n",
      "4091  411.928568  416.496017\n",
      "4092  485.750000  472.712864\n",
      "4093  412.250001  412.143583\n",
      "4094  471.000000  470.777399\n",
      "4095  421.499998  422.122079\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.1292\n",
      "Epoch 20/25, Validation Loss: 0.1252\n",
      "          actual   predicted\n",
      "0     496.250000  423.343973\n",
      "1     423.000003  439.180339\n",
      "2     455.750000  463.810988\n",
      "3     428.000001  441.760024\n",
      "4     367.499997  386.070785\n",
      "...          ...         ...\n",
      "4091  411.928568  414.493368\n",
      "4092  485.750000  477.332568\n",
      "4093  412.250001  414.979992\n",
      "4094  471.000000  468.958022\n",
      "4095  421.499998  426.061049\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.1295\n",
      "Epoch 21/25, Validation Loss: 0.1259\n",
      "          actual   predicted\n",
      "0     496.250000  423.745940\n",
      "1     423.000003  441.865392\n",
      "2     455.750000  467.585692\n",
      "3     428.000001  439.187121\n",
      "4     367.499997  382.713517\n",
      "...          ...         ...\n",
      "4091  411.928568  411.619185\n",
      "4092  485.750000  477.884632\n",
      "4093  412.250001  412.102076\n",
      "4094  471.000000  465.685289\n",
      "4095  421.499998  416.358917\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.1314\n",
      "Epoch 22/25, Validation Loss: 0.1231\n",
      "          actual   predicted\n",
      "0     496.250000  420.351771\n",
      "1     423.000003  440.567760\n",
      "2     455.750000  467.917400\n",
      "3     428.000001  436.058469\n",
      "4     367.499997  378.635387\n",
      "...          ...         ...\n",
      "4091  411.928568  415.127849\n",
      "4092  485.750000  477.849079\n",
      "4093  412.250001  414.620121\n",
      "4094  471.000000  466.783219\n",
      "4095  421.499998  417.766375\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.1266\n",
      "Epoch 23/25, Validation Loss: 0.1218\n",
      "          actual   predicted\n",
      "0     496.250000  422.910569\n",
      "1     423.000003  439.486639\n",
      "2     455.750000  467.289714\n",
      "3     428.000001  438.117369\n",
      "4     367.499997  383.359629\n",
      "...          ...         ...\n",
      "4091  411.928568  417.027052\n",
      "4092  485.750000  478.797186\n",
      "4093  412.250001  409.893102\n",
      "4094  471.000000  469.103423\n",
      "4095  421.499998  420.822091\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.1261\n",
      "Epoch 24/25, Validation Loss: 0.1220\n",
      "          actual   predicted\n",
      "0     496.250000  422.856767\n",
      "1     423.000003  440.446393\n",
      "2     455.750000  462.925020\n",
      "3     428.000001  437.112704\n",
      "4     367.499997  378.630340\n",
      "...          ...         ...\n",
      "4091  411.928568  415.517730\n",
      "4092  485.750000  477.447341\n",
      "4093  412.250001  414.766246\n",
      "4094  471.000000  467.051740\n",
      "4095  421.499998  413.604814\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.1260\n",
      "Epoch 25/25, Validation Loss: 0.1210\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.6661]], device='cuda:0')\n",
      "[572.79]\n",
      "           actual   predicted\n",
      "0      496.250000  422.856767\n",
      "1      423.000003  440.446393\n",
      "2      455.750000  462.925020\n",
      "3      428.000001  437.112704\n",
      "4      367.499997  378.630340\n",
      "...           ...         ...\n",
      "19155  451.500001  449.102390\n",
      "19156  567.714289  828.383661\n",
      "19157  439.000000  440.331189\n",
      "19158  483.333333  462.037493\n",
      "19159  585.666668  575.904874\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 43.4488\n",
      "Score (MAE): 16.4013\n",
      "Score (ME): 1.4362\n",
      "Score (MAPE): 2.8605%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual   predicted\n",
      "0      916.499996  683.714683\n",
      "1      587.041658  650.883760\n",
      "2      619.250003  672.572882\n",
      "3      698.499999  692.880140\n",
      "4      615.999997  658.666311\n",
      "...           ...         ...\n",
      "4091   654.250002  665.667678\n",
      "4092   762.250001  706.483361\n",
      "4093   975.000000  899.701152\n",
      "4094  1049.750006  925.396422\n",
      "4095  1081.361119  938.231958\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8152\n",
      "Epoch 1/25, Validation Loss: 0.5172\n",
      "           actual    predicted\n",
      "0      916.499996   877.232366\n",
      "1      587.041658   648.359591\n",
      "2      619.250003   587.738015\n",
      "3      698.499999   686.186336\n",
      "4      615.999997   616.880949\n",
      "...           ...          ...\n",
      "4091   654.250002   730.325582\n",
      "4092   762.250001   745.748972\n",
      "4093   975.000000  1090.612524\n",
      "4094  1049.750006  1061.622898\n",
      "4095  1081.361119  1000.070834\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.4715\n",
      "Epoch 2/25, Validation Loss: 0.4036\n",
      "           actual    predicted\n",
      "0      916.499996   862.801313\n",
      "1      587.041658   642.812269\n",
      "2      619.250003   575.442757\n",
      "3      698.499999   662.232061\n",
      "4      615.999997   603.268025\n",
      "...           ...          ...\n",
      "4091   654.250002   748.041837\n",
      "4092   762.250001   780.216861\n",
      "4093   975.000000  1070.429233\n",
      "4094  1049.750006  1053.648195\n",
      "4095  1081.361119   897.032353\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.3682\n",
      "Epoch 3/25, Validation Loss: 0.3078\n",
      "           actual    predicted\n",
      "0      916.499996   805.513247\n",
      "1      587.041658   655.861394\n",
      "2      619.250003   579.358545\n",
      "3      698.499999   638.080207\n",
      "4      615.999997   613.402050\n",
      "...           ...          ...\n",
      "4091   654.250002   732.851957\n",
      "4092   762.250001   775.908174\n",
      "4093   975.000000  1168.690588\n",
      "4094  1049.750006  1012.750690\n",
      "4095  1081.361119   913.800848\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.2706\n",
      "Epoch 4/25, Validation Loss: 0.2260\n",
      "           actual    predicted\n",
      "0      916.499996   758.850772\n",
      "1      587.041658   648.046079\n",
      "2      619.250003   568.338079\n",
      "3      698.499999   618.008339\n",
      "4      615.999997   603.133114\n",
      "...           ...          ...\n",
      "4091   654.250002   756.561223\n",
      "4092   762.250001   786.914670\n",
      "4093   975.000000  1135.043320\n",
      "4094  1049.750006   966.900549\n",
      "4095  1081.361119   917.172162\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.2041\n",
      "Epoch 5/25, Validation Loss: 0.1650\n",
      "           actual    predicted\n",
      "0      916.499996   764.598576\n",
      "1      587.041658   668.631593\n",
      "2      619.250003   602.498116\n",
      "3      698.499999   625.685877\n",
      "4      615.999997   629.231276\n",
      "...           ...          ...\n",
      "4091   654.250002   755.271278\n",
      "4092   762.250001   787.746289\n",
      "4093   975.000000  1095.548083\n",
      "4094  1049.750006   971.570160\n",
      "4095  1081.361119   928.442811\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1644\n",
      "Epoch 6/25, Validation Loss: 0.1312\n",
      "           actual    predicted\n",
      "0      916.499996   763.965557\n",
      "1      587.041658   637.918300\n",
      "2      619.250003   585.331541\n",
      "3      698.499999   617.284275\n",
      "4      615.999997   607.373046\n",
      "...           ...          ...\n",
      "4091   654.250002   740.823585\n",
      "4092   762.250001   763.547327\n",
      "4093   975.000000  1053.077890\n",
      "4094  1049.750006   964.465238\n",
      "4095  1081.361119   901.939352\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.1427\n",
      "Epoch 7/25, Validation Loss: 0.1211\n",
      "           actual    predicted\n",
      "0      916.499996   767.438027\n",
      "1      587.041658   627.235702\n",
      "2      619.250003   599.365336\n",
      "3      698.499999   635.752480\n",
      "4      615.999997   619.699660\n",
      "...           ...          ...\n",
      "4091   654.250002   734.562743\n",
      "4092   762.250001   763.917669\n",
      "4093   975.000000  1058.529466\n",
      "4094  1049.750006   991.174355\n",
      "4095  1081.361119   924.302125\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1328\n",
      "Epoch 8/25, Validation Loss: 0.1127\n",
      "           actual    predicted\n",
      "0      916.499996   759.234865\n",
      "1      587.041658   635.097261\n",
      "2      619.250003   602.915061\n",
      "3      698.499999   629.402425\n",
      "4      615.999997   612.830094\n",
      "...           ...          ...\n",
      "4091   654.250002   740.196299\n",
      "4092   762.250001   751.323477\n",
      "4093   975.000000  1039.577606\n",
      "4094  1049.750006   991.990075\n",
      "4095  1081.361119   922.681301\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1254\n",
      "Epoch 9/25, Validation Loss: 0.1074\n",
      "           actual    predicted\n",
      "0      916.499996   771.695087\n",
      "1      587.041658   629.744427\n",
      "2      619.250003   597.818290\n",
      "3      698.499999   637.564762\n",
      "4      615.999997   608.559397\n",
      "...           ...          ...\n",
      "4091   654.250002   736.070858\n",
      "4092   762.250001   753.825900\n",
      "4093   975.000000  1064.661591\n",
      "4094  1049.750006  1002.950955\n",
      "4095  1081.361119   924.973953\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.1184\n",
      "Epoch 10/25, Validation Loss: 0.1041\n",
      "           actual    predicted\n",
      "0      916.499996   774.015620\n",
      "1      587.041658   625.588732\n",
      "2      619.250003   604.594985\n",
      "3      698.499999   640.894401\n",
      "4      615.999997   610.321456\n",
      "...           ...          ...\n",
      "4091   654.250002   738.975382\n",
      "4092   762.250001   751.223605\n",
      "4093   975.000000  1064.631224\n",
      "4094  1049.750006  1024.105792\n",
      "4095  1081.361119   926.953301\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.1154\n",
      "Epoch 11/25, Validation Loss: 0.1015\n",
      "           actual    predicted\n",
      "0      916.499996   785.406945\n",
      "1      587.041658   627.635553\n",
      "2      619.250003   609.576493\n",
      "3      698.499999   642.776344\n",
      "4      615.999997   611.607922\n",
      "...           ...          ...\n",
      "4091   654.250002   748.138064\n",
      "4092   762.250001   755.514667\n",
      "4093   975.000000  1086.343043\n",
      "4094  1049.750006  1039.320640\n",
      "4095  1081.361119   956.195994\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1131\n",
      "Epoch 12/25, Validation Loss: 0.1001\n",
      "           actual    predicted\n",
      "0      916.499996   785.510811\n",
      "1      587.041658   621.169209\n",
      "2      619.250003   600.362802\n",
      "3      698.499999   646.976337\n",
      "4      615.999997   606.288877\n",
      "...           ...          ...\n",
      "4091   654.250002   731.672463\n",
      "4092   762.250001   752.964648\n",
      "4093   975.000000  1064.194127\n",
      "4094  1049.750006  1021.795212\n",
      "4095  1081.361119   928.144733\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1093\n",
      "Epoch 13/25, Validation Loss: 0.0990\n",
      "           actual    predicted\n",
      "0      916.499996   795.027215\n",
      "1      587.041658   622.556329\n",
      "2      619.250003   604.270251\n",
      "3      698.499999   653.621622\n",
      "4      615.999997   611.869249\n",
      "...           ...          ...\n",
      "4091   654.250002   746.709344\n",
      "4092   762.250001   757.695141\n",
      "4093   975.000000  1061.355334\n",
      "4094  1049.750006  1038.393392\n",
      "4095  1081.361119   933.988951\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.1061\n",
      "Epoch 14/25, Validation Loss: 0.0967\n",
      "           actual    predicted\n",
      "0      916.499996   808.819699\n",
      "1      587.041658   619.617681\n",
      "2      619.250003   603.223344\n",
      "3      698.499999   650.714209\n",
      "4      615.999997   610.878229\n",
      "...           ...          ...\n",
      "4091   654.250002   739.823292\n",
      "4092   762.250001   760.461977\n",
      "4093   975.000000  1056.609009\n",
      "4094  1049.750006  1042.724770\n",
      "4095  1081.361119   934.985487\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.1041\n",
      "Epoch 15/25, Validation Loss: 0.0951\n",
      "           actual    predicted\n",
      "0      916.499996   800.796190\n",
      "1      587.041658   619.271249\n",
      "2      619.250003   610.490468\n",
      "3      698.499999   654.272629\n",
      "4      615.999997   612.679983\n",
      "...           ...          ...\n",
      "4091   654.250002   726.295917\n",
      "4092   762.250001   746.236999\n",
      "4093   975.000000  1046.373654\n",
      "4094  1049.750006  1019.771861\n",
      "4095  1081.361119   921.790352\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.1028\n",
      "Epoch 16/25, Validation Loss: 0.0946\n",
      "           actual    predicted\n",
      "0      916.499996   809.112016\n",
      "1      587.041658   615.444702\n",
      "2      619.250003   609.064800\n",
      "3      698.499999   650.093048\n",
      "4      615.999997   609.715886\n",
      "...           ...          ...\n",
      "4091   654.250002   734.378630\n",
      "4092   762.250001   756.134584\n",
      "4093   975.000000  1037.997258\n",
      "4094  1049.750006  1030.160437\n",
      "4095  1081.361119   930.910442\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.1005\n",
      "Epoch 17/25, Validation Loss: 0.0935\n",
      "           actual    predicted\n",
      "0      916.499996   815.169426\n",
      "1      587.041658   619.757422\n",
      "2      619.250003   609.752124\n",
      "3      698.499999   652.157770\n",
      "4      615.999997   606.842852\n",
      "...           ...          ...\n",
      "4091   654.250002   729.922578\n",
      "4092   762.250001   750.758092\n",
      "4093   975.000000  1060.009144\n",
      "4094  1049.750006  1042.354242\n",
      "4095  1081.361119   948.550091\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0989\n",
      "Epoch 18/25, Validation Loss: 0.0940\n",
      "           actual    predicted\n",
      "0      916.499996   810.076224\n",
      "1      587.041658   621.565847\n",
      "2      619.250003   608.153604\n",
      "3      698.499999   653.611902\n",
      "4      615.999997   605.775430\n",
      "...           ...          ...\n",
      "4091   654.250002   721.323395\n",
      "4092   762.250001   746.344480\n",
      "4093   975.000000  1038.838984\n",
      "4094  1049.750006  1021.398749\n",
      "4095  1081.361119   925.351872\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0975\n",
      "Epoch 19/25, Validation Loss: 0.0929\n",
      "           actual    predicted\n",
      "0      916.499996   816.025653\n",
      "1      587.041658   615.640901\n",
      "2      619.250003   610.494985\n",
      "3      698.499999   654.808954\n",
      "4      615.999997   607.827028\n",
      "...           ...          ...\n",
      "4091   654.250002   720.173495\n",
      "4092   762.250001   746.845627\n",
      "4093   975.000000  1019.434931\n",
      "4094  1049.750006  1022.785677\n",
      "4095  1081.361119   924.598057\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0965\n",
      "Epoch 20/25, Validation Loss: 0.0923\n",
      "           actual    predicted\n",
      "0      916.499996   840.801892\n",
      "1      587.041658   614.843057\n",
      "2      619.250003   599.340512\n",
      "3      698.499999   655.322306\n",
      "4      615.999997   598.214563\n",
      "...           ...          ...\n",
      "4091   654.250002   735.620406\n",
      "4092   762.250001   758.991668\n",
      "4093   975.000000  1069.227716\n",
      "4094  1049.750006  1058.316816\n",
      "4095  1081.361119   953.046346\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0959\n",
      "Epoch 21/25, Validation Loss: 0.0922\n",
      "           actual    predicted\n",
      "0      916.499996   831.861072\n",
      "1      587.041658   618.542841\n",
      "2      619.250003   608.453028\n",
      "3      698.499999   655.203672\n",
      "4      615.999997   606.325115\n",
      "...           ...          ...\n",
      "4091   654.250002   725.525278\n",
      "4092   762.250001   751.483994\n",
      "4093   975.000000  1044.481061\n",
      "4094  1049.750006  1031.289774\n",
      "4095  1081.361119   926.355800\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0952\n",
      "Epoch 22/25, Validation Loss: 0.0914\n",
      "           actual    predicted\n",
      "0      916.499996   839.563004\n",
      "1      587.041658   620.737202\n",
      "2      619.250003   615.372173\n",
      "3      698.499999   663.996909\n",
      "4      615.999997   614.431619\n",
      "...           ...          ...\n",
      "4091   654.250002   727.416094\n",
      "4092   762.250001   753.482677\n",
      "4093   975.000000  1043.230712\n",
      "4094  1049.750006  1039.722784\n",
      "4095  1081.361119   942.922067\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0937\n",
      "Epoch 23/25, Validation Loss: 0.0910\n",
      "           actual    predicted\n",
      "0      916.499996   824.897582\n",
      "1      587.041658   609.179473\n",
      "2      619.250003   610.593119\n",
      "3      698.499999   656.424064\n",
      "4      615.999997   604.526331\n",
      "...           ...          ...\n",
      "4091   654.250002   712.635932\n",
      "4092   762.250001   743.086852\n",
      "4093   975.000000  1031.324014\n",
      "4094  1049.750006  1047.421411\n",
      "4095  1081.361119   932.271929\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0932\n",
      "Epoch 24/25, Validation Loss: 0.0911\n",
      "           actual    predicted\n",
      "0      916.499996   841.697510\n",
      "1      587.041658   616.840003\n",
      "2      619.250003   607.565528\n",
      "3      698.499999   654.245624\n",
      "4      615.999997   604.249317\n",
      "...           ...          ...\n",
      "4091   654.250002   724.358096\n",
      "4092   762.250001   747.269599\n",
      "4093   975.000000  1009.069859\n",
      "4094  1049.750006  1021.756334\n",
      "4095  1081.361119   929.657739\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0923\n",
      "Epoch 25/25, Validation Loss: 0.0916\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.4302]], device='cuda:0')\n",
      "[665.67]\n",
      "            actual    predicted\n",
      "0       916.499996   841.697510\n",
      "1       587.041658   616.840003\n",
      "2       619.250003   607.565528\n",
      "3       698.499999   654.245624\n",
      "4       615.999997   604.249317\n",
      "...            ...          ...\n",
      "19155  1763.499987  1655.849189\n",
      "19156   540.750008   564.363418\n",
      "19157   588.250002   600.674959\n",
      "19158   842.000000   798.352271\n",
      "19159   609.000003   661.069962\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 87.8695\n",
      "Score (MAE): 41.3346\n",
      "Score (ME): 8.2683\n",
      "Score (MAPE): 4.7821%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       actual  predicted\n",
      "0     44.1400  41.331316\n",
      "1     34.9300  31.946779\n",
      "2     44.3925  39.095021\n",
      "3     41.5500  44.812517\n",
      "4     27.9475  30.061359\n",
      "...       ...        ...\n",
      "4091  41.8425  34.933179\n",
      "4092  26.0300  31.344147\n",
      "4093  27.8700  31.530480\n",
      "4094  43.0450  35.907463\n",
      "4095  39.5425  30.886384\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.7650\n",
      "Epoch 1/25, Validation Loss: 0.3183\n",
      "       actual  predicted\n",
      "0     44.1400  44.365680\n",
      "1     34.9300  33.738823\n",
      "2     44.3925  47.412698\n",
      "3     41.5500  37.874104\n",
      "4     27.9475  25.000972\n",
      "...       ...        ...\n",
      "4091  41.8425  42.998129\n",
      "4092  26.0300  26.715606\n",
      "4093  27.8700  28.104729\n",
      "4094  43.0450  42.628159\n",
      "4095  39.5425  31.148584\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2155\n",
      "Epoch 2/25, Validation Loss: 0.1434\n",
      "       actual  predicted\n",
      "0     44.1400  44.261567\n",
      "1     34.9300  34.335671\n",
      "2     44.3925  45.066007\n",
      "3     41.5500  40.044749\n",
      "4     27.9475  27.660601\n",
      "...       ...        ...\n",
      "4091  41.8425  42.258629\n",
      "4092  26.0300  26.821610\n",
      "4093  27.8700  29.230217\n",
      "4094  43.0450  42.427348\n",
      "4095  39.5425  31.781316\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1218\n",
      "Epoch 3/25, Validation Loss: 0.1007\n",
      "       actual  predicted\n",
      "0     44.1400  44.968914\n",
      "1     34.9300  34.154144\n",
      "2     44.3925  45.343045\n",
      "3     41.5500  39.712737\n",
      "4     27.9475  27.894258\n",
      "...       ...        ...\n",
      "4091  41.8425  41.893450\n",
      "4092  26.0300  26.697207\n",
      "4093  27.8700  28.489755\n",
      "4094  43.0450  42.787546\n",
      "4095  39.5425  32.228692\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0968\n",
      "Epoch 4/25, Validation Loss: 0.0853\n",
      "       actual  predicted\n",
      "0     44.1400  44.093493\n",
      "1     34.9300  34.055104\n",
      "2     44.3925  45.523237\n",
      "3     41.5500  39.654927\n",
      "4     27.9475  29.035139\n",
      "...       ...        ...\n",
      "4091  41.8425  40.252492\n",
      "4092  26.0300  26.832705\n",
      "4093  27.8700  27.975137\n",
      "4094  43.0450  43.224703\n",
      "4095  39.5425  33.885636\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0829\n",
      "Epoch 5/25, Validation Loss: 0.0720\n",
      "       actual  predicted\n",
      "0     44.1400  43.573561\n",
      "1     34.9300  34.255082\n",
      "2     44.3925  46.135885\n",
      "3     41.5500  39.498416\n",
      "4     27.9475  28.610619\n",
      "...       ...        ...\n",
      "4091  41.8425  39.668227\n",
      "4092  26.0300  26.069809\n",
      "4093  27.8700  27.399661\n",
      "4094  43.0450  43.609317\n",
      "4095  39.5425  34.809874\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0709\n",
      "Epoch 6/25, Validation Loss: 0.0605\n",
      "       actual  predicted\n",
      "0     44.1400  43.152981\n",
      "1     34.9300  34.892257\n",
      "2     44.3925  46.889598\n",
      "3     41.5500  39.363132\n",
      "4     27.9475  28.883698\n",
      "...       ...        ...\n",
      "4091  41.8425  39.808005\n",
      "4092  26.0300  26.171823\n",
      "4093  27.8700  27.348971\n",
      "4094  43.0450  43.837061\n",
      "4095  39.5425  35.638034\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0618\n",
      "Epoch 7/25, Validation Loss: 0.0518\n",
      "       actual  predicted\n",
      "0     44.1400  42.360244\n",
      "1     34.9300  34.863545\n",
      "2     44.3925  45.997257\n",
      "3     41.5500  39.355030\n",
      "4     27.9475  28.802089\n",
      "...       ...        ...\n",
      "4091  41.8425  39.722932\n",
      "4092  26.0300  26.090941\n",
      "4093  27.8700  27.193340\n",
      "4094  43.0450  43.285115\n",
      "4095  39.5425  35.862456\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0537\n",
      "Epoch 8/25, Validation Loss: 0.0452\n",
      "       actual  predicted\n",
      "0     44.1400  42.673859\n",
      "1     34.9300  35.199622\n",
      "2     44.3925  45.726499\n",
      "3     41.5500  39.961060\n",
      "4     27.9475  28.583223\n",
      "...       ...        ...\n",
      "4091  41.8425  40.167300\n",
      "4092  26.0300  26.418556\n",
      "4093  27.8700  27.394858\n",
      "4094  43.0450  43.481436\n",
      "4095  39.5425  35.916147\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0483\n",
      "Epoch 9/25, Validation Loss: 0.0385\n",
      "       actual  predicted\n",
      "0     44.1400  42.660338\n",
      "1     34.9300  35.255301\n",
      "2     44.3925  45.011565\n",
      "3     41.5500  40.322812\n",
      "4     27.9475  28.162999\n",
      "...       ...        ...\n",
      "4091  41.8425  40.078275\n",
      "4092  26.0300  26.011503\n",
      "4093  27.8700  27.208372\n",
      "4094  43.0450  43.268962\n",
      "4095  39.5425  35.991300\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0434\n",
      "Epoch 10/25, Validation Loss: 0.0336\n",
      "       actual  predicted\n",
      "0     44.1400  43.254348\n",
      "1     34.9300  35.445065\n",
      "2     44.3925  45.043064\n",
      "3     41.5500  40.793170\n",
      "4     27.9475  27.875338\n",
      "...       ...        ...\n",
      "4091  41.8425  40.468601\n",
      "4092  26.0300  25.783319\n",
      "4093  27.8700  27.080257\n",
      "4094  43.0450  43.322373\n",
      "4095  39.5425  36.507357\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0399\n",
      "Epoch 11/25, Validation Loss: 0.0304\n",
      "       actual  predicted\n",
      "0     44.1400  43.368386\n",
      "1     34.9300  35.383886\n",
      "2     44.3925  44.768134\n",
      "3     41.5500  40.920922\n",
      "4     27.9475  27.807765\n",
      "...       ...        ...\n",
      "4091  41.8425  40.607309\n",
      "4092  26.0300  25.945776\n",
      "4093  27.8700  27.150655\n",
      "4094  43.0450  43.208411\n",
      "4095  39.5425  36.824032\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0367\n",
      "Epoch 12/25, Validation Loss: 0.0268\n",
      "       actual  predicted\n",
      "0     44.1400  43.198208\n",
      "1     34.9300  35.207617\n",
      "2     44.3925  44.324409\n",
      "3     41.5500  40.778512\n",
      "4     27.9475  27.734883\n",
      "...       ...        ...\n",
      "4091  41.8425  40.572652\n",
      "4092  26.0300  25.915714\n",
      "4093  27.8700  27.180996\n",
      "4094  43.0450  43.192723\n",
      "4095  39.5425  37.051871\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0338\n",
      "Epoch 13/25, Validation Loss: 0.0244\n",
      "       actual  predicted\n",
      "0     44.1400  43.577715\n",
      "1     34.9300  35.166955\n",
      "2     44.3925  44.942479\n",
      "3     41.5500  41.057426\n",
      "4     27.9475  27.962488\n",
      "...       ...        ...\n",
      "4091  41.8425  40.682417\n",
      "4092  26.0300  26.180815\n",
      "4093  27.8700  27.095293\n",
      "4094  43.0450  43.536131\n",
      "4095  39.5425  37.162015\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0316\n",
      "Epoch 14/25, Validation Loss: 0.0227\n",
      "       actual  predicted\n",
      "0     44.1400  43.398152\n",
      "1     34.9300  35.301660\n",
      "2     44.3925  44.942019\n",
      "3     41.5500  41.073672\n",
      "4     27.9475  27.813818\n",
      "...       ...        ...\n",
      "4091  41.8425  40.751675\n",
      "4092  26.0300  25.987584\n",
      "4093  27.8700  27.049638\n",
      "4094  43.0450  43.366086\n",
      "4095  39.5425  37.715592\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0297\n",
      "Epoch 15/25, Validation Loss: 0.0208\n",
      "       actual  predicted\n",
      "0     44.1400  43.378499\n",
      "1     34.9300  35.125303\n",
      "2     44.3925  44.524177\n",
      "3     41.5500  40.799025\n",
      "4     27.9475  27.702435\n",
      "...       ...        ...\n",
      "4091  41.8425  40.848219\n",
      "4092  26.0300  25.816505\n",
      "4093  27.8700  27.129579\n",
      "4094  43.0450  43.294666\n",
      "4095  39.5425  37.703575\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0282\n",
      "Epoch 16/25, Validation Loss: 0.0193\n",
      "       actual  predicted\n",
      "0     44.1400  43.600213\n",
      "1     34.9300  35.097980\n",
      "2     44.3925  44.655898\n",
      "3     41.5500  40.714103\n",
      "4     27.9475  28.009874\n",
      "...       ...        ...\n",
      "4091  41.8425  40.964143\n",
      "4092  26.0300  25.964018\n",
      "4093  27.8700  27.337723\n",
      "4094  43.0450  43.595227\n",
      "4095  39.5425  37.501477\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0269\n",
      "Epoch 17/25, Validation Loss: 0.0182\n",
      "       actual  predicted\n",
      "0     44.1400  43.564979\n",
      "1     34.9300  34.876881\n",
      "2     44.3925  44.564695\n",
      "3     41.5500  40.944383\n",
      "4     27.9475  27.821002\n",
      "...       ...        ...\n",
      "4091  41.8425  40.850776\n",
      "4092  26.0300  25.959776\n",
      "4093  27.8700  27.236452\n",
      "4094  43.0450  43.355903\n",
      "4095  39.5425  37.746458\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0257\n",
      "Epoch 18/25, Validation Loss: 0.0172\n",
      "       actual  predicted\n",
      "0     44.1400  43.768012\n",
      "1     34.9300  35.276137\n",
      "2     44.3925  44.563848\n",
      "3     41.5500  40.968135\n",
      "4     27.9475  28.247476\n",
      "...       ...        ...\n",
      "4091  41.8425  41.222085\n",
      "4092  26.0300  26.183715\n",
      "4093  27.8700  27.695231\n",
      "4094  43.0450  43.359368\n",
      "4095  39.5425  38.180957\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0246\n",
      "Epoch 19/25, Validation Loss: 0.0166\n",
      "       actual  predicted\n",
      "0     44.1400  43.395016\n",
      "1     34.9300  35.091552\n",
      "2     44.3925  44.579885\n",
      "3     41.5500  40.994381\n",
      "4     27.9475  27.820724\n",
      "...       ...        ...\n",
      "4091  41.8425  40.853809\n",
      "4092  26.0300  26.031502\n",
      "4093  27.8700  27.324830\n",
      "4094  43.0450  43.198920\n",
      "4095  39.5425  38.288391\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0238\n",
      "Epoch 20/25, Validation Loss: 0.0156\n",
      "       actual  predicted\n",
      "0     44.1400  43.683198\n",
      "1     34.9300  35.097289\n",
      "2     44.3925  44.901150\n",
      "3     41.5500  40.997288\n",
      "4     27.9475  27.821936\n",
      "...       ...        ...\n",
      "4091  41.8425  41.096329\n",
      "4092  26.0300  26.068190\n",
      "4093  27.8700  27.198115\n",
      "4094  43.0450  43.372468\n",
      "4095  39.5425  38.626889\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0229\n",
      "Epoch 21/25, Validation Loss: 0.0158\n",
      "       actual  predicted\n",
      "0     44.1400  43.552425\n",
      "1     34.9300  35.047852\n",
      "2     44.3925  44.393268\n",
      "3     41.5500  40.859271\n",
      "4     27.9475  28.162081\n",
      "...       ...        ...\n",
      "4091  41.8425  40.988357\n",
      "4092  26.0300  26.009093\n",
      "4093  27.8700  27.467812\n",
      "4094  43.0450  43.176296\n",
      "4095  39.5425  38.624005\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0220\n",
      "Epoch 22/25, Validation Loss: 0.0146\n",
      "       actual  predicted\n",
      "0     44.1400  43.449198\n",
      "1     34.9300  34.796995\n",
      "2     44.3925  44.392836\n",
      "3     41.5500  40.603536\n",
      "4     27.9475  27.891266\n",
      "...       ...        ...\n",
      "4091  41.8425  40.774683\n",
      "4092  26.0300  26.061148\n",
      "4093  27.8700  27.341892\n",
      "4094  43.0450  43.112446\n",
      "4095  39.5425  38.820985\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0213\n",
      "Epoch 23/25, Validation Loss: 0.0140\n",
      "       actual  predicted\n",
      "0     44.1400  43.723121\n",
      "1     34.9300  35.122580\n",
      "2     44.3925  44.491176\n",
      "3     41.5500  40.781423\n",
      "4     27.9475  27.826028\n",
      "...       ...        ...\n",
      "4091  41.8425  41.053031\n",
      "4092  26.0300  26.003388\n",
      "4093  27.8700  27.383691\n",
      "4094  43.0450  43.115637\n",
      "4095  39.5425  38.883554\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0207\n",
      "Epoch 24/25, Validation Loss: 0.0137\n",
      "       actual  predicted\n",
      "0     44.1400  43.682939\n",
      "1     34.9300  35.170497\n",
      "2     44.3925  44.438832\n",
      "3     41.5500  40.757268\n",
      "4     27.9475  28.238269\n",
      "...       ...        ...\n",
      "4091  41.8425  41.180905\n",
      "4092  26.0300  26.077190\n",
      "4093  27.8700  27.678134\n",
      "4094  43.0450  43.111419\n",
      "4095  39.5425  39.095605\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0205\n",
      "Epoch 25/25, Validation Loss: 0.0134\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[1.1012]], device='cuda:0')\n",
      "[46.09]\n",
      "        actual  predicted\n",
      "0      44.1400  43.682939\n",
      "1      34.9300  35.170497\n",
      "2      44.3925  44.438832\n",
      "3      41.5500  40.757268\n",
      "4      27.9475  28.238269\n",
      "...        ...        ...\n",
      "19155  56.2900  55.548213\n",
      "19156  33.1950  33.287117\n",
      "19157  42.1700  42.112119\n",
      "19158  32.5500  31.604358\n",
      "19159  42.5050  41.625133\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 1.1101\n",
      "Score (MAE): 0.7057\n",
      "Score (ME): -0.0674\n",
      "Score (MAPE): 1.9561%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        actual  predicted\n",
      "0     19.07000  22.189606\n",
      "1     23.25500  25.741082\n",
      "2     24.08750  26.317904\n",
      "3     26.62000  25.889522\n",
      "4     24.74375  22.988254\n",
      "...        ...        ...\n",
      "4091  27.60250  26.256883\n",
      "4092  19.52750  23.037590\n",
      "4093  29.56875  27.378903\n",
      "4094  30.61250  27.295585\n",
      "4095  16.22250  22.166928\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8100\n",
      "Epoch 1/25, Validation Loss: 0.3699\n",
      "        actual  predicted\n",
      "0     19.07000  19.377250\n",
      "1     23.25500  23.959262\n",
      "2     24.08750  25.667661\n",
      "3     26.62000  29.088463\n",
      "4     24.74375  23.564086\n",
      "...        ...        ...\n",
      "4091  27.60250  28.156037\n",
      "4092  19.52750  20.727120\n",
      "4093  29.56875  30.444346\n",
      "4094  30.61250  31.527872\n",
      "4095  16.22250  18.836116\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2351\n",
      "Epoch 2/25, Validation Loss: 0.1531\n",
      "        actual  predicted\n",
      "0     19.07000  19.642599\n",
      "1     23.25500  23.780533\n",
      "2     24.08750  24.628258\n",
      "3     26.62000  28.002794\n",
      "4     24.74375  24.090299\n",
      "...        ...        ...\n",
      "4091  27.60250  27.559399\n",
      "4092  19.52750  20.080399\n",
      "4093  29.56875  29.074824\n",
      "4094  30.61250  31.350178\n",
      "4095  16.22250  15.293549\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1188\n",
      "Epoch 3/25, Validation Loss: 0.0978\n",
      "        actual  predicted\n",
      "0     19.07000  19.736840\n",
      "1     23.25500  23.680041\n",
      "2     24.08750  24.618624\n",
      "3     26.62000  28.173602\n",
      "4     24.74375  24.614882\n",
      "...        ...        ...\n",
      "4091  27.60250  27.934817\n",
      "4092  19.52750  19.620515\n",
      "4093  29.56875  29.310017\n",
      "4094  30.61250  31.295082\n",
      "4095  16.22250  15.189758\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0932\n",
      "Epoch 4/25, Validation Loss: 0.0881\n",
      "        actual  predicted\n",
      "0     19.07000  19.469925\n",
      "1     23.25500  23.585620\n",
      "2     24.08750  24.611772\n",
      "3     26.62000  28.276247\n",
      "4     24.74375  24.830400\n",
      "...        ...        ...\n",
      "4091  27.60250  27.990743\n",
      "4092  19.52750  19.520478\n",
      "4093  29.56875  29.461207\n",
      "4094  30.61250  31.532000\n",
      "4095  16.22250  15.454004\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0865\n",
      "Epoch 5/25, Validation Loss: 0.0824\n",
      "        actual  predicted\n",
      "0     19.07000  19.379852\n",
      "1     23.25500  23.606208\n",
      "2     24.08750  24.693524\n",
      "3     26.62000  28.168908\n",
      "4     24.74375  24.838660\n",
      "...        ...        ...\n",
      "4091  27.60250  27.913167\n",
      "4092  19.52750  19.698435\n",
      "4093  29.56875  29.402734\n",
      "4094  30.61250  31.520086\n",
      "4095  16.22250  15.603852\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0809\n",
      "Epoch 6/25, Validation Loss: 0.0766\n",
      "        actual  predicted\n",
      "0     19.07000  19.250462\n",
      "1     23.25500  23.478786\n",
      "2     24.08750  24.750227\n",
      "3     26.62000  28.095217\n",
      "4     24.74375  24.874306\n",
      "...        ...        ...\n",
      "4091  27.60250  28.002228\n",
      "4092  19.52750  19.633260\n",
      "4093  29.56875  29.502035\n",
      "4094  30.61250  31.834106\n",
      "4095  16.22250  15.648948\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0755\n",
      "Epoch 7/25, Validation Loss: 0.0720\n",
      "        actual  predicted\n",
      "0     19.07000  19.329573\n",
      "1     23.25500  23.356796\n",
      "2     24.08750  24.786805\n",
      "3     26.62000  27.760347\n",
      "4     24.74375  24.821516\n",
      "...        ...        ...\n",
      "4091  27.60250  27.744091\n",
      "4092  19.52750  19.659212\n",
      "4093  29.56875  29.221254\n",
      "4094  30.61250  31.394806\n",
      "4095  16.22250  15.839966\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0703\n",
      "Epoch 8/25, Validation Loss: 0.0664\n",
      "        actual  predicted\n",
      "0     19.07000  19.423859\n",
      "1     23.25500  23.402568\n",
      "2     24.08750  24.734516\n",
      "3     26.62000  27.591573\n",
      "4     24.74375  24.933974\n",
      "...        ...        ...\n",
      "4091  27.60250  27.824774\n",
      "4092  19.52750  19.605778\n",
      "4093  29.56875  29.249950\n",
      "4094  30.61250  31.362796\n",
      "4095  16.22250  15.973707\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0658\n",
      "Epoch 9/25, Validation Loss: 0.0618\n",
      "        actual  predicted\n",
      "0     19.07000  19.279449\n",
      "1     23.25500  23.468840\n",
      "2     24.08750  24.791227\n",
      "3     26.62000  27.387847\n",
      "4     24.74375  24.838914\n",
      "...        ...        ...\n",
      "4091  27.60250  27.773314\n",
      "4092  19.52750  19.453371\n",
      "4093  29.56875  29.361237\n",
      "4094  30.61250  30.991896\n",
      "4095  16.22250  16.066141\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0612\n",
      "Epoch 10/25, Validation Loss: 0.0565\n",
      "        actual  predicted\n",
      "0     19.07000  19.228942\n",
      "1     23.25500  23.361868\n",
      "2     24.08750  24.955587\n",
      "3     26.62000  27.340555\n",
      "4     24.74375  24.729068\n",
      "...        ...        ...\n",
      "4091  27.60250  27.852777\n",
      "4092  19.52750  19.447420\n",
      "4093  29.56875  29.372240\n",
      "4094  30.61250  31.066982\n",
      "4095  16.22250  15.884388\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0562\n",
      "Epoch 11/25, Validation Loss: 0.0510\n",
      "        actual  predicted\n",
      "0     19.07000  19.293504\n",
      "1     23.25500  23.276802\n",
      "2     24.08750  24.814815\n",
      "3     26.62000  27.233425\n",
      "4     24.74375  24.666634\n",
      "...        ...        ...\n",
      "4091  27.60250  27.866482\n",
      "4092  19.52750  19.287920\n",
      "4093  29.56875  29.382498\n",
      "4094  30.61250  31.124739\n",
      "4095  16.22250  16.040064\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0519\n",
      "Epoch 12/25, Validation Loss: 0.0456\n",
      "        actual  predicted\n",
      "0     19.07000  19.391333\n",
      "1     23.25500  23.274469\n",
      "2     24.08750  24.839495\n",
      "3     26.62000  27.176839\n",
      "4     24.74375  24.620031\n",
      "...        ...        ...\n",
      "4091  27.60250  27.749111\n",
      "4092  19.52750  19.489282\n",
      "4093  29.56875  29.438982\n",
      "4094  30.61250  30.887631\n",
      "4095  16.22250  16.194970\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0457\n",
      "Epoch 13/25, Validation Loss: 0.0395\n",
      "        actual  predicted\n",
      "0     19.07000  19.360666\n",
      "1     23.25500  23.124106\n",
      "2     24.08750  24.587560\n",
      "3     26.62000  27.042310\n",
      "4     24.74375  24.551529\n",
      "...        ...        ...\n",
      "4091  27.60250  27.623072\n",
      "4092  19.52750  19.246455\n",
      "4093  29.56875  29.198448\n",
      "4094  30.61250  30.677381\n",
      "4095  16.22250  16.147100\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0411\n",
      "Epoch 14/25, Validation Loss: 0.0343\n",
      "        actual  predicted\n",
      "0     19.07000  19.217151\n",
      "1     23.25500  23.213528\n",
      "2     24.08750  24.524167\n",
      "3     26.62000  26.926353\n",
      "4     24.74375  24.629345\n",
      "...        ...        ...\n",
      "4091  27.60250  27.633204\n",
      "4092  19.52750  19.515375\n",
      "4093  29.56875  29.274396\n",
      "4094  30.61250  30.740276\n",
      "4095  16.22250  16.121213\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0371\n",
      "Epoch 15/25, Validation Loss: 0.0300\n",
      "        actual  predicted\n",
      "0     19.07000  19.220145\n",
      "1     23.25500  23.164040\n",
      "2     24.08750  24.322487\n",
      "3     26.62000  26.923502\n",
      "4     24.74375  24.597045\n",
      "...        ...        ...\n",
      "4091  27.60250  27.520776\n",
      "4092  19.52750  19.612822\n",
      "4093  29.56875  29.191280\n",
      "4094  30.61250  30.675173\n",
      "4095  16.22250  16.142225\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0335\n",
      "Epoch 16/25, Validation Loss: 0.0272\n",
      "        actual  predicted\n",
      "0     19.07000  19.197818\n",
      "1     23.25500  23.198379\n",
      "2     24.08750  24.360412\n",
      "3     26.62000  27.115147\n",
      "4     24.74375  24.644831\n",
      "...        ...        ...\n",
      "4091  27.60250  27.872005\n",
      "4092  19.52750  19.652438\n",
      "4093  29.56875  29.536607\n",
      "4094  30.61250  31.143121\n",
      "4095  16.22250  16.238331\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0314\n",
      "Epoch 17/25, Validation Loss: 0.0278\n",
      "        actual  predicted\n",
      "0     19.07000  19.351287\n",
      "1     23.25500  23.169285\n",
      "2     24.08750  24.232447\n",
      "3     26.62000  26.777981\n",
      "4     24.74375  24.538998\n",
      "...        ...        ...\n",
      "4091  27.60250  27.508178\n",
      "4092  19.52750  19.702487\n",
      "4093  29.56875  29.168639\n",
      "4094  30.61250  30.612130\n",
      "4095  16.22250  16.356673\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0303\n",
      "Epoch 18/25, Validation Loss: 0.0231\n",
      "        actual  predicted\n",
      "0     19.07000  19.272956\n",
      "1     23.25500  23.184943\n",
      "2     24.08750  24.215758\n",
      "3     26.62000  26.779650\n",
      "4     24.74375  24.625294\n",
      "...        ...        ...\n",
      "4091  27.60250  27.534336\n",
      "4092  19.52750  19.690204\n",
      "4093  29.56875  29.162633\n",
      "4094  30.61250  30.593558\n",
      "4095  16.22250  16.141482\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0279\n",
      "Epoch 19/25, Validation Loss: 0.0218\n",
      "        actual  predicted\n",
      "0     19.07000  19.249232\n",
      "1     23.25500  23.242108\n",
      "2     24.08750  24.195963\n",
      "3     26.62000  26.848569\n",
      "4     24.74375  24.676148\n",
      "...        ...        ...\n",
      "4091  27.60250  27.698550\n",
      "4092  19.52750  19.671307\n",
      "4093  29.56875  29.278448\n",
      "4094  30.61250  30.801209\n",
      "4095  16.22250  16.276817\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0264\n",
      "Epoch 20/25, Validation Loss: 0.0211\n",
      "        actual  predicted\n",
      "0     19.07000  19.250897\n",
      "1     23.25500  23.198158\n",
      "2     24.08750  24.262037\n",
      "3     26.62000  26.757887\n",
      "4     24.74375  24.601813\n",
      "...        ...        ...\n",
      "4091  27.60250  27.564200\n",
      "4092  19.52750  19.762807\n",
      "4093  29.56875  29.105679\n",
      "4094  30.61250  30.469321\n",
      "4095  16.22250  16.306142\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0257\n",
      "Epoch 21/25, Validation Loss: 0.0198\n",
      "        actual  predicted\n",
      "0     19.07000  19.243466\n",
      "1     23.25500  23.187226\n",
      "2     24.08750  24.083880\n",
      "3     26.62000  26.802806\n",
      "4     24.74375  24.650488\n",
      "...        ...        ...\n",
      "4091  27.60250  27.726006\n",
      "4092  19.52750  19.647095\n",
      "4093  29.56875  29.402530\n",
      "4094  30.61250  30.985998\n",
      "4095  16.22250  16.225403\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0247\n",
      "Epoch 22/25, Validation Loss: 0.0196\n",
      "        actual  predicted\n",
      "0     19.07000  19.016029\n",
      "1     23.25500  23.177740\n",
      "2     24.08750  23.940666\n",
      "3     26.62000  26.665073\n",
      "4     24.74375  24.629083\n",
      "...        ...        ...\n",
      "4091  27.60250  27.531573\n",
      "4092  19.52750  19.577358\n",
      "4093  29.56875  29.178860\n",
      "4094  30.61250  30.551570\n",
      "4095  16.22250  16.111397\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0239\n",
      "Epoch 23/25, Validation Loss: 0.0183\n",
      "        actual  predicted\n",
      "0     19.07000  18.968276\n",
      "1     23.25500  23.201735\n",
      "2     24.08750  24.035585\n",
      "3     26.62000  26.724541\n",
      "4     24.74375  24.565530\n",
      "...        ...        ...\n",
      "4091  27.60250  27.598963\n",
      "4092  19.52750  19.574642\n",
      "4093  29.56875  29.326504\n",
      "4094  30.61250  30.799934\n",
      "4095  16.22250  16.045715\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0230\n",
      "Epoch 24/25, Validation Loss: 0.0179\n",
      "        actual  predicted\n",
      "0     19.07000  19.128012\n",
      "1     23.25500  23.248892\n",
      "2     24.08750  23.948955\n",
      "3     26.62000  26.536504\n",
      "4     24.74375  24.578075\n",
      "...        ...        ...\n",
      "4091  27.60250  27.533618\n",
      "4092  19.52750  19.713481\n",
      "4093  29.56875  29.112576\n",
      "4094  30.61250  30.648334\n",
      "4095  16.22250  16.196171\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0223\n",
      "Epoch 25/25, Validation Loss: 0.0171\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.2940]], device='cuda:0')\n",
      "[25.11]\n",
      "          actual  predicted\n",
      "0      19.070000  19.128012\n",
      "1      23.255000  23.248892\n",
      "2      24.087500  23.948955\n",
      "3      26.620000  26.536504\n",
      "4      24.743750  24.578075\n",
      "...          ...        ...\n",
      "19155  24.007500  24.095702\n",
      "19156  22.568750  22.261335\n",
      "19157  27.387500  27.162133\n",
      "19158  23.703333  24.055520\n",
      "19159  21.860000  21.794996\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 0.4695\n",
      "Score (MAE): 0.2802\n",
      "Score (ME): 0.0344\n",
      "Score (MAPE): 1.1409%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100645, 26) to (100793, 26)\n",
      "training data cutoff:  2023-07-13 23:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "snr                         float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([76484, 20, 24]) torch.Size([76484]) torch.Size([76484, 1])\n",
      "Testing data shape: torch.Size([19315, 20, 24]) torch.Size([19315]) torch.Size([19315, 1])\n",
      "Shuffled Training data shape: torch.Size([76639, 20, 24]) torch.Size([76639]) torch.Size([76639, 1])\n",
      "Shuffled Testing data shape: torch.Size([19160, 20, 24]) torch.Size([19160]) torch.Size([19160, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0       7.500007  206.393850\n",
      "1     203.461538  192.169627\n",
      "2       4.999994  456.866401\n",
      "3      11.249995   45.806929\n",
      "4      24.499993  116.804401\n",
      "...          ...         ...\n",
      "4091  266.384616  214.550772\n",
      "4092  910.500029  522.002813\n",
      "4093    8.499999  340.977483\n",
      "4094    4.000001  145.782121\n",
      "4095  379.043483  297.319932\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.9308\n",
      "Epoch 1/25, Validation Loss: 0.7272\n",
      "          actual   predicted\n",
      "0       7.500007  195.064551\n",
      "1     203.461538  286.057588\n",
      "2       4.999994  363.406123\n",
      "3      11.249995   35.270104\n",
      "4      24.499993  139.370906\n",
      "...          ...         ...\n",
      "4091  266.384616  229.226148\n",
      "4092  910.500029  976.970197\n",
      "4093    8.499999   93.114324\n",
      "4094    4.000001   -4.840214\n",
      "4095  379.043483  152.999933\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.8885\n",
      "Epoch 2/25, Validation Loss: 0.6999\n",
      "          actual   predicted\n",
      "0       7.500007  124.795465\n",
      "1     203.461538  411.274653\n",
      "2       4.999994  479.344101\n",
      "3      11.249995   59.541050\n",
      "4      24.499993  114.882829\n",
      "...          ...         ...\n",
      "4091  266.384616  170.150859\n",
      "4092  910.500029  520.690876\n",
      "4093    8.499999   36.115153\n",
      "4094    4.000001   55.161275\n",
      "4095  379.043483  272.799329\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.8461\n",
      "Epoch 3/25, Validation Loss: 0.6735\n",
      "          actual   predicted\n",
      "0       7.500007   93.886034\n",
      "1     203.461538  313.007151\n",
      "2       4.999994  479.499204\n",
      "3      11.249995   27.039011\n",
      "4      24.499993   82.474769\n",
      "...          ...         ...\n",
      "4091  266.384616  227.515711\n",
      "4092  910.500029  364.228166\n",
      "4093    8.499999   18.813863\n",
      "4094    4.000001   54.620740\n",
      "4095  379.043483  411.587768\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.8105\n",
      "Epoch 4/25, Validation Loss: 0.6998\n",
      "          actual   predicted\n",
      "0       7.500007  138.490746\n",
      "1     203.461538  293.955234\n",
      "2       4.999994  324.781153\n",
      "3      11.249995   53.271791\n",
      "4      24.499993  111.224424\n",
      "...          ...         ...\n",
      "4091  266.384616  218.737680\n",
      "4092  910.500029  636.204323\n",
      "4093    8.499999   92.657380\n",
      "4094    4.000001   49.513359\n",
      "4095  379.043483  235.091891\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.8128\n",
      "Epoch 5/25, Validation Loss: 0.6484\n",
      "          actual   predicted\n",
      "0       7.500007   57.415559\n",
      "1     203.461538  246.495346\n",
      "2       4.999994  410.336733\n",
      "3      11.249995   20.045118\n",
      "4      24.499993   80.414711\n",
      "...          ...         ...\n",
      "4091  266.384616  263.487949\n",
      "4092  910.500029  592.726492\n",
      "4093    8.499999   15.480296\n",
      "4094    4.000001   34.945338\n",
      "4095  379.043483  357.865166\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.7616\n",
      "Epoch 6/25, Validation Loss: 0.6444\n",
      "          actual   predicted\n",
      "0       7.500007   19.901778\n",
      "1     203.461538  212.049976\n",
      "2       4.999994  476.847756\n",
      "3      11.249995   -0.657772\n",
      "4      24.499993   73.361623\n",
      "...          ...         ...\n",
      "4091  266.384616  245.606925\n",
      "4092  910.500029  384.798216\n",
      "4093    8.499999  -39.452233\n",
      "4094    4.000001    5.534220\n",
      "4095  379.043483  386.021645\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.7329\n",
      "Epoch 7/25, Validation Loss: 0.6158\n",
      "          actual   predicted\n",
      "0       7.500007   75.628958\n",
      "1     203.461538  230.907900\n",
      "2       4.999994  393.527281\n",
      "3      11.249995  -81.026792\n",
      "4      24.499993   -3.325932\n",
      "...          ...         ...\n",
      "4091  266.384616  200.919575\n",
      "4092  910.500029  423.738229\n",
      "4093    8.499999 -164.936119\n",
      "4094    4.000001  -89.993682\n",
      "4095  379.043483  413.439575\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.8043\n",
      "Epoch 8/25, Validation Loss: 0.6432\n",
      "          actual   predicted\n",
      "0       7.500007   52.316953\n",
      "1     203.461538  180.281595\n",
      "2       4.999994  344.538012\n",
      "3      11.249995    8.141999\n",
      "4      24.499993   85.291177\n",
      "...          ...         ...\n",
      "4091  266.384616  224.002234\n",
      "4092  910.500029  281.508971\n",
      "4093    8.499999  -76.366062\n",
      "4094    4.000001  -22.801962\n",
      "4095  379.043483  292.839128\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.7646\n",
      "Epoch 9/25, Validation Loss: 0.6424\n",
      "          actual   predicted\n",
      "0       7.500007   28.628077\n",
      "1     203.461538  196.117063\n",
      "2       4.999994  439.983461\n",
      "3      11.249995   10.013775\n",
      "4      24.499993   87.917415\n",
      "...          ...         ...\n",
      "4091  266.384616  225.749890\n",
      "4092  910.500029  345.994538\n",
      "4093    8.499999 -108.860385\n",
      "4094    4.000001  -14.994136\n",
      "4095  379.043483  352.542887\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.7327\n",
      "Epoch 10/25, Validation Loss: 0.6535\n",
      "          actual   predicted\n",
      "0       7.500007   59.708832\n",
      "1     203.461538  296.422029\n",
      "2       4.999994  433.193794\n",
      "3      11.249995   49.199676\n",
      "4      24.499993  129.960033\n",
      "...          ...         ...\n",
      "4091  266.384616  296.246895\n",
      "4092  910.500029  464.002416\n",
      "4093    8.499999  -74.278928\n",
      "4094    4.000001    8.888214\n",
      "4095  379.043483  384.663778\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.7275\n",
      "Epoch 11/25, Validation Loss: 0.6066\n",
      "          actual   predicted\n",
      "0       7.500007   72.910639\n",
      "1     203.461538  235.565614\n",
      "2       4.999994  396.453534\n",
      "3      11.249995   61.426471\n",
      "4      24.499993  131.911883\n",
      "...          ...         ...\n",
      "4091  266.384616  287.163599\n",
      "4092  910.500029  476.915932\n",
      "4093    8.499999   -6.252770\n",
      "4094    4.000001   33.285218\n",
      "4095  379.043483  340.920090\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.6856\n",
      "Epoch 12/25, Validation Loss: 0.6395\n",
      "          actual   predicted\n",
      "0       7.500007  136.529971\n",
      "1     203.461538  292.783538\n",
      "2       4.999994  364.770188\n",
      "3      11.249995  101.863248\n",
      "4      24.499993  168.808660\n",
      "...          ...         ...\n",
      "4091  266.384616  348.752644\n",
      "4092  910.500029  419.686779\n",
      "4093    8.499999  -24.360277\n",
      "4094    4.000001   70.752911\n",
      "4095  379.043483  365.172442\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.6951\n",
      "Epoch 13/25, Validation Loss: 0.6319\n",
      "          actual   predicted\n",
      "0       7.500007   47.731087\n",
      "1     203.461538  290.436074\n",
      "2       4.999994  352.799288\n",
      "3      11.249995   48.541383\n",
      "4      24.499993  121.553663\n",
      "...          ...         ...\n",
      "4091  266.384616  325.490135\n",
      "4092  910.500029  458.983733\n",
      "4093    8.499999  -40.656673\n",
      "4094    4.000001   24.435786\n",
      "4095  379.043483  360.536567\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.6928\n",
      "Epoch 14/25, Validation Loss: 0.5651\n",
      "          actual   predicted\n",
      "0       7.500007    4.955281\n",
      "1     203.461538  176.691484\n",
      "2       4.999994  284.444600\n",
      "3      11.249995   35.893550\n",
      "4      24.499993  113.045219\n",
      "...          ...         ...\n",
      "4091  266.384616  254.509960\n",
      "4092  910.500029  314.882351\n",
      "4093    8.499999   -2.489105\n",
      "4094    4.000001    4.687670\n",
      "4095  379.043483  246.279341\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.6160\n",
      "Epoch 15/25, Validation Loss: 0.5269\n",
      "          actual   predicted\n",
      "0       7.500007   17.054753\n",
      "1     203.461538  211.254553\n",
      "2       4.999994  202.524009\n",
      "3      11.249995   54.045011\n",
      "4      24.499993  124.349533\n",
      "...          ...         ...\n",
      "4091  266.384616  295.434329\n",
      "4092  910.500029  411.851331\n",
      "4093    8.499999   46.172772\n",
      "4094    4.000001   10.174522\n",
      "4095  379.043483  257.579611\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.5898\n",
      "Epoch 16/25, Validation Loss: 0.5393\n",
      "          actual   predicted\n",
      "0       7.500007   16.425789\n",
      "1     203.461538  121.981752\n",
      "2       4.999994  208.514495\n",
      "3      11.249995   44.505364\n",
      "4      24.499993  119.804656\n",
      "...          ...         ...\n",
      "4091  266.384616  285.025634\n",
      "4092  910.500029  415.855223\n",
      "4093    8.499999   32.429593\n",
      "4094    4.000001   -7.268510\n",
      "4095  379.043483  236.414504\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.6515\n",
      "Epoch 17/25, Validation Loss: 0.5955\n",
      "          actual   predicted\n",
      "0       7.500007   -8.341516\n",
      "1     203.461538  194.996403\n",
      "2       4.999994  279.203477\n",
      "3      11.249995   59.467419\n",
      "4      24.499993  158.096384\n",
      "...          ...         ...\n",
      "4091  266.384616  344.836265\n",
      "4092  910.500029  554.900846\n",
      "4093    8.499999   26.038781\n",
      "4094    4.000001   -0.398493\n",
      "4095  379.043483  388.378397\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.5917\n",
      "Epoch 18/25, Validation Loss: 0.6005\n",
      "          actual   predicted\n",
      "0       7.500007   62.580175\n",
      "1     203.461538  185.748345\n",
      "2       4.999994  237.372702\n",
      "3      11.249995  119.191582\n",
      "4      24.499993  211.111902\n",
      "...          ...         ...\n",
      "4091  266.384616  366.579449\n",
      "4092  910.500029  508.002157\n",
      "4093    8.499999  109.837773\n",
      "4094    4.000001   39.343879\n",
      "4095  379.043483  308.045774\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.5910\n",
      "Epoch 19/25, Validation Loss: 0.5129\n",
      "          actual   predicted\n",
      "0       7.500007    4.920909\n",
      "1     203.461538  157.408637\n",
      "2       4.999994  195.087346\n",
      "3      11.249995   53.767977\n",
      "4      24.499993  166.427575\n",
      "...          ...         ...\n",
      "4091  266.384616  296.693158\n",
      "4092  910.500029  493.270226\n",
      "4093    8.499999   74.552932\n",
      "4094    4.000001  -26.016159\n",
      "4095  379.043483  309.648026\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.5824\n",
      "Epoch 20/25, Validation Loss: 0.5460\n",
      "          actual   predicted\n",
      "0       7.500007   -6.330511\n",
      "1     203.461538  179.025264\n",
      "2       4.999994  258.601332\n",
      "3      11.249995   37.151747\n",
      "4      24.499993  188.956051\n",
      "...          ...         ...\n",
      "4091  266.384616  405.474417\n",
      "4092  910.500029  480.145575\n",
      "4093    8.499999    6.978840\n",
      "4094    4.000001  -50.224890\n",
      "4095  379.043483  350.583045\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.5425\n",
      "Epoch 21/25, Validation Loss: 0.4766\n",
      "          actual   predicted\n",
      "0       7.500007   66.872247\n",
      "1     203.461538  181.480454\n",
      "2       4.999994  230.080394\n",
      "3      11.249995   96.332473\n",
      "4      24.499993  240.984706\n",
      "...          ...         ...\n",
      "4091  266.384616  396.463795\n",
      "4092  910.500029  528.862680\n",
      "4093    8.499999  121.932636\n",
      "4094    4.000001    7.980793\n",
      "4095  379.043483  333.021970\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.4894\n",
      "Epoch 22/25, Validation Loss: 0.4563\n",
      "          actual   predicted\n",
      "0       7.500007  -26.725663\n",
      "1     203.461538  140.416595\n",
      "2       4.999994  128.719948\n",
      "3      11.249995  -14.087601\n",
      "4      24.499993  149.301048\n",
      "...          ...         ...\n",
      "4091  266.384616  335.941566\n",
      "4092  910.500029  559.862705\n",
      "4093    8.499999    7.330026\n",
      "4094    4.000001  -95.496203\n",
      "4095  379.043483  274.604092\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.4568\n",
      "Epoch 23/25, Validation Loss: 0.4979\n",
      "          actual   predicted\n",
      "0       7.500007    4.704808\n",
      "1     203.461538  140.028262\n",
      "2       4.999994  168.651337\n",
      "3      11.249995    7.023157\n",
      "4      24.499993  182.112407\n",
      "...          ...         ...\n",
      "4091  266.384616  359.306213\n",
      "4092  910.500029  614.467700\n",
      "4093    8.499999   61.305583\n",
      "4094    4.000001  -59.560152\n",
      "4095  379.043483  341.607538\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.5080\n",
      "Epoch 24/25, Validation Loss: 0.5383\n",
      "          actual   predicted\n",
      "0       7.500007   74.210418\n",
      "1     203.461538  236.733674\n",
      "2       4.999994  293.905454\n",
      "3      11.249995   70.214621\n",
      "4      24.499993  242.132623\n",
      "...          ...         ...\n",
      "4091  266.384616  471.785277\n",
      "4092  910.500029  642.787218\n",
      "4093    8.499999  128.371797\n",
      "4094    4.000001   15.700791\n",
      "4095  379.043483  436.917284\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.4420\n",
      "Epoch 25/25, Validation Loss: 0.5216\n",
      "loading latest dataframe: data/quarter_hour_25f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3157e-01,  1.6505e+00,  1.1788e+00, -6.9057e-01, -1.0034e-01,\n",
      "          -8.3387e-02, -1.8855e+00, -5.9167e-01, -2.0578e+00, -4.7333e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0916e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.3911e-01,\n",
      "          -1.0397e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.3882e-01,  1.6221e+00,  1.1389e+00, -6.5352e-01, -1.1749e-01,\n",
      "          -8.7823e-02, -1.7440e+00, -5.9167e-01, -2.0964e+00, -4.7780e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0903e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -9.9869e-01,\n",
      "          -9.7392e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.4607e-01,  1.5938e+00,  1.0989e+00, -6.1646e-01, -1.3465e-01,\n",
      "          -9.2260e-02, -1.6024e+00, -5.9167e-01, -2.1350e+00, -4.8227e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0889e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.0583e+00,\n",
      "          -9.0815e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.5332e-01,  1.5654e+00,  1.0590e+00, -5.7941e-01, -1.5180e-01,\n",
      "          -9.6696e-02, -1.4609e+00, -5.9167e-01, -2.1736e+00, -4.8673e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0876e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1179e+00,\n",
      "          -8.4238e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.6058e-01,  1.5371e+00,  1.0191e+00, -5.4235e-01, -1.6895e-01,\n",
      "          -1.0113e-01, -1.3194e+00, -5.9167e-01, -2.2122e+00, -4.9120e+00,\n",
      "           4.1565e+00,  0.0000e+00, -9.0863e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.9752e+00,  1.4079e-01, -1.1774e+00,\n",
      "          -7.7661e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-6.7465e-01,  2.1841e+00,  1.2187e+00, -1.1702e+00,  2.6930e-01,\n",
      "           9.0312e-04, -6.1164e-01, -4.1315e-01,  1.6003e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.1036e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -7.7474e-01,\n",
      "          -1.1821e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [-1.9685e-01,  1.7023e+00,  3.5126e+00, -1.1433e+00,  4.5695e-02,\n",
      "          -5.3442e-02, -1.3547e+00, -4.8902e-01,  9.1505e-01,  1.0314e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0895e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -8.4527e-01,\n",
      "          -1.1319e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 8.7745e-02,  1.2606e+00,  2.1609e+00, -1.0410e+00,  2.5768e-03,\n",
      "          -6.1205e-02, -8.4755e-01, -3.5365e-01,  1.3590e+00,  7.2617e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0729e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -9.1407e-01,\n",
      "          -1.0761e+00, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 1.9560e-01,  1.1203e+00,  1.7936e+00, -1.0364e+00, -9.6754e-03,\n",
      "          -7.0078e-02, -6.1164e-01, -5.0241e-01,  1.6003e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0703e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1974e+00,\n",
      "          -7.4526e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.3186e-01,  9.3797e-01,  9.1531e-01, -1.0399e+00, -1.0618e-02,\n",
      "          -7.0078e-02, -2.0271e+00, -5.9167e-01, -1.7296e+00, -3.0575e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0689e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0330e+00,\n",
      "          -9.6193e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 2.8206e-01,  8.4002e-01,  8.6741e-01, -9.7467e-01, -3.3237e-02,\n",
      "          -7.4514e-02, -2.0271e+00, -5.0241e-01,  1.8416e+00,  6.0700e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0676e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.0937e+00,\n",
      "          -8.9167e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.2948e-01,  7.6188e-01,  8.3547e-01, -9.7123e-01, -3.8892e-02,\n",
      "          -7.8951e-02, -1.3194e+00, -3.2390e-01,  1.6486e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0663e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.1496e+00,\n",
      "          -8.1758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4901e-01,  8.1293e-01,  7.7159e-01, -9.2320e-01, -4.2662e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  1.1209e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0649e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3510e+00,\n",
      "          -4.0134e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.4622e-01,  8.1919e-01,  7.2369e-01, -7.7566e-01, -4.3604e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5038e+00,  9.6451e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0636e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.2464e+00,\n",
      "          -6.5921e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.6295e-01,  9.1887e-01,  5.9328e-01, -8.0997e-01, -8.4131e-02,\n",
      "          -9.2999e-02, -1.3194e+00, -4.1315e-01,  1.6003e+00,  9.7196e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0609e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3028e+00,\n",
      "          -5.3751e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.8666e-01,  1.0271e+00,  1.1349e+00, -5.8181e-01, -5.2558e-02,\n",
      "          -8.5605e-02, -1.6732e+00, -5.9167e-01,  1.6969e+00,  6.5169e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0576e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3381e+00,\n",
      "          -4.4273e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.2153e-01,  9.8173e-01,  6.7578e-01, -7.2763e-01, -4.4547e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.7934e+00,  4.5059e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0556e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3657e+00,\n",
      "          -3.4758e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1037e-01,  1.0026e+00,  6.8377e-01, -7.0704e-01, -4.6432e-02,\n",
      "          -8.5605e-02, -2.0271e+00, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0543e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3856e+00,\n",
      "          -2.5681e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 4.1316e-01,  1.1068e+00,  7.4764e-01, -6.4871e-01, -4.4547e-02,\n",
      "          -8.3387e-02, -6.1164e-01, -5.9167e-01,  1.6003e+00,  1.0539e+00,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0530e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.3995e+00,\n",
      "          -1.6494e-01, -1.1599e+00,  1.1599e+00,  0.0000e+00],\n",
      "         [ 3.9642e-01,  1.1537e+00,  7.7159e-01, -5.8695e-01, -4.4547e-02,\n",
      "          -8.1169e-02, -2.0271e+00, -5.9167e-01,  1.5521e+00,  8.9748e-01,\n",
      "          -8.0150e-01,  0.0000e+00, -9.0516e-01,  0.0000e+00, -4.4680e-01,\n",
      "          -4.1455e-02,  1.4122e+00, -1.7692e+00,  8.5768e-01, -1.4074e+00,\n",
      "          -7.2359e-02, -1.1599e+00,  1.1599e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.1913]], device='cuda:0')\n",
      "[1.32]\n",
      "           actual   predicted\n",
      "0        7.500007   74.210418\n",
      "1      203.461538  236.733674\n",
      "2        4.999994  293.905454\n",
      "3       11.249995   70.214621\n",
      "4       24.499993  242.132623\n",
      "...           ...         ...\n",
      "19155   35.666667  102.776440\n",
      "19156  703.650009  889.671634\n",
      "19157    6.000002   33.206829\n",
      "19158    4.000001   17.642846\n",
      "19159    8.999996  188.179020\n",
      "\n",
      "[19160 rows x 2 columns]\n",
      "Score (RMSE): 769.8993\n",
      "Score (MAE): 164.8195\n",
      "Score (ME): -31.9840\n",
      "Score (MAPE): 571.3633%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    }
   ],
   "source": [
    "# Reload the utils module if you have made changes\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Read existing model performance records\n",
    "performance_df = pd.read_csv('model_performances.csv')\n",
    "\n",
    "# Define LSTM-specific hyperparameters\n",
    "hidden_dim = 256\n",
    "num_layers = 5\n",
    "dropout = 0.3\n",
    "batch_size = 4096\n",
    "learning_rate = 0.00031\n",
    "epochs = 25\n",
    "aggregation_level = 'quarter_hour'\n",
    "y_feature = 'CO2'\n",
    "freq = 15 if aggregation_level == 'quarter_hour' else 30 if aggregation_level == 'half_hour' else 60\n",
    "window_size = 5 * 60 // freq\n",
    "\n",
    "# Loop over each feature to create and evaluate the LSTM model\n",
    "for aggregation_level in ['quarter_hour', 'half_hour', 'hour']:\n",
    "    for y_feature in ['CO2', 'VOC', 'hum', 'tmp', 'vis']:\n",
    "        model, scaler, rmse, mae, me, mape, train_loss, val_loss, n_features = utils.create_multivariate_lstm_model_for_feature(\n",
    "            df, \n",
    "            hidden_dim=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout, \n",
    "            batch_size=batch_size, \n",
    "            learning_rate=learning_rate, \n",
    "            epochs=epochs, \n",
    "            y_feature=y_feature, \n",
    "            aggregation_level=aggregation_level, \n",
    "            window_size=window_size\n",
    "        )\n",
    "        performance_df = performance_df.append({\n",
    "            'model_name': 'multivariate_lstm',\n",
    "            'aggregation_level': aggregation_level,\n",
    "            'y_feature': y_feature,\n",
    "            'n_features': n_features,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'me': me,\n",
    "            'mape': mape,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'num_layers': num_layers,\n",
    "            'dropout': dropout,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epochs': epochs,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'note': 'LSTM model with device_id embedding'\n",
    "        }, ignore_index=True)\n",
    "        performance_df.to_csv('model_performances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'please_test_this.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main abb8b35] Updating models with 25 features instead of 26. Adding back snr feature\n",
      " 76 files changed, 958052 insertions(+), 949959 deletions(-)\n",
      " delete mode 100644 data/half_hour_23f_20ws_dataframe_v1.parquet\n",
      " create mode 100644 data/half_hour_25f_20ws_dataframe_v1.parquet\n",
      " delete mode 100644 data/half_hour_26f_20ws_dataframe_v1.parquet\n",
      " delete mode 100644 data/hour_23f_20ws_dataframe_v1.parquet\n",
      " create mode 100644 data/hour_25f_20ws_dataframe_v1.parquet\n",
      " delete mode 100644 data/hour_26f_20ws_dataframe_v1.parquet\n",
      " create mode 100644 data/lstm_multivariate_quarter_hour_25f_20ws_CO2_predictions.csv\n",
      " create mode 100644 data/lstm_multivariate_quarter_hour_25f_20ws_VOC_predictions.csv\n",
      " create mode 100644 data/lstm_multivariate_quarter_hour_25f_20ws_hum_predictions.csv\n",
      " create mode 100644 data/lstm_multivariate_quarter_hour_25f_20ws_tmp_predictions.csv\n",
      " create mode 100644 data/lstm_multivariate_quarter_hour_25f_20ws_vis_predictions.csv\n",
      " delete mode 100644 data/lstm_multivariate_quarter_hour_26f_20ws_CO2_predictions.csv\n",
      " delete mode 100644 data/lstm_multivariate_quarter_hour_26f_20ws_VOC_predictions.csv\n",
      " delete mode 100644 data/lstm_multivariate_quarter_hour_26f_20ws_hum_predictions.csv\n",
      " delete mode 100644 data/lstm_multivariate_quarter_hour_26f_20ws_tmp_predictions.csv\n",
      " delete mode 100644 data/lstm_multivariate_quarter_hour_26f_20ws_vis_predictions.csv\n",
      " delete mode 100644 data/quarter_hour_23f_20ws_dataframe_v1.parquet\n",
      " rename data/{quarter_hour_26f_20ws_dataframe_v1.parquet => quarter_hour_25f_20ws_dataframe_v1.parquet} (56%)\n",
      " create mode 100644 data/transformer_multivariate_quarter_hour_25f_20ws_CO2_predictions.csv\n",
      " create mode 100644 data/transformer_multivariate_quarter_hour_25f_20ws_VOC_predictions.csv\n",
      " create mode 100644 data/transformer_multivariate_quarter_hour_25f_20ws_hum_predictions.csv\n",
      " create mode 100644 data/transformer_multivariate_quarter_hour_25f_20ws_tmp_predictions.csv\n",
      " create mode 100644 data/transformer_multivariate_quarter_hour_25f_20ws_vis_predictions.csv\n",
      " delete mode 100644 data/transformer_multivariate_quarter_hour_26f_20ws_CO2_predictions.csv\n",
      " delete mode 100644 data/transformer_multivariate_quarter_hour_26f_20ws_VOC_predictions.csv\n",
      " delete mode 100644 data/transformer_multivariate_quarter_hour_26f_20ws_hum_predictions.csv\n",
      " delete mode 100644 data/transformer_multivariate_quarter_hour_26f_20ws_tmp_predictions.csv\n",
      " delete mode 100644 data/transformer_multivariate_quarter_hour_26f_20ws_vis_predictions.csv\n",
      " create mode 100644 models/half_hour_25f_20ws_scaler_v1.pth\n",
      " delete mode 100644 models/half_hour_26f_20ws_scaler_v1.pth\n",
      " create mode 100644 models/hour_25f_20ws_scaler_v1.pth\n",
      " delete mode 100644 models/hour_26f_20ws_scaler_v1.pth\n",
      " rename models/{lstm_multivariate_quarter_hour_26f_20ws_CO2_model_v1.pth.gz => lstm_multivariate_half_hour_25f_20ws_CO2_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_hour_26f_20ws_hum_model_v1.pth.gz => lstm_multivariate_half_hour_25f_20ws_VOC_model_v1.pth.gz} (53%)\n",
      " create mode 100644 models/lstm_multivariate_half_hour_25f_20ws_hum_model_v1.pth.gz\n",
      " rename models/{lstm_multivariate_half_hour_26f_20ws_hum_model_v1.pth.gz => lstm_multivariate_half_hour_25f_20ws_tmp_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_hour_26f_20ws_VOC_model_v1.pth.gz => lstm_multivariate_half_hour_25f_20ws_vis_model_v1.pth.gz} (53%)\n",
      " delete mode 100644 models/lstm_multivariate_half_hour_26f_20ws_vis_model_v1.pth.gz\n",
      " rename models/{lstm_multivariate_quarter_hour_26f_20ws_vis_model_v1.pth.gz => lstm_multivariate_hour_25f_20ws_CO2_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_half_hour_26f_20ws_tmp_model_v1.pth.gz => lstm_multivariate_hour_25f_20ws_VOC_model_v1.pth.gz} (53%)\n",
      " create mode 100644 models/lstm_multivariate_hour_25f_20ws_hum_model_v1.pth.gz\n",
      " create mode 100644 models/lstm_multivariate_hour_25f_20ws_tmp_model_v1.pth.gz\n",
      " rename models/{lstm_multivariate_hour_26f_20ws_tmp_model_v1.pth.gz => lstm_multivariate_hour_25f_20ws_vis_model_v1.pth.gz} (53%)\n",
      " delete mode 100644 models/lstm_multivariate_hour_26f_20ws_vis_model_v1.pth.gz\n",
      " rename models/{lstm_multivariate_half_hour_26f_20ws_VOC_model_v1.pth.gz => lstm_multivariate_quarter_hour_25f_20ws_CO2_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_half_hour_26f_20ws_CO2_model_v1.pth.gz => lstm_multivariate_quarter_hour_25f_20ws_VOC_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_quarter_hour_26f_20ws_VOC_model_v1.pth.gz => lstm_multivariate_quarter_hour_25f_20ws_hum_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_quarter_hour_26f_20ws_tmp_model_v1.pth.gz => lstm_multivariate_quarter_hour_25f_20ws_tmp_model_v1.pth.gz} (53%)\n",
      " rename models/{lstm_multivariate_hour_26f_20ws_CO2_model_v1.pth.gz => lstm_multivariate_quarter_hour_25f_20ws_vis_model_v1.pth.gz} (53%)\n",
      " delete mode 100644 models/lstm_multivariate_quarter_hour_26f_20ws_hum_model_v1.pth.gz\n",
      " create mode 100644 models/quarter_hour_25f_20ws_scaler_v1.pth\n",
      " delete mode 100644 models/quarter_hour_26f_20ws_scaler_v1.pth\n",
      " delete mode 100644 models/quarter_hour_26f_40ws_scaler_v1.pth\n",
      " rename models/{transformer_multivariate_half_hour_26f_20ws_tmp_model_v1.pth.gz => transformer_multivariate_half_hour_25f_20ws_CO2_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_quarter_hour_26f_20ws_vis_model_v1.pth.gz => transformer_multivariate_half_hour_25f_20ws_VOC_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_quarter_hour_26f_20ws_CO2_model_v2.pth.gz => transformer_multivariate_half_hour_25f_20ws_hum_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_hour_26f_20ws_vis_model_v1.pth.gz => transformer_multivariate_half_hour_25f_20ws_tmp_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_half_hour_26f_20ws_vis_model_v1.pth.gz => transformer_multivariate_half_hour_25f_20ws_vis_model_v1.pth.gz} (73%)\n",
      " delete mode 100644 models/transformer_multivariate_half_hour_26f_20ws_CO2_model_v1.pth.gz\n",
      " delete mode 100644 models/transformer_multivariate_half_hour_26f_20ws_hum_model_v1.pth.gz\n",
      " rename models/{transformer_multivariate_hour_26f_20ws_VOC_model_v1.pth.gz => transformer_multivariate_hour_25f_20ws_CO2_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_hour_26f_20ws_hum_model_v1.pth.gz => transformer_multivariate_hour_25f_20ws_VOC_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_hour_26f_20ws_tmp_model_v1.pth.gz => transformer_multivariate_hour_25f_20ws_hum_model_v1.pth.gz} (73%)\n",
      " create mode 100644 models/transformer_multivariate_hour_25f_20ws_tmp_model_v1.pth.gz\n",
      " create mode 100644 models/transformer_multivariate_hour_25f_20ws_vis_model_v1.pth.gz\n",
      " delete mode 100644 models/transformer_multivariate_hour_26f_20ws_CO2_model_v1.pth.gz\n",
      " rename models/{transformer_multivariate_half_hour_26f_20ws_VOC_model_v1.pth.gz => transformer_multivariate_quarter_hour_25f_20ws_CO2_model_v1.pth.gz} (73%)\n",
      " create mode 100644 models/transformer_multivariate_quarter_hour_25f_20ws_VOC_model_v1.pth.gz\n",
      " rename models/{transformer_multivariate_quarter_hour_26f_20ws_VOC_model_v1.pth.gz => transformer_multivariate_quarter_hour_25f_20ws_hum_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_quarter_hour_26f_20ws_hum_model_v1.pth.gz => transformer_multivariate_quarter_hour_25f_20ws_tmp_model_v1.pth.gz} (73%)\n",
      " rename models/{transformer_multivariate_quarter_hour_26f_40ws_CO2_model_v1.pth.gz => transformer_multivariate_quarter_hour_25f_20ws_vis_model_v1.pth.gz} (73%)\n",
      " delete mode 100644 models/transformer_multivariate_quarter_hour_26f_20ws_tmp_model_v1.pth.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: RPC failed; curl 16 Error in the HTTP2 framing layer\n",
      "send-pack: unexpected disconnect while reading sideband packet\n",
      "fatal: the remote end hung up unexpectedly\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!git add -A\n",
    "!git commit -m \"Updating models with 25 features instead of 26. Adding back snr feature\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device_ids = torch.tensor([0])\n",
    "\n",
    "device = utils.get_device()\n",
    "\n",
    "model = utils.load_transformer_model(y_feature=\"CO2\", model_name = 'transformer_multivariate_quarter_hour_26f', device=device)\n",
    "scaler = utils.load_scaler(y_feature=\"CO2\", model_name = 'transformer_multivariate_quarter_hour_26f')\n",
    "\n",
    "real_data = utils.load_dataframe(model_name='transformer_multivariate_quarter_hour_26f')\n",
    "pd.set_option('display.max_columns', None)\n",
    "real_data.iloc[7:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = real_data.iloc[7:27].drop(columns=['device_id', 'date_time_rounded'])\n",
    "\n",
    "data_df_scaled = scaler.transform(data_df)\n",
    "\n",
    "input_data = torch.tensor(data_df_scaled, dtype=torch.float32).view(-1, 20, data_df_scaled.shape[1])\n",
    "print(input_data.shape)\n",
    "device_ids = torch.tensor([0])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_data.to(device), device_ids.to(device))\n",
    "    print(output)\n",
    "\n",
    "    prediction = output.cpu().numpy().reshape(-1, 1)\n",
    "    print(prediction)\n",
    "    print(prediction.shape)\n",
    "    zeroes_for_scaler = np.zeros((prediction.shape[0], 25))\n",
    "\n",
    "    zeroes_for_scaler[:, 2] = prediction  # Insert predicted values into the correct column\n",
    "    print(zeroes_for_scaler)\n",
    "    inverse_transformed = scaler.inverse_transform(zeroes_for_scaler)\n",
    "    predicted_unscaled = inverse_transformed[:, 2].round(2)\n",
    "    print(predicted_unscaled)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
