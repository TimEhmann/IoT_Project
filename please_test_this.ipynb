{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608036, 18)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv('hka-aqm-am/' + f.removeprefix('._'), skiprows=1, sep=';', engine='python') for f in os.listdir('hka-aqm-am/')])\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395686, 27) to (401930, 27)\n",
      "training data cutoff:  2023-07-14 03:45:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316477, 20, 25]) torch.Size([316477]) torch.Size([316477, 1])\n",
      "Testing data shape: torch.Size([79441, 20, 25]) torch.Size([79441]) torch.Size([79441, 1])\n",
      "Shuffled Training data shape: torch.Size([316734, 20, 25]) torch.Size([316734]) torch.Size([316734, 1])\n",
      "Shuffled Testing data shape: torch.Size([79184, 20, 25]) torch.Size([79184]) torch.Size([79184, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     457.000001  432.422542\n",
      "1     401.000001  410.000218\n",
      "2     472.000000  471.593813\n",
      "3     583.000002  596.252556\n",
      "4     601.000004  622.904292\n",
      "...          ...         ...\n",
      "1019  433.000000  426.727638\n",
      "1020  412.333329  411.173397\n",
      "1021  450.000000  435.744775\n",
      "1022  434.000001  438.315401\n",
      "1023  469.999999  464.471902\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2294\n",
      "Epoch 1/25, Validation Loss: 0.1842\n",
      "          actual   predicted\n",
      "0     457.000001  440.701184\n",
      "1     401.000001  409.838766\n",
      "2     472.000000  476.649004\n",
      "3     583.000002  577.867889\n",
      "4     601.000004  595.435962\n",
      "...          ...         ...\n",
      "1019  433.000000  436.979284\n",
      "1020  412.333329  421.580467\n",
      "1021  450.000000  447.667869\n",
      "1022  434.000001  445.744292\n",
      "1023  469.999999  469.981890\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.1392\n",
      "Epoch 2/25, Validation Loss: 0.1393\n",
      "          actual   predicted\n",
      "0     457.000001  432.138962\n",
      "1     401.000001  388.020915\n",
      "2     472.000000  452.926650\n",
      "3     583.000002  575.692369\n",
      "4     601.000004  582.796328\n",
      "...          ...         ...\n",
      "1019  433.000000  421.500400\n",
      "1020  412.333329  407.636306\n",
      "1021  450.000000  436.412593\n",
      "1022  434.000001  428.657770\n",
      "1023  469.999999  452.898793\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1332\n",
      "Epoch 3/25, Validation Loss: 0.1428\n",
      "          actual   predicted\n",
      "0     457.000001  436.309848\n",
      "1     401.000001  386.888690\n",
      "2     472.000000  474.331455\n",
      "3     583.000002  584.050867\n",
      "4     601.000004  599.217284\n",
      "...          ...         ...\n",
      "1019  433.000000  415.307543\n",
      "1020  412.333329  405.068755\n",
      "1021  450.000000  435.349542\n",
      "1022  434.000001  427.545702\n",
      "1023  469.999999  458.571346\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1070\n",
      "Epoch 4/25, Validation Loss: 0.2005\n",
      "          actual   predicted\n",
      "0     457.000001  427.006470\n",
      "1     401.000001  395.416986\n",
      "2     472.000000  462.924538\n",
      "3     583.000002  558.333283\n",
      "4     601.000004  577.514321\n",
      "...          ...         ...\n",
      "1019  433.000000  415.651186\n",
      "1020  412.333329  406.741943\n",
      "1021  450.000000  431.166644\n",
      "1022  434.000001  429.654916\n",
      "1023  469.999999  453.996366\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.1044\n",
      "Epoch 5/25, Validation Loss: 0.1097\n",
      "          actual   predicted\n",
      "0     457.000001  419.765920\n",
      "1     401.000001  394.140906\n",
      "2     472.000000  454.706844\n",
      "3     583.000002  563.520626\n",
      "4     601.000004  582.003413\n",
      "...          ...         ...\n",
      "1019  433.000000  413.769881\n",
      "1020  412.333329  402.987375\n",
      "1021  450.000000  425.588613\n",
      "1022  434.000001  423.271138\n",
      "1023  469.999999  448.438981\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1031\n",
      "Epoch 6/25, Validation Loss: 0.1631\n",
      "          actual   predicted\n",
      "0     457.000001  439.249232\n",
      "1     401.000001  384.213786\n",
      "2     472.000000  470.550401\n",
      "3     583.000002  565.351406\n",
      "4     601.000004  589.915358\n",
      "...          ...         ...\n",
      "1019  433.000000  419.877412\n",
      "1020  412.333329  403.712743\n",
      "1021  450.000000  439.767729\n",
      "1022  434.000001  428.789908\n",
      "1023  469.999999  458.710893\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0975\n",
      "Epoch 7/25, Validation Loss: 0.0706\n",
      "          actual   predicted\n",
      "0     457.000001  444.273814\n",
      "1     401.000001  394.657684\n",
      "2     472.000000  484.347460\n",
      "3     583.000002  587.157608\n",
      "4     601.000004  600.956668\n",
      "...          ...         ...\n",
      "1019  433.000000  426.368358\n",
      "1020  412.333329  408.074351\n",
      "1021  450.000000  442.042015\n",
      "1022  434.000001  435.335088\n",
      "1023  469.999999  469.070416\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0957\n",
      "Epoch 8/25, Validation Loss: 0.0683\n",
      "          actual   predicted\n",
      "0     457.000001  439.964978\n",
      "1     401.000001  391.720065\n",
      "2     472.000000  472.226417\n",
      "3     583.000002  567.964921\n",
      "4     601.000004  582.289722\n",
      "...          ...         ...\n",
      "1019  433.000000  421.755431\n",
      "1020  412.333329  406.389556\n",
      "1021  450.000000  441.794389\n",
      "1022  434.000001  432.476092\n",
      "1023  469.999999  462.268754\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0897\n",
      "Epoch 9/25, Validation Loss: 0.0666\n",
      "          actual   predicted\n",
      "0     457.000001  440.389245\n",
      "1     401.000001  397.992242\n",
      "2     472.000000  467.095255\n",
      "3     583.000002  577.914951\n",
      "4     601.000004  594.443918\n",
      "...          ...         ...\n",
      "1019  433.000000  423.537864\n",
      "1020  412.333329  407.996540\n",
      "1021  450.000000  437.590008\n",
      "1022  434.000001  434.396907\n",
      "1023  469.999999  459.926582\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0783\n",
      "Epoch 10/25, Validation Loss: 0.0631\n",
      "          actual   predicted\n",
      "0     457.000001  442.984611\n",
      "1     401.000001  389.920559\n",
      "2     472.000000  473.602472\n",
      "3     583.000002  556.685058\n",
      "4     601.000004  573.892517\n",
      "...          ...         ...\n",
      "1019  433.000000  419.586513\n",
      "1020  412.333329  403.521293\n",
      "1021  450.000000  436.320589\n",
      "1022  434.000001  434.947648\n",
      "1023  469.999999  464.602518\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0848\n",
      "Epoch 11/25, Validation Loss: 0.0678\n",
      "          actual   predicted\n",
      "0     457.000001  445.766070\n",
      "1     401.000001  389.698326\n",
      "2     472.000000  472.176415\n",
      "3     583.000002  563.461687\n",
      "4     601.000004  578.181171\n",
      "...          ...         ...\n",
      "1019  433.000000  423.810707\n",
      "1020  412.333329  403.995605\n",
      "1021  450.000000  440.687017\n",
      "1022  434.000001  434.076824\n",
      "1023  469.999999  461.292212\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0777\n",
      "Epoch 12/25, Validation Loss: 0.0615\n",
      "          actual   predicted\n",
      "0     457.000001  448.679638\n",
      "1     401.000001  398.080385\n",
      "2     472.000000  472.060366\n",
      "3     583.000002  568.887956\n",
      "4     601.000004  586.230526\n",
      "...          ...         ...\n",
      "1019  433.000000  430.012316\n",
      "1020  412.333329  408.811064\n",
      "1021  450.000000  446.175127\n",
      "1022  434.000001  439.009627\n",
      "1023  469.999999  462.929774\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1012\n",
      "Epoch 13/25, Validation Loss: 0.1302\n",
      "          actual   predicted\n",
      "0     457.000001  444.719484\n",
      "1     401.000001  397.378662\n",
      "2     472.000000  475.391325\n",
      "3     583.000002  588.329418\n",
      "4     601.000004  609.119855\n",
      "...          ...         ...\n",
      "1019  433.000000  420.553205\n",
      "1020  412.333329  403.718115\n",
      "1021  450.000000  438.621004\n",
      "1022  434.000001  429.572784\n",
      "1023  469.999999  457.013025\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0923\n",
      "Epoch 14/25, Validation Loss: 0.0604\n",
      "          actual   predicted\n",
      "0     457.000001  445.611828\n",
      "1     401.000001  393.577767\n",
      "2     472.000000  476.646757\n",
      "3     583.000002  561.466423\n",
      "4     601.000004  580.172009\n",
      "...          ...         ...\n",
      "1019  433.000000  426.511740\n",
      "1020  412.333329  408.453238\n",
      "1021  450.000000  446.358581\n",
      "1022  434.000001  437.339028\n",
      "1023  469.999999  469.824543\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.1041\n",
      "Epoch 15/25, Validation Loss: 0.0746\n",
      "          actual   predicted\n",
      "0     457.000001  445.453996\n",
      "1     401.000001  393.670405\n",
      "2     472.000000  478.256950\n",
      "3     583.000002  558.827328\n",
      "4     601.000004  578.480295\n",
      "...          ...         ...\n",
      "1019  433.000000  420.976590\n",
      "1020  412.333329  403.025592\n",
      "1021  450.000000  441.766452\n",
      "1022  434.000001  435.923409\n",
      "1023  469.999999  465.238400\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0915\n",
      "Epoch 16/25, Validation Loss: 0.0531\n",
      "          actual   predicted\n",
      "0     457.000001  439.562408\n",
      "1     401.000001  397.183850\n",
      "2     472.000000  470.880904\n",
      "3     583.000002  564.240689\n",
      "4     601.000004  588.049183\n",
      "...          ...         ...\n",
      "1019  433.000000  421.567445\n",
      "1020  412.333329  406.834005\n",
      "1021  450.000000  438.429504\n",
      "1022  434.000001  433.202275\n",
      "1023  469.999999  460.322531\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0836\n",
      "Epoch 17/25, Validation Loss: 0.0667\n",
      "          actual   predicted\n",
      "0     457.000001  445.015845\n",
      "1     401.000001  391.919532\n",
      "2     472.000000  478.548134\n",
      "3     583.000002  578.309390\n",
      "4     601.000004  598.531341\n",
      "...          ...         ...\n",
      "1019  433.000000  426.735904\n",
      "1020  412.333329  409.721419\n",
      "1021  450.000000  450.918355\n",
      "1022  434.000001  438.198515\n",
      "1023  469.999999  468.756999\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0894\n",
      "Epoch 18/25, Validation Loss: 0.0957\n",
      "          actual   predicted\n",
      "0     457.000001  441.688358\n",
      "1     401.000001  397.553858\n",
      "2     472.000000  469.909717\n",
      "3     583.000002  567.233260\n",
      "4     601.000004  591.311674\n",
      "...          ...         ...\n",
      "1019  433.000000  420.105875\n",
      "1020  412.333329  405.672712\n",
      "1021  450.000000  440.546207\n",
      "1022  434.000001  428.539481\n",
      "1023  469.999999  457.609008\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0815\n",
      "Epoch 19/25, Validation Loss: 0.1092\n",
      "          actual   predicted\n",
      "0     457.000001  446.958192\n",
      "1     401.000001  396.855954\n",
      "2     472.000000  468.326273\n",
      "3     583.000002  578.071301\n",
      "4     601.000004  594.685285\n",
      "...          ...         ...\n",
      "1019  433.000000  426.803983\n",
      "1020  412.333329  410.161011\n",
      "1021  450.000000  443.550524\n",
      "1022  434.000001  434.447571\n",
      "1023  469.999999  461.426730\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0926\n",
      "Epoch 20/25, Validation Loss: 0.0703\n",
      "          actual   predicted\n",
      "0     457.000001  443.608539\n",
      "1     401.000001  396.000120\n",
      "2     472.000000  474.537192\n",
      "3     583.000002  591.159667\n",
      "4     601.000004  611.506491\n",
      "...          ...         ...\n",
      "1019  433.000000  420.917211\n",
      "1020  412.333329  403.621111\n",
      "1021  450.000000  442.774023\n",
      "1022  434.000001  432.720247\n",
      "1023  469.999999  462.451606\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/transformer_multivariate_quarter_hour_26f_dataframe_v2.parquet\n",
      "input_data: tensor([[[ 0.4279,  1.6477,  1.0389, -0.6762, -0.1096, -0.1022, -1.5047,\n",
      "          -0.5239, -1.8377, -1.8377, -0.5868,  4.1524,  0.0000, -0.9095,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -0.9383,\n",
      "          -1.0391, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4351,  1.6194,  1.0036, -0.6399, -0.1338, -0.1094, -1.3924,\n",
      "          -0.5239, -1.8721, -1.8721, -0.0431,  4.1524,  0.0000, -0.9094,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -0.9978,\n",
      "          -0.9735, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4424,  1.5911,  0.9683, -0.6037, -0.1580, -0.1165, -1.2801,\n",
      "          -0.5239, -1.9066, -1.9066,  0.5006,  4.1524,  0.0000, -0.9092,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.0572,\n",
      "          -0.9080, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4496,  1.5628,  0.9330, -0.5675, -0.1822, -0.1237, -1.1677,\n",
      "          -0.5239, -1.9410, -1.9410,  1.0442,  4.1524,  0.0000, -0.9091,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.1167,\n",
      "          -0.8424, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4569,  1.5345,  0.8978, -0.5312, -0.2064, -0.1308, -1.0554,\n",
      "          -0.5239, -1.9755, -1.9755,  1.5879,  4.1524,  0.0000, -0.9090,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.1762,\n",
      "          -0.7768, -1.1628,  1.1628,  0.0000],\n",
      "         [-0.6774,  2.1809,  1.0742, -1.1452,  0.4119,  0.0339, -0.4937,\n",
      "          -0.3663,  1.4266,  1.4266,  1.1348, -0.8020,  0.0000, -0.9107,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.7743,\n",
      "          -1.1811, -1.1628,  1.1628,  0.0000],\n",
      "         [-0.2000,  1.6995,  3.1013, -1.1189,  0.0964, -0.0539, -1.0835,\n",
      "          -0.4333,  0.8151,  0.8151, -0.2243, -0.8020,  0.0000, -0.9093,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.8447,\n",
      "          -1.1311, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.0843,  1.2582,  1.9067, -1.0189,  0.0356, -0.0664, -0.6809,\n",
      "          -0.3138,  1.2112,  1.2112,  1.1348, -0.8020,  0.0000, -0.9076,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.9133,\n",
      "          -1.0754, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.1921,  1.1180,  1.5822, -1.0144,  0.0183, -0.0807, -0.4937,\n",
      "          -0.4451,  1.4266,  1.4266,  1.1348, -0.8020,  0.0000, -0.9074,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.1961,\n",
      "          -0.7456, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.2283,  0.9359,  0.8060, -1.0177,  0.0170, -0.0807, -1.6171,\n",
      "          -0.5239, -1.5448, -1.5448,  1.5879, -0.8020,  0.0000, -0.9072,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.0321,\n",
      "          -0.9616, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.2785,  0.8380,  0.7637, -0.9540, -0.0149, -0.0879, -1.6171,\n",
      "          -0.4451,  1.6419,  1.6419, -0.6774, -0.8020,  0.0000, -0.9071,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.0926,\n",
      "          -0.8915, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3259,  0.7599,  0.7355, -0.9506, -0.0229, -0.0950, -1.0554,\n",
      "          -0.2875,  1.4696,  1.4696, -1.1304, -0.8020,  0.0000, -0.9070,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.1484,\n",
      "          -0.8177, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3454,  0.8109,  0.6790, -0.9037, -0.0282, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3404,  1.3404, -0.2243, -0.8020,  0.0000, -0.9068,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3494,\n",
      "          -0.4027, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3426,  0.8172,  0.6367, -0.7594, -0.0296, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3404,  1.3404, -1.5835, -0.8020,  0.0000, -0.9067,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.2450,\n",
      "          -0.6598, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3593,  0.9168,  0.5215, -0.7929, -0.0867, -0.1177, -1.0554,\n",
      "          -0.3663,  1.4266,  1.4266,  0.0777, -0.8020,  0.0000, -0.9064,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3013,\n",
      "          -0.5385, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3830,  1.0249,  1.0001, -0.5698, -0.0422, -0.1058, -1.3362,\n",
      "          -0.5239,  1.5127,  1.5127,  1.1348, -0.8020,  0.0000, -0.9061,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3365,\n",
      "          -0.4440, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4179,  0.9796,  0.5944, -0.7124, -0.0309, -0.1058, -1.6171,\n",
      "          -0.5239,  1.5988,  1.5988, -0.6774, -0.8020,  0.0000, -0.9059,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3641,\n",
      "          -0.3491, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4067,  1.0004,  0.6014, -0.6923, -0.0335, -0.1058, -1.6171,\n",
      "          -0.5239,  1.4266,  1.4266, -0.2243, -0.8020,  0.0000, -0.9058,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3839,\n",
      "          -0.2587, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4095,  1.1045,  0.6579, -0.6352, -0.0309, -0.1022, -0.4937,\n",
      "          -0.5239,  1.4266,  1.4266,  0.2287, -0.8020,  0.0000, -0.9056,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3978,\n",
      "          -0.1671, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3928,  1.1514,  0.6790, -0.5749, -0.0309, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3835,  1.3835, -1.5835, -0.8020,  0.0000, -0.9055,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.4056,\n",
      "          -0.0748, -1.1628,  1.1628,  0.0000]]])\n",
      "predicted: tensor([[-0.1106]], device='cuda:0')\n",
      "[474.09]\n",
      "           actual   predicted\n",
      "0      457.000001  443.608539\n",
      "1      401.000001  396.000120\n",
      "2      472.000000  474.537192\n",
      "3      583.000002  591.159667\n",
      "4      601.000004  611.506491\n",
      "...           ...         ...\n",
      "79179  402.999996  406.041250\n",
      "79180  440.000002  437.141796\n",
      "79181  418.999999  423.464082\n",
      "79182  442.000001  437.593573\n",
      "79183  510.000000  511.312696\n",
      "\n",
      "[79184 rows x 2 columns]\n",
      "Score (RMSE): 38.8715\n",
      "Score (MAE): 10.8034\n",
      "Score (ME): 1.5282\n",
      "Score (MAPE): 1.9598%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_42280\\1796540001.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395686, 27) to (401930, 27)\n",
      "training data cutoff:  2023-07-14 03:45:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316477, 20, 25]) torch.Size([316477]) torch.Size([316477, 1])\n",
      "Testing data shape: torch.Size([79441, 20, 25]) torch.Size([79441]) torch.Size([79441, 1])\n",
      "Shuffled Training data shape: torch.Size([316734, 20, 25]) torch.Size([316734]) torch.Size([316734, 1])\n",
      "Shuffled Testing data shape: torch.Size([79184, 20, 25]) torch.Size([79184]) torch.Size([79184, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      640.000000   655.619465\n",
      "1      667.000003   627.545970\n",
      "2      683.999998   632.789152\n",
      "3      691.999996   667.412786\n",
      "4     1268.999983  1333.254075\n",
      "...           ...          ...\n",
      "1019  2255.499954  2469.730523\n",
      "1020   596.999994   701.374627\n",
      "1021   763.000000   788.732272\n",
      "1022   805.000000   749.803969\n",
      "1023   638.000001   609.783988\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.1553\n",
      "Epoch 1/25, Validation Loss: 0.0874\n",
      "           actual    predicted\n",
      "0      640.000000   661.404607\n",
      "1      667.000003   652.895123\n",
      "2      683.999998   634.508431\n",
      "3      691.999996   669.171148\n",
      "4     1268.999983  1290.756173\n",
      "...           ...          ...\n",
      "1019  2255.499954  2252.855554\n",
      "1020   596.999994   729.907227\n",
      "1021   763.000000   815.831199\n",
      "1022   805.000000   756.159723\n",
      "1023   638.000001   610.675551\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0729\n",
      "Epoch 2/25, Validation Loss: 0.0461\n",
      "           actual    predicted\n",
      "0      640.000000   637.893252\n",
      "1      667.000003   645.635314\n",
      "2      683.999998   635.763996\n",
      "3      691.999996   673.952036\n",
      "4     1268.999983  1268.295856\n",
      "...           ...          ...\n",
      "1019  2255.499954  2165.988830\n",
      "1020   596.999994   695.045882\n",
      "1021   763.000000   785.160806\n",
      "1022   805.000000   731.249666\n",
      "1023   638.000001   608.635083\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0643\n",
      "Epoch 3/25, Validation Loss: 0.0473\n",
      "           actual    predicted\n",
      "0      640.000000   619.650465\n",
      "1      667.000003   639.897462\n",
      "2      683.999998   609.991801\n",
      "3      691.999996   643.999520\n",
      "4     1268.999983  1231.671041\n",
      "...           ...          ...\n",
      "1019  2255.499954  2194.660830\n",
      "1020   596.999994   705.927956\n",
      "1021   763.000000   798.669622\n",
      "1022   805.000000   747.025586\n",
      "1023   638.000001   599.726184\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0608\n",
      "Epoch 4/25, Validation Loss: 0.0470\n",
      "           actual    predicted\n",
      "0      640.000000   657.665227\n",
      "1      667.000003   651.575338\n",
      "2      683.999998   637.732445\n",
      "3      691.999996   680.051508\n",
      "4     1268.999983  1276.123749\n",
      "...           ...          ...\n",
      "1019  2255.499954  2190.323371\n",
      "1020   596.999994   706.166973\n",
      "1021   763.000000   802.752393\n",
      "1022   805.000000   772.687324\n",
      "1023   638.000001   610.719679\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0581\n",
      "Epoch 5/25, Validation Loss: 0.0455\n",
      "           actual    predicted\n",
      "0      640.000000   659.936406\n",
      "1      667.000003   665.956177\n",
      "2      683.999998   642.739114\n",
      "3      691.999996   675.791139\n",
      "4     1268.999983  1281.431772\n",
      "...           ...          ...\n",
      "1019  2255.499954  2249.494307\n",
      "1020   596.999994   724.579943\n",
      "1021   763.000000   805.225064\n",
      "1022   805.000000   764.154687\n",
      "1023   638.000001   612.787203\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0563\n",
      "Epoch 6/25, Validation Loss: 0.0444\n",
      "           actual    predicted\n",
      "0      640.000000   646.700359\n",
      "1      667.000003   675.592706\n",
      "2      683.999998   634.579455\n",
      "3      691.999996   669.064098\n",
      "4     1268.999983  1267.719814\n",
      "...           ...          ...\n",
      "1019  2255.499954  2220.903029\n",
      "1020   596.999994   702.178690\n",
      "1021   763.000000   780.469005\n",
      "1022   805.000000   748.774249\n",
      "1023   638.000001   602.809695\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0539\n",
      "Epoch 7/25, Validation Loss: 0.0487\n",
      "           actual    predicted\n",
      "0      640.000000   650.007006\n",
      "1      667.000003   661.025381\n",
      "2      683.999998   635.287383\n",
      "3      691.999996   665.748950\n",
      "4     1268.999983  1274.233820\n",
      "...           ...          ...\n",
      "1019  2255.499954  2090.274010\n",
      "1020   596.999994   720.150049\n",
      "1021   763.000000   807.729617\n",
      "1022   805.000000   778.324058\n",
      "1023   638.000001   611.550309\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0532\n",
      "Epoch 8/25, Validation Loss: 0.0439\n",
      "           actual    predicted\n",
      "0      640.000000   668.284426\n",
      "1      667.000003   688.747896\n",
      "2      683.999998   648.092811\n",
      "3      691.999996   693.275084\n",
      "4     1268.999983  1294.528818\n",
      "...           ...          ...\n",
      "1019  2255.499954  2299.603477\n",
      "1020   596.999994   753.078399\n",
      "1021   763.000000   820.523440\n",
      "1022   805.000000   781.110356\n",
      "1023   638.000001   601.236140\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0558\n",
      "Epoch 9/25, Validation Loss: 0.0497\n",
      "           actual    predicted\n",
      "0      640.000000   650.895718\n",
      "1      667.000003   670.327017\n",
      "2      683.999998   635.785083\n",
      "3      691.999996   682.434917\n",
      "4     1268.999983  1279.923538\n",
      "...           ...          ...\n",
      "1019  2255.499954  2091.494382\n",
      "1020   596.999994   712.554618\n",
      "1021   763.000000   793.740662\n",
      "1022   805.000000   768.394283\n",
      "1023   638.000001   603.529970\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0514\n",
      "Epoch 10/25, Validation Loss: 0.0450\n",
      "           actual    predicted\n",
      "0      640.000000   648.073021\n",
      "1      667.000003   669.720987\n",
      "2      683.999998   639.060508\n",
      "3      691.999996   667.949977\n",
      "4     1268.999983  1318.147863\n",
      "...           ...          ...\n",
      "1019  2255.499954  2203.814831\n",
      "1020   596.999994   694.423642\n",
      "1021   763.000000   791.212178\n",
      "1022   805.000000   749.249737\n",
      "1023   638.000001   599.399791\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0505\n",
      "Epoch 11/25, Validation Loss: 0.0486\n",
      "           actual    predicted\n",
      "0      640.000000   653.199345\n",
      "1      667.000003   684.048523\n",
      "2      683.999998   645.908537\n",
      "3      691.999996   684.108692\n",
      "4     1268.999983  1305.442282\n",
      "...           ...          ...\n",
      "1019  2255.499954  2119.775841\n",
      "1020   596.999994   708.230615\n",
      "1021   763.000000   800.649922\n",
      "1022   805.000000   758.567226\n",
      "1023   638.000001   611.792266\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0509\n",
      "Epoch 12/25, Validation Loss: 0.0447\n",
      "           actual    predicted\n",
      "0      640.000000   655.055858\n",
      "1      667.000003   675.848875\n",
      "2      683.999998   644.348181\n",
      "3      691.999996   688.204017\n",
      "4     1268.999983  1292.133871\n",
      "...           ...          ...\n",
      "1019  2255.499954  2156.432633\n",
      "1020   596.999994   714.836634\n",
      "1021   763.000000   805.653550\n",
      "1022   805.000000   776.878580\n",
      "1023   638.000001   612.431533\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0483\n",
      "Epoch 13/25, Validation Loss: 0.0424\n",
      "           actual    predicted\n",
      "0      640.000000   660.571667\n",
      "1      667.000003   678.734122\n",
      "2      683.999998   646.330576\n",
      "3      691.999996   678.274404\n",
      "4     1268.999983  1287.620326\n",
      "...           ...          ...\n",
      "1019  2255.499954  2116.280150\n",
      "1020   596.999994   720.012180\n",
      "1021   763.000000   823.282661\n",
      "1022   805.000000   779.712225\n",
      "1023   638.000001   613.763397\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0477\n",
      "Epoch 14/25, Validation Loss: 0.0433\n",
      "           actual    predicted\n",
      "0      640.000000   644.198051\n",
      "1      667.000003   668.238566\n",
      "2      683.999998   632.378056\n",
      "3      691.999996   671.623316\n",
      "4     1268.999983  1258.416553\n",
      "...           ...          ...\n",
      "1019  2255.499954  2201.652639\n",
      "1020   596.999994   708.591419\n",
      "1021   763.000000   796.836434\n",
      "1022   805.000000   756.290366\n",
      "1023   638.000001   604.133601\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0478\n",
      "Epoch 15/25, Validation Loss: 0.0432\n",
      "           actual    predicted\n",
      "0      640.000000   652.470410\n",
      "1      667.000003   675.935168\n",
      "2      683.999998   639.132455\n",
      "3      691.999996   674.108428\n",
      "4     1268.999983  1287.294839\n",
      "...           ...          ...\n",
      "1019  2255.499954  2235.027202\n",
      "1020   596.999994   709.670445\n",
      "1021   763.000000   803.160450\n",
      "1022   805.000000   753.327218\n",
      "1023   638.000001   599.561735\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0475\n",
      "Epoch 16/25, Validation Loss: 0.0447\n",
      "           actual    predicted\n",
      "0      640.000000   658.045323\n",
      "1      667.000003   683.312624\n",
      "2      683.999998   642.429527\n",
      "3      691.999996   679.146595\n",
      "4     1268.999983  1285.742549\n",
      "...           ...          ...\n",
      "1019  2255.499954  2175.954186\n",
      "1020   596.999994   723.433979\n",
      "1021   763.000000   806.620217\n",
      "1022   805.000000   767.434411\n",
      "1023   638.000001   608.911486\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0474\n",
      "Epoch 17/25, Validation Loss: 0.0426\n",
      "           actual    predicted\n",
      "0      640.000000   666.859190\n",
      "1      667.000003   692.212241\n",
      "2      683.999998   652.067992\n",
      "3      691.999996   696.022921\n",
      "4     1268.999983  1301.313062\n",
      "...           ...          ...\n",
      "1019  2255.499954  2233.123950\n",
      "1020   596.999994   722.167204\n",
      "1021   763.000000   803.661626\n",
      "1022   805.000000   771.234523\n",
      "1023   638.000001   608.504618\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/transformer_multivariate_quarter_hour_26f_dataframe_v2.parquet\n",
      "input_data: tensor([[[ 0.4279,  1.6477,  1.0389, -0.6762, -0.1096, -0.1022, -1.5047,\n",
      "          -0.5239, -1.8377, -1.8377, -0.5868,  4.1524,  0.0000, -0.9095,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -0.9383,\n",
      "          -1.0391, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4351,  1.6194,  1.0036, -0.6399, -0.1338, -0.1094, -1.3924,\n",
      "          -0.5239, -1.8721, -1.8721, -0.0431,  4.1524,  0.0000, -0.9094,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -0.9978,\n",
      "          -0.9735, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4424,  1.5911,  0.9683, -0.6037, -0.1580, -0.1165, -1.2801,\n",
      "          -0.5239, -1.9066, -1.9066,  0.5006,  4.1524,  0.0000, -0.9092,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.0572,\n",
      "          -0.9080, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4496,  1.5628,  0.9330, -0.5675, -0.1822, -0.1237, -1.1677,\n",
      "          -0.5239, -1.9410, -1.9410,  1.0442,  4.1524,  0.0000, -0.9091,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.1167,\n",
      "          -0.8424, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4569,  1.5345,  0.8978, -0.5312, -0.2064, -0.1308, -1.0554,\n",
      "          -0.5239, -1.9755, -1.9755,  1.5879,  4.1524,  0.0000, -0.9090,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.1762,\n",
      "          -0.7768, -1.1628,  1.1628,  0.0000],\n",
      "         [-0.6774,  2.1809,  1.0742, -1.1452,  0.4119,  0.0339, -0.4937,\n",
      "          -0.3663,  1.4266,  1.4266,  1.1348, -0.8020,  0.0000, -0.9107,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.7743,\n",
      "          -1.1811, -1.1628,  1.1628,  0.0000],\n",
      "         [-0.2000,  1.6995,  3.1013, -1.1189,  0.0964, -0.0539, -1.0835,\n",
      "          -0.4333,  0.8151,  0.8151, -0.2243, -0.8020,  0.0000, -0.9093,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.8447,\n",
      "          -1.1311, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.0843,  1.2582,  1.9067, -1.0189,  0.0356, -0.0664, -0.6809,\n",
      "          -0.3138,  1.2112,  1.2112,  1.1348, -0.8020,  0.0000, -0.9076,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.9133,\n",
      "          -1.0754, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.1921,  1.1180,  1.5822, -1.0144,  0.0183, -0.0807, -0.4937,\n",
      "          -0.4451,  1.4266,  1.4266,  1.1348, -0.8020,  0.0000, -0.9074,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.1961,\n",
      "          -0.7456, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.2283,  0.9359,  0.8060, -1.0177,  0.0170, -0.0807, -1.6171,\n",
      "          -0.5239, -1.5448, -1.5448,  1.5879, -0.8020,  0.0000, -0.9072,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.0321,\n",
      "          -0.9616, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.2785,  0.8380,  0.7637, -0.9540, -0.0149, -0.0879, -1.6171,\n",
      "          -0.4451,  1.6419,  1.6419, -0.6774, -0.8020,  0.0000, -0.9071,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.0926,\n",
      "          -0.8915, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3259,  0.7599,  0.7355, -0.9506, -0.0229, -0.0950, -1.0554,\n",
      "          -0.2875,  1.4696,  1.4696, -1.1304, -0.8020,  0.0000, -0.9070,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.1484,\n",
      "          -0.8177, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3454,  0.8109,  0.6790, -0.9037, -0.0282, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3404,  1.3404, -0.2243, -0.8020,  0.0000, -0.9068,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3494,\n",
      "          -0.4027, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3426,  0.8172,  0.6367, -0.7594, -0.0296, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3404,  1.3404, -1.5835, -0.8020,  0.0000, -0.9067,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.2450,\n",
      "          -0.6598, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3593,  0.9168,  0.5215, -0.7929, -0.0867, -0.1177, -1.0554,\n",
      "          -0.3663,  1.4266,  1.4266,  0.0777, -0.8020,  0.0000, -0.9064,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3013,\n",
      "          -0.5385, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3830,  1.0249,  1.0001, -0.5698, -0.0422, -0.1058, -1.3362,\n",
      "          -0.5239,  1.5127,  1.5127,  1.1348, -0.8020,  0.0000, -0.9061,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3365,\n",
      "          -0.4440, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4179,  0.9796,  0.5944, -0.7124, -0.0309, -0.1058, -1.6171,\n",
      "          -0.5239,  1.5988,  1.5988, -0.6774, -0.8020,  0.0000, -0.9059,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3641,\n",
      "          -0.3491, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4067,  1.0004,  0.6014, -0.6923, -0.0335, -0.1058, -1.6171,\n",
      "          -0.5239,  1.4266,  1.4266, -0.2243, -0.8020,  0.0000, -0.9058,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3839,\n",
      "          -0.2587, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4095,  1.1045,  0.6579, -0.6352, -0.0309, -0.1022, -0.4937,\n",
      "          -0.5239,  1.4266,  1.4266,  0.2287, -0.8020,  0.0000, -0.9056,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3978,\n",
      "          -0.1671, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3928,  1.1514,  0.6790, -0.5749, -0.0309, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3835,  1.3835, -1.5835, -0.8020,  0.0000, -0.9055,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.4056,\n",
      "          -0.0748, -1.1628,  1.1628,  0.0000]]])\n",
      "predicted: tensor([[-0.4565]], device='cuda:0')\n",
      "[655.28]\n",
      "            actual    predicted\n",
      "0       640.000000   666.859190\n",
      "1       667.000003   692.212241\n",
      "2       683.999998   652.067992\n",
      "3       691.999996   696.022921\n",
      "4      1268.999983  1301.313062\n",
      "...            ...          ...\n",
      "79179   576.999999   561.525611\n",
      "79180   573.999999   570.714413\n",
      "79181   570.000001   582.734680\n",
      "79182   693.999996   700.456221\n",
      "79183   626.000004   593.914830\n",
      "\n",
      "[79184 rows x 2 columns]\n",
      "Score (RMSE): 61.8442\n",
      "Score (MAE): 31.3966\n",
      "Score (ME): -7.5576\n",
      "Score (MAPE): 3.8817%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_42280\\1796540001.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395686, 27) to (401930, 27)\n",
      "training data cutoff:  2023-07-14 03:45:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316477, 20, 25]) torch.Size([316477]) torch.Size([316477, 1])\n",
      "Testing data shape: torch.Size([79441, 20, 25]) torch.Size([79441]) torch.Size([79441, 1])\n",
      "Shuffled Training data shape: torch.Size([316734, 20, 25]) torch.Size([316734]) torch.Size([316734, 1])\n",
      "Shuffled Testing data shape: torch.Size([79184, 20, 25]) torch.Size([79184]) torch.Size([79184, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual  predicted\n",
      "0      36.19  35.675307\n",
      "1      39.95  39.844160\n",
      "2      20.85  21.983214\n",
      "3      35.46  33.372930\n",
      "4      37.45  36.794921\n",
      "...      ...        ...\n",
      "1019   51.86  49.977174\n",
      "1020   48.35  47.690961\n",
      "1021   34.06  33.773674\n",
      "1022   33.59  33.120000\n",
      "1023   30.13  29.722613\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.0867\n",
      "Epoch 1/25, Validation Loss: 0.0106\n",
      "      actual  predicted\n",
      "0      36.19  36.137265\n",
      "1      39.95  40.045559\n",
      "2      20.85  19.598437\n",
      "3      35.46  35.139621\n",
      "4      37.45  37.343464\n",
      "...      ...        ...\n",
      "1019   51.86  51.470596\n",
      "1020   48.35  48.907083\n",
      "1021   34.06  34.783923\n",
      "1022   33.59  34.019221\n",
      "1023   30.13  30.354904\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0267\n",
      "Epoch 2/25, Validation Loss: 0.0101\n",
      "      actual  predicted\n",
      "0      36.19  36.612644\n",
      "1      39.95  40.702068\n",
      "2      20.85  20.534586\n",
      "3      35.46  35.341294\n",
      "4      37.45  37.511831\n",
      "...      ...        ...\n",
      "1019   51.86  51.579367\n",
      "1020   48.35  49.096182\n",
      "1021   34.06  34.349041\n",
      "1022   33.59  33.480954\n",
      "1023   30.13  30.022421\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0197\n",
      "Epoch 3/25, Validation Loss: 0.0085\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m freq\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y_feature \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCO2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVOC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 17\u001b[0m     model, scaler, rmse, mae, me, mape, train_loss, val_loss, n_features \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_multivariate_transformer_model_for_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_pe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregation_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     performance_df \u001b[38;5;241m=\u001b[39m performance_df\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultivariate_transformer\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maggregation_level\u001b[39m\u001b[38;5;124m'\u001b[39m: aggregation_level, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_feature\u001b[39m\u001b[38;5;124m'\u001b[39m: y_feature, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_features\u001b[39m\u001b[38;5;124m'\u001b[39m: n_features, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m: rmse, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m: mae, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mme\u001b[39m\u001b[38;5;124m'\u001b[39m: me, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmape\u001b[39m\u001b[38;5;124m'\u001b[39m: mape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m'\u001b[39m: d_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnhead\u001b[39m\u001b[38;5;124m'\u001b[39m: nhead, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: num_layers, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_pe\u001b[39m\u001b[38;5;124m'\u001b[39m: dropout_pe,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m: dropout_encoder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: batch_size, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: learning_rate, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: epochs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: val_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnote\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2 decoder Linear layers with LeakyReLu, shuffled training data\u001b[39m\u001b[38;5;124m'\u001b[39m}, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m     performance_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_performances.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Studium\\Semester_6\\IoT_Project\\utils.py:1310\u001b[0m, in \u001b[0;36mcreate_multivariate_transformer_model_for_feature\u001b[1;34m(df, y_feature, aggregation_level, device, window_size, batch_size, epochs, clean_data, input_dim, d_model, nhead, num_layers, dropout_pe, dropout_encoder, learning_rate, drop_columns)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m num_unique_devices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_preprocessed_df_unscaled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 1310\u001b[0m model, train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_transformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_pe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_feature_scaler_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_feature_scaler_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_devices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_unique_devices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m# Save the model, scaler and used columns\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m save_model(model, y_feature\u001b[38;5;241m=\u001b[39my_feature, model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n",
      "File \u001b[1;32mc:\\Studium\\Semester_6\\IoT_Project\\utils.py:398\u001b[0m, in \u001b[0;36mtrain_transformer_model\u001b[1;34m(device, train_loader, test_loader, scaler, epochs, input_dim, d_model, nhead, num_layers, dropout_pe, dropout_encoder, learning_rate, y_feature_scaler_index, num_devices, device_embedding_dim)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    397\u001b[0m     x_batch, device_ids_batch, y_batch \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m--> 398\u001b[0m     x_batch, device_ids_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, device_ids_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    400\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    401\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(x_batch, device_ids_batch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "performance_df = pd.read_csv('model_performances.csv')\n",
    "d_model=256\n",
    "nhead=8\n",
    "num_layers=5 \n",
    "dropout_pe=0.15\n",
    "dropout_encoder=0.15\n",
    "batch_size=1024\n",
    "learning_rate=0.00031\n",
    "epochs=3\n",
    "aggregation_level = 'quarter_hour'\n",
    "y_feature = 'CO2'\n",
    "freq = 15 if aggregation_level == 'quarter_hour' else 30 if aggregation_level == 'half_hour' else 60\n",
    "window_size = 5 * 60 // freq\n",
    "for y_feature in ['CO2']:\n",
    "    \n",
    "    model, scaler, rmse, mae, me, mape, train_loss, val_loss, n_features = utils.create_multivariate_transformer_model_for_feature(df, d_model=d_model, nhead=nhead, num_layers=num_layers, dropout_pe=dropout_pe, dropout_encoder=dropout_encoder, batch_size=batch_size, learning_rate=learning_rate, epochs=epochs, y_feature=y_feature, aggregation_level=aggregation_level, window_size=window_size)\n",
    "    performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n",
    "    performance_df.to_csv('model_performances.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395686, 27) to (401930, 27)\n",
      "training data cutoff:  2023-07-14 03:45:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n"
     ]
    }
   ],
   "source": [
    "# Reload the utils module if you have made changes\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Read existing model performance records\n",
    "performance_df = pd.read_csv('model_performances.csv')\n",
    "\n",
    "# Define LSTM-specific hyperparameters\n",
    "hidden_dim = 256\n",
    "num_layers = 5\n",
    "dropout = 0.3\n",
    "batch_size = 4096\n",
    "learning_rate = 0.00031\n",
    "epochs = 25\n",
    "aggregation_level = 'quarter_hour'\n",
    "y_feature = 'CO2'\n",
    "freq = 15 if aggregation_level == 'quarter_hour' else 30 if aggregation_level == 'half_hour' else 60\n",
    "window_size = 5 * 60 // freq\n",
    "\n",
    "# Loop over each feature to create and evaluate the LSTM model\n",
    "for y_feature in ['VOC', 'hum', 'tmp', 'vis']:\n",
    "    model, scaler, rmse, mae, me, mape, train_loss, val_loss, n_features = utils.create_multivariate_lstm_model_for_feature(\n",
    "        df, \n",
    "        hidden_dim=hidden_dim, \n",
    "        num_layers=num_layers, \n",
    "        dropout=dropout, \n",
    "        batch_size=batch_size, \n",
    "        learning_rate=learning_rate, \n",
    "        epochs=epochs, \n",
    "        y_feature=y_feature, \n",
    "        aggregation_level=aggregation_level, \n",
    "        window_size=window_size\n",
    "    )\n",
    "    performance_df = performance_df.append({\n",
    "        'model_name': 'multivariate_lstm',\n",
    "        'aggregation_level': aggregation_level,\n",
    "        'y_feature': y_feature,\n",
    "        'n_features': n_features,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'me': me,\n",
    "        'mape': mape,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout': dropout,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'epochs': epochs,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'note': 'LSTM model with device_id embedding'\n",
    "    }, ignore_index=True)\n",
    "    performance_df.to_csv('model_performances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device_ids = torch.tensor([0])\n",
    "\n",
    "device = utils.get_device()\n",
    "\n",
    "model = utils.load_transformer_model(y_feature=\"CO2\", model_name = 'transformer_multivariate_quarter_hour_26f', device=device)\n",
    "scaler = utils.load_scaler(y_feature=\"CO2\", model_name = 'transformer_multivariate_quarter_hour_26f')\n",
    "\n",
    "real_data = utils.load_dataframe(model_name='transformer_multivariate_quarter_hour_26f')\n",
    "pd.set_option('display.max_columns', None)\n",
    "real_data.iloc[7:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = real_data.iloc[7:27].drop(columns=['device_id', 'date_time_rounded'])\n",
    "\n",
    "data_df_scaled = scaler.transform(data_df)\n",
    "\n",
    "input_data = torch.tensor(data_df_scaled, dtype=torch.float32).view(-1, 20, data_df_scaled.shape[1])\n",
    "print(input_data.shape)\n",
    "device_ids = torch.tensor([0])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_data.to(device), device_ids.to(device))\n",
    "    print(output)\n",
    "\n",
    "    prediction = output.cpu().numpy().reshape(-1, 1)\n",
    "    print(prediction)\n",
    "    print(prediction.shape)\n",
    "    zeroes_for_scaler = np.zeros((prediction.shape[0], 25))\n",
    "\n",
    "    zeroes_for_scaler[:, 2] = prediction  # Insert predicted values into the correct column\n",
    "    print(zeroes_for_scaler)\n",
    "    inverse_transformed = scaler.inverse_transform(zeroes_for_scaler)\n",
    "    predicted_unscaled = inverse_transformed[:, 2].round(2)\n",
    "    print(predicted_unscaled)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
