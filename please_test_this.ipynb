{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608036, 18)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv('hka-aqm-am/' + f.removeprefix('._'), skiprows=1, sep=';', engine='python') for f in os.listdir('hka-aqm-am/')])\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395686, 27) to (401930, 27)\n",
      "training data cutoff:  2023-07-14 03:45:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316477, 20, 25]) torch.Size([316477]) torch.Size([316477, 1])\n",
      "Testing data shape: torch.Size([79441, 20, 25]) torch.Size([79441]) torch.Size([79441, 1])\n",
      "Shuffled Training data shape: torch.Size([316734, 20, 25]) torch.Size([316734]) torch.Size([316734, 1])\n",
      "Shuffled Testing data shape: torch.Size([79184, 20, 25]) torch.Size([79184]) torch.Size([79184, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     447.000000  420.123641\n",
      "1     431.000001  421.473891\n",
      "2     445.000001  430.241168\n",
      "3     623.999997  661.232737\n",
      "4     480.000000  448.026895\n",
      "...          ...         ...\n",
      "1019  423.999999  444.747124\n",
      "1020  495.000000  477.513857\n",
      "1021  510.999999  493.279501\n",
      "1022  488.000000  472.447271\n",
      "1023  425.000000  436.473044\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2660\n",
      "Epoch 1/25, Validation Loss: 0.1119\n",
      "          actual   predicted\n",
      "0     447.000000  418.286876\n",
      "1     431.000001  419.374151\n",
      "2     445.000001  435.542233\n",
      "3     623.999997  675.227402\n",
      "4     480.000000  473.656121\n",
      "...          ...         ...\n",
      "1019  423.999999  437.821871\n",
      "1020  495.000000  487.357301\n",
      "1021  510.999999  519.197087\n",
      "1022  488.000000  473.060118\n",
      "1023  425.000000  423.066704\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.1587\n",
      "Epoch 2/25, Validation Loss: 0.0942\n",
      "          actual   predicted\n",
      "0     447.000000  411.726863\n",
      "1     431.000001  421.881990\n",
      "2     445.000001  439.113241\n",
      "3     623.999997  647.267452\n",
      "4     480.000000  494.820034\n",
      "...          ...         ...\n",
      "1019  423.999999  420.048216\n",
      "1020  495.000000  479.208462\n",
      "1021  510.999999  501.086270\n",
      "1022  488.000000  486.256958\n",
      "1023  425.000000  413.647779\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1449\n",
      "Epoch 3/25, Validation Loss: 0.0778\n",
      "          actual   predicted\n",
      "0     447.000000  420.891602\n",
      "1     431.000001  425.258414\n",
      "2     445.000001  438.493372\n",
      "3     623.999997  650.195296\n",
      "4     480.000000  472.787888\n",
      "...          ...         ...\n",
      "1019  423.999999  428.482641\n",
      "1020  495.000000  473.944118\n",
      "1021  510.999999  499.942985\n",
      "1022  488.000000  471.810530\n",
      "1023  425.000000  418.056154\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1281\n",
      "Epoch 4/25, Validation Loss: 0.0742\n",
      "          actual   predicted\n",
      "0     447.000000  419.877793\n",
      "1     431.000001  421.333132\n",
      "2     445.000001  435.166540\n",
      "3     623.999997  603.054714\n",
      "4     480.000000  473.885267\n",
      "...          ...         ...\n",
      "1019  423.999999  426.694692\n",
      "1020  495.000000  470.606099\n",
      "1021  510.999999  491.016307\n",
      "1022  488.000000  477.934386\n",
      "1023  425.000000  420.351031\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.1158\n",
      "Epoch 5/25, Validation Loss: 0.0757\n",
      "          actual   predicted\n",
      "0     447.000000  438.857442\n",
      "1     431.000001  446.613566\n",
      "2     445.000001  471.527395\n",
      "3     623.999997  629.627388\n",
      "4     480.000000  502.254346\n",
      "...          ...         ...\n",
      "1019  423.999999  447.173701\n",
      "1020  495.000000  502.553407\n",
      "1021  510.999999  518.444632\n",
      "1022  488.000000  499.461741\n",
      "1023  425.000000  422.979402\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1009\n",
      "Epoch 6/25, Validation Loss: 0.0601\n",
      "          actual   predicted\n",
      "0     447.000000  425.885320\n",
      "1     431.000001  422.974730\n",
      "2     445.000001  444.260644\n",
      "3     623.999997  676.157710\n",
      "4     480.000000  488.483938\n",
      "...          ...         ...\n",
      "1019  423.999999  425.713334\n",
      "1020  495.000000  494.823449\n",
      "1021  510.999999  517.165559\n",
      "1022  488.000000  495.521847\n",
      "1023  425.000000  421.309808\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0995\n",
      "Epoch 7/25, Validation Loss: 0.0621\n",
      "          actual   predicted\n",
      "0     447.000000  429.568522\n",
      "1     431.000001  423.627570\n",
      "2     445.000001  452.716315\n",
      "3     623.999997  663.210911\n",
      "4     480.000000  483.966375\n",
      "...          ...         ...\n",
      "1019  423.999999  426.272730\n",
      "1020  495.000000  499.166157\n",
      "1021  510.999999  522.253822\n",
      "1022  488.000000  490.023295\n",
      "1023  425.000000  416.380026\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1006\n",
      "Epoch 8/25, Validation Loss: 0.0562\n",
      "          actual   predicted\n",
      "0     447.000000  428.794124\n",
      "1     431.000001  427.807419\n",
      "2     445.000001  447.713148\n",
      "3     623.999997  671.091642\n",
      "4     480.000000  495.665998\n",
      "...          ...         ...\n",
      "1019  423.999999  429.600902\n",
      "1020  495.000000  496.665921\n",
      "1021  510.999999  517.466993\n",
      "1022  488.000000  496.809786\n",
      "1023  425.000000  419.877260\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1035\n",
      "Epoch 9/25, Validation Loss: 0.0614\n",
      "          actual   predicted\n",
      "0     447.000000  425.711653\n",
      "1     431.000001  425.042057\n",
      "2     445.000001  443.643668\n",
      "3     623.999997  662.943829\n",
      "4     480.000000  480.035850\n",
      "...          ...         ...\n",
      "1019  423.999999  424.853298\n",
      "1020  495.000000  482.611847\n",
      "1021  510.999999  505.643719\n",
      "1022  488.000000  476.121712\n",
      "1023  425.000000  418.931560\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0939\n",
      "Epoch 10/25, Validation Loss: 0.0696\n",
      "          actual   predicted\n",
      "0     447.000000  425.888742\n",
      "1     431.000001  421.112715\n",
      "2     445.000001  443.905626\n",
      "3     623.999997  622.335062\n",
      "4     480.000000  471.535423\n",
      "...          ...         ...\n",
      "1019  423.999999  420.902664\n",
      "1020  495.000000  479.995130\n",
      "1021  510.999999  494.069534\n",
      "1022  488.000000  474.199934\n",
      "1023  425.000000  416.752087\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0880\n",
      "Epoch 11/25, Validation Loss: 0.0571\n",
      "          actual   predicted\n",
      "0     447.000000  431.106598\n",
      "1     431.000001  428.939512\n",
      "2     445.000001  446.626347\n",
      "3     623.999997  626.725351\n",
      "4     480.000000  474.826244\n",
      "...          ...         ...\n",
      "1019  423.999999  425.647210\n",
      "1020  495.000000  478.944642\n",
      "1021  510.999999  495.871109\n",
      "1022  488.000000  474.230341\n",
      "1023  425.000000  419.090364\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1058\n",
      "Epoch 12/25, Validation Loss: 0.0567\n",
      "          actual   predicted\n",
      "0     447.000000  435.763301\n",
      "1     431.000001  432.966927\n",
      "2     445.000001  454.378344\n",
      "3     623.999997  666.643335\n",
      "4     480.000000  482.929263\n",
      "...          ...         ...\n",
      "1019  423.999999  430.440862\n",
      "1020  495.000000  486.366587\n",
      "1021  510.999999  506.498998\n",
      "1022  488.000000  481.613327\n",
      "1023  425.000000  419.169489\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/transformer_multivariate_quarter_hour_26f_dataframe_v2.parquet\n",
      "input_data: tensor([[[ 0.4279,  1.6477,  1.0389, -0.6762, -0.1096, -0.1022, -1.5047,\n",
      "          -0.5239, -1.8377, -1.8377, -0.5868,  4.1524,  0.0000, -0.9095,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -0.9383,\n",
      "          -1.0391, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4351,  1.6194,  1.0036, -0.6399, -0.1338, -0.1094, -1.3924,\n",
      "          -0.5239, -1.8721, -1.8721, -0.0431,  4.1524,  0.0000, -0.9094,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -0.9978,\n",
      "          -0.9735, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4424,  1.5911,  0.9683, -0.6037, -0.1580, -0.1165, -1.2801,\n",
      "          -0.5239, -1.9066, -1.9066,  0.5006,  4.1524,  0.0000, -0.9092,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.0572,\n",
      "          -0.9080, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4496,  1.5628,  0.9330, -0.5675, -0.1822, -0.1237, -1.1677,\n",
      "          -0.5239, -1.9410, -1.9410,  1.0442,  4.1524,  0.0000, -0.9091,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.1167,\n",
      "          -0.8424, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4569,  1.5345,  0.8978, -0.5312, -0.2064, -0.1308, -1.0554,\n",
      "          -0.5239, -1.9755, -1.9755,  1.5879,  4.1524,  0.0000, -0.9090,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.1762,\n",
      "          -0.7768, -1.1628,  1.1628,  0.0000],\n",
      "         [-0.6774,  2.1809,  1.0742, -1.1452,  0.4119,  0.0339, -0.4937,\n",
      "          -0.3663,  1.4266,  1.4266,  1.1348, -0.8020,  0.0000, -0.9107,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.7743,\n",
      "          -1.1811, -1.1628,  1.1628,  0.0000],\n",
      "         [-0.2000,  1.6995,  3.1013, -1.1189,  0.0964, -0.0539, -1.0835,\n",
      "          -0.4333,  0.8151,  0.8151, -0.2243, -0.8020,  0.0000, -0.9093,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.8447,\n",
      "          -1.1311, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.0843,  1.2582,  1.9067, -1.0189,  0.0356, -0.0664, -0.6809,\n",
      "          -0.3138,  1.2112,  1.2112,  1.1348, -0.8020,  0.0000, -0.9076,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.9133,\n",
      "          -1.0754, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.1921,  1.1180,  1.5822, -1.0144,  0.0183, -0.0807, -0.4937,\n",
      "          -0.4451,  1.4266,  1.4266,  1.1348, -0.8020,  0.0000, -0.9074,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.1961,\n",
      "          -0.7456, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.2283,  0.9359,  0.8060, -1.0177,  0.0170, -0.0807, -1.6171,\n",
      "          -0.5239, -1.5448, -1.5448,  1.5879, -0.8020,  0.0000, -0.9072,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.0321,\n",
      "          -0.9616, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.2785,  0.8380,  0.7637, -0.9540, -0.0149, -0.0879, -1.6171,\n",
      "          -0.4451,  1.6419,  1.6419, -0.6774, -0.8020,  0.0000, -0.9071,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.0926,\n",
      "          -0.8915, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3259,  0.7599,  0.7355, -0.9506, -0.0229, -0.0950, -1.0554,\n",
      "          -0.2875,  1.4696,  1.4696, -1.1304, -0.8020,  0.0000, -0.9070,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.1484,\n",
      "          -0.8177, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3454,  0.8109,  0.6790, -0.9037, -0.0282, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3404,  1.3404, -0.2243, -0.8020,  0.0000, -0.9068,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3494,\n",
      "          -0.4027, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3426,  0.8172,  0.6367, -0.7594, -0.0296, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3404,  1.3404, -1.5835, -0.8020,  0.0000, -0.9067,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.2450,\n",
      "          -0.6598, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3593,  0.9168,  0.5215, -0.7929, -0.0867, -0.1177, -1.0554,\n",
      "          -0.3663,  1.4266,  1.4266,  0.0777, -0.8020,  0.0000, -0.9064,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3013,\n",
      "          -0.5385, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3830,  1.0249,  1.0001, -0.5698, -0.0422, -0.1058, -1.3362,\n",
      "          -0.5239,  1.5127,  1.5127,  1.1348, -0.8020,  0.0000, -0.9061,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3365,\n",
      "          -0.4440, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4179,  0.9796,  0.5944, -0.7124, -0.0309, -0.1058, -1.6171,\n",
      "          -0.5239,  1.5988,  1.5988, -0.6774, -0.8020,  0.0000, -0.9059,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3641,\n",
      "          -0.3491, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4067,  1.0004,  0.6014, -0.6923, -0.0335, -0.1058, -1.6171,\n",
      "          -0.5239,  1.4266,  1.4266, -0.2243, -0.8020,  0.0000, -0.9058,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3839,\n",
      "          -0.2587, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4095,  1.1045,  0.6579, -0.6352, -0.0309, -0.1022, -0.4937,\n",
      "          -0.5239,  1.4266,  1.4266,  0.2287, -0.8020,  0.0000, -0.9056,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3978,\n",
      "          -0.1671, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3928,  1.1514,  0.6790, -0.5749, -0.0309, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3835,  1.3835, -1.5835, -0.8020,  0.0000, -0.9055,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.4056,\n",
      "          -0.0748, -1.1628,  1.1628,  0.0000]]])\n",
      "predicted: tensor([[-0.1766]], device='cuda:0')\n",
      "[464.74]\n",
      "           actual   predicted\n",
      "0      447.000000  435.763301\n",
      "1      431.000001  432.966927\n",
      "2      445.000001  454.378344\n",
      "3      623.999997  666.643335\n",
      "4      480.000000  482.929263\n",
      "...           ...         ...\n",
      "79179  408.000004  411.073241\n",
      "79180  485.000000  492.858339\n",
      "79181  412.000001  416.156864\n",
      "79182  543.000000  536.893778\n",
      "79183  524.999999  528.409710\n",
      "\n",
      "[79184 rows x 2 columns]\n",
      "Score (RMSE): 36.8028\n",
      "Score (MAE): 10.6005\n",
      "Score (ME): -1.7518\n",
      "Score (MAPE): 1.8249%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_42280\\1427812750.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395686, 27) to (401930, 27)\n",
      "training data cutoff:  2023-07-14 03:45:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316477, 20, 25]) torch.Size([316477]) torch.Size([316477, 1])\n",
      "Testing data shape: torch.Size([79441, 20, 25]) torch.Size([79441]) torch.Size([79441, 1])\n",
      "Shuffled Training data shape: torch.Size([316734, 20, 25]) torch.Size([316734]) torch.Size([316734, 1])\n",
      "Shuffled Testing data shape: torch.Size([79184, 20, 25]) torch.Size([79184]) torch.Size([79184, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     36.950000  36.186426\n",
      "1     50.360000  52.797788\n",
      "2     57.393334  61.410086\n",
      "3     31.442500  30.413733\n",
      "4     46.850000  46.967761\n",
      "...         ...        ...\n",
      "1019  40.220000  39.032828\n",
      "1020  45.850000  45.689495\n",
      "1021  36.973333  35.936150\n",
      "1022  32.620000  31.641047\n",
      "1023  21.410000  18.548109\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.0891\n",
      "Epoch 1/25, Validation Loss: 0.0259\n",
      "         actual  predicted\n",
      "0     36.950000  37.214592\n",
      "1     50.360000  52.687134\n",
      "2     57.393334  61.118115\n",
      "3     31.442500  31.246970\n",
      "4     46.850000  47.601921\n",
      "...         ...        ...\n",
      "1019  40.220000  40.195708\n",
      "1020  45.850000  46.778044\n",
      "1021  36.973333  37.023953\n",
      "1022  32.620000  32.113112\n",
      "1023  21.410000  19.297023\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0280\n",
      "Epoch 2/25, Validation Loss: 0.0185\n",
      "         actual  predicted\n",
      "0     36.950000  35.381997\n",
      "1     50.360000  52.200606\n",
      "2     57.393334  61.498387\n",
      "3     31.442500  30.053241\n",
      "4     46.850000  46.766428\n",
      "...         ...        ...\n",
      "1019  40.220000  38.959067\n",
      "1020  45.850000  46.187273\n",
      "1021  36.973333  35.679205\n",
      "1022  32.620000  31.200251\n",
      "1023  21.410000  19.089176\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0200\n",
      "Epoch 3/25, Validation Loss: 0.0342\n",
      "         actual  predicted\n",
      "0     36.950000  36.813388\n",
      "1     50.360000  51.017116\n",
      "2     57.393334  59.334587\n",
      "3     31.442500  32.242713\n",
      "4     46.850000  46.442789\n",
      "...         ...        ...\n",
      "1019  40.220000  39.903712\n",
      "1020  45.850000  46.074755\n",
      "1021  36.973333  36.283207\n",
      "1022  32.620000  32.636899\n",
      "1023  21.410000  20.913067\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0166\n",
      "Epoch 4/25, Validation Loss: 0.0062\n",
      "         actual  predicted\n",
      "0     36.950000  36.993236\n",
      "1     50.360000  51.169057\n",
      "2     57.393334  58.268471\n",
      "3     31.442500  32.384098\n",
      "4     46.850000  46.732332\n",
      "...         ...        ...\n",
      "1019  40.220000  39.980121\n",
      "1020  45.850000  46.402022\n",
      "1021  36.973333  37.051422\n",
      "1022  32.620000  33.175011\n",
      "1023  21.410000  22.023880\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0141\n",
      "Epoch 5/25, Validation Loss: 0.0093\n",
      "         actual  predicted\n",
      "0     36.950000  36.528586\n",
      "1     50.360000  51.568103\n",
      "2     57.393334  58.848575\n",
      "3     31.442500  31.250025\n",
      "4     46.850000  47.343695\n",
      "...         ...        ...\n",
      "1019  40.220000  39.655712\n",
      "1020  45.850000  46.033273\n",
      "1021  36.973333  36.441015\n",
      "1022  32.620000  32.657314\n",
      "1023  21.410000  19.890424\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0122\n",
      "Epoch 6/25, Validation Loss: 0.0065\n",
      "         actual  predicted\n",
      "0     36.950000  36.980942\n",
      "1     50.360000  51.151620\n",
      "2     57.393334  57.655276\n",
      "3     31.442500  31.756242\n",
      "4     46.850000  47.367532\n",
      "...         ...        ...\n",
      "1019  40.220000  39.355986\n",
      "1020  45.850000  46.327802\n",
      "1021  36.973333  36.215809\n",
      "1022  32.620000  32.265678\n",
      "1023  21.410000  20.326506\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0107\n",
      "Epoch 7/25, Validation Loss: 0.0064\n",
      "         actual  predicted\n",
      "0     36.950000  36.150607\n",
      "1     50.360000  50.895473\n",
      "2     57.393334  57.948559\n",
      "3     31.442500  30.776202\n",
      "4     46.850000  46.452897\n",
      "...         ...        ...\n",
      "1019  40.220000  38.816328\n",
      "1020  45.850000  45.241834\n",
      "1021  36.973333  36.050901\n",
      "1022  32.620000  32.202402\n",
      "1023  21.410000  19.436251\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0098\n",
      "Epoch 8/25, Validation Loss: 0.0106\n",
      "         actual  predicted\n",
      "0     36.950000  36.873042\n",
      "1     50.360000  51.458539\n",
      "2     57.393334  57.693456\n",
      "3     31.442500  31.516120\n",
      "4     46.850000  47.104489\n",
      "...         ...        ...\n",
      "1019  40.220000  39.572361\n",
      "1020  45.850000  46.408720\n",
      "1021  36.973333  36.682571\n",
      "1022  32.620000  32.858546\n",
      "1023  21.410000  20.555364\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0087\n",
      "Epoch 9/25, Validation Loss: 0.0044\n",
      "         actual  predicted\n",
      "0     36.950000  36.413401\n",
      "1     50.360000  51.078884\n",
      "2     57.393334  57.605264\n",
      "3     31.442500  30.788278\n",
      "4     46.850000  46.925414\n",
      "...         ...        ...\n",
      "1019  40.220000  39.142526\n",
      "1020  45.850000  46.081801\n",
      "1021  36.973333  36.088955\n",
      "1022  32.620000  32.089962\n",
      "1023  21.410000  20.047766\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0083\n",
      "Epoch 10/25, Validation Loss: 0.0058\n",
      "         actual  predicted\n",
      "0     36.950000  37.287697\n",
      "1     50.360000  51.469535\n",
      "2     57.393334  58.368209\n",
      "3     31.442500  31.788588\n",
      "4     46.850000  47.256217\n",
      "...         ...        ...\n",
      "1019  40.220000  39.571259\n",
      "1020  45.850000  46.066945\n",
      "1021  36.973333  36.468542\n",
      "1022  32.620000  32.726656\n",
      "1023  21.410000  20.380690\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0079\n",
      "Epoch 11/25, Validation Loss: 0.0051\n",
      "         actual  predicted\n",
      "0     36.950000  36.661919\n",
      "1     50.360000  50.678929\n",
      "2     57.393334  57.571215\n",
      "3     31.442500  30.834197\n",
      "4     46.850000  46.739407\n",
      "...         ...        ...\n",
      "1019  40.220000  39.131570\n",
      "1020  45.850000  45.844384\n",
      "1021  36.973333  36.172034\n",
      "1022  32.620000  32.274844\n",
      "1023  21.410000  20.315940\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0079\n",
      "Epoch 12/25, Validation Loss: 0.0054\n",
      "         actual  predicted\n",
      "0     36.950000  36.933782\n",
      "1     50.360000  50.503845\n",
      "2     57.393334  57.256261\n",
      "3     31.442500  31.639112\n",
      "4     46.850000  46.658787\n",
      "...         ...        ...\n",
      "1019  40.220000  39.370900\n",
      "1020  45.850000  45.607883\n",
      "1021  36.973333  36.409586\n",
      "1022  32.620000  32.654469\n",
      "1023  21.410000  20.260783\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0077\n",
      "Epoch 13/25, Validation Loss: 0.0038\n",
      "         actual  predicted\n",
      "0     36.950000  36.970919\n",
      "1     50.360000  50.116918\n",
      "2     57.393334  57.070527\n",
      "3     31.442500  31.586418\n",
      "4     46.850000  46.379316\n",
      "...         ...        ...\n",
      "1019  40.220000  39.284092\n",
      "1020  45.850000  45.808621\n",
      "1021  36.973333  36.380288\n",
      "1022  32.620000  32.602087\n",
      "1023  21.410000  20.429182\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0075\n",
      "Epoch 14/25, Validation Loss: 0.0035\n",
      "         actual  predicted\n",
      "0     36.950000  36.430722\n",
      "1     50.360000  51.128367\n",
      "2     57.393334  58.160919\n",
      "3     31.442500  30.716857\n",
      "4     46.850000  47.156242\n",
      "...         ...        ...\n",
      "1019  40.220000  39.488430\n",
      "1020  45.850000  46.011954\n",
      "1021  36.973333  36.363693\n",
      "1022  32.620000  32.454634\n",
      "1023  21.410000  19.379656\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0072\n",
      "Epoch 15/25, Validation Loss: 0.0078\n",
      "         actual  predicted\n",
      "0     36.950000  37.467004\n",
      "1     50.360000  49.619225\n",
      "2     57.393334  55.845297\n",
      "3     31.442500  32.306898\n",
      "4     46.850000  46.144172\n",
      "...         ...        ...\n",
      "1019  40.220000  39.530817\n",
      "1020  45.850000  45.489419\n",
      "1021  36.973333  36.777018\n",
      "1022  32.620000  33.158450\n",
      "1023  21.410000  21.237216\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0070\n",
      "Epoch 16/25, Validation Loss: 0.0070\n",
      "         actual  predicted\n",
      "0     36.950000  37.125557\n",
      "1     50.360000  52.126578\n",
      "2     57.393334  59.016092\n",
      "3     31.442500  30.904721\n",
      "4     46.850000  47.431612\n",
      "...         ...        ...\n",
      "1019  40.220000  39.403645\n",
      "1020  45.850000  46.251822\n",
      "1021  36.973333  36.441545\n",
      "1022  32.620000  32.373794\n",
      "1023  21.410000  19.525274\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0072\n",
      "Epoch 17/25, Validation Loss: 0.0085\n",
      "         actual  predicted\n",
      "0     36.950000  36.933614\n",
      "1     50.360000  49.884570\n",
      "2     57.393334  55.991572\n",
      "3     31.442500  31.325323\n",
      "4     46.850000  46.214592\n",
      "...         ...        ...\n",
      "1019  40.220000  38.985030\n",
      "1020  45.850000  45.085000\n",
      "1021  36.973333  36.442735\n",
      "1022  32.620000  32.749940\n",
      "1023  21.410000  21.251844\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0067\n",
      "Epoch 18/25, Validation Loss: 0.0055\n",
      "         actual  predicted\n",
      "0     36.950000  36.880560\n",
      "1     50.360000  50.153311\n",
      "2     57.393334  56.396004\n",
      "3     31.442500  31.250520\n",
      "4     46.850000  46.256691\n",
      "...         ...        ...\n",
      "1019  40.220000  39.404067\n",
      "1020  45.850000  45.585573\n",
      "1021  36.973333  36.381844\n",
      "1022  32.620000  32.383898\n",
      "1023  21.410000  20.398537\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0064\n",
      "Epoch 19/25, Validation Loss: 0.0034\n",
      "         actual  predicted\n",
      "0     36.950000  36.922951\n",
      "1     50.360000  50.357909\n",
      "2     57.393334  56.867898\n",
      "3     31.442500  31.310577\n",
      "4     46.850000  46.445580\n",
      "...         ...        ...\n",
      "1019  40.220000  39.390770\n",
      "1020  45.850000  45.719616\n",
      "1021  36.973333  36.484012\n",
      "1022  32.620000  32.579599\n",
      "1023  21.410000  20.608361\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0062\n",
      "Epoch 20/25, Validation Loss: 0.0037\n",
      "         actual  predicted\n",
      "0     36.950000  36.966174\n",
      "1     50.360000  50.457788\n",
      "2     57.393334  56.985303\n",
      "3     31.442500  31.328387\n",
      "4     46.850000  46.684593\n",
      "...         ...        ...\n",
      "1019  40.220000  39.262062\n",
      "1020  45.850000  45.461503\n",
      "1021  36.973333  36.417507\n",
      "1022  32.620000  32.572633\n",
      "1023  21.410000  20.368448\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0062\n",
      "Epoch 21/25, Validation Loss: 0.0034\n",
      "         actual  predicted\n",
      "0     36.950000  37.133509\n",
      "1     50.360000  50.265492\n",
      "2     57.393334  57.084729\n",
      "3     31.442500  31.639041\n",
      "4     46.850000  46.598268\n",
      "...         ...        ...\n",
      "1019  40.220000  39.191948\n",
      "1020  45.850000  45.358064\n",
      "1021  36.973333  36.385126\n",
      "1022  32.620000  32.701565\n",
      "1023  21.410000  20.316776\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0061\n",
      "Epoch 22/25, Validation Loss: 0.0036\n",
      "         actual  predicted\n",
      "0     36.950000  37.065772\n",
      "1     50.360000  50.912559\n",
      "2     57.393334  58.007439\n",
      "3     31.442500  31.287719\n",
      "4     46.850000  46.939615\n",
      "...         ...        ...\n",
      "1019  40.220000  39.477575\n",
      "1020  45.850000  45.833939\n",
      "1021  36.973333  36.534061\n",
      "1022  32.620000  32.848773\n",
      "1023  21.410000  20.591001\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0061\n",
      "Epoch 23/25, Validation Loss: 0.0036\n",
      "         actual  predicted\n",
      "0     36.950000  37.241530\n",
      "1     50.360000  50.373218\n",
      "2     57.393334  56.687956\n",
      "3     31.442500  31.629912\n",
      "4     46.850000  46.606870\n",
      "...         ...        ...\n",
      "1019  40.220000  39.420506\n",
      "1020  45.850000  45.792098\n",
      "1021  36.973333  36.603901\n",
      "1022  32.620000  32.943640\n",
      "1023  21.410000  20.629174\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0060\n",
      "Epoch 24/25, Validation Loss: 0.0033\n",
      "         actual  predicted\n",
      "0     36.950000  37.075432\n",
      "1     50.360000  49.859492\n",
      "2     57.393334  56.232723\n",
      "3     31.442500  31.524145\n",
      "4     46.850000  46.086723\n",
      "...         ...        ...\n",
      "1019  40.220000  39.249393\n",
      "1020  45.850000  45.210099\n",
      "1021  36.973333  36.436780\n",
      "1022  32.620000  32.771118\n",
      "1023  21.410000  20.689918\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0059\n",
      "Epoch 25/25, Validation Loss: 0.0050\n",
      "loading latest dataframe: data/transformer_multivariate_quarter_hour_26f_dataframe_v2.parquet\n",
      "input_data: tensor([[[ 0.4279,  1.6477,  1.0389, -0.6762, -0.1096, -0.1022, -1.5047,\n",
      "          -0.5239, -1.8377, -1.8377, -0.5868,  4.1524,  0.0000, -0.9095,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -0.9383,\n",
      "          -1.0391, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4351,  1.6194,  1.0036, -0.6399, -0.1338, -0.1094, -1.3924,\n",
      "          -0.5239, -1.8721, -1.8721, -0.0431,  4.1524,  0.0000, -0.9094,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -0.9978,\n",
      "          -0.9735, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4424,  1.5911,  0.9683, -0.6037, -0.1580, -0.1165, -1.2801,\n",
      "          -0.5239, -1.9066, -1.9066,  0.5006,  4.1524,  0.0000, -0.9092,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.0572,\n",
      "          -0.9080, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4496,  1.5628,  0.9330, -0.5675, -0.1822, -0.1237, -1.1677,\n",
      "          -0.5239, -1.9410, -1.9410,  1.0442,  4.1524,  0.0000, -0.9091,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.1167,\n",
      "          -0.8424, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4569,  1.5345,  0.8978, -0.5312, -0.2064, -0.1308, -1.0554,\n",
      "          -0.5239, -1.9755, -1.9755,  1.5879,  4.1524,  0.0000, -0.9090,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.1762,\n",
      "          -0.7768, -1.1628,  1.1628,  0.0000],\n",
      "         [-0.6774,  2.1809,  1.0742, -1.1452,  0.4119,  0.0339, -0.4937,\n",
      "          -0.3663,  1.4266,  1.4266,  1.1348, -0.8020,  0.0000, -0.9107,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.7743,\n",
      "          -1.1811, -1.1628,  1.1628,  0.0000],\n",
      "         [-0.2000,  1.6995,  3.1013, -1.1189,  0.0964, -0.0539, -1.0835,\n",
      "          -0.4333,  0.8151,  0.8151, -0.2243, -0.8020,  0.0000, -0.9093,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.8447,\n",
      "          -1.1311, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.0843,  1.2582,  1.9067, -1.0189,  0.0356, -0.0664, -0.6809,\n",
      "          -0.3138,  1.2112,  1.2112,  1.1348, -0.8020,  0.0000, -0.9076,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.9133,\n",
      "          -1.0754, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.1921,  1.1180,  1.5822, -1.0144,  0.0183, -0.0807, -0.4937,\n",
      "          -0.4451,  1.4266,  1.4266,  1.1348, -0.8020,  0.0000, -0.9074,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.1961,\n",
      "          -0.7456, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.2283,  0.9359,  0.8060, -1.0177,  0.0170, -0.0807, -1.6171,\n",
      "          -0.5239, -1.5448, -1.5448,  1.5879, -0.8020,  0.0000, -0.9072,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.0321,\n",
      "          -0.9616, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.2785,  0.8380,  0.7637, -0.9540, -0.0149, -0.0879, -1.6171,\n",
      "          -0.4451,  1.6419,  1.6419, -0.6774, -0.8020,  0.0000, -0.9071,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.0926,\n",
      "          -0.8915, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3259,  0.7599,  0.7355, -0.9506, -0.0229, -0.0950, -1.0554,\n",
      "          -0.2875,  1.4696,  1.4696, -1.1304, -0.8020,  0.0000, -0.9070,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.1484,\n",
      "          -0.8177, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3454,  0.8109,  0.6790, -0.9037, -0.0282, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3404,  1.3404, -0.2243, -0.8020,  0.0000, -0.9068,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3494,\n",
      "          -0.4027, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3426,  0.8172,  0.6367, -0.7594, -0.0296, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3404,  1.3404, -1.5835, -0.8020,  0.0000, -0.9067,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.2450,\n",
      "          -0.6598, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3593,  0.9168,  0.5215, -0.7929, -0.0867, -0.1177, -1.0554,\n",
      "          -0.3663,  1.4266,  1.4266,  0.0777, -0.8020,  0.0000, -0.9064,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3013,\n",
      "          -0.5385, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3830,  1.0249,  1.0001, -0.5698, -0.0422, -0.1058, -1.3362,\n",
      "          -0.5239,  1.5127,  1.5127,  1.1348, -0.8020,  0.0000, -0.9061,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3365,\n",
      "          -0.4440, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4179,  0.9796,  0.5944, -0.7124, -0.0309, -0.1058, -1.6171,\n",
      "          -0.5239,  1.5988,  1.5988, -0.6774, -0.8020,  0.0000, -0.9059,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3641,\n",
      "          -0.3491, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4067,  1.0004,  0.6014, -0.6923, -0.0335, -0.1058, -1.6171,\n",
      "          -0.5239,  1.4266,  1.4266, -0.2243, -0.8020,  0.0000, -0.9058,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3839,\n",
      "          -0.2587, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4095,  1.1045,  0.6579, -0.6352, -0.0309, -0.1022, -0.4937,\n",
      "          -0.5239,  1.4266,  1.4266,  0.2287, -0.8020,  0.0000, -0.9056,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3978,\n",
      "          -0.1671, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3928,  1.1514,  0.6790, -0.5749, -0.0309, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3835,  1.3835, -1.5835, -0.8020,  0.0000, -0.9055,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.4056,\n",
      "          -0.0748, -1.1628,  1.1628,  0.0000]]])\n",
      "predicted: tensor([[1.2813]], device='cuda:0')\n",
      "[47.84]\n",
      "          actual  predicted\n",
      "0      36.950000  37.075432\n",
      "1      50.360000  49.859492\n",
      "2      57.393334  56.232723\n",
      "3      31.442500  31.524145\n",
      "4      46.850000  46.086723\n",
      "...          ...        ...\n",
      "79179  36.757727  36.753282\n",
      "79180  33.520000  33.466458\n",
      "79181  37.880000  37.965119\n",
      "79182  44.635000  44.255793\n",
      "79183  41.200000  40.921059\n",
      "\n",
      "[79184 rows x 2 columns]\n",
      "Score (RMSE): 0.6759\n",
      "Score (MAE): 0.4069\n",
      "Score (ME): 0.2274\n",
      "Score (MAPE): 1.0318%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_42280\\1427812750.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395686, 27) to (401930, 27)\n",
      "training data cutoff:  2023-07-14 03:45:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316477, 20, 25]) torch.Size([316477]) torch.Size([316477, 1])\n",
      "Testing data shape: torch.Size([79441, 20, 25]) torch.Size([79441]) torch.Size([79441, 1])\n",
      "Shuffled Training data shape: torch.Size([316734, 20, 25]) torch.Size([316734]) torch.Size([316734, 1])\n",
      "Shuffled Testing data shape: torch.Size([79184, 20, 25]) torch.Size([79184]) torch.Size([79184, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     22.230000  22.217415\n",
      "1     21.940000  21.900738\n",
      "2     26.840000  26.952173\n",
      "3     19.840000  18.417838\n",
      "4     18.840000  16.818404\n",
      "...         ...        ...\n",
      "1019  25.630000  25.743582\n",
      "1020  29.052222  29.540438\n",
      "1021  25.320000  25.298888\n",
      "1022  20.570000  19.267527\n",
      "1023  24.320000  24.459981\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.0816\n",
      "Epoch 1/25, Validation Loss: 0.0340\n",
      "         actual  predicted\n",
      "0     22.230000  22.239312\n",
      "1     21.940000  21.843627\n",
      "2     26.840000  26.892044\n",
      "3     19.840000  19.246664\n",
      "4     18.840000  17.966701\n",
      "...         ...        ...\n",
      "1019  25.630000  25.906853\n",
      "1020  29.052222  29.274489\n",
      "1021  25.320000  25.443065\n",
      "1022  20.570000  19.711486\n",
      "1023  24.320000  24.310339\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0251\n",
      "Epoch 2/25, Validation Loss: 0.0162\n",
      "         actual  predicted\n",
      "0     22.230000  22.512785\n",
      "1     21.940000  22.298735\n",
      "2     26.840000  26.312048\n",
      "3     19.840000  20.016384\n",
      "4     18.840000  19.196246\n",
      "...         ...        ...\n",
      "1019  25.630000  25.368513\n",
      "1020  29.052222  28.385131\n",
      "1021  25.320000  25.034706\n",
      "1022  20.570000  20.593782\n",
      "1023  24.320000  24.034083\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0184\n",
      "Epoch 3/25, Validation Loss: 0.0161\n",
      "         actual  predicted\n",
      "0     22.230000  22.517918\n",
      "1     21.940000  22.426409\n",
      "2     26.840000  27.384915\n",
      "3     19.840000  19.944030\n",
      "4     18.840000  19.134207\n",
      "...         ...        ...\n",
      "1019  25.630000  26.060323\n",
      "1020  29.052222  29.609921\n",
      "1021  25.320000  25.706772\n",
      "1022  20.570000  20.773625\n",
      "1023  24.320000  24.609279\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0148\n",
      "Epoch 4/25, Validation Loss: 0.0210\n",
      "         actual  predicted\n",
      "0     22.230000  21.989263\n",
      "1     21.940000  21.933241\n",
      "2     26.840000  27.060870\n",
      "3     19.840000  19.598885\n",
      "4     18.840000  18.761083\n",
      "...         ...        ...\n",
      "1019  25.630000  25.670183\n",
      "1020  29.052222  29.283802\n",
      "1021  25.320000  25.388503\n",
      "1022  20.570000  20.203418\n",
      "1023  24.320000  24.113413\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0135\n",
      "Epoch 5/25, Validation Loss: 0.0101\n",
      "         actual  predicted\n",
      "0     22.230000  22.156466\n",
      "1     21.940000  21.994904\n",
      "2     26.840000  27.072919\n",
      "3     19.840000  19.608888\n",
      "4     18.840000  18.712937\n",
      "...         ...        ...\n",
      "1019  25.630000  25.901963\n",
      "1020  29.052222  29.604696\n",
      "1021  25.320000  25.575662\n",
      "1022  20.570000  20.468725\n",
      "1023  24.320000  24.286106\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0114\n",
      "Epoch 6/25, Validation Loss: 0.0105\n",
      "         actual  predicted\n",
      "0     22.230000  22.303735\n",
      "1     21.940000  22.058865\n",
      "2     26.840000  26.571666\n",
      "3     19.840000  19.824318\n",
      "4     18.840000  18.996867\n",
      "...         ...        ...\n",
      "1019  25.630000  25.332338\n",
      "1020  29.052222  28.668001\n",
      "1021  25.320000  24.988947\n",
      "1022  20.570000  20.559129\n",
      "1023  24.320000  23.956261\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0099\n",
      "Epoch 7/25, Validation Loss: 0.0052\n",
      "         actual  predicted\n",
      "0     22.230000  22.141765\n",
      "1     21.940000  21.941793\n",
      "2     26.840000  26.753974\n",
      "3     19.840000  19.477858\n",
      "4     18.840000  18.606382\n",
      "...         ...        ...\n",
      "1019  25.630000  25.391277\n",
      "1020  29.052222  28.933885\n",
      "1021  25.320000  25.144869\n",
      "1022  20.570000  20.150903\n",
      "1023  24.320000  23.928042\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0092\n",
      "Epoch 8/25, Validation Loss: 0.0055\n",
      "         actual  predicted\n",
      "0     22.230000  22.131849\n",
      "1     21.940000  21.805290\n",
      "2     26.840000  26.816675\n",
      "3     19.840000  19.433125\n",
      "4     18.840000  18.588839\n",
      "...         ...        ...\n",
      "1019  25.630000  25.811611\n",
      "1020  29.052222  29.333157\n",
      "1021  25.320000  25.423611\n",
      "1022  20.570000  20.177403\n",
      "1023  24.320000  24.160906\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0090\n",
      "Epoch 9/25, Validation Loss: 0.0072\n",
      "         actual  predicted\n",
      "0     22.230000  22.411521\n",
      "1     21.940000  22.186650\n",
      "2     26.840000  26.978375\n",
      "3     19.840000  20.126410\n",
      "4     18.840000  19.185824\n",
      "...         ...        ...\n",
      "1019  25.630000  25.541957\n",
      "1020  29.052222  29.131002\n",
      "1021  25.320000  25.274008\n",
      "1022  20.570000  20.745342\n",
      "1023  24.320000  24.118860\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0080\n",
      "Epoch 10/25, Validation Loss: 0.0042\n",
      "         actual  predicted\n",
      "0     22.230000  21.943696\n",
      "1     21.940000  21.688974\n",
      "2     26.840000  26.175818\n",
      "3     19.840000  19.614147\n",
      "4     18.840000  18.730710\n",
      "...         ...        ...\n",
      "1019  25.630000  25.455070\n",
      "1020  29.052222  28.790246\n",
      "1021  25.320000  25.075877\n",
      "1022  20.570000  20.415533\n",
      "1023  24.320000  23.968525\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0080\n",
      "Epoch 11/25, Validation Loss: 0.0100\n",
      "         actual  predicted\n",
      "0     22.230000  22.509651\n",
      "1     21.940000  22.372544\n",
      "2     26.840000  27.333422\n",
      "3     19.840000  20.160673\n",
      "4     18.840000  19.328415\n",
      "...         ...        ...\n",
      "1019  25.630000  25.828677\n",
      "1020  29.052222  29.686378\n",
      "1021  25.320000  25.642451\n",
      "1022  20.570000  20.859309\n",
      "1023  24.320000  24.473157\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0084\n",
      "Epoch 12/25, Validation Loss: 0.0213\n",
      "         actual  predicted\n",
      "0     22.230000  22.134389\n",
      "1     21.940000  21.835943\n",
      "2     26.840000  26.540728\n",
      "3     19.840000  19.692021\n",
      "4     18.840000  18.684751\n",
      "...         ...        ...\n",
      "1019  25.630000  25.454071\n",
      "1020  29.052222  28.730895\n",
      "1021  25.320000  25.157496\n",
      "1022  20.570000  20.508155\n",
      "1023  24.320000  24.112511\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0073\n",
      "Epoch 13/25, Validation Loss: 0.0068\n",
      "         actual  predicted\n",
      "0     22.230000  22.504274\n",
      "1     21.940000  22.178500\n",
      "2     26.840000  26.939053\n",
      "3     19.840000  20.048228\n",
      "4     18.840000  19.213639\n",
      "...         ...        ...\n",
      "1019  25.630000  25.732963\n",
      "1020  29.052222  29.264104\n",
      "1021  25.320000  25.423075\n",
      "1022  20.570000  20.593367\n",
      "1023  24.320000  24.252567\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0070\n",
      "Epoch 14/25, Validation Loss: 0.0035\n",
      "         actual  predicted\n",
      "0     22.230000  22.322917\n",
      "1     21.940000  22.059816\n",
      "2     26.840000  26.907970\n",
      "3     19.840000  19.753174\n",
      "4     18.840000  18.951382\n",
      "...         ...        ...\n",
      "1019  25.630000  25.630306\n",
      "1020  29.052222  29.486308\n",
      "1021  25.320000  25.270654\n",
      "1022  20.570000  20.538684\n",
      "1023  24.320000  24.192125\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0067\n",
      "Epoch 15/25, Validation Loss: 0.0050\n",
      "         actual  predicted\n",
      "0     22.230000  22.385895\n",
      "1     21.940000  22.088676\n",
      "2     26.840000  27.049222\n",
      "3     19.840000  19.803728\n",
      "4     18.840000  18.853718\n",
      "...         ...        ...\n",
      "1019  25.630000  25.668493\n",
      "1020  29.052222  29.253022\n",
      "1021  25.320000  25.365570\n",
      "1022  20.570000  20.588215\n",
      "1023  24.320000  24.163840\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0064\n",
      "Epoch 16/25, Validation Loss: 0.0037\n",
      "         actual  predicted\n",
      "0     22.230000  22.376936\n",
      "1     21.940000  22.084443\n",
      "2     26.840000  26.563666\n",
      "3     19.840000  20.103293\n",
      "4     18.840000  19.335892\n",
      "...         ...        ...\n",
      "1019  25.630000  25.503233\n",
      "1020  29.052222  28.833403\n",
      "1021  25.320000  25.210879\n",
      "1022  20.570000  20.662092\n",
      "1023  24.320000  24.127903\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0063\n",
      "Epoch 17/25, Validation Loss: 0.0069\n",
      "         actual  predicted\n",
      "0     22.230000  22.304552\n",
      "1     21.940000  21.974493\n",
      "2     26.840000  27.199332\n",
      "3     19.840000  19.560871\n",
      "4     18.840000  18.556583\n",
      "...         ...        ...\n",
      "1019  25.630000  25.808737\n",
      "1020  29.052222  29.530652\n",
      "1021  25.320000  25.503501\n",
      "1022  20.570000  20.399855\n",
      "1023  24.320000  24.215097\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0062\n",
      "Epoch 18/25, Validation Loss: 0.0093\n",
      "         actual  predicted\n",
      "0     22.230000  22.266260\n",
      "1     21.940000  21.957229\n",
      "2     26.840000  26.776480\n",
      "3     19.840000  19.751398\n",
      "4     18.840000  18.818265\n",
      "...         ...        ...\n",
      "1019  25.630000  25.619395\n",
      "1020  29.052222  29.101667\n",
      "1021  25.320000  25.334190\n",
      "1022  20.570000  20.515595\n",
      "1023  24.320000  24.223609\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0055\n",
      "Epoch 19/25, Validation Loss: 0.0016\n",
      "         actual  predicted\n",
      "0     22.230000  22.306337\n",
      "1     21.940000  22.036106\n",
      "2     26.840000  26.686641\n",
      "3     19.840000  19.834671\n",
      "4     18.840000  18.797183\n",
      "...         ...        ...\n",
      "1019  25.630000  25.398682\n",
      "1020  29.052222  29.069547\n",
      "1021  25.320000  25.166311\n",
      "1022  20.570000  20.524129\n",
      "1023  24.320000  24.021083\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0053\n",
      "Epoch 20/25, Validation Loss: 0.0028\n",
      "         actual  predicted\n",
      "0     22.230000  22.213303\n",
      "1     21.940000  21.960235\n",
      "2     26.840000  26.635564\n",
      "3     19.840000  19.715964\n",
      "4     18.840000  18.787344\n",
      "...         ...        ...\n",
      "1019  25.630000  25.463195\n",
      "1020  29.052222  29.029478\n",
      "1021  25.320000  25.165954\n",
      "1022  20.570000  20.547583\n",
      "1023  24.320000  24.108018\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0053\n",
      "Epoch 21/25, Validation Loss: 0.0026\n",
      "         actual  predicted\n",
      "0     22.230000  22.330080\n",
      "1     21.940000  22.049535\n",
      "2     26.840000  26.886896\n",
      "3     19.840000  19.769666\n",
      "4     18.840000  18.778009\n",
      "...         ...        ...\n",
      "1019  25.630000  25.571053\n",
      "1020  29.052222  29.165232\n",
      "1021  25.320000  25.275219\n",
      "1022  20.570000  20.427350\n",
      "1023  24.320000  24.153343\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0052\n",
      "Epoch 22/25, Validation Loss: 0.0018\n",
      "         actual  predicted\n",
      "0     22.230000  22.299788\n",
      "1     21.940000  22.018905\n",
      "2     26.840000  26.703522\n",
      "3     19.840000  19.845194\n",
      "4     18.840000  18.831024\n",
      "...         ...        ...\n",
      "1019  25.630000  25.515533\n",
      "1020  29.052222  29.049493\n",
      "1021  25.320000  25.241079\n",
      "1022  20.570000  20.545133\n",
      "1023  24.320000  24.174323\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0052\n",
      "Epoch 23/25, Validation Loss: 0.0025\n",
      "         actual  predicted\n",
      "0     22.230000  22.370713\n",
      "1     21.940000  22.048243\n",
      "2     26.840000  26.929592\n",
      "3     19.840000  19.904913\n",
      "4     18.840000  18.839438\n",
      "...         ...        ...\n",
      "1019  25.630000  25.574880\n",
      "1020  29.052222  29.184721\n",
      "1021  25.320000  25.239303\n",
      "1022  20.570000  20.584110\n",
      "1023  24.320000  24.130734\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/transformer_multivariate_quarter_hour_26f_dataframe_v2.parquet\n",
      "input_data: tensor([[[ 0.4279,  1.6477,  1.0389, -0.6762, -0.1096, -0.1022, -1.5047,\n",
      "          -0.5239, -1.8377, -1.8377, -0.5868,  4.1524,  0.0000, -0.9095,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -0.9383,\n",
      "          -1.0391, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4351,  1.6194,  1.0036, -0.6399, -0.1338, -0.1094, -1.3924,\n",
      "          -0.5239, -1.8721, -1.8721, -0.0431,  4.1524,  0.0000, -0.9094,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -0.9978,\n",
      "          -0.9735, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4424,  1.5911,  0.9683, -0.6037, -0.1580, -0.1165, -1.2801,\n",
      "          -0.5239, -1.9066, -1.9066,  0.5006,  4.1524,  0.0000, -0.9092,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.0572,\n",
      "          -0.9080, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4496,  1.5628,  0.9330, -0.5675, -0.1822, -0.1237, -1.1677,\n",
      "          -0.5239, -1.9410, -1.9410,  1.0442,  4.1524,  0.0000, -0.9091,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.1167,\n",
      "          -0.8424, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4569,  1.5345,  0.8978, -0.5312, -0.2064, -0.1308, -1.0554,\n",
      "          -0.5239, -1.9755, -1.9755,  1.5879,  4.1524,  0.0000, -0.9090,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.9707,  0.1445, -1.1762,\n",
      "          -0.7768, -1.1628,  1.1628,  0.0000],\n",
      "         [-0.6774,  2.1809,  1.0742, -1.1452,  0.4119,  0.0339, -0.4937,\n",
      "          -0.3663,  1.4266,  1.4266,  1.1348, -0.8020,  0.0000, -0.9107,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.7743,\n",
      "          -1.1811, -1.1628,  1.1628,  0.0000],\n",
      "         [-0.2000,  1.6995,  3.1013, -1.1189,  0.0964, -0.0539, -1.0835,\n",
      "          -0.4333,  0.8151,  0.8151, -0.2243, -0.8020,  0.0000, -0.9093,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.8447,\n",
      "          -1.1311, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.0843,  1.2582,  1.9067, -1.0189,  0.0356, -0.0664, -0.6809,\n",
      "          -0.3138,  1.2112,  1.2112,  1.1348, -0.8020,  0.0000, -0.9076,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -0.9133,\n",
      "          -1.0754, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.1921,  1.1180,  1.5822, -1.0144,  0.0183, -0.0807, -0.4937,\n",
      "          -0.4451,  1.4266,  1.4266,  1.1348, -0.8020,  0.0000, -0.9074,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.1961,\n",
      "          -0.7456, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.2283,  0.9359,  0.8060, -1.0177,  0.0170, -0.0807, -1.6171,\n",
      "          -0.5239, -1.5448, -1.5448,  1.5879, -0.8020,  0.0000, -0.9072,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.0321,\n",
      "          -0.9616, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.2785,  0.8380,  0.7637, -0.9540, -0.0149, -0.0879, -1.6171,\n",
      "          -0.4451,  1.6419,  1.6419, -0.6774, -0.8020,  0.0000, -0.9071,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.0926,\n",
      "          -0.8915, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3259,  0.7599,  0.7355, -0.9506, -0.0229, -0.0950, -1.0554,\n",
      "          -0.2875,  1.4696,  1.4696, -1.1304, -0.8020,  0.0000, -0.9070,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.1484,\n",
      "          -0.8177, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3454,  0.8109,  0.6790, -0.9037, -0.0282, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3404,  1.3404, -0.2243, -0.8020,  0.0000, -0.9068,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3494,\n",
      "          -0.4027, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3426,  0.8172,  0.6367, -0.7594, -0.0296, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3404,  1.3404, -1.5835, -0.8020,  0.0000, -0.9067,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.2450,\n",
      "          -0.6598, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3593,  0.9168,  0.5215, -0.7929, -0.0867, -0.1177, -1.0554,\n",
      "          -0.3663,  1.4266,  1.4266,  0.0777, -0.8020,  0.0000, -0.9064,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3013,\n",
      "          -0.5385, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3830,  1.0249,  1.0001, -0.5698, -0.0422, -0.1058, -1.3362,\n",
      "          -0.5239,  1.5127,  1.5127,  1.1348, -0.8020,  0.0000, -0.9061,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3365,\n",
      "          -0.4440, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4179,  0.9796,  0.5944, -0.7124, -0.0309, -0.1058, -1.6171,\n",
      "          -0.5239,  1.5988,  1.5988, -0.6774, -0.8020,  0.0000, -0.9059,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3641,\n",
      "          -0.3491, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4067,  1.0004,  0.6014, -0.6923, -0.0335, -0.1058, -1.6171,\n",
      "          -0.5239,  1.4266,  1.4266, -0.2243, -0.8020,  0.0000, -0.9058,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3839,\n",
      "          -0.2587, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.4095,  1.1045,  0.6579, -0.6352, -0.0309, -0.1022, -0.4937,\n",
      "          -0.5239,  1.4266,  1.4266,  0.2287, -0.8020,  0.0000, -0.9056,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.3978,\n",
      "          -0.1671, -1.1628,  1.1628,  0.0000],\n",
      "         [ 0.3928,  1.1514,  0.6790, -0.5749, -0.0309, -0.0986, -1.6171,\n",
      "          -0.5239,  1.3835,  1.3835, -1.5835, -0.8020,  0.0000, -0.9055,\n",
      "           0.0000, -0.4483, -0.0401,  1.4130, -1.7649,  0.8617, -1.4056,\n",
      "          -0.0748, -1.1628,  1.1628,  0.0000]]])\n",
      "predicted: tensor([[0.3422]], device='cuda:0')\n",
      "[25.3]\n",
      "       actual  predicted\n",
      "0       22.23  22.370713\n",
      "1       21.94  22.048243\n",
      "2       26.84  26.929592\n",
      "3       19.84  19.904913\n",
      "4       18.84  18.839438\n",
      "...       ...        ...\n",
      "79179   22.39  22.417416\n",
      "79180   29.02  28.769694\n",
      "79181   18.64  18.907276\n",
      "79182   24.58  24.521260\n",
      "79183   26.53  26.029387\n",
      "\n",
      "[79184 rows x 2 columns]\n",
      "Score (RMSE): 0.1708\n",
      "Score (MAE): 0.1066\n",
      "Score (ME): -0.0653\n",
      "Score (MAPE): 0.4309%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_42280\\1427812750.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "performance_df = pd.read_csv('model_performances.csv')\n",
    "d_model=256\n",
    "nhead=8\n",
    "num_layers=5 \n",
    "dropout_pe=0.15\n",
    "dropout_encoder=0.15\n",
    "batch_size=1024\n",
    "learning_rate=0.00031\n",
    "epochs=25\n",
    "aggregation_level = 'quarter_hour'\n",
    "y_feature = 'CO2'\n",
    "freq = 15 if aggregation_level == 'quarter_hour' else 30 if aggregation_level == 'half_hour' else 60\n",
    "window_size = 5 * 60 // freq\n",
    "for y_feature in ['CO2', 'VOC', 'hum', 'tmp']:\n",
    "    \n",
    "    model, scaler, rmse, mae, me, mape, train_loss, val_loss, n_features = utils.create_multivariate_transformer_model_for_feature(df, d_model=d_model, nhead=nhead, num_layers=num_layers, dropout_pe=dropout_pe, dropout_encoder=dropout_encoder, batch_size=batch_size, learning_rate=learning_rate, epochs=epochs, y_feature=y_feature, aggregation_level=aggregation_level, window_size=window_size)\n",
    "    performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n",
    "    performance_df.to_csv('model_performances.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device_ids = torch.tensor([0])\n",
    "\n",
    "device = utils.get_device()\n",
    "\n",
    "model = utils.load_transformer_model(y_feature=\"CO2\", model_name = 'transformer_multivariate_quarter_hour_26f', device=device)\n",
    "scaler = utils.load_scaler(y_feature=\"CO2\", model_name = 'transformer_multivariate_quarter_hour_26f')\n",
    "\n",
    "real_data = utils.load_dataframe(model_name='transformer_multivariate_quarter_hour_26f')\n",
    "pd.set_option('display.max_columns', None)\n",
    "real_data.iloc[7:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = real_data.iloc[7:27].drop(columns=['device_id', 'date_time_rounded'])\n",
    "\n",
    "data_df_scaled = scaler.transform(data_df)\n",
    "\n",
    "input_data = torch.tensor(data_df_scaled, dtype=torch.float32).view(-1, 20, data_df_scaled.shape[1])\n",
    "print(input_data.shape)\n",
    "device_ids = torch.tensor([0])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_data.to(device), device_ids.to(device))\n",
    "    print(output)\n",
    "\n",
    "    prediction = output.cpu().numpy().reshape(-1, 1)\n",
    "    print(prediction)\n",
    "    print(prediction.shape)\n",
    "    zeroes_for_scaler = np.zeros((prediction.shape[0], 25))\n",
    "\n",
    "    zeroes_for_scaler[:, 2] = prediction  # Insert predicted values into the correct column\n",
    "    print(zeroes_for_scaler)\n",
    "    inverse_transformed = scaler.inverse_transform(zeroes_for_scaler)\n",
    "    predicted_unscaled = inverse_transformed[:, 2].round(2)\n",
    "    print(predicted_unscaled)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
