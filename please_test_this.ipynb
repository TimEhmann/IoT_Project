{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r'C:\\Studium\\Semester_6\\IoT_Project\\data\\hka-aqm-am-combined-RAW.parquet')\n",
    "# drop channel_rssi and channel_index\n",
    "df = df.drop(columns=['channel_rssi', 'channel_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (392384, 27) to (399291, 27)\n",
      "training data cutoff:  2023-07-14 04:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([312832, 20, 25]) torch.Size([312832]) torch.Size([312832, 1])\n",
      "Testing data shape: torch.Size([78609, 20, 25]) torch.Size([78609]) torch.Size([78609, 1])\n",
      "Shuffled Training data shape: torch.Size([313152, 20, 25]) torch.Size([313152]) torch.Size([313152, 1])\n",
      "Shuffled Testing data shape: torch.Size([78289, 20, 25]) torch.Size([78289]) torch.Size([78289, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     394.000001  411.189080\n",
      "1     456.999998  455.986390\n",
      "2     425.000001  416.269727\n",
      "3     573.000002  624.286545\n",
      "4     459.000000  450.363028\n",
      "...          ...         ...\n",
      "1019  466.999999  462.929830\n",
      "1020  434.000001  437.627503\n",
      "1021  429.000001  429.458821\n",
      "1022  533.999998  561.661945\n",
      "1023  407.000003  425.985794\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.1416\n",
      "Epoch 1/25, Validation Loss: 0.0616\n",
      "          actual   predicted\n",
      "0     394.000001  394.673091\n",
      "1     456.999998  438.174765\n",
      "2     425.000001  406.988448\n",
      "3     573.000002  597.319750\n",
      "4     459.000000  437.458457\n",
      "...          ...         ...\n",
      "1019  466.999999  461.145339\n",
      "1020  434.000001  429.668849\n",
      "1021  429.000001  426.967576\n",
      "1022  533.999998  553.935520\n",
      "1023  407.000003  416.063092\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0693\n",
      "Epoch 2/25, Validation Loss: 0.0414\n",
      "          actual   predicted\n",
      "0     394.000001  393.658803\n",
      "1     456.999998  450.547628\n",
      "2     425.000001  408.731654\n",
      "3     573.000002  608.432839\n",
      "4     459.000000  449.446339\n",
      "...          ...         ...\n",
      "1019  466.999999  473.568061\n",
      "1020  434.000001  434.354839\n",
      "1021  429.000001  425.380791\n",
      "1022  533.999998  543.148579\n",
      "1023  407.000003  413.758773\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0622\n",
      "Epoch 3/25, Validation Loss: 0.0413\n",
      "          actual   predicted\n",
      "0     394.000001  400.205234\n",
      "1     456.999998  453.650008\n",
      "2     425.000001  415.101694\n",
      "3     573.000002  586.858448\n",
      "4     459.000000  452.525835\n",
      "...          ...         ...\n",
      "1019  466.999999  459.669984\n",
      "1020  434.000001  431.774543\n",
      "1021  429.000001  424.379946\n",
      "1022  533.999998  532.776104\n",
      "1023  407.000003  406.763371\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0573\n",
      "Epoch 4/25, Validation Loss: 0.0380\n",
      "          actual   predicted\n",
      "0     394.000001  395.920874\n",
      "1     456.999998  447.714641\n",
      "2     425.000001  409.949375\n",
      "3     573.000002  586.222062\n",
      "4     459.000000  447.331141\n",
      "...          ...         ...\n",
      "1019  466.999999  459.636428\n",
      "1020  434.000001  426.769782\n",
      "1021  429.000001  421.743676\n",
      "1022  533.999998  530.129368\n",
      "1023  407.000003  405.033339\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0548\n",
      "Epoch 5/25, Validation Loss: 0.0416\n",
      "          actual   predicted\n",
      "0     394.000001  388.418329\n",
      "1     456.999998  448.584513\n",
      "2     425.000001  411.588653\n",
      "3     573.000002  573.937565\n",
      "4     459.000000  453.333227\n",
      "...          ...         ...\n",
      "1019  466.999999  467.886630\n",
      "1020  434.000001  430.071296\n",
      "1021  429.000001  423.073075\n",
      "1022  533.999998  540.099817\n",
      "1023  407.000003  410.455176\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0533\n",
      "Epoch 6/25, Validation Loss: 0.0344\n",
      "          actual   predicted\n",
      "0     394.000001  391.099686\n",
      "1     456.999998  449.216512\n",
      "2     425.000001  409.896798\n",
      "3     573.000002  562.510169\n",
      "4     459.000000  452.002234\n",
      "...          ...         ...\n",
      "1019  466.999999  466.056472\n",
      "1020  434.000001  433.437341\n",
      "1021  429.000001  424.638402\n",
      "1022  533.999998  531.239142\n",
      "1023  407.000003  409.746848\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0507\n",
      "Epoch 7/25, Validation Loss: 0.0378\n",
      "          actual   predicted\n",
      "0     394.000001  400.039215\n",
      "1     456.999998  452.995034\n",
      "2     425.000001  418.473538\n",
      "3     573.000002  574.597315\n",
      "4     459.000000  454.549830\n",
      "...          ...         ...\n",
      "1019  466.999999  466.140907\n",
      "1020  434.000001  432.723065\n",
      "1021  429.000001  424.973945\n",
      "1022  533.999998  530.774501\n",
      "1023  407.000003  412.480949\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0514\n",
      "Epoch 8/25, Validation Loss: 0.0353\n",
      "          actual   predicted\n",
      "0     394.000001  396.696976\n",
      "1     456.999998  453.810801\n",
      "2     425.000001  419.543392\n",
      "3     573.000002  567.720356\n",
      "4     459.000000  455.729066\n",
      "...          ...         ...\n",
      "1019  466.999999  470.551830\n",
      "1020  434.000001  431.105325\n",
      "1021  429.000001  422.251501\n",
      "1022  533.999998  541.697111\n",
      "1023  407.000003  411.377156\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0478\n",
      "Epoch 9/25, Validation Loss: 0.0359\n",
      "          actual   predicted\n",
      "0     394.000001  398.864127\n",
      "1     456.999998  460.534459\n",
      "2     425.000001  420.649594\n",
      "3     573.000002  568.789898\n",
      "4     459.000000  457.149309\n",
      "...          ...         ...\n",
      "1019  466.999999  466.840112\n",
      "1020  434.000001  432.646857\n",
      "1021  429.000001  425.895512\n",
      "1022  533.999998  529.307015\n",
      "1023  407.000003  412.045115\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0477\n",
      "Epoch 10/25, Validation Loss: 0.0344\n",
      "          actual   predicted\n",
      "0     394.000001  396.200001\n",
      "1     456.999998  454.486377\n",
      "2     425.000001  417.855812\n",
      "3     573.000002  569.507966\n",
      "4     459.000000  454.490184\n",
      "...          ...         ...\n",
      "1019  466.999999  470.505148\n",
      "1020  434.000001  430.760357\n",
      "1021  429.000001  421.711283\n",
      "1022  533.999998  536.526851\n",
      "1023  407.000003  408.299413\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0447\n",
      "Epoch 11/25, Validation Loss: 0.0333\n",
      "          actual   predicted\n",
      "0     394.000001  390.089281\n",
      "1     456.999998  459.214352\n",
      "2     425.000001  413.040342\n",
      "3     573.000002  579.569730\n",
      "4     459.000000  456.710587\n",
      "...          ...         ...\n",
      "1019  466.999999  473.301313\n",
      "1020  434.000001  434.029189\n",
      "1021  429.000001  423.047432\n",
      "1022  533.999998  543.569440\n",
      "1023  407.000003  408.752034\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0440\n",
      "Epoch 12/25, Validation Loss: 0.0340\n",
      "          actual   predicted\n",
      "0     394.000001  393.164361\n",
      "1     456.999998  456.103573\n",
      "2     425.000001  416.092146\n",
      "3     573.000002  575.623737\n",
      "4     459.000000  458.105598\n",
      "...          ...         ...\n",
      "1019  466.999999  470.482703\n",
      "1020  434.000001  431.136799\n",
      "1021  429.000001  420.865410\n",
      "1022  533.999998  536.942167\n",
      "1023  407.000003  406.336922\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0439\n",
      "Epoch 13/25, Validation Loss: 0.0336\n",
      "          actual   predicted\n",
      "0     394.000001  392.052396\n",
      "1     456.999998  453.538138\n",
      "2     425.000001  415.757924\n",
      "3     573.000002  566.043837\n",
      "4     459.000000  451.768597\n",
      "...          ...         ...\n",
      "1019  466.999999  468.214534\n",
      "1020  434.000001  430.960626\n",
      "1021  429.000001  421.988509\n",
      "1022  533.999998  528.761712\n",
      "1023  407.000003  407.227388\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0439\n",
      "Epoch 14/25, Validation Loss: 0.0333\n",
      "          actual   predicted\n",
      "0     394.000001  394.623751\n",
      "1     456.999998  455.344535\n",
      "2     425.000001  416.674025\n",
      "3     573.000002  579.092003\n",
      "4     459.000000  451.885258\n",
      "...          ...         ...\n",
      "1019  466.999999  466.517682\n",
      "1020  434.000001  431.134567\n",
      "1021  429.000001  421.199589\n",
      "1022  533.999998  541.367599\n",
      "1023  407.000003  407.839772\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0437\n",
      "Epoch 15/25, Validation Loss: 0.0381\n",
      "          actual   predicted\n",
      "0     394.000001  393.472179\n",
      "1     456.999998  454.613610\n",
      "2     425.000001  416.593969\n",
      "3     573.000002  575.305802\n",
      "4     459.000000  453.044871\n",
      "...          ...         ...\n",
      "1019  466.999999  468.254436\n",
      "1020  434.000001  430.903023\n",
      "1021  429.000001  421.165592\n",
      "1022  533.999998  539.138629\n",
      "1023  407.000003  408.363029\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4371,  1.6471,  1.2275, -0.7223, -0.1084, -0.1014, -1.5026,\n",
      "          -0.5233, -1.8361, -1.8361, -0.5868,  4.1524,  0.0000, -0.9068,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9400,\n",
      "          -1.0373, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4443,  1.6188,  1.1864, -0.6811, -0.1326, -0.1086, -1.3903,\n",
      "          -0.5233, -1.8705, -1.8705, -0.0432,  4.1524,  0.0000, -0.9066,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9995,\n",
      "          -0.9717, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4516,  1.5905,  1.1454, -0.6400, -0.1568, -0.1157, -1.2780,\n",
      "          -0.5233, -1.9049, -1.9049,  0.5005,  4.1524,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.0590,\n",
      "          -0.9061, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4589,  1.5622,  1.1043, -0.5989, -0.1810, -0.1229, -1.1657,\n",
      "          -0.5233, -1.9393, -1.9393,  1.0442,  4.1524,  0.0000, -0.9064,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1184,\n",
      "          -0.8406, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4661,  1.5339,  1.0632, -0.5578, -0.2052, -0.1300, -1.0534,\n",
      "          -0.5233, -1.9737, -1.9737,  1.5879,  4.1524,  0.0000, -0.9062,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1779,\n",
      "          -0.7750, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.6714,  2.1800,  1.2685, -1.2546,  0.4127,  0.0343, -0.4919,\n",
      "          -0.3653,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9080,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.7760,\n",
      "          -1.1793, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.1926,  1.6989,  3.6279, -1.2248,  0.0975, -0.0532, -1.0815,\n",
      "          -0.4325,  0.8111,  0.8111, -0.2244, -0.7996,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.8464,\n",
      "          -1.1292, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.0926,  1.2578,  2.2376, -1.1112,  0.0367, -0.0657, -0.6791,\n",
      "          -0.3127,  1.2064,  1.2064,  1.1348, -0.7996,  0.0000, -0.9049,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.9151,\n",
      "          -1.0735, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2006,  1.1177,  1.8598, -1.1061,  0.0194, -0.0800, -0.4919,\n",
      "          -0.4443,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9046,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1978,\n",
      "          -0.7437, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2370,  0.9357,  0.9565, -1.1099,  0.0181, -0.0800, -1.6149,\n",
      "          -0.5233, -1.5439, -1.5439,  1.5879, -0.7996,  0.0000, -0.9045,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0338,\n",
      "          -0.9597, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2873,  0.8379,  0.9072, -1.0376, -0.0138, -0.0871, -1.6149,\n",
      "          -0.4443,  1.6362,  1.6362, -0.6774, -0.7996,  0.0000, -0.9044,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0943,\n",
      "          -0.8897, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3348,  0.7598,  0.8744, -1.0338, -0.0218, -0.0943, -1.0534,\n",
      "          -0.2864,  1.4643,  1.4643, -1.1305, -0.7996,  0.0000, -0.9042,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1501,\n",
      "          -0.8158, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3543,  0.8108,  0.8087, -0.9805, -0.0271, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -0.2244, -0.7996,  0.0000, -0.9041,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3511,\n",
      "          -0.4009, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3516,  0.8170,  0.7594, -0.8167, -0.0284, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -1.5836, -0.7996,  0.0000, -0.9039,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.2467,\n",
      "          -0.6580, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3683,  0.9166,  0.6252, -0.8548, -0.0856, -0.1169, -1.0534,\n",
      "          -0.3653,  1.4213,  1.4213,  0.0777, -0.7996,  0.0000, -0.9037,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3030,\n",
      "          -0.5367, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3921,  1.0246,  1.1823, -0.6016, -0.0411, -0.1050, -1.3342,\n",
      "          -0.5233,  1.5072,  1.5072,  1.1348, -0.7996,  0.0000, -0.9033,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3382,\n",
      "          -0.4422, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4270,  0.9794,  0.7101, -0.7634, -0.0298, -0.1050, -1.6149,\n",
      "          -0.5233,  1.5932,  1.5932, -0.6774, -0.7996,  0.0000, -0.9031,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3658,\n",
      "          -0.3473, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4158,  1.0002,  0.7183, -0.7405, -0.0324, -0.1050, -1.6149,\n",
      "          -0.5233,  1.4213,  1.4213, -0.2244, -0.7996,  0.0000, -0.9030,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3856,\n",
      "          -0.2568, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4186,  1.1042,  0.7840, -0.6758, -0.0298, -0.1014, -0.4919,\n",
      "          -0.5233,  1.4213,  1.4213,  0.2287, -0.7996,  0.0000, -0.9029,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3995,\n",
      "          -0.1653, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4019,  1.1510,  0.8087, -0.6073, -0.0298, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3783,  1.3783, -1.5836, -0.7996,  0.0000, -0.9027,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.4074,\n",
      "          -0.0730, -1.1564,  1.1564,  0.0000]]])\n",
      "predicted: tensor([[0.3448]], device='cuda:0')\n",
      "[529.52]\n",
      "           actual   predicted\n",
      "0      394.000001  393.472179\n",
      "1      456.999998  454.613610\n",
      "2      425.000001  416.593969\n",
      "3      573.000002  575.305802\n",
      "4      459.000000  453.044871\n",
      "...           ...         ...\n",
      "78284  431.000001  431.489776\n",
      "78285  694.000001  750.178876\n",
      "78286  415.999999  415.812525\n",
      "78287  626.999997  642.187988\n",
      "78288  475.000000  470.929014\n",
      "\n",
      "[78289 rows x 2 columns]\n",
      "Score (RMSE): 22.6540\n",
      "Score (MAE): 8.6153\n",
      "Score (ME): -1.1923\n",
      "Score (MAPE): 1.5674%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (392384, 27) to (399291, 27)\n",
      "training data cutoff:  2023-07-14 04:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([312832, 20, 25]) torch.Size([312832]) torch.Size([312832, 1])\n",
      "Testing data shape: torch.Size([78609, 20, 25]) torch.Size([78609]) torch.Size([78609, 1])\n",
      "Shuffled Training data shape: torch.Size([313152, 20, 25]) torch.Size([313152]) torch.Size([313152, 1])\n",
      "Shuffled Testing data shape: torch.Size([78289, 20, 25]) torch.Size([78289]) torch.Size([78289, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0     1900.000051  1878.744481\n",
      "1      786.000000   754.463415\n",
      "2      777.000000   784.122832\n",
      "3      617.000002   563.079733\n",
      "4      998.000005   955.015802\n",
      "...           ...          ...\n",
      "1019   605.999999   604.407853\n",
      "1020   632.999998   577.729297\n",
      "1021   539.000008   563.023981\n",
      "1022   807.000001   807.815026\n",
      "1023   595.999997   558.056591\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.1363\n",
      "Epoch 1/25, Validation Loss: 0.0504\n",
      "           actual    predicted\n",
      "0     1900.000051  1910.132609\n",
      "1      786.000000   804.925938\n",
      "2      777.000000   836.789110\n",
      "3      617.000002   570.167600\n",
      "4      998.000005   904.790578\n",
      "...           ...          ...\n",
      "1019   605.999999   611.227085\n",
      "1020   632.999998   591.591209\n",
      "1021   539.000008   548.597504\n",
      "1022   807.000001   838.178438\n",
      "1023   595.999997   559.581230\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0629\n",
      "Epoch 2/25, Validation Loss: 0.0462\n",
      "           actual    predicted\n",
      "0     1900.000051  1878.264030\n",
      "1      786.000000   800.372867\n",
      "2      777.000000   835.970221\n",
      "3      617.000002   587.599200\n",
      "4      998.000005   897.283240\n",
      "...           ...          ...\n",
      "1019   605.999999   618.660233\n",
      "1020   632.999998   581.280516\n",
      "1021   539.000008   548.032564\n",
      "1022   807.000001   804.980427\n",
      "1023   595.999997   551.945108\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0572\n",
      "Epoch 3/25, Validation Loss: 0.0447\n",
      "           actual    predicted\n",
      "0     1900.000051  1898.215731\n",
      "1      786.000000   789.354566\n",
      "2      777.000000   831.759586\n",
      "3      617.000002   563.001974\n",
      "4      998.000005   886.705995\n",
      "...           ...          ...\n",
      "1019   605.999999   622.705381\n",
      "1020   632.999998   590.100065\n",
      "1021   539.000008   549.013893\n",
      "1022   807.000001   836.934865\n",
      "1023   595.999997   550.997446\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0534\n",
      "Epoch 4/25, Validation Loss: 0.0430\n",
      "           actual    predicted\n",
      "0     1900.000051  1897.949649\n",
      "1      786.000000   788.248665\n",
      "2      777.000000   814.205760\n",
      "3      617.000002   573.712981\n",
      "4      998.000005   878.293146\n",
      "...           ...          ...\n",
      "1019   605.999999   607.886103\n",
      "1020   632.999998   581.421102\n",
      "1021   539.000008   542.668945\n",
      "1022   807.000001   829.879942\n",
      "1023   595.999997   541.292436\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0517\n",
      "Epoch 5/25, Validation Loss: 0.0431\n",
      "           actual    predicted\n",
      "0     1900.000051  1955.024845\n",
      "1      786.000000   788.314497\n",
      "2      777.000000   813.370999\n",
      "3      617.000002   581.467431\n",
      "4      998.000005   896.689171\n",
      "...           ...          ...\n",
      "1019   605.999999   610.158340\n",
      "1020   632.999998   584.460675\n",
      "1021   539.000008   542.843167\n",
      "1022   807.000001   828.602161\n",
      "1023   595.999997   545.977434\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0499\n",
      "Epoch 6/25, Validation Loss: 0.0481\n",
      "           actual    predicted\n",
      "0     1900.000051  1898.776947\n",
      "1      786.000000   771.862843\n",
      "2      777.000000   818.580849\n",
      "3      617.000002   566.206879\n",
      "4      998.000005   890.378078\n",
      "...           ...          ...\n",
      "1019   605.999999   608.435282\n",
      "1020   632.999998   583.430324\n",
      "1021   539.000008   543.751902\n",
      "1022   807.000001   828.089474\n",
      "1023   595.999997   539.689945\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0489\n",
      "Epoch 7/25, Validation Loss: 0.0429\n",
      "           actual    predicted\n",
      "0     1900.000051  1903.242129\n",
      "1      786.000000   796.488187\n",
      "2      777.000000   830.276194\n",
      "3      617.000002   579.034950\n",
      "4      998.000005   900.569033\n",
      "...           ...          ...\n",
      "1019   605.999999   618.555084\n",
      "1020   632.999998   587.545279\n",
      "1021   539.000008   554.601522\n",
      "1022   807.000001   836.182973\n",
      "1023   595.999997   552.925873\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0479\n",
      "Epoch 8/25, Validation Loss: 0.0417\n",
      "           actual    predicted\n",
      "0     1900.000051  1808.574756\n",
      "1      786.000000   782.915830\n",
      "2      777.000000   823.509689\n",
      "3      617.000002   569.548988\n",
      "4      998.000005   885.552315\n",
      "...           ...          ...\n",
      "1019   605.999999   613.924648\n",
      "1020   632.999998   582.695200\n",
      "1021   539.000008   531.695018\n",
      "1022   807.000001   819.927557\n",
      "1023   595.999997   532.075641\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0473\n",
      "Epoch 9/25, Validation Loss: 0.0453\n",
      "           actual    predicted\n",
      "0     1900.000051  1898.917313\n",
      "1      786.000000   795.328869\n",
      "2      777.000000   835.876364\n",
      "3      617.000002   570.946064\n",
      "4      998.000005   895.501088\n",
      "...           ...          ...\n",
      "1019   605.999999   621.474607\n",
      "1020   632.999998   591.955069\n",
      "1021   539.000008   549.485595\n",
      "1022   807.000001   831.461455\n",
      "1023   595.999997   540.650770\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0466\n",
      "Epoch 10/25, Validation Loss: 0.0424\n",
      "           actual    predicted\n",
      "0     1900.000051  1918.733031\n",
      "1      786.000000   808.335320\n",
      "2      777.000000   850.450790\n",
      "3      617.000002   568.418324\n",
      "4      998.000005   932.515622\n",
      "...           ...          ...\n",
      "1019   605.999999   614.494692\n",
      "1020   632.999998   589.351965\n",
      "1021   539.000008   535.413196\n",
      "1022   807.000001   862.348401\n",
      "1023   595.999997   530.388066\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0467\n",
      "Epoch 11/25, Validation Loss: 0.0467\n",
      "           actual    predicted\n",
      "0     1900.000051  1899.448477\n",
      "1      786.000000   794.105035\n",
      "2      777.000000   829.050628\n",
      "3      617.000002   574.272302\n",
      "4      998.000005   905.446206\n",
      "...           ...          ...\n",
      "1019   605.999999   617.477245\n",
      "1020   632.999998   586.205834\n",
      "1021   539.000008   544.235719\n",
      "1022   807.000001   834.976785\n",
      "1023   595.999997   540.465826\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0460\n",
      "Epoch 12/25, Validation Loss: 0.0411\n",
      "           actual    predicted\n",
      "0     1900.000051  1905.144402\n",
      "1      786.000000   808.251177\n",
      "2      777.000000   837.970772\n",
      "3      617.000002   574.713405\n",
      "4      998.000005   914.243451\n",
      "...           ...          ...\n",
      "1019   605.999999   620.495375\n",
      "1020   632.999998   580.020739\n",
      "1021   539.000008   526.502240\n",
      "1022   807.000001   846.980687\n",
      "1023   595.999997   524.585332\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0457\n",
      "Epoch 13/25, Validation Loss: 0.0417\n",
      "           actual    predicted\n",
      "0     1900.000051  1873.703809\n",
      "1      786.000000   795.506717\n",
      "2      777.000000   833.362707\n",
      "3      617.000002   574.165181\n",
      "4      998.000005   903.901783\n",
      "...           ...          ...\n",
      "1019   605.999999   622.119544\n",
      "1020   632.999998   587.879948\n",
      "1021   539.000008   543.422304\n",
      "1022   807.000001   831.787998\n",
      "1023   595.999997   540.003437\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0454\n",
      "Epoch 14/25, Validation Loss: 0.0413\n",
      "           actual    predicted\n",
      "0     1900.000051  1854.370046\n",
      "1      786.000000   777.826554\n",
      "2      777.000000   812.313840\n",
      "3      617.000002   586.665703\n",
      "4      998.000005   882.754446\n",
      "...           ...          ...\n",
      "1019   605.999999   621.235445\n",
      "1020   632.999998   590.668261\n",
      "1021   539.000008   554.311726\n",
      "1022   807.000001   832.183365\n",
      "1023   595.999997   550.174859\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0451\n",
      "Epoch 15/25, Validation Loss: 0.0412\n",
      "           actual    predicted\n",
      "0     1900.000051  1888.785632\n",
      "1      786.000000   799.159529\n",
      "2      777.000000   838.756823\n",
      "3      617.000002   581.108423\n",
      "4      998.000005   909.147269\n",
      "...           ...          ...\n",
      "1019   605.999999   623.938486\n",
      "1020   632.999998   591.341717\n",
      "1021   539.000008   547.533878\n",
      "1022   807.000001   856.054220\n",
      "1023   595.999997   548.236712\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0450\n",
      "Epoch 16/25, Validation Loss: 0.0408\n",
      "           actual    predicted\n",
      "0     1900.000051  1833.839473\n",
      "1      786.000000   803.128745\n",
      "2      777.000000   834.879383\n",
      "3      617.000002   586.005473\n",
      "4      998.000005   901.779663\n",
      "...           ...          ...\n",
      "1019   605.999999   631.839831\n",
      "1020   632.999998   592.212606\n",
      "1021   539.000008   542.141929\n",
      "1022   807.000001   850.944437\n",
      "1023   595.999997   547.629949\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0452\n",
      "Epoch 17/25, Validation Loss: 0.0415\n",
      "           actual    predicted\n",
      "0     1900.000051  1832.479884\n",
      "1      786.000000   793.304718\n",
      "2      777.000000   831.359945\n",
      "3      617.000002   586.544385\n",
      "4      998.000005   892.272243\n",
      "...           ...          ...\n",
      "1019   605.999999   628.576607\n",
      "1020   632.999998   600.745709\n",
      "1021   539.000008   559.622019\n",
      "1022   807.000001   847.743312\n",
      "1023   595.999997   555.465712\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0446\n",
      "Epoch 18/25, Validation Loss: 0.0417\n",
      "           actual    predicted\n",
      "0     1900.000051  1881.990065\n",
      "1      786.000000   796.594250\n",
      "2      777.000000   827.003896\n",
      "3      617.000002   589.851403\n",
      "4      998.000005   894.671441\n",
      "...           ...          ...\n",
      "1019   605.999999   635.443734\n",
      "1020   632.999998   601.377624\n",
      "1021   539.000008   558.166201\n",
      "1022   807.000001   836.925677\n",
      "1023   595.999997   562.401660\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0446\n",
      "Epoch 19/25, Validation Loss: 0.0404\n",
      "           actual    predicted\n",
      "0     1900.000051  1905.265861\n",
      "1      786.000000   797.935108\n",
      "2      777.000000   839.543689\n",
      "3      617.000002   571.199876\n",
      "4      998.000005   905.137135\n",
      "...           ...          ...\n",
      "1019   605.999999   619.196531\n",
      "1020   632.999998   588.772578\n",
      "1021   539.000008   536.367948\n",
      "1022   807.000001   851.427596\n",
      "1023   595.999997   539.879583\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0441\n",
      "Epoch 20/25, Validation Loss: 0.0434\n",
      "           actual    predicted\n",
      "0     1900.000051  1904.496537\n",
      "1      786.000000   788.372134\n",
      "2      777.000000   837.342582\n",
      "3      617.000002   584.285045\n",
      "4      998.000005   891.886689\n",
      "...           ...          ...\n",
      "1019   605.999999   622.620015\n",
      "1020   632.999998   593.111793\n",
      "1021   539.000008   548.845478\n",
      "1022   807.000001   863.254037\n",
      "1023   595.999997   548.214298\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0440\n",
      "Epoch 21/25, Validation Loss: 0.0410\n",
      "           actual    predicted\n",
      "0     1900.000051  1914.734276\n",
      "1      786.000000   788.489368\n",
      "2      777.000000   827.382328\n",
      "3      617.000002   580.051855\n",
      "4      998.000005   880.177943\n",
      "...           ...          ...\n",
      "1019   605.999999   628.781256\n",
      "1020   632.999998   596.998919\n",
      "1021   539.000008   547.900868\n",
      "1022   807.000001   839.080157\n",
      "1023   595.999997   553.189139\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0438\n",
      "Epoch 22/25, Validation Loss: 0.0411\n",
      "           actual    predicted\n",
      "0     1900.000051  1852.324652\n",
      "1      786.000000   784.293653\n",
      "2      777.000000   827.425140\n",
      "3      617.000002   578.771168\n",
      "4      998.000005   890.503435\n",
      "...           ...          ...\n",
      "1019   605.999999   616.054187\n",
      "1020   632.999998   587.715212\n",
      "1021   539.000008   537.813341\n",
      "1022   807.000001   828.452509\n",
      "1023   595.999997   541.296850\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0435\n",
      "Epoch 23/25, Validation Loss: 0.0408\n",
      "           actual    predicted\n",
      "0     1900.000051  1853.320365\n",
      "1      786.000000   797.603352\n",
      "2      777.000000   833.646160\n",
      "3      617.000002   575.699570\n",
      "4      998.000005   897.936559\n",
      "...           ...          ...\n",
      "1019   605.999999   618.790645\n",
      "1020   632.999998   586.649237\n",
      "1021   539.000008   539.221310\n",
      "1022   807.000001   842.543342\n",
      "1023   595.999997   539.761208\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0430\n",
      "Epoch 24/25, Validation Loss: 0.0399\n",
      "           actual    predicted\n",
      "0     1900.000051  1904.841005\n",
      "1      786.000000   809.742763\n",
      "2      777.000000   851.444688\n",
      "3      617.000002   575.878754\n",
      "4      998.000005   914.274222\n",
      "...           ...          ...\n",
      "1019   605.999999   621.016584\n",
      "1020   632.999998   587.238470\n",
      "1021   539.000008   538.099724\n",
      "1022   807.000001   846.137299\n",
      "1023   595.999997   541.219185\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0426\n",
      "Epoch 25/25, Validation Loss: 0.0411\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4371,  1.6471,  1.2275, -0.7223, -0.1084, -0.1014, -1.5026,\n",
      "          -0.5233, -1.8361, -1.8361, -0.5868,  4.1524,  0.0000, -0.9068,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9400,\n",
      "          -1.0373, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4443,  1.6188,  1.1864, -0.6811, -0.1326, -0.1086, -1.3903,\n",
      "          -0.5233, -1.8705, -1.8705, -0.0432,  4.1524,  0.0000, -0.9066,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9995,\n",
      "          -0.9717, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4516,  1.5905,  1.1454, -0.6400, -0.1568, -0.1157, -1.2780,\n",
      "          -0.5233, -1.9049, -1.9049,  0.5005,  4.1524,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.0590,\n",
      "          -0.9061, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4589,  1.5622,  1.1043, -0.5989, -0.1810, -0.1229, -1.1657,\n",
      "          -0.5233, -1.9393, -1.9393,  1.0442,  4.1524,  0.0000, -0.9064,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1184,\n",
      "          -0.8406, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4661,  1.5339,  1.0632, -0.5578, -0.2052, -0.1300, -1.0534,\n",
      "          -0.5233, -1.9737, -1.9737,  1.5879,  4.1524,  0.0000, -0.9062,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1779,\n",
      "          -0.7750, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.6714,  2.1800,  1.2685, -1.2546,  0.4127,  0.0343, -0.4919,\n",
      "          -0.3653,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9080,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.7760,\n",
      "          -1.1793, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.1926,  1.6989,  3.6279, -1.2248,  0.0975, -0.0532, -1.0815,\n",
      "          -0.4325,  0.8111,  0.8111, -0.2244, -0.7996,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.8464,\n",
      "          -1.1292, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.0926,  1.2578,  2.2376, -1.1112,  0.0367, -0.0657, -0.6791,\n",
      "          -0.3127,  1.2064,  1.2064,  1.1348, -0.7996,  0.0000, -0.9049,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.9151,\n",
      "          -1.0735, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2006,  1.1177,  1.8598, -1.1061,  0.0194, -0.0800, -0.4919,\n",
      "          -0.4443,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9046,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1978,\n",
      "          -0.7437, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2370,  0.9357,  0.9565, -1.1099,  0.0181, -0.0800, -1.6149,\n",
      "          -0.5233, -1.5439, -1.5439,  1.5879, -0.7996,  0.0000, -0.9045,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0338,\n",
      "          -0.9597, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2873,  0.8379,  0.9072, -1.0376, -0.0138, -0.0871, -1.6149,\n",
      "          -0.4443,  1.6362,  1.6362, -0.6774, -0.7996,  0.0000, -0.9044,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0943,\n",
      "          -0.8897, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3348,  0.7598,  0.8744, -1.0338, -0.0218, -0.0943, -1.0534,\n",
      "          -0.2864,  1.4643,  1.4643, -1.1305, -0.7996,  0.0000, -0.9042,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1501,\n",
      "          -0.8158, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3543,  0.8108,  0.8087, -0.9805, -0.0271, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -0.2244, -0.7996,  0.0000, -0.9041,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3511,\n",
      "          -0.4009, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3516,  0.8170,  0.7594, -0.8167, -0.0284, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -1.5836, -0.7996,  0.0000, -0.9039,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.2467,\n",
      "          -0.6580, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3683,  0.9166,  0.6252, -0.8548, -0.0856, -0.1169, -1.0534,\n",
      "          -0.3653,  1.4213,  1.4213,  0.0777, -0.7996,  0.0000, -0.9037,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3030,\n",
      "          -0.5367, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3921,  1.0246,  1.1823, -0.6016, -0.0411, -0.1050, -1.3342,\n",
      "          -0.5233,  1.5072,  1.5072,  1.1348, -0.7996,  0.0000, -0.9033,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3382,\n",
      "          -0.4422, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4270,  0.9794,  0.7101, -0.7634, -0.0298, -0.1050, -1.6149,\n",
      "          -0.5233,  1.5932,  1.5932, -0.6774, -0.7996,  0.0000, -0.9031,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3658,\n",
      "          -0.3473, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4158,  1.0002,  0.7183, -0.7405, -0.0324, -0.1050, -1.6149,\n",
      "          -0.5233,  1.4213,  1.4213, -0.2244, -0.7996,  0.0000, -0.9030,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3856,\n",
      "          -0.2568, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4186,  1.1042,  0.7840, -0.6758, -0.0298, -0.1014, -0.4919,\n",
      "          -0.5233,  1.4213,  1.4213,  0.2287, -0.7996,  0.0000, -0.9029,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3995,\n",
      "          -0.1653, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4019,  1.1510,  0.8087, -0.6073, -0.0298, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3783,  1.3783, -1.5836, -0.7996,  0.0000, -0.9027,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.4074,\n",
      "          -0.0730, -1.1564,  1.1564,  0.0000]]])\n",
      "predicted: tensor([[-0.5727]], device='cuda:0')\n",
      "[629.07]\n",
      "            actual    predicted\n",
      "0      1900.000051  1904.841005\n",
      "1       786.000000   809.742763\n",
      "2       777.000000   851.444688\n",
      "3       617.000002   575.878754\n",
      "4       998.000005   914.274222\n",
      "...            ...          ...\n",
      "78284   672.999997   675.686385\n",
      "78285  1155.000012  1202.498889\n",
      "78286   927.000007   856.398797\n",
      "78287   614.999999   630.867439\n",
      "78288   663.000004   675.654009\n",
      "\n",
      "[78289 rows x 2 columns]\n",
      "Score (RMSE): 53.2385\n",
      "Score (MAE): 28.7112\n",
      "Score (ME): -0.5221\n",
      "Score (MAPE): 3.6450%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (392384, 27) to (399291, 27)\n",
      "training data cutoff:  2023-07-14 04:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([312832, 20, 25]) torch.Size([312832]) torch.Size([312832, 1])\n",
      "Testing data shape: torch.Size([78609, 20, 25]) torch.Size([78609]) torch.Size([78609, 1])\n",
      "Shuffled Training data shape: torch.Size([313152, 20, 25]) torch.Size([313152]) torch.Size([313152, 1])\n",
      "Shuffled Testing data shape: torch.Size([78289, 20, 25]) torch.Size([78289]) torch.Size([78289, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     57.820001  61.977716\n",
      "1     27.850000  27.477183\n",
      "2     45.914667  46.903631\n",
      "3     26.320000  26.017969\n",
      "4     34.360000  33.980632\n",
      "...         ...        ...\n",
      "1019  31.580000  31.934425\n",
      "1020  20.920000  21.202254\n",
      "1021  34.320000  33.929178\n",
      "1022  33.020000  32.750441\n",
      "1023  58.410000  63.333950\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.0898\n",
      "Epoch 1/25, Validation Loss: 0.0200\n",
      "         actual  predicted\n",
      "0     57.820001  61.202358\n",
      "1     27.850000  27.841096\n",
      "2     45.914667  46.934723\n",
      "3     26.320000  25.202586\n",
      "4     34.360000  34.868170\n",
      "...         ...        ...\n",
      "1019  31.580000  31.576506\n",
      "1020  20.920000  18.991126\n",
      "1021  34.320000  34.002178\n",
      "1022  33.020000  32.601061\n",
      "1023  58.410000  61.795367\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0273\n",
      "Epoch 2/25, Validation Loss: 0.0146\n",
      "         actual  predicted\n",
      "0     57.820001  60.828492\n",
      "1     27.850000  27.424302\n",
      "2     45.914667  46.045285\n",
      "3     26.320000  25.600586\n",
      "4     34.360000  34.275476\n",
      "...         ...        ...\n",
      "1019  31.580000  31.621353\n",
      "1020  20.920000  19.221236\n",
      "1021  34.320000  34.150731\n",
      "1022  33.020000  32.976106\n",
      "1023  58.410000  60.388021\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0194\n",
      "Epoch 3/25, Validation Loss: 0.0060\n",
      "         actual  predicted\n",
      "0     57.820001  59.095231\n",
      "1     27.850000  27.361191\n",
      "2     45.914667  45.922111\n",
      "3     26.320000  26.036993\n",
      "4     34.360000  34.129542\n",
      "...         ...        ...\n",
      "1019  31.580000  31.999883\n",
      "1020  20.920000  19.370189\n",
      "1021  34.320000  34.121080\n",
      "1022  33.020000  32.841794\n",
      "1023  58.410000  60.961426\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0153\n",
      "Epoch 4/25, Validation Loss: 0.0041\n",
      "         actual  predicted\n",
      "0     57.820001  60.394172\n",
      "1     27.850000  28.078887\n",
      "2     45.914667  46.823990\n",
      "3     26.320000  26.674559\n",
      "4     34.360000  34.633343\n",
      "...         ...        ...\n",
      "1019  31.580000  31.980552\n",
      "1020  20.920000  21.083679\n",
      "1021  34.320000  34.512888\n",
      "1022  33.020000  33.263975\n",
      "1023  58.410000  60.797595\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0132\n",
      "Epoch 5/25, Validation Loss: 0.0056\n",
      "         actual  predicted\n",
      "0     57.820001  59.548046\n",
      "1     27.850000  27.903123\n",
      "2     45.914667  46.175658\n",
      "3     26.320000  26.483419\n",
      "4     34.360000  34.449254\n",
      "...         ...        ...\n",
      "1019  31.580000  31.501024\n",
      "1020  20.920000  20.379326\n",
      "1021  34.320000  33.933406\n",
      "1022  33.020000  32.771929\n",
      "1023  58.410000  60.271738\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0114\n",
      "Epoch 6/25, Validation Loss: 0.0040\n",
      "         actual  predicted\n",
      "0     57.820001  59.321264\n",
      "1     27.850000  27.630832\n",
      "2     45.914667  46.432713\n",
      "3     26.320000  26.184049\n",
      "4     34.360000  34.823683\n",
      "...         ...        ...\n",
      "1019  31.580000  31.204441\n",
      "1020  20.920000  19.570383\n",
      "1021  34.320000  34.068241\n",
      "1022  33.020000  32.722917\n",
      "1023  58.410000  60.654143\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0106\n",
      "Epoch 7/25, Validation Loss: 0.0047\n",
      "         actual  predicted\n",
      "0     57.820001  58.524771\n",
      "1     27.850000  27.878025\n",
      "2     45.914667  46.134930\n",
      "3     26.320000  26.522691\n",
      "4     34.360000  34.271980\n",
      "...         ...        ...\n",
      "1019  31.580000  31.272583\n",
      "1020  20.920000  20.747553\n",
      "1021  34.320000  33.802581\n",
      "1022  33.020000  32.737986\n",
      "1023  58.410000  59.426128\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0097\n",
      "Epoch 8/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     57.820001  59.560008\n",
      "1     27.850000  27.698809\n",
      "2     45.914667  46.488406\n",
      "3     26.320000  26.192973\n",
      "4     34.360000  34.799725\n",
      "...         ...        ...\n",
      "1019  31.580000  31.795604\n",
      "1020  20.920000  21.020516\n",
      "1021  34.320000  34.406826\n",
      "1022  33.020000  33.119229\n",
      "1023  58.410000  59.660784\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0088\n",
      "Epoch 9/25, Validation Loss: 0.0031\n",
      "         actual  predicted\n",
      "0     57.820001  59.874046\n",
      "1     27.850000  27.601184\n",
      "2     45.914667  46.621492\n",
      "3     26.320000  25.902490\n",
      "4     34.360000  34.307112\n",
      "...         ...        ...\n",
      "1019  31.580000  31.897020\n",
      "1020  20.920000  19.662247\n",
      "1021  34.320000  34.634485\n",
      "1022  33.020000  33.176075\n",
      "1023  58.410000  59.027410\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0084\n",
      "Epoch 10/25, Validation Loss: 0.0045\n",
      "         actual  predicted\n",
      "0     57.820001  58.964843\n",
      "1     27.850000  27.623453\n",
      "2     45.914667  45.947626\n",
      "3     26.320000  26.146660\n",
      "4     34.360000  34.335130\n",
      "...         ...        ...\n",
      "1019  31.580000  31.682254\n",
      "1020  20.920000  20.563989\n",
      "1021  34.320000  34.111791\n",
      "1022  33.020000  32.990503\n",
      "1023  58.410000  58.796768\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0080\n",
      "Epoch 11/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     57.820001  58.189380\n",
      "1     27.850000  28.144920\n",
      "2     45.914667  46.112845\n",
      "3     26.320000  26.599188\n",
      "4     34.360000  34.531242\n",
      "...         ...        ...\n",
      "1019  31.580000  31.829577\n",
      "1020  20.920000  20.800632\n",
      "1021  34.320000  34.183508\n",
      "1022  33.020000  33.000754\n",
      "1023  58.410000  58.580472\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0076\n",
      "Epoch 12/25, Validation Loss: 0.0024\n",
      "         actual  predicted\n",
      "0     57.820001  58.999251\n",
      "1     27.850000  27.587946\n",
      "2     45.914667  45.956202\n",
      "3     26.320000  25.857612\n",
      "4     34.360000  34.398151\n",
      "...         ...        ...\n",
      "1019  31.580000  31.479134\n",
      "1020  20.920000  19.831505\n",
      "1021  34.320000  34.117891\n",
      "1022  33.020000  33.035347\n",
      "1023  58.410000  59.857419\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0075\n",
      "Epoch 13/25, Validation Loss: 0.0033\n",
      "         actual  predicted\n",
      "0     57.820001  58.840620\n",
      "1     27.850000  27.424030\n",
      "2     45.914667  46.261439\n",
      "3     26.320000  25.753100\n",
      "4     34.360000  34.372613\n",
      "...         ...        ...\n",
      "1019  31.580000  31.811405\n",
      "1020  20.920000  20.166595\n",
      "1021  34.320000  34.304521\n",
      "1022  33.020000  33.238931\n",
      "1023  58.410000  58.827352\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0070\n",
      "Epoch 14/25, Validation Loss: 0.0035\n",
      "         actual  predicted\n",
      "0     57.820001  58.535658\n",
      "1     27.850000  27.970172\n",
      "2     45.914667  46.383264\n",
      "3     26.320000  26.330909\n",
      "4     34.360000  34.620871\n",
      "...         ...        ...\n",
      "1019  31.580000  31.935473\n",
      "1020  20.920000  20.399602\n",
      "1021  34.320000  34.674074\n",
      "1022  33.020000  33.646332\n",
      "1023  58.410000  58.719022\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0068\n",
      "Epoch 15/25, Validation Loss: 0.0025\n",
      "         actual  predicted\n",
      "0     57.820001  59.163600\n",
      "1     27.850000  27.676081\n",
      "2     45.914667  46.698808\n",
      "3     26.320000  25.972458\n",
      "4     34.360000  34.711316\n",
      "...         ...        ...\n",
      "1019  31.580000  31.694722\n",
      "1020  20.920000  20.490584\n",
      "1021  34.320000  34.367567\n",
      "1022  33.020000  33.296675\n",
      "1023  58.410000  58.886809\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0067\n",
      "Epoch 16/25, Validation Loss: 0.0026\n",
      "         actual  predicted\n",
      "0     57.820001  59.292814\n",
      "1     27.850000  28.037820\n",
      "2     45.914667  46.419913\n",
      "3     26.320000  26.265268\n",
      "4     34.360000  34.691180\n",
      "...         ...        ...\n",
      "1019  31.580000  31.729121\n",
      "1020  20.920000  20.502111\n",
      "1021  34.320000  34.243853\n",
      "1022  33.020000  33.166281\n",
      "1023  58.410000  59.856410\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4371,  1.6471,  1.2275, -0.7223, -0.1084, -0.1014, -1.5026,\n",
      "          -0.5233, -1.8361, -1.8361, -0.5868,  4.1524,  0.0000, -0.9068,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9400,\n",
      "          -1.0373, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4443,  1.6188,  1.1864, -0.6811, -0.1326, -0.1086, -1.3903,\n",
      "          -0.5233, -1.8705, -1.8705, -0.0432,  4.1524,  0.0000, -0.9066,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9995,\n",
      "          -0.9717, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4516,  1.5905,  1.1454, -0.6400, -0.1568, -0.1157, -1.2780,\n",
      "          -0.5233, -1.9049, -1.9049,  0.5005,  4.1524,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.0590,\n",
      "          -0.9061, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4589,  1.5622,  1.1043, -0.5989, -0.1810, -0.1229, -1.1657,\n",
      "          -0.5233, -1.9393, -1.9393,  1.0442,  4.1524,  0.0000, -0.9064,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1184,\n",
      "          -0.8406, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4661,  1.5339,  1.0632, -0.5578, -0.2052, -0.1300, -1.0534,\n",
      "          -0.5233, -1.9737, -1.9737,  1.5879,  4.1524,  0.0000, -0.9062,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1779,\n",
      "          -0.7750, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.6714,  2.1800,  1.2685, -1.2546,  0.4127,  0.0343, -0.4919,\n",
      "          -0.3653,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9080,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.7760,\n",
      "          -1.1793, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.1926,  1.6989,  3.6279, -1.2248,  0.0975, -0.0532, -1.0815,\n",
      "          -0.4325,  0.8111,  0.8111, -0.2244, -0.7996,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.8464,\n",
      "          -1.1292, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.0926,  1.2578,  2.2376, -1.1112,  0.0367, -0.0657, -0.6791,\n",
      "          -0.3127,  1.2064,  1.2064,  1.1348, -0.7996,  0.0000, -0.9049,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.9151,\n",
      "          -1.0735, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2006,  1.1177,  1.8598, -1.1061,  0.0194, -0.0800, -0.4919,\n",
      "          -0.4443,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9046,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1978,\n",
      "          -0.7437, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2370,  0.9357,  0.9565, -1.1099,  0.0181, -0.0800, -1.6149,\n",
      "          -0.5233, -1.5439, -1.5439,  1.5879, -0.7996,  0.0000, -0.9045,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0338,\n",
      "          -0.9597, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2873,  0.8379,  0.9072, -1.0376, -0.0138, -0.0871, -1.6149,\n",
      "          -0.4443,  1.6362,  1.6362, -0.6774, -0.7996,  0.0000, -0.9044,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0943,\n",
      "          -0.8897, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3348,  0.7598,  0.8744, -1.0338, -0.0218, -0.0943, -1.0534,\n",
      "          -0.2864,  1.4643,  1.4643, -1.1305, -0.7996,  0.0000, -0.9042,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1501,\n",
      "          -0.8158, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3543,  0.8108,  0.8087, -0.9805, -0.0271, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -0.2244, -0.7996,  0.0000, -0.9041,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3511,\n",
      "          -0.4009, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3516,  0.8170,  0.7594, -0.8167, -0.0284, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -1.5836, -0.7996,  0.0000, -0.9039,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.2467,\n",
      "          -0.6580, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3683,  0.9166,  0.6252, -0.8548, -0.0856, -0.1169, -1.0534,\n",
      "          -0.3653,  1.4213,  1.4213,  0.0777, -0.7996,  0.0000, -0.9037,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3030,\n",
      "          -0.5367, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3921,  1.0246,  1.1823, -0.6016, -0.0411, -0.1050, -1.3342,\n",
      "          -0.5233,  1.5072,  1.5072,  1.1348, -0.7996,  0.0000, -0.9033,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3382,\n",
      "          -0.4422, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4270,  0.9794,  0.7101, -0.7634, -0.0298, -0.1050, -1.6149,\n",
      "          -0.5233,  1.5932,  1.5932, -0.6774, -0.7996,  0.0000, -0.9031,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3658,\n",
      "          -0.3473, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4158,  1.0002,  0.7183, -0.7405, -0.0324, -0.1050, -1.6149,\n",
      "          -0.5233,  1.4213,  1.4213, -0.2244, -0.7996,  0.0000, -0.9030,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3856,\n",
      "          -0.2568, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4186,  1.1042,  0.7840, -0.6758, -0.0298, -0.1014, -0.4919,\n",
      "          -0.5233,  1.4213,  1.4213,  0.2287, -0.7996,  0.0000, -0.9029,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3995,\n",
      "          -0.1653, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4019,  1.1510,  0.8087, -0.6073, -0.0298, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3783,  1.3783, -1.5836, -0.7996,  0.0000, -0.9027,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.4074,\n",
      "          -0.0730, -1.1564,  1.1564,  0.0000]]])\n",
      "predicted: tensor([[1.4049]], device='cuda:0')\n",
      "[49.03]\n",
      "          actual  predicted\n",
      "0      57.820001  59.292814\n",
      "1      27.850000  28.037820\n",
      "2      45.914667  46.419913\n",
      "3      26.320000  26.265268\n",
      "4      34.360000  34.691180\n",
      "...          ...        ...\n",
      "78284  42.440000  42.447849\n",
      "78285  28.740000  28.357285\n",
      "78286  31.340000  31.039805\n",
      "78287  45.100000  44.944238\n",
      "78288  42.080000  42.045527\n",
      "\n",
      "[78289 rows x 2 columns]\n",
      "Score (RMSE): 0.4775\n",
      "Score (MAE): 0.2668\n",
      "Score (ME): -0.0950\n",
      "Score (MAPE): 0.7316%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (392384, 27) to (399291, 27)\n",
      "training data cutoff:  2023-07-14 04:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([312832, 20, 25]) torch.Size([312832]) torch.Size([312832, 1])\n",
      "Testing data shape: torch.Size([78609, 20, 25]) torch.Size([78609]) torch.Size([78609, 1])\n",
      "Shuffled Training data shape: torch.Size([313152, 20, 25]) torch.Size([313152]) torch.Size([313152, 1])\n",
      "Shuffled Testing data shape: torch.Size([78289, 20, 25]) torch.Size([78289]) torch.Size([78289, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual  predicted\n",
      "0      19.01  18.479597\n",
      "1      15.24  14.287356\n",
      "2      22.86  23.273346\n",
      "3      23.45  23.645323\n",
      "4      24.79  24.706863\n",
      "...      ...        ...\n",
      "1019   30.58  31.337429\n",
      "1020   16.87  14.947445\n",
      "1021   22.22  22.240789\n",
      "1022   29.30  29.799708\n",
      "1023   25.47  25.010113\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.0888\n",
      "Epoch 1/25, Validation Loss: 0.0300\n",
      "      actual  predicted\n",
      "0      19.01  18.694281\n",
      "1      15.24  14.697904\n",
      "2      22.86  23.338796\n",
      "3      23.45  23.568480\n",
      "4      24.79  24.937646\n",
      "...      ...        ...\n",
      "1019   30.58  31.027479\n",
      "1020   16.87  16.035231\n",
      "1021   22.22  22.359989\n",
      "1022   29.30  29.644361\n",
      "1023   25.47  24.824317\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0279\n",
      "Epoch 2/25, Validation Loss: 0.0099\n",
      "      actual  predicted\n",
      "0      19.01  18.614685\n",
      "1      15.24  14.457840\n",
      "2      22.86  22.868010\n",
      "3      23.45  23.119414\n",
      "4      24.79  24.656452\n",
      "...      ...        ...\n",
      "1019   30.58  30.561405\n",
      "1020   16.87  15.574562\n",
      "1021   22.22  21.933442\n",
      "1022   29.30  29.098759\n",
      "1023   25.47  24.639016\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0202\n",
      "Epoch 3/25, Validation Loss: 0.0090\n",
      "      actual  predicted\n",
      "0      19.01  18.633613\n",
      "1      15.24  14.459504\n",
      "2      22.86  23.044620\n",
      "3      23.45  23.371742\n",
      "4      24.79  24.748271\n",
      "...      ...        ...\n",
      "1019   30.58  31.065986\n",
      "1020   16.87  15.790472\n",
      "1021   22.22  22.046044\n",
      "1022   29.30  29.530090\n",
      "1023   25.47  24.679417\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0164\n",
      "Epoch 4/25, Validation Loss: 0.0111\n",
      "      actual  predicted\n",
      "0      19.01  18.780653\n",
      "1      15.24  14.703742\n",
      "2      22.86  22.994050\n",
      "3      23.45  23.323378\n",
      "4      24.79  24.638725\n",
      "...      ...        ...\n",
      "1019   30.58  30.797244\n",
      "1020   16.87  15.818081\n",
      "1021   22.22  22.188581\n",
      "1022   29.30  29.491208\n",
      "1023   25.47  24.859313\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0138\n",
      "Epoch 5/25, Validation Loss: 0.0082\n",
      "      actual  predicted\n",
      "0      19.01  18.690257\n",
      "1      15.24  14.293978\n",
      "2      22.86  22.757270\n",
      "3      23.45  23.227571\n",
      "4      24.79  24.716398\n",
      "...      ...        ...\n",
      "1019   30.58  30.901018\n",
      "1020   16.87  15.895976\n",
      "1021   22.22  22.069021\n",
      "1022   29.30  29.521535\n",
      "1023   25.47  24.754582\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0116\n",
      "Epoch 6/25, Validation Loss: 0.0065\n",
      "      actual  predicted\n",
      "0      19.01  18.976105\n",
      "1      15.24  14.581499\n",
      "2      22.86  22.954360\n",
      "3      23.45  23.408639\n",
      "4      24.79  24.845675\n",
      "...      ...        ...\n",
      "1019   30.58  30.609147\n",
      "1020   16.87  16.144346\n",
      "1021   22.22  22.184743\n",
      "1022   29.30  29.482463\n",
      "1023   25.47  24.923766\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0106\n",
      "Epoch 7/25, Validation Loss: 0.0050\n",
      "      actual  predicted\n",
      "0      19.01  18.931549\n",
      "1      15.24  14.635820\n",
      "2      22.86  22.873113\n",
      "3      23.45  23.384018\n",
      "4      24.79  24.746891\n",
      "...      ...        ...\n",
      "1019   30.58  30.455649\n",
      "1020   16.87  15.956650\n",
      "1021   22.22  22.240307\n",
      "1022   29.30  29.156397\n",
      "1023   25.47  24.757308\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0097\n",
      "Epoch 8/25, Validation Loss: 0.0027\n",
      "      actual  predicted\n",
      "0      19.01  18.809894\n",
      "1      15.24  14.494418\n",
      "2      22.86  22.800999\n",
      "3      23.45  23.253039\n",
      "4      24.79  24.717672\n",
      "...      ...        ...\n",
      "1019   30.58  30.629896\n",
      "1020   16.87  16.043194\n",
      "1021   22.22  22.255301\n",
      "1022   29.30  29.334121\n",
      "1023   25.47  24.984384\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0088\n",
      "Epoch 9/25, Validation Loss: 0.0033\n",
      "      actual  predicted\n",
      "0      19.01  18.922714\n",
      "1      15.24  14.588100\n",
      "2      22.86  22.824259\n",
      "3      23.45  23.408433\n",
      "4      24.79  24.884054\n",
      "...      ...        ...\n",
      "1019   30.58  30.518935\n",
      "1020   16.87  16.476493\n",
      "1021   22.22  22.132245\n",
      "1022   29.30  29.277991\n",
      "1023   25.47  24.940903\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0085\n",
      "Epoch 10/25, Validation Loss: 0.0030\n",
      "      actual  predicted\n",
      "0      19.01  18.942645\n",
      "1      15.24  14.628766\n",
      "2      22.86  22.774792\n",
      "3      23.45  23.338120\n",
      "4      24.79  24.761208\n",
      "...      ...        ...\n",
      "1019   30.58  30.618104\n",
      "1020   16.87  16.012279\n",
      "1021   22.22  22.130111\n",
      "1022   29.30  29.240239\n",
      "1023   25.47  24.890060\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0079\n",
      "Epoch 11/25, Validation Loss: 0.0032\n",
      "      actual  predicted\n",
      "0      19.01  18.832655\n",
      "1      15.24  14.635784\n",
      "2      22.86  22.863122\n",
      "3      23.45  23.401466\n",
      "4      24.79  24.804778\n",
      "...      ...        ...\n",
      "1019   30.58  30.719843\n",
      "1020   16.87  16.122697\n",
      "1021   22.22  22.354060\n",
      "1022   29.30  29.408445\n",
      "1023   25.47  24.935351\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0081\n",
      "Epoch 12/25, Validation Loss: 0.0040\n",
      "      actual  predicted\n",
      "0      19.01  18.814681\n",
      "1      15.24  14.767428\n",
      "2      22.86  22.775959\n",
      "3      23.45  23.310498\n",
      "4      24.79  24.758980\n",
      "...      ...        ...\n",
      "1019   30.58  30.821583\n",
      "1020   16.87  16.337166\n",
      "1021   22.22  22.188801\n",
      "1022   29.30  29.350509\n",
      "1023   25.47  24.780049\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4371,  1.6471,  1.2275, -0.7223, -0.1084, -0.1014, -1.5026,\n",
      "          -0.5233, -1.8361, -1.8361, -0.5868,  4.1524,  0.0000, -0.9068,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9400,\n",
      "          -1.0373, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4443,  1.6188,  1.1864, -0.6811, -0.1326, -0.1086, -1.3903,\n",
      "          -0.5233, -1.8705, -1.8705, -0.0432,  4.1524,  0.0000, -0.9066,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9995,\n",
      "          -0.9717, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4516,  1.5905,  1.1454, -0.6400, -0.1568, -0.1157, -1.2780,\n",
      "          -0.5233, -1.9049, -1.9049,  0.5005,  4.1524,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.0590,\n",
      "          -0.9061, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4589,  1.5622,  1.1043, -0.5989, -0.1810, -0.1229, -1.1657,\n",
      "          -0.5233, -1.9393, -1.9393,  1.0442,  4.1524,  0.0000, -0.9064,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1184,\n",
      "          -0.8406, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4661,  1.5339,  1.0632, -0.5578, -0.2052, -0.1300, -1.0534,\n",
      "          -0.5233, -1.9737, -1.9737,  1.5879,  4.1524,  0.0000, -0.9062,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1779,\n",
      "          -0.7750, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.6714,  2.1800,  1.2685, -1.2546,  0.4127,  0.0343, -0.4919,\n",
      "          -0.3653,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9080,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.7760,\n",
      "          -1.1793, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.1926,  1.6989,  3.6279, -1.2248,  0.0975, -0.0532, -1.0815,\n",
      "          -0.4325,  0.8111,  0.8111, -0.2244, -0.7996,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.8464,\n",
      "          -1.1292, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.0926,  1.2578,  2.2376, -1.1112,  0.0367, -0.0657, -0.6791,\n",
      "          -0.3127,  1.2064,  1.2064,  1.1348, -0.7996,  0.0000, -0.9049,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.9151,\n",
      "          -1.0735, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2006,  1.1177,  1.8598, -1.1061,  0.0194, -0.0800, -0.4919,\n",
      "          -0.4443,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9046,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1978,\n",
      "          -0.7437, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2370,  0.9357,  0.9565, -1.1099,  0.0181, -0.0800, -1.6149,\n",
      "          -0.5233, -1.5439, -1.5439,  1.5879, -0.7996,  0.0000, -0.9045,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0338,\n",
      "          -0.9597, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2873,  0.8379,  0.9072, -1.0376, -0.0138, -0.0871, -1.6149,\n",
      "          -0.4443,  1.6362,  1.6362, -0.6774, -0.7996,  0.0000, -0.9044,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0943,\n",
      "          -0.8897, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3348,  0.7598,  0.8744, -1.0338, -0.0218, -0.0943, -1.0534,\n",
      "          -0.2864,  1.4643,  1.4643, -1.1305, -0.7996,  0.0000, -0.9042,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1501,\n",
      "          -0.8158, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3543,  0.8108,  0.8087, -0.9805, -0.0271, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -0.2244, -0.7996,  0.0000, -0.9041,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3511,\n",
      "          -0.4009, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3516,  0.8170,  0.7594, -0.8167, -0.0284, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -1.5836, -0.7996,  0.0000, -0.9039,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.2467,\n",
      "          -0.6580, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3683,  0.9166,  0.6252, -0.8548, -0.0856, -0.1169, -1.0534,\n",
      "          -0.3653,  1.4213,  1.4213,  0.0777, -0.7996,  0.0000, -0.9037,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3030,\n",
      "          -0.5367, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3921,  1.0246,  1.1823, -0.6016, -0.0411, -0.1050, -1.3342,\n",
      "          -0.5233,  1.5072,  1.5072,  1.1348, -0.7996,  0.0000, -0.9033,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3382,\n",
      "          -0.4422, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4270,  0.9794,  0.7101, -0.7634, -0.0298, -0.1050, -1.6149,\n",
      "          -0.5233,  1.5932,  1.5932, -0.6774, -0.7996,  0.0000, -0.9031,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3658,\n",
      "          -0.3473, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4158,  1.0002,  0.7183, -0.7405, -0.0324, -0.1050, -1.6149,\n",
      "          -0.5233,  1.4213,  1.4213, -0.2244, -0.7996,  0.0000, -0.9030,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3856,\n",
      "          -0.2568, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4186,  1.1042,  0.7840, -0.6758, -0.0298, -0.1014, -0.4919,\n",
      "          -0.5233,  1.4213,  1.4213,  0.2287, -0.7996,  0.0000, -0.9029,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3995,\n",
      "          -0.1653, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4019,  1.1510,  0.8087, -0.6073, -0.0298, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3783,  1.3783, -1.5836, -0.7996,  0.0000, -0.9027,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.4074,\n",
      "          -0.0730, -1.1564,  1.1564,  0.0000]]])\n",
      "predicted: tensor([[0.3810]], device='cuda:0')\n",
      "[25.41]\n",
      "       actual  predicted\n",
      "0      19.010  18.814681\n",
      "1      15.240  14.767428\n",
      "2      22.860  22.775959\n",
      "3      23.450  23.310498\n",
      "4      24.790  24.758980\n",
      "...       ...        ...\n",
      "78284  26.355  26.382304\n",
      "78285  20.900  21.018639\n",
      "78286  27.350  27.438944\n",
      "78287  19.950  20.052063\n",
      "78288  19.490  19.450793\n",
      "\n",
      "[78289 rows x 2 columns]\n",
      "Score (RMSE): 0.1883\n",
      "Score (MAE): 0.1254\n",
      "Score (ME): -0.0046\n",
      "Score (MAPE): 0.5280%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (392384, 27) to (399291, 27)\n",
      "training data cutoff:  2023-07-14 04:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([312832, 20, 25]) torch.Size([312832]) torch.Size([312832, 1])\n",
      "Testing data shape: torch.Size([78609, 20, 25]) torch.Size([78609]) torch.Size([78609, 1])\n",
      "Shuffled Training data shape: torch.Size([313152, 20, 25]) torch.Size([313152]) torch.Size([313152, 1])\n",
      "Shuffled Testing data shape: torch.Size([78289, 20, 25]) torch.Size([78289]) torch.Size([78289, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0       9.000000   17.792957\n",
      "1       4.999997  -10.066938\n",
      "2       4.999997    7.730457\n",
      "3       4.999997   -3.895515\n",
      "4     828.000009  673.696303\n",
      "...          ...         ...\n",
      "1019    4.999997    8.618720\n",
      "1020   20.999997  -23.384251\n",
      "1021    4.999997  -23.861134\n",
      "1022   60.500005   48.906824\n",
      "1023   82.999999   61.361140\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.5621\n",
      "Epoch 1/25, Validation Loss: 0.4478\n",
      "          actual   predicted\n",
      "0       9.000000    9.881994\n",
      "1       4.999997  -14.105641\n",
      "2       4.999997   14.758978\n",
      "3       4.999997    2.486447\n",
      "4     828.000009  839.610147\n",
      "...          ...         ...\n",
      "1019    4.999997   12.091621\n",
      "1020   20.999997  -23.027048\n",
      "1021    4.999997  -22.170848\n",
      "1022   60.500005   59.925702\n",
      "1023   82.999999   94.629606\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.4754\n",
      "Epoch 2/25, Validation Loss: 0.4723\n",
      "          actual   predicted\n",
      "0       9.000000   26.313167\n",
      "1       4.999997   16.034801\n",
      "2       4.999997   41.112940\n",
      "3       4.999997   31.684304\n",
      "4     828.000009  753.822317\n",
      "...          ...         ...\n",
      "1019    4.999997   56.037653\n",
      "1020   20.999997   15.264693\n",
      "1021    4.999997   17.060306\n",
      "1022   60.500005  112.940381\n",
      "1023   82.999999  233.964963\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.4664\n",
      "Epoch 3/25, Validation Loss: 0.4462\n",
      "          actual   predicted\n",
      "0       9.000000   -9.748806\n",
      "1       4.999997  -18.627643\n",
      "2       4.999997   -1.303184\n",
      "3       4.999997   -6.327783\n",
      "4     828.000009  874.953554\n",
      "...          ...         ...\n",
      "1019    4.999997    8.204705\n",
      "1020   20.999997  -20.337577\n",
      "1021    4.999997  -20.030077\n",
      "1022   60.500005   65.117408\n",
      "1023   82.999999   79.424239\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.4621\n",
      "Epoch 4/25, Validation Loss: 0.4232\n",
      "          actual    predicted\n",
      "0       9.000000    20.471516\n",
      "1       4.999997    17.884691\n",
      "2       4.999997    25.061186\n",
      "3       4.999997    24.572561\n",
      "4     828.000009  1011.584593\n",
      "...          ...          ...\n",
      "1019    4.999997    24.350908\n",
      "1020   20.999997    15.548497\n",
      "1021    4.999997    15.347590\n",
      "1022   60.500005    49.288407\n",
      "1023   82.999999    53.300992\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.4761\n",
      "Epoch 5/25, Validation Loss: 0.4301\n",
      "          actual   predicted\n",
      "0       9.000000    6.457674\n",
      "1       4.999997    2.236477\n",
      "2       4.999997   17.663082\n",
      "3       4.999997   14.555817\n",
      "4     828.000009  618.127550\n",
      "...          ...         ...\n",
      "1019    4.999997   16.174914\n",
      "1020   20.999997   -4.297138\n",
      "1021    4.999997   -3.023984\n",
      "1022   60.500005   57.069371\n",
      "1023   82.999999   75.440027\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.4491\n",
      "Epoch 6/25, Validation Loss: 0.4316\n",
      "          actual   predicted\n",
      "0       9.000000   -4.917219\n",
      "1       4.999997  -10.951376\n",
      "2       4.999997   -0.916532\n",
      "3       4.999997   -3.059321\n",
      "4     828.000009  889.230129\n",
      "...          ...         ...\n",
      "1019    4.999997   -2.088419\n",
      "1020   20.999997  -12.861331\n",
      "1021    4.999997  -13.124220\n",
      "1022   60.500005   65.852269\n",
      "1023   82.999999   54.519711\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.4430\n",
      "Epoch 7/25, Validation Loss: 0.4252\n",
      "          actual    predicted\n",
      "0       9.000000     9.694410\n",
      "1       4.999997     3.450542\n",
      "2       4.999997    13.688895\n",
      "3       4.999997    10.557945\n",
      "4     828.000009  1006.166580\n",
      "...          ...          ...\n",
      "1019    4.999997    10.623706\n",
      "1020   20.999997     1.090327\n",
      "1021    4.999997     0.993591\n",
      "1022   60.500005    55.854432\n",
      "1023   82.999999    68.905043\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.4413\n",
      "Epoch 8/25, Validation Loss: 0.4015\n",
      "          actual    predicted\n",
      "0       9.000000    -0.352468\n",
      "1       4.999997    -2.673320\n",
      "2       4.999997     7.844385\n",
      "3       4.999997     7.016938\n",
      "4     828.000009  1313.934973\n",
      "...          ...          ...\n",
      "1019    4.999997    11.318744\n",
      "1020   20.999997    -2.584995\n",
      "1021    4.999997    -1.025648\n",
      "1022   60.500005    39.210958\n",
      "1023   82.999999    55.657988\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.4536\n",
      "Epoch 9/25, Validation Loss: 0.4175\n",
      "          actual   predicted\n",
      "0       9.000000   15.667056\n",
      "1       4.999997    1.657105\n",
      "2       4.999997   27.561200\n",
      "3       4.999997   18.366766\n",
      "4     828.000009  924.891533\n",
      "...          ...         ...\n",
      "1019    4.999997   14.404276\n",
      "1020   20.999997   -0.550638\n",
      "1021    4.999997    0.269968\n",
      "1022   60.500005  227.069665\n",
      "1023   82.999999  169.858648\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.4421\n",
      "Epoch 10/25, Validation Loss: 0.3908\n",
      "          actual    predicted\n",
      "0       9.000000    14.245491\n",
      "1       4.999997     4.938217\n",
      "2       4.999997    22.202275\n",
      "3       4.999997    15.405468\n",
      "4     828.000009  1138.676399\n",
      "...          ...          ...\n",
      "1019    4.999997    21.118265\n",
      "1020   20.999997     5.459061\n",
      "1021    4.999997     6.317527\n",
      "1022   60.500005   184.701851\n",
      "1023   82.999999   112.349873\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.4304\n",
      "Epoch 11/25, Validation Loss: 0.4167\n",
      "          actual   predicted\n",
      "0       9.000000   -2.039737\n",
      "1       4.999997   -7.282917\n",
      "2       4.999997   -0.162237\n",
      "3       4.999997   -2.586598\n",
      "4     828.000009  839.801914\n",
      "...          ...         ...\n",
      "1019    4.999997   -0.957005\n",
      "1020   20.999997   -6.878031\n",
      "1021    4.999997   -6.358174\n",
      "1022   60.500005   30.545207\n",
      "1023   82.999999   27.568916\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.4363\n",
      "Epoch 12/25, Validation Loss: 0.4047\n",
      "          actual   predicted\n",
      "0       9.000000    2.679123\n",
      "1       4.999997   -4.317257\n",
      "2       4.999997    6.829500\n",
      "3       4.999997    1.668140\n",
      "4     828.000009  802.628354\n",
      "...          ...         ...\n",
      "1019    4.999997    5.154925\n",
      "1020   20.999997   -9.644691\n",
      "1021    4.999997  -10.934061\n",
      "1022   60.500005   60.980881\n",
      "1023   82.999999   53.244336\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.4315\n",
      "Epoch 13/25, Validation Loss: 0.3993\n",
      "          actual   predicted\n",
      "0       9.000000   16.217372\n",
      "1       4.999997    0.737666\n",
      "2       4.999997   32.781671\n",
      "3       4.999997   10.712940\n",
      "4     828.000009  784.269407\n",
      "...          ...         ...\n",
      "1019    4.999997   15.658354\n",
      "1020   20.999997   -0.944097\n",
      "1021    4.999997   -1.251889\n",
      "1022   60.500005  154.644785\n",
      "1023   82.999999  146.958297\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.4272\n",
      "Epoch 14/25, Validation Loss: 0.4007\n",
      "          actual   predicted\n",
      "0       9.000000   14.621769\n",
      "1       4.999997    4.515208\n",
      "2       4.999997   22.836049\n",
      "3       4.999997   11.571316\n",
      "4     828.000009  730.985082\n",
      "...          ...         ...\n",
      "1019    4.999997   10.701176\n",
      "1020   20.999997    1.225001\n",
      "1021    4.999997    0.273613\n",
      "1022   60.500005   78.000700\n",
      "1023   82.999999   67.342231\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.4146\n",
      "Epoch 15/25, Validation Loss: 0.3907\n",
      "          actual   predicted\n",
      "0       9.000000   15.403730\n",
      "1       4.999997    0.675897\n",
      "2       4.999997   14.897656\n",
      "3       4.999997    5.182860\n",
      "4     828.000009  856.478336\n",
      "...          ...         ...\n",
      "1019    4.999997    6.887501\n",
      "1020   20.999997   -1.369909\n",
      "1021    4.999997   -2.737444\n",
      "1022   60.500005   68.404901\n",
      "1023   82.999999   58.007123\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.4044\n",
      "Epoch 16/25, Validation Loss: 0.3948\n",
      "          actual   predicted\n",
      "0       9.000000   18.193985\n",
      "1       4.999997    7.117779\n",
      "2       4.999997   30.094409\n",
      "3       4.999997   11.935225\n",
      "4     828.000009  937.705519\n",
      "...          ...         ...\n",
      "1019    4.999997   13.147148\n",
      "1020   20.999997    3.155266\n",
      "1021    4.999997    0.538442\n",
      "1022   60.500005   92.755582\n",
      "1023   82.999999   83.574112\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.3970\n",
      "Epoch 17/25, Validation Loss: 0.3886\n",
      "          actual   predicted\n",
      "0       9.000000    7.734797\n",
      "1       4.999997   -4.064438\n",
      "2       4.999997    8.121090\n",
      "3       4.999997   -0.086405\n",
      "4     828.000009  902.226911\n",
      "...          ...         ...\n",
      "1019    4.999997    2.886725\n",
      "1020   20.999997   -6.587554\n",
      "1021    4.999997   -8.819465\n",
      "1022   60.500005   47.017492\n",
      "1023   82.999999   54.539123\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.4025\n",
      "Epoch 18/25, Validation Loss: 0.3863\n",
      "          actual    predicted\n",
      "0       9.000000    28.621313\n",
      "1       4.999997     6.147718\n",
      "2       4.999997    38.877060\n",
      "3       4.999997    15.738661\n",
      "4     828.000009  1003.134911\n",
      "...          ...          ...\n",
      "1019    4.999997    34.590674\n",
      "1020   20.999997     0.104678\n",
      "1021    4.999997    -0.068383\n",
      "1022   60.500005    93.755064\n",
      "1023   82.999999    86.504856\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.3991\n",
      "Epoch 19/25, Validation Loss: 0.3819\n",
      "          actual   predicted\n",
      "0       9.000000   28.791840\n",
      "1       4.999997   20.308335\n",
      "2       4.999997   30.741494\n",
      "3       4.999997   24.160486\n",
      "4     828.000009  942.544867\n",
      "...          ...         ...\n",
      "1019    4.999997   25.678519\n",
      "1020   20.999997   18.063180\n",
      "1021    4.999997   17.089138\n",
      "1022   60.500005   80.383377\n",
      "1023   82.999999   90.231658\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.4015\n",
      "Epoch 20/25, Validation Loss: 0.3649\n",
      "          actual   predicted\n",
      "0       9.000000   22.274059\n",
      "1       4.999997   11.744803\n",
      "2       4.999997   24.783112\n",
      "3       4.999997   16.170776\n",
      "4     828.000009  980.866159\n",
      "...          ...         ...\n",
      "1019    4.999997   15.859171\n",
      "1020   20.999997    6.630892\n",
      "1021    4.999997    4.876616\n",
      "1022   60.500005   99.822792\n",
      "1023   82.999999   77.702597\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.4075\n",
      "Epoch 21/25, Validation Loss: 0.3692\n",
      "          actual    predicted\n",
      "0       9.000000    22.682826\n",
      "1       4.999997    16.668563\n",
      "2       4.999997    27.536382\n",
      "3       4.999997    22.543565\n",
      "4     828.000009  1043.479199\n",
      "...          ...          ...\n",
      "1019    4.999997    23.273793\n",
      "1020   20.999997    13.265023\n",
      "1021    4.999997    11.599969\n",
      "1022   60.500005    93.186306\n",
      "1023   82.999999   101.010548\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.3835\n",
      "Epoch 22/25, Validation Loss: 0.3962\n",
      "          actual    predicted\n",
      "0       9.000000     7.970716\n",
      "1       4.999997    -2.112126\n",
      "2       4.999997    10.753054\n",
      "3       4.999997     4.689660\n",
      "4     828.000009  1137.143249\n",
      "...          ...          ...\n",
      "1019    4.999997     2.628714\n",
      "1020   20.999997    -6.127302\n",
      "1021    4.999997    -8.821596\n",
      "1022   60.500005    87.151285\n",
      "1023   82.999999    79.038216\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.3934\n",
      "Epoch 23/25, Validation Loss: 0.3808\n",
      "          actual   predicted\n",
      "0       9.000000   28.766854\n",
      "1       4.999997   20.643008\n",
      "2       4.999997   25.876050\n",
      "3       4.999997   23.032918\n",
      "4     828.000009  989.407799\n",
      "...          ...         ...\n",
      "1019    4.999997   25.681524\n",
      "1020   20.999997   19.284590\n",
      "1021    4.999997   18.047031\n",
      "1022   60.500005   48.449207\n",
      "1023   82.999999   54.138834\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.3648\n",
      "Epoch 24/25, Validation Loss: 0.3696\n",
      "          actual   predicted\n",
      "0       9.000000   12.697157\n",
      "1       4.999997   -0.307139\n",
      "2       4.999997   10.199408\n",
      "3       4.999997    3.886033\n",
      "4     828.000009  987.640761\n",
      "...          ...         ...\n",
      "1019    4.999997    3.805782\n",
      "1020   20.999997   -3.912314\n",
      "1021    4.999997   -5.469171\n",
      "1022   60.500005   92.037111\n",
      "1023   82.999999   72.636987\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.3842\n",
      "Epoch 25/25, Validation Loss: 0.3540\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4371,  1.6471,  1.2275, -0.7223, -0.1084, -0.1014, -1.5026,\n",
      "          -0.5233, -1.8361, -1.8361, -0.5868,  4.1524,  0.0000, -0.9068,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9400,\n",
      "          -1.0373, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4443,  1.6188,  1.1864, -0.6811, -0.1326, -0.1086, -1.3903,\n",
      "          -0.5233, -1.8705, -1.8705, -0.0432,  4.1524,  0.0000, -0.9066,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9995,\n",
      "          -0.9717, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4516,  1.5905,  1.1454, -0.6400, -0.1568, -0.1157, -1.2780,\n",
      "          -0.5233, -1.9049, -1.9049,  0.5005,  4.1524,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.0590,\n",
      "          -0.9061, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4589,  1.5622,  1.1043, -0.5989, -0.1810, -0.1229, -1.1657,\n",
      "          -0.5233, -1.9393, -1.9393,  1.0442,  4.1524,  0.0000, -0.9064,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1184,\n",
      "          -0.8406, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4661,  1.5339,  1.0632, -0.5578, -0.2052, -0.1300, -1.0534,\n",
      "          -0.5233, -1.9737, -1.9737,  1.5879,  4.1524,  0.0000, -0.9062,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1779,\n",
      "          -0.7750, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.6714,  2.1800,  1.2685, -1.2546,  0.4127,  0.0343, -0.4919,\n",
      "          -0.3653,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9080,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.7760,\n",
      "          -1.1793, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.1926,  1.6989,  3.6279, -1.2248,  0.0975, -0.0532, -1.0815,\n",
      "          -0.4325,  0.8111,  0.8111, -0.2244, -0.7996,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.8464,\n",
      "          -1.1292, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.0926,  1.2578,  2.2376, -1.1112,  0.0367, -0.0657, -0.6791,\n",
      "          -0.3127,  1.2064,  1.2064,  1.1348, -0.7996,  0.0000, -0.9049,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.9151,\n",
      "          -1.0735, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2006,  1.1177,  1.8598, -1.1061,  0.0194, -0.0800, -0.4919,\n",
      "          -0.4443,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9046,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1978,\n",
      "          -0.7437, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2370,  0.9357,  0.9565, -1.1099,  0.0181, -0.0800, -1.6149,\n",
      "          -0.5233, -1.5439, -1.5439,  1.5879, -0.7996,  0.0000, -0.9045,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0338,\n",
      "          -0.9597, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2873,  0.8379,  0.9072, -1.0376, -0.0138, -0.0871, -1.6149,\n",
      "          -0.4443,  1.6362,  1.6362, -0.6774, -0.7996,  0.0000, -0.9044,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0943,\n",
      "          -0.8897, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3348,  0.7598,  0.8744, -1.0338, -0.0218, -0.0943, -1.0534,\n",
      "          -0.2864,  1.4643,  1.4643, -1.1305, -0.7996,  0.0000, -0.9042,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1501,\n",
      "          -0.8158, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3543,  0.8108,  0.8087, -0.9805, -0.0271, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -0.2244, -0.7996,  0.0000, -0.9041,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3511,\n",
      "          -0.4009, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3516,  0.8170,  0.7594, -0.8167, -0.0284, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -1.5836, -0.7996,  0.0000, -0.9039,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.2467,\n",
      "          -0.6580, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3683,  0.9166,  0.6252, -0.8548, -0.0856, -0.1169, -1.0534,\n",
      "          -0.3653,  1.4213,  1.4213,  0.0777, -0.7996,  0.0000, -0.9037,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3030,\n",
      "          -0.5367, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3921,  1.0246,  1.1823, -0.6016, -0.0411, -0.1050, -1.3342,\n",
      "          -0.5233,  1.5072,  1.5072,  1.1348, -0.7996,  0.0000, -0.9033,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3382,\n",
      "          -0.4422, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4270,  0.9794,  0.7101, -0.7634, -0.0298, -0.1050, -1.6149,\n",
      "          -0.5233,  1.5932,  1.5932, -0.6774, -0.7996,  0.0000, -0.9031,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3658,\n",
      "          -0.3473, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4158,  1.0002,  0.7183, -0.7405, -0.0324, -0.1050, -1.6149,\n",
      "          -0.5233,  1.4213,  1.4213, -0.2244, -0.7996,  0.0000, -0.9030,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3856,\n",
      "          -0.2568, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4186,  1.1042,  0.7840, -0.6758, -0.0298, -0.1014, -0.4919,\n",
      "          -0.5233,  1.4213,  1.4213,  0.2287, -0.7996,  0.0000, -0.9029,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3995,\n",
      "          -0.1653, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4019,  1.1510,  0.8087, -0.6073, -0.0298, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3783,  1.3783, -1.5836, -0.7996,  0.0000, -0.9027,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.4074,\n",
      "          -0.0730, -1.1564,  1.1564,  0.0000]]])\n",
      "predicted: tensor([[-0.1050]], device='cuda:0')\n",
      "[100.36]\n",
      "           actual   predicted\n",
      "0        9.000000   12.697157\n",
      "1        4.999997   -0.307139\n",
      "2        4.999997   10.199408\n",
      "3        4.999997    3.886033\n",
      "4      828.000009  987.640761\n",
      "...           ...         ...\n",
      "78284  718.999978  430.249628\n",
      "78285    6.000001   -6.768253\n",
      "78286    4.000005   -4.852623\n",
      "78287  167.000000   24.248531\n",
      "78288    6.000001    1.344570\n",
      "\n",
      "[78289 rows x 2 columns]\n",
      "Score (RMSE): 447.8505\n",
      "Score (MAE): 51.4616\n",
      "Score (ME): 8.3202\n",
      "Score (MAPE): 92157.2390%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (199269, 27) to (200228, 27)\n",
      "training data cutoff:  2023-07-14 02:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([154985, 20, 25]) torch.Size([154985]) torch.Size([154985, 1])\n",
      "Testing data shape: torch.Size([39048, 20, 25]) torch.Size([39048]) torch.Size([39048, 1])\n",
      "Shuffled Training data shape: torch.Size([155226, 20, 25]) torch.Size([155226]) torch.Size([155226, 1])\n",
      "Shuffled Testing data shape: torch.Size([38807, 20, 25]) torch.Size([38807]) torch.Size([38807, 1])\n",
      "Index(['device_id', 'date_time_rounded', 'tmp', 'hum', 'CO2', 'VOC', 'vis',\n",
      "       'IR', 'WIFI', 'BLE', 'rssi', 'channel_rssi', 'channel_index',\n",
      "       'spreading_factor', 'bandwidth', 'f_cnt', 'isHoliday', 'isExamTime',\n",
      "       'weekday_sin', 'weekday_cos', 'month_sin', 'month_cos', 'time_sin',\n",
      "       'time_cos', 'semester_SS23', 'semester_WS22/23', 'semester_WS23/24'],\n",
      "      dtype='object')\n",
      "Index(['date_time_rounded', 'device_id', 'tmp', 'hum', 'CO2', 'VOC', 'vis',\n",
      "       'IR', 'WIFI', 'BLE', 'rssi', 'channel_rssi', 'channel_index',\n",
      "       'spreading_factor', 'bandwidth', 'f_cnt', 'isHoliday', 'isExamTime',\n",
      "       'weekday_sin', 'weekday_cos', 'month_sin', 'month_cos', 'time_sin',\n",
      "       'time_cos', 'semester_SS23', 'semester_WS22/23', 'semester_WS23/24'],\n",
      "      dtype='object')\n",
      "device_id                    object\n",
      "date_time_rounded    datetime64[ns]\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "dtype: object\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "dtype: object\n",
      "1612  rows differ from the last saved dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     708.500005  827.863946\n",
      "1     514.000000  525.307844\n",
      "2     590.000003  587.991827\n",
      "3     453.000002  439.821506\n",
      "4     420.000003  425.164582\n",
      "...          ...         ...\n",
      "1019  424.000000  419.780133\n",
      "1020  559.000003  548.112187\n",
      "1021  428.499999  423.602857\n",
      "1022  411.103452  423.664308\n",
      "1023  436.000002  435.510259\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2749\n",
      "Epoch 1/25, Validation Loss: 0.1096\n",
      "          actual   predicted\n",
      "0     708.500005  803.117016\n",
      "1     514.000000  541.848297\n",
      "2     590.000003  614.482320\n",
      "3     453.000002  440.654552\n",
      "4     420.000003  421.248082\n",
      "...          ...         ...\n",
      "1019  424.000000  417.961890\n",
      "1020  559.000003  577.529139\n",
      "1021  428.499999  431.164577\n",
      "1022  411.103452  414.371301\n",
      "1023  436.000002  430.088789\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.1206\n",
      "Epoch 2/25, Validation Loss: 0.1193\n",
      "          actual   predicted\n",
      "0     708.500005  760.598373\n",
      "1     514.000000  533.213361\n",
      "2     590.000003  587.505759\n",
      "3     453.000002  451.063781\n",
      "4     420.000003  427.561288\n",
      "...          ...         ...\n",
      "1019  424.000000  414.991534\n",
      "1020  559.000003  560.572612\n",
      "1021  428.499999  424.248155\n",
      "1022  411.103452  418.184798\n",
      "1023  436.000002  428.570030\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1088\n",
      "Epoch 3/25, Validation Loss: 0.0837\n",
      "          actual   predicted\n",
      "0     708.500005  752.459070\n",
      "1     514.000000  510.898658\n",
      "2     590.000003  569.794050\n",
      "3     453.000002  437.266384\n",
      "4     420.000003  420.513377\n",
      "...          ...         ...\n",
      "1019  424.000000  411.838084\n",
      "1020  559.000003  541.939177\n",
      "1021  428.499999  422.420414\n",
      "1022  411.103452  416.753076\n",
      "1023  436.000002  422.236442\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1031\n",
      "Epoch 4/25, Validation Loss: 0.0779\n",
      "          actual   predicted\n",
      "0     708.500005  700.895276\n",
      "1     514.000000  527.393214\n",
      "2     590.000003  580.126780\n",
      "3     453.000002  447.154557\n",
      "4     420.000003  431.413258\n",
      "...          ...         ...\n",
      "1019  424.000000  417.732779\n",
      "1020  559.000003  540.675180\n",
      "1021  428.499999  422.910062\n",
      "1022  411.103452  418.318460\n",
      "1023  436.000002  423.335935\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.1010\n",
      "Epoch 5/25, Validation Loss: 0.0744\n",
      "          actual   predicted\n",
      "0     708.500005  719.547270\n",
      "1     514.000000  524.565085\n",
      "2     590.000003  572.773311\n",
      "3     453.000002  435.430718\n",
      "4     420.000003  416.699931\n",
      "...          ...         ...\n",
      "1019  424.000000  410.473356\n",
      "1020  559.000003  555.666868\n",
      "1021  428.499999  420.753905\n",
      "1022  411.103452  411.820990\n",
      "1023  436.000002  421.806719\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0966\n",
      "Epoch 6/25, Validation Loss: 0.0768\n",
      "          actual   predicted\n",
      "0     708.500005  710.552436\n",
      "1     514.000000  534.357748\n",
      "2     590.000003  586.832234\n",
      "3     453.000002  436.222238\n",
      "4     420.000003  413.080764\n",
      "...          ...         ...\n",
      "1019  424.000000  407.616194\n",
      "1020  559.000003  566.438336\n",
      "1021  428.499999  422.631919\n",
      "1022  411.103452  413.116318\n",
      "1023  436.000002  425.647289\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0965\n",
      "Epoch 7/25, Validation Loss: 0.0731\n",
      "          actual   predicted\n",
      "0     708.500005  730.713900\n",
      "1     514.000000  518.441743\n",
      "2     590.000003  569.637670\n",
      "3     453.000002  442.335824\n",
      "4     420.000003  423.313260\n",
      "...          ...         ...\n",
      "1019  424.000000  414.584238\n",
      "1020  559.000003  546.444626\n",
      "1021  428.499999  425.554083\n",
      "1022  411.103452  422.916110\n",
      "1023  436.000002  433.636230\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0932\n",
      "Epoch 8/25, Validation Loss: 0.0738\n",
      "          actual   predicted\n",
      "0     708.500005  724.965110\n",
      "1     514.000000  516.000464\n",
      "2     590.000003  571.048697\n",
      "3     453.000002  437.100302\n",
      "4     420.000003  419.184372\n",
      "...          ...         ...\n",
      "1019  424.000000  418.544414\n",
      "1020  559.000003  548.430025\n",
      "1021  428.499999  425.178198\n",
      "1022  411.103452  421.187092\n",
      "1023  436.000002  431.202282\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0908\n",
      "Epoch 9/25, Validation Loss: 0.0698\n",
      "          actual   predicted\n",
      "0     708.500005  756.702876\n",
      "1     514.000000  527.861183\n",
      "2     590.000003  572.878848\n",
      "3     453.000002  427.603954\n",
      "4     420.000003  410.912197\n",
      "...          ...         ...\n",
      "1019  424.000000  407.148206\n",
      "1020  559.000003  542.303786\n",
      "1021  428.499999  415.715618\n",
      "1022  411.103452  408.025792\n",
      "1023  436.000002  418.838548\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0912\n",
      "Epoch 10/25, Validation Loss: 0.1170\n",
      "          actual   predicted\n",
      "0     708.500005  714.597644\n",
      "1     514.000000  528.614483\n",
      "2     590.000003  574.265454\n",
      "3     453.000002  433.189848\n",
      "4     420.000003  416.062099\n",
      "...          ...         ...\n",
      "1019  424.000000  409.498452\n",
      "1020  559.000003  560.102184\n",
      "1021  428.499999  420.384771\n",
      "1022  411.103452  411.731685\n",
      "1023  436.000002  424.745049\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0917\n",
      "Epoch 11/25, Validation Loss: 0.0718\n",
      "          actual   predicted\n",
      "0     708.500005  742.699435\n",
      "1     514.000000  532.980941\n",
      "2     590.000003  601.446924\n",
      "3     453.000002  437.330545\n",
      "4     420.000003  419.293693\n",
      "...          ...         ...\n",
      "1019  424.000000  409.228024\n",
      "1020  559.000003  577.312265\n",
      "1021  428.499999  421.140127\n",
      "1022  411.103452  410.792831\n",
      "1023  436.000002  427.392518\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0866\n",
      "Epoch 12/25, Validation Loss: 0.0733\n",
      "          actual   predicted\n",
      "0     708.500005  727.089101\n",
      "1     514.000000  517.538743\n",
      "2     590.000003  562.340596\n",
      "3     453.000002  434.210296\n",
      "4     420.000003  416.201649\n",
      "...          ...         ...\n",
      "1019  424.000000  409.049143\n",
      "1020  559.000003  541.627128\n",
      "1021  428.499999  421.906558\n",
      "1022  411.103452  414.092180\n",
      "1023  436.000002  423.938367\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0861\n",
      "Epoch 13/25, Validation Loss: 0.0698\n",
      "          actual   predicted\n",
      "0     708.500005  730.191000\n",
      "1     514.000000  520.840078\n",
      "2     590.000003  571.019615\n",
      "3     453.000002  439.672404\n",
      "4     420.000003  418.745130\n",
      "...          ...         ...\n",
      "1019  424.000000  414.410800\n",
      "1020  559.000003  548.751603\n",
      "1021  428.499999  422.302167\n",
      "1022  411.103452  414.401865\n",
      "1023  436.000002  425.687650\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0826\n",
      "Epoch 14/25, Validation Loss: 0.0687\n",
      "          actual   predicted\n",
      "0     708.500005  763.016688\n",
      "1     514.000000  527.548495\n",
      "2     590.000003  594.683036\n",
      "3     453.000002  439.385682\n",
      "4     420.000003  417.516263\n",
      "...          ...         ...\n",
      "1019  424.000000  412.338630\n",
      "1020  559.000003  553.019121\n",
      "1021  428.499999  424.579033\n",
      "1022  411.103452  415.963035\n",
      "1023  436.000002  427.578809\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0829\n",
      "Epoch 15/25, Validation Loss: 0.0671\n",
      "          actual   predicted\n",
      "0     708.500005  745.775561\n",
      "1     514.000000  515.645428\n",
      "2     590.000003  572.529261\n",
      "3     453.000002  436.446808\n",
      "4     420.000003  413.365803\n",
      "...          ...         ...\n",
      "1019  424.000000  409.064009\n",
      "1020  559.000003  548.619041\n",
      "1021  428.499999  419.948363\n",
      "1022  411.103452  411.205870\n",
      "1023  436.000002  424.858519\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0809\n",
      "Epoch 16/25, Validation Loss: 0.0671\n",
      "          actual   predicted\n",
      "0     708.500005  715.764651\n",
      "1     514.000000  522.935095\n",
      "2     590.000003  567.692032\n",
      "3     453.000002  434.178869\n",
      "4     420.000003  411.148948\n",
      "...          ...         ...\n",
      "1019  424.000000  409.841332\n",
      "1020  559.000003  551.387319\n",
      "1021  428.499999  422.008888\n",
      "1022  411.103452  414.471308\n",
      "1023  436.000002  425.062368\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0807\n",
      "Epoch 17/25, Validation Loss: 0.0663\n",
      "          actual   predicted\n",
      "0     708.500005  739.742732\n",
      "1     514.000000  526.054905\n",
      "2     590.000003  582.610089\n",
      "3     453.000002  444.352197\n",
      "4     420.000003  419.320101\n",
      "...          ...         ...\n",
      "1019  424.000000  411.530991\n",
      "1020  559.000003  569.593693\n",
      "1021  428.499999  425.349043\n",
      "1022  411.103452  414.851495\n",
      "1023  436.000002  431.939003\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0790\n",
      "Epoch 18/25, Validation Loss: 0.0685\n",
      "          actual   predicted\n",
      "0     708.500005  754.677751\n",
      "1     514.000000  521.943710\n",
      "2     590.000003  587.458056\n",
      "3     453.000002  439.735736\n",
      "4     420.000003  412.979238\n",
      "...          ...         ...\n",
      "1019  424.000000  410.254919\n",
      "1020  559.000003  564.223154\n",
      "1021  428.499999  421.092454\n",
      "1022  411.103452  414.244455\n",
      "1023  436.000002  427.549147\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0796\n",
      "Epoch 19/25, Validation Loss: 0.0686\n",
      "          actual   predicted\n",
      "0     708.500005  746.249751\n",
      "1     514.000000  527.072756\n",
      "2     590.000003  579.637907\n",
      "3     453.000002  440.398654\n",
      "4     420.000003  413.438971\n",
      "...          ...         ...\n",
      "1019  424.000000  410.496389\n",
      "1020  559.000003  559.360970\n",
      "1021  428.499999  422.503218\n",
      "1022  411.103452  412.759960\n",
      "1023  436.000002  426.364878\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0786\n",
      "Epoch 20/25, Validation Loss: 0.0732\n",
      "          actual   predicted\n",
      "0     708.500005  767.315457\n",
      "1     514.000000  523.259792\n",
      "2     590.000003  584.893544\n",
      "3     453.000002  440.826339\n",
      "4     420.000003  410.830080\n",
      "...          ...         ...\n",
      "1019  424.000000  408.207864\n",
      "1020  559.000003  562.609724\n",
      "1021  428.499999  423.425116\n",
      "1022  411.103452  411.437025\n",
      "1023  436.000002  429.974723\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0842\n",
      "Epoch 21/25, Validation Loss: 0.0710\n",
      "          actual   predicted\n",
      "0     708.500005  751.587950\n",
      "1     514.000000  519.720777\n",
      "2     590.000003  576.262271\n",
      "3     453.000002  437.236550\n",
      "4     420.000003  411.459642\n",
      "...          ...         ...\n",
      "1019  424.000000  408.192027\n",
      "1020  559.000003  555.890901\n",
      "1021  428.499999  421.492825\n",
      "1022  411.103452  409.712813\n",
      "1023  436.000002  426.266420\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0778\n",
      "Epoch 22/25, Validation Loss: 0.0660\n",
      "          actual   predicted\n",
      "0     708.500005  757.695437\n",
      "1     514.000000  523.394879\n",
      "2     590.000003  584.220772\n",
      "3     453.000002  440.817372\n",
      "4     420.000003  414.754556\n",
      "...          ...         ...\n",
      "1019  424.000000  412.537366\n",
      "1020  559.000003  562.241984\n",
      "1021  428.499999  424.807931\n",
      "1022  411.103452  414.108054\n",
      "1023  436.000002  429.932744\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0774\n",
      "Epoch 23/25, Validation Loss: 0.0695\n",
      "          actual   predicted\n",
      "0     708.500005  749.306737\n",
      "1     514.000000  521.489967\n",
      "2     590.000003  577.872085\n",
      "3     453.000002  441.767033\n",
      "4     420.000003  415.535276\n",
      "...          ...         ...\n",
      "1019  424.000000  412.523742\n",
      "1020  559.000003  553.810674\n",
      "1021  428.499999  421.342800\n",
      "1022  411.103452  411.913195\n",
      "1023  436.000002  426.750559\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0767\n",
      "Epoch 24/25, Validation Loss: 0.0694\n",
      "          actual   predicted\n",
      "0     708.500005  755.063023\n",
      "1     514.000000  520.204877\n",
      "2     590.000003  575.748159\n",
      "3     453.000002  439.316901\n",
      "4     420.000003  414.712289\n",
      "...          ...         ...\n",
      "1019  424.000000  413.237978\n",
      "1020  559.000003  552.296221\n",
      "1021  428.499999  424.240492\n",
      "1022  411.103452  415.393688\n",
      "1023  436.000002  428.763944\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0754\n",
      "Epoch 25/25, Validation Loss: 0.0667\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4369,  1.6490,  1.2161, -0.7247, -0.1103, -0.1009, -1.7508,\n",
      "          -0.5681, -1.9687, -1.9687, -0.8792,  4.1508,  0.0000, -0.9065,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -0.9411,\n",
      "          -1.0386, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4441,  1.6207,  1.1753, -0.6836, -0.1333, -0.1076, -1.6197,\n",
      "          -0.5681, -2.0056, -2.0056, -0.0644,  4.1508,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0007,\n",
      "          -0.9729, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4514,  1.5924,  1.1345, -0.6426, -0.1562, -0.1142, -1.4886,\n",
      "          -0.5681, -2.0425, -2.0425,  0.7504,  4.1508,  0.0000, -0.9062,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0603,\n",
      "          -0.9071, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4586,  1.5641,  1.0937, -0.6016, -0.1791, -0.1209, -1.3575,\n",
      "          -0.5681, -2.0794, -2.0794,  1.5652,  4.1508,  0.0000, -0.9061,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1199,\n",
      "          -0.8414, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4659,  1.5358,  1.0529, -0.5605, -0.2021, -0.1275, -1.2264,\n",
      "          -0.5681, -2.1163, -2.1163,  2.3800,  4.1508,  0.0000, -0.9059,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1795,\n",
      "          -0.7757, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.6707,  2.1820,  1.2569, -1.2558,  0.3838,  0.0256, -0.5708,\n",
      "          -0.3964,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9077,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.7766,\n",
      "          -1.1809, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.1923,  1.7008,  3.6011, -1.2260,  0.0849, -0.0560, -1.2592,\n",
      "          -0.4694,  0.8735,  0.8735, -0.3360, -0.7989,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.8472,\n",
      "          -1.1308, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.0926,  1.2596,  2.2197, -1.1127,  0.0272, -0.0676, -0.7894,\n",
      "          -0.3392,  1.2980,  1.2980,  1.7010, -0.7989,  0.0000, -0.9046,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.9160,\n",
      "          -1.0749, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2006,  1.1195,  1.8444, -1.1076,  0.0108, -0.0809, -0.5708,\n",
      "          -0.4822,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9043,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1995,\n",
      "          -0.7444, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2369,  0.9374,  0.9469, -1.1114,  0.0096, -0.0809, -1.8819,\n",
      "          -0.5681, -1.6549, -1.6549,  2.3800, -0.7989,  0.0000, -0.9042,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0351,\n",
      "          -0.9609, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2872,  0.8395,  0.8979, -1.0392, -0.0206, -0.0876, -1.8819,\n",
      "          -0.4822,  1.7594,  1.7594, -1.0150, -0.7989,  0.0000, -0.9041,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0958,\n",
      "          -0.8907, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3347,  0.7615,  0.8653, -1.0354, -0.0282, -0.0942, -1.2264,\n",
      "          -0.3105,  1.5748,  1.5748, -1.6940, -0.7989,  0.0000, -0.9039,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1517,\n",
      "          -0.8166, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3542,  0.8125,  0.8000, -0.9823, -0.0332, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -0.3360, -0.7989,  0.0000, -0.9038,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3532,\n",
      "          -0.4007, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3514,  0.8187,  0.7511, -0.8189, -0.0345, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -2.3730, -0.7989,  0.0000, -0.9037,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.2486,\n",
      "          -0.6584, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3682,  0.9183,  0.6178, -0.8569, -0.0887, -0.1153, -1.2264,\n",
      "          -0.3964,  1.5287,  1.5287,  0.1166, -0.7989,  0.0000, -0.9034,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3050,\n",
      "          -0.5368, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3919,  1.0263,  1.1713, -0.6042, -0.0465, -0.1042, -1.5541,\n",
      "          -0.5681,  1.6210,  1.6210,  1.7010, -0.7989,  0.0000, -0.9031,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3402,\n",
      "          -0.4420, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4268,  0.9811,  0.7021, -0.7657, -0.0358, -0.1042, -1.8819,\n",
      "          -0.5681,  1.7132,  1.7132, -1.0150, -0.7989,  0.0000, -0.9029,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3679,\n",
      "          -0.3469, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4156,  1.0019,  0.7103, -0.7429, -0.0383, -0.1042, -1.8819,\n",
      "          -0.5681,  1.5287,  1.5287, -0.3360, -0.7989,  0.0000, -0.9027,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3878,\n",
      "          -0.2562, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4184,  1.1060,  0.7755, -0.6783, -0.0358, -0.1009, -0.5708,\n",
      "          -0.5681,  1.5287,  1.5287,  0.3430, -0.7989,  0.0000, -0.9026,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4017,\n",
      "          -0.1644, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4017,  1.1528,  0.8000, -0.6099, -0.0358, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4825,  1.4825, -2.3730, -0.7989,  0.0000, -0.9025,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4096,\n",
      "          -0.0719, -1.1552,  1.1552,  0.0000]]])\n",
      "predicted: tensor([[0.1020]], device='cuda:0')\n",
      "[500.46]\n",
      "           actual   predicted\n",
      "0      708.500005  755.063023\n",
      "1      514.000000  520.204877\n",
      "2      590.000003  575.748159\n",
      "3      453.000002  439.316901\n",
      "4      420.000003  414.712289\n",
      "...           ...         ...\n",
      "38802  453.500001  466.325658\n",
      "38803  415.500000  416.168937\n",
      "38804  428.000000  420.532137\n",
      "38805  551.384616  574.882210\n",
      "38806  495.000000  495.763468\n",
      "\n",
      "[38807 rows x 2 columns]\n",
      "Score (RMSE): 31.6383\n",
      "Score (MAE): 10.7044\n",
      "Score (ME): -1.6339\n",
      "Score (MAPE): 1.8649%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (199269, 27) to (200228, 27)\n",
      "training data cutoff:  2023-07-14 02:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([154985, 20, 25]) torch.Size([154985]) torch.Size([154985, 1])\n",
      "Testing data shape: torch.Size([39048, 20, 25]) torch.Size([39048]) torch.Size([39048, 1])\n",
      "Shuffled Training data shape: torch.Size([155226, 20, 25]) torch.Size([155226]) torch.Size([155226, 1])\n",
      "Shuffled Testing data shape: torch.Size([38807, 20, 25]) torch.Size([38807]) torch.Size([38807, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     638.500002  595.569963\n",
      "1     799.199999  723.973436\n",
      "2     564.999993  556.931144\n",
      "3     619.500005  590.102892\n",
      "4     809.333334  782.870452\n",
      "...          ...         ...\n",
      "1019  669.999997  608.668087\n",
      "1020  615.999995  606.143025\n",
      "1021  611.718754  577.803538\n",
      "1022  605.999995  585.184455\n",
      "1023  629.047617  629.312640\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2176\n",
      "Epoch 1/25, Validation Loss: 0.0766\n",
      "          actual   predicted\n",
      "0     638.500002  600.864966\n",
      "1     799.199999  757.480717\n",
      "2     564.999993  559.457492\n",
      "3     619.500005  615.114673\n",
      "4     809.333334  794.698444\n",
      "...          ...         ...\n",
      "1019  669.999997  626.762618\n",
      "1020  615.999995  609.377057\n",
      "1021  611.718754  578.244426\n",
      "1022  605.999995  585.574717\n",
      "1023  629.047617  633.998340\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0916\n",
      "Epoch 2/25, Validation Loss: 0.0711\n",
      "          actual   predicted\n",
      "0     638.500002  591.656582\n",
      "1     799.199999  791.985051\n",
      "2     564.999993  542.894939\n",
      "3     619.500005  584.367190\n",
      "4     809.333334  825.498620\n",
      "...          ...         ...\n",
      "1019  669.999997  628.872364\n",
      "1020  615.999995  617.924226\n",
      "1021  611.718754  577.413480\n",
      "1022  605.999995  586.855572\n",
      "1023  629.047617  644.565111\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0831\n",
      "Epoch 3/25, Validation Loss: 0.0831\n",
      "          actual   predicted\n",
      "0     638.500002  602.680007\n",
      "1     799.199999  773.034068\n",
      "2     564.999993  550.237436\n",
      "3     619.500005  608.370527\n",
      "4     809.333334  785.786494\n",
      "...          ...         ...\n",
      "1019  669.999997  636.733233\n",
      "1020  615.999995  615.893816\n",
      "1021  611.718754  589.078650\n",
      "1022  605.999995  594.540168\n",
      "1023  629.047617  645.735112\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0788\n",
      "Epoch 4/25, Validation Loss: 0.0673\n",
      "          actual   predicted\n",
      "0     638.500002  648.121223\n",
      "1     799.199999  786.103221\n",
      "2     564.999993  570.433508\n",
      "3     619.500005  658.875092\n",
      "4     809.333334  817.291039\n",
      "...          ...         ...\n",
      "1019  669.999997  626.897992\n",
      "1020  615.999995  613.300228\n",
      "1021  611.718754  584.815435\n",
      "1022  605.999995  597.379592\n",
      "1023  629.047617  629.296245\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0756\n",
      "Epoch 5/25, Validation Loss: 0.0591\n",
      "          actual   predicted\n",
      "0     638.500002  636.796520\n",
      "1     799.199999  803.300707\n",
      "2     564.999993  572.375514\n",
      "3     619.500005  643.411582\n",
      "4     809.333334  820.663367\n",
      "...          ...         ...\n",
      "1019  669.999997  632.025538\n",
      "1020  615.999995  612.174687\n",
      "1021  611.718754  586.718174\n",
      "1022  605.999995  588.573785\n",
      "1023  629.047617  636.152955\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0731\n",
      "Epoch 6/25, Validation Loss: 0.0620\n",
      "          actual   predicted\n",
      "0     638.500002  652.725909\n",
      "1     799.199999  780.910561\n",
      "2     564.999993  571.501002\n",
      "3     619.500005  642.751215\n",
      "4     809.333334  794.419583\n",
      "...          ...         ...\n",
      "1019  669.999997  648.720061\n",
      "1020  615.999995  632.561497\n",
      "1021  611.718754  592.985976\n",
      "1022  605.999995  603.278719\n",
      "1023  629.047617  649.324593\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0713\n",
      "Epoch 7/25, Validation Loss: 0.0607\n",
      "          actual   predicted\n",
      "0     638.500002  645.291871\n",
      "1     799.199999  787.785209\n",
      "2     564.999993  570.415607\n",
      "3     619.500005  636.346281\n",
      "4     809.333334  812.862613\n",
      "...          ...         ...\n",
      "1019  669.999997  619.850513\n",
      "1020  615.999995  613.845317\n",
      "1021  611.718754  582.955290\n",
      "1022  605.999995  592.559506\n",
      "1023  629.047617  630.346702\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0712\n",
      "Epoch 8/25, Validation Loss: 0.0587\n",
      "          actual   predicted\n",
      "0     638.500002  634.507520\n",
      "1     799.199999  804.903456\n",
      "2     564.999993  562.412241\n",
      "3     619.500005  627.493661\n",
      "4     809.333334  819.653135\n",
      "...          ...         ...\n",
      "1019  669.999997  622.863199\n",
      "1020  615.999995  611.004638\n",
      "1021  611.718754  584.111643\n",
      "1022  605.999995  591.729830\n",
      "1023  629.047617  637.988751\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0688\n",
      "Epoch 9/25, Validation Loss: 0.0619\n",
      "          actual   predicted\n",
      "0     638.500002  630.431403\n",
      "1     799.199999  785.521366\n",
      "2     564.999993  568.106918\n",
      "3     619.500005  634.200280\n",
      "4     809.333334  801.345963\n",
      "...          ...         ...\n",
      "1019  669.999997  619.433816\n",
      "1020  615.999995  608.017839\n",
      "1021  611.718754  586.251338\n",
      "1022  605.999995  591.758618\n",
      "1023  629.047617  632.022839\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0682\n",
      "Epoch 10/25, Validation Loss: 0.0595\n",
      "          actual   predicted\n",
      "0     638.500002  628.413026\n",
      "1     799.199999  784.183965\n",
      "2     564.999993  566.546027\n",
      "3     619.500005  640.940629\n",
      "4     809.333334  807.355389\n",
      "...          ...         ...\n",
      "1019  669.999997  622.280253\n",
      "1020  615.999995  616.189697\n",
      "1021  611.718754  585.866002\n",
      "1022  605.999995  594.900450\n",
      "1023  629.047617  623.126731\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0676\n",
      "Epoch 11/25, Validation Loss: 0.0597\n",
      "          actual   predicted\n",
      "0     638.500002  639.896887\n",
      "1     799.199999  808.651470\n",
      "2     564.999993  568.860756\n",
      "3     619.500005  629.828424\n",
      "4     809.333334  842.565325\n",
      "...          ...         ...\n",
      "1019  669.999997  629.921487\n",
      "1020  615.999995  621.561635\n",
      "1021  611.718754  600.159511\n",
      "1022  605.999995  602.673183\n",
      "1023  629.047617  646.313100\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0670\n",
      "Epoch 12/25, Validation Loss: 0.0619\n",
      "          actual   predicted\n",
      "0     638.500002  642.132861\n",
      "1     799.199999  811.363661\n",
      "2     564.999993  569.570197\n",
      "3     619.500005  635.647415\n",
      "4     809.333334  824.377794\n",
      "...          ...         ...\n",
      "1019  669.999997  638.605239\n",
      "1020  615.999995  623.319665\n",
      "1021  611.718754  595.334969\n",
      "1022  605.999995  601.256499\n",
      "1023  629.047617  645.957510\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0645\n",
      "Epoch 13/25, Validation Loss: 0.0569\n",
      "          actual   predicted\n",
      "0     638.500002  642.762557\n",
      "1     799.199999  802.591088\n",
      "2     564.999993  569.383741\n",
      "3     619.500005  635.351926\n",
      "4     809.333334  821.662080\n",
      "...          ...         ...\n",
      "1019  669.999997  632.762244\n",
      "1020  615.999995  618.040853\n",
      "1021  611.718754  588.181170\n",
      "1022  605.999995  596.562562\n",
      "1023  629.047617  644.372286\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0642\n",
      "Epoch 14/25, Validation Loss: 0.0568\n",
      "          actual   predicted\n",
      "0     638.500002  641.937855\n",
      "1     799.199999  807.620529\n",
      "2     564.999993  570.312582\n",
      "3     619.500005  641.549664\n",
      "4     809.333334  828.826483\n",
      "...          ...         ...\n",
      "1019  669.999997  626.811267\n",
      "1020  615.999995  618.637322\n",
      "1021  611.718754  589.944220\n",
      "1022  605.999995  598.626498\n",
      "1023  629.047617  640.593588\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0639\n",
      "Epoch 15/25, Validation Loss: 0.0563\n",
      "          actual   predicted\n",
      "0     638.500002  632.681904\n",
      "1     799.199999  805.474057\n",
      "2     564.999993  555.678826\n",
      "3     619.500005  631.029096\n",
      "4     809.333334  824.114811\n",
      "...          ...         ...\n",
      "1019  669.999997  625.514081\n",
      "1020  615.999995  605.426839\n",
      "1021  611.718754  576.819458\n",
      "1022  605.999995  586.630728\n",
      "1023  629.047617  636.919108\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0637\n",
      "Epoch 16/25, Validation Loss: 0.0577\n",
      "          actual   predicted\n",
      "0     638.500002  639.238732\n",
      "1     799.199999  807.836006\n",
      "2     564.999993  562.364549\n",
      "3     619.500005  629.410629\n",
      "4     809.333334  832.852593\n",
      "...          ...         ...\n",
      "1019  669.999997  633.783128\n",
      "1020  615.999995  615.055465\n",
      "1021  611.718754  588.528399\n",
      "1022  605.999995  593.657874\n",
      "1023  629.047617  647.726286\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0634\n",
      "Epoch 17/25, Validation Loss: 0.0573\n",
      "          actual   predicted\n",
      "0     638.500002  633.785199\n",
      "1     799.199999  807.637453\n",
      "2     564.999993  561.971416\n",
      "3     619.500005  629.300921\n",
      "4     809.333334  834.836574\n",
      "...          ...         ...\n",
      "1019  669.999997  629.909877\n",
      "1020  615.999995  613.046156\n",
      "1021  611.718754  583.598950\n",
      "1022  605.999995  589.936110\n",
      "1023  629.047617  636.393127\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0631\n",
      "Epoch 18/25, Validation Loss: 0.0571\n",
      "          actual   predicted\n",
      "0     638.500002  627.351086\n",
      "1     799.199999  783.806730\n",
      "2     564.999993  563.657593\n",
      "3     619.500005  624.876367\n",
      "4     809.333334  816.070996\n",
      "...          ...         ...\n",
      "1019  669.999997  623.680119\n",
      "1020  615.999995  608.641825\n",
      "1021  611.718754  582.602586\n",
      "1022  605.999995  586.895938\n",
      "1023  629.047617  638.655159\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0627\n",
      "Epoch 19/25, Validation Loss: 0.0575\n",
      "          actual   predicted\n",
      "0     638.500002  639.917125\n",
      "1     799.199999  793.961160\n",
      "2     564.999993  567.011907\n",
      "3     619.500005  635.696535\n",
      "4     809.333334  818.864516\n",
      "...          ...         ...\n",
      "1019  669.999997  632.481879\n",
      "1020  615.999995  613.331306\n",
      "1021  611.718754  585.822828\n",
      "1022  605.999995  591.674466\n",
      "1023  629.047617  639.244066\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4369,  1.6490,  1.2161, -0.7247, -0.1103, -0.1009, -1.7508,\n",
      "          -0.5681, -1.9687, -1.9687, -0.8792,  4.1508,  0.0000, -0.9065,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -0.9411,\n",
      "          -1.0386, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4441,  1.6207,  1.1753, -0.6836, -0.1333, -0.1076, -1.6197,\n",
      "          -0.5681, -2.0056, -2.0056, -0.0644,  4.1508,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0007,\n",
      "          -0.9729, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4514,  1.5924,  1.1345, -0.6426, -0.1562, -0.1142, -1.4886,\n",
      "          -0.5681, -2.0425, -2.0425,  0.7504,  4.1508,  0.0000, -0.9062,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0603,\n",
      "          -0.9071, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4586,  1.5641,  1.0937, -0.6016, -0.1791, -0.1209, -1.3575,\n",
      "          -0.5681, -2.0794, -2.0794,  1.5652,  4.1508,  0.0000, -0.9061,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1199,\n",
      "          -0.8414, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4659,  1.5358,  1.0529, -0.5605, -0.2021, -0.1275, -1.2264,\n",
      "          -0.5681, -2.1163, -2.1163,  2.3800,  4.1508,  0.0000, -0.9059,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1795,\n",
      "          -0.7757, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.6707,  2.1820,  1.2569, -1.2558,  0.3838,  0.0256, -0.5708,\n",
      "          -0.3964,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9077,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.7766,\n",
      "          -1.1809, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.1923,  1.7008,  3.6011, -1.2260,  0.0849, -0.0560, -1.2592,\n",
      "          -0.4694,  0.8735,  0.8735, -0.3360, -0.7989,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.8472,\n",
      "          -1.1308, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.0926,  1.2596,  2.2197, -1.1127,  0.0272, -0.0676, -0.7894,\n",
      "          -0.3392,  1.2980,  1.2980,  1.7010, -0.7989,  0.0000, -0.9046,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.9160,\n",
      "          -1.0749, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2006,  1.1195,  1.8444, -1.1076,  0.0108, -0.0809, -0.5708,\n",
      "          -0.4822,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9043,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1995,\n",
      "          -0.7444, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2369,  0.9374,  0.9469, -1.1114,  0.0096, -0.0809, -1.8819,\n",
      "          -0.5681, -1.6549, -1.6549,  2.3800, -0.7989,  0.0000, -0.9042,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0351,\n",
      "          -0.9609, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2872,  0.8395,  0.8979, -1.0392, -0.0206, -0.0876, -1.8819,\n",
      "          -0.4822,  1.7594,  1.7594, -1.0150, -0.7989,  0.0000, -0.9041,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0958,\n",
      "          -0.8907, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3347,  0.7615,  0.8653, -1.0354, -0.0282, -0.0942, -1.2264,\n",
      "          -0.3105,  1.5748,  1.5748, -1.6940, -0.7989,  0.0000, -0.9039,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1517,\n",
      "          -0.8166, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3542,  0.8125,  0.8000, -0.9823, -0.0332, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -0.3360, -0.7989,  0.0000, -0.9038,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3532,\n",
      "          -0.4007, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3514,  0.8187,  0.7511, -0.8189, -0.0345, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -2.3730, -0.7989,  0.0000, -0.9037,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.2486,\n",
      "          -0.6584, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3682,  0.9183,  0.6178, -0.8569, -0.0887, -0.1153, -1.2264,\n",
      "          -0.3964,  1.5287,  1.5287,  0.1166, -0.7989,  0.0000, -0.9034,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3050,\n",
      "          -0.5368, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3919,  1.0263,  1.1713, -0.6042, -0.0465, -0.1042, -1.5541,\n",
      "          -0.5681,  1.6210,  1.6210,  1.7010, -0.7989,  0.0000, -0.9031,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3402,\n",
      "          -0.4420, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4268,  0.9811,  0.7021, -0.7657, -0.0358, -0.1042, -1.8819,\n",
      "          -0.5681,  1.7132,  1.7132, -1.0150, -0.7989,  0.0000, -0.9029,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3679,\n",
      "          -0.3469, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4156,  1.0019,  0.7103, -0.7429, -0.0383, -0.1042, -1.8819,\n",
      "          -0.5681,  1.5287,  1.5287, -0.3360, -0.7989,  0.0000, -0.9027,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3878,\n",
      "          -0.2562, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4184,  1.1060,  0.7755, -0.6783, -0.0358, -0.1009, -0.5708,\n",
      "          -0.5681,  1.5287,  1.5287,  0.3430, -0.7989,  0.0000, -0.9026,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4017,\n",
      "          -0.1644, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4017,  1.1528,  0.8000, -0.6099, -0.0358, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4825,  1.4825, -2.3730, -0.7989,  0.0000, -0.9025,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4096,\n",
      "          -0.0719, -1.1552,  1.1552,  0.0000]]])\n",
      "predicted: tensor([[-0.4514]], device='cuda:0')\n",
      "[661.72]\n",
      "           actual   predicted\n",
      "0      638.500002  639.917125\n",
      "1      799.199999  793.961160\n",
      "2      564.999993  567.011907\n",
      "3      619.500005  635.696535\n",
      "4      809.333334  818.864516\n",
      "...           ...         ...\n",
      "38802  739.999999  708.711077\n",
      "38803  549.499992  575.399778\n",
      "38804  664.800003  654.784284\n",
      "38805  818.499999  808.609617\n",
      "38806  601.499997  603.135548\n",
      "\n",
      "[38807 rows x 2 columns]\n",
      "Score (RMSE): 62.8562\n",
      "Score (MAE): 31.8041\n",
      "Score (ME): -1.3442\n",
      "Score (MAPE): 3.8822%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (199269, 27) to (200228, 27)\n",
      "training data cutoff:  2023-07-14 02:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([154985, 20, 25]) torch.Size([154985]) torch.Size([154985, 1])\n",
      "Testing data shape: torch.Size([39048, 20, 25]) torch.Size([39048]) torch.Size([39048, 1])\n",
      "Shuffled Training data shape: torch.Size([155226, 20, 25]) torch.Size([155226]) torch.Size([155226, 1])\n",
      "Shuffled Testing data shape: torch.Size([38807, 20, 25]) torch.Size([38807]) torch.Size([38807, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     35.590000  34.248538\n",
      "1     62.500000  65.217751\n",
      "2     42.475000  42.041443\n",
      "3     40.870000  41.556526\n",
      "4     34.440000  35.631778\n",
      "...         ...        ...\n",
      "1019  43.740000  43.733741\n",
      "1020  46.815000  47.094912\n",
      "1021  27.110000  26.767154\n",
      "1022  24.680001  23.624693\n",
      "1023  33.285000  32.640308\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.1303\n",
      "Epoch 1/25, Validation Loss: 0.0202\n",
      "         actual  predicted\n",
      "0     35.590000  35.468934\n",
      "1     62.500000  64.089427\n",
      "2     42.475000  42.088565\n",
      "3     40.870000  41.135728\n",
      "4     34.440000  35.690588\n",
      "...         ...        ...\n",
      "1019  43.740000  42.641140\n",
      "1020  46.815000  45.498903\n",
      "1021  27.110000  26.675528\n",
      "1022  24.680001  24.113025\n",
      "1023  33.285000  32.118283\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0424\n",
      "Epoch 2/25, Validation Loss: 0.0158\n",
      "         actual  predicted\n",
      "0     35.590000  36.000485\n",
      "1     62.500000  66.825806\n",
      "2     42.475000  42.417801\n",
      "3     40.870000  41.540568\n",
      "4     34.440000  35.620371\n",
      "...         ...        ...\n",
      "1019  43.740000  43.223326\n",
      "1020  46.815000  46.633266\n",
      "1021  27.110000  26.761571\n",
      "1022  24.680001  23.941379\n",
      "1023  33.285000  33.032061\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0304\n",
      "Epoch 3/25, Validation Loss: 0.0187\n",
      "         actual  predicted\n",
      "0     35.590000  34.999751\n",
      "1     62.500000  63.595625\n",
      "2     42.475000  41.816940\n",
      "3     40.870000  40.912198\n",
      "4     34.440000  34.640161\n",
      "...         ...        ...\n",
      "1019  43.740000  43.060241\n",
      "1020  46.815000  45.757921\n",
      "1021  27.110000  25.743526\n",
      "1022  24.680001  23.032745\n",
      "1023  33.285000  32.225745\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0247\n",
      "Epoch 4/25, Validation Loss: 0.0240\n",
      "         actual  predicted\n",
      "0     35.590000  35.677140\n",
      "1     62.500000  63.807081\n",
      "2     42.475000  42.889963\n",
      "3     40.870000  41.873559\n",
      "4     34.440000  34.744656\n",
      "...         ...        ...\n",
      "1019  43.740000  43.292250\n",
      "1020  46.815000  46.959837\n",
      "1021  27.110000  26.958637\n",
      "1022  24.680001  24.377483\n",
      "1023  33.285000  33.631750\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0231\n",
      "Epoch 5/25, Validation Loss: 0.0110\n",
      "         actual  predicted\n",
      "0     35.590000  35.996509\n",
      "1     62.500000  65.134984\n",
      "2     42.475000  43.258267\n",
      "3     40.870000  41.640930\n",
      "4     34.440000  35.096235\n",
      "...         ...        ...\n",
      "1019  43.740000  43.626715\n",
      "1020  46.815000  46.891416\n",
      "1021  27.110000  26.887550\n",
      "1022  24.680001  24.488329\n",
      "1023  33.285000  33.753955\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0197\n",
      "Epoch 6/25, Validation Loss: 0.0123\n",
      "         actual  predicted\n",
      "0     35.590000  35.597877\n",
      "1     62.500000  63.634526\n",
      "2     42.475000  42.416970\n",
      "3     40.870000  40.850473\n",
      "4     34.440000  34.258159\n",
      "...         ...        ...\n",
      "1019  43.740000  42.346088\n",
      "1020  46.815000  44.913225\n",
      "1021  27.110000  26.397830\n",
      "1022  24.680001  24.254747\n",
      "1023  33.285000  32.479050\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0179\n",
      "Epoch 7/25, Validation Loss: 0.0102\n",
      "         actual  predicted\n",
      "0     35.590000  36.115555\n",
      "1     62.500000  64.417231\n",
      "2     42.475000  42.898366\n",
      "3     40.870000  41.624912\n",
      "4     34.440000  35.260654\n",
      "...         ...        ...\n",
      "1019  43.740000  44.146515\n",
      "1020  46.815000  47.783457\n",
      "1021  27.110000  27.914255\n",
      "1022  24.680001  25.044195\n",
      "1023  33.285000  34.403759\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0171\n",
      "Epoch 8/25, Validation Loss: 0.0124\n",
      "         actual  predicted\n",
      "0     35.590000  36.348000\n",
      "1     62.500000  67.607053\n",
      "2     42.475000  43.775395\n",
      "3     40.870000  41.957300\n",
      "4     34.440000  35.320787\n",
      "...         ...        ...\n",
      "1019  43.740000  44.070668\n",
      "1020  46.815000  47.511938\n",
      "1021  27.110000  27.284479\n",
      "1022  24.680001  24.589428\n",
      "1023  33.285000  34.317361\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0156\n",
      "Epoch 9/25, Validation Loss: 0.0184\n",
      "         actual  predicted\n",
      "0     35.590000  36.047920\n",
      "1     62.500000  64.351204\n",
      "2     42.475000  42.236901\n",
      "3     40.870000  40.962766\n",
      "4     34.440000  34.829817\n",
      "...         ...        ...\n",
      "1019  43.740000  43.353736\n",
      "1020  46.815000  46.224469\n",
      "1021  27.110000  27.645317\n",
      "1022  24.680001  24.952447\n",
      "1023  33.285000  33.893515\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0149\n",
      "Epoch 10/25, Validation Loss: 0.0079\n",
      "         actual  predicted\n",
      "0     35.590000  35.094152\n",
      "1     62.500000  63.856311\n",
      "2     42.475000  41.907041\n",
      "3     40.870000  40.100580\n",
      "4     34.440000  34.201090\n",
      "...         ...        ...\n",
      "1019  43.740000  42.937771\n",
      "1020  46.815000  46.555167\n",
      "1021  27.110000  26.178848\n",
      "1022  24.680001  23.527496\n",
      "1023  33.285000  33.057858\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0140\n",
      "Epoch 11/25, Validation Loss: 0.0099\n",
      "         actual  predicted\n",
      "0     35.590000  35.888284\n",
      "1     62.500000  64.770760\n",
      "2     42.475000  42.566621\n",
      "3     40.870000  40.918085\n",
      "4     34.440000  34.641467\n",
      "...         ...        ...\n",
      "1019  43.740000  42.869709\n",
      "1020  46.815000  46.529886\n",
      "1021  27.110000  27.021628\n",
      "1022  24.680001  24.488787\n",
      "1023  33.285000  33.128417\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0132\n",
      "Epoch 12/25, Validation Loss: 0.0076\n",
      "         actual  predicted\n",
      "0     35.590000  35.948867\n",
      "1     62.500000  65.706884\n",
      "2     42.475000  43.285573\n",
      "3     40.870000  41.422881\n",
      "4     34.440000  34.777135\n",
      "...         ...        ...\n",
      "1019  43.740000  43.627758\n",
      "1020  46.815000  46.599989\n",
      "1021  27.110000  27.156673\n",
      "1022  24.680001  24.652287\n",
      "1023  33.285000  33.891761\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0125\n",
      "Epoch 13/25, Validation Loss: 0.0099\n",
      "         actual  predicted\n",
      "0     35.590000  35.988332\n",
      "1     62.500000  65.348462\n",
      "2     42.475000  42.797456\n",
      "3     40.870000  41.292472\n",
      "4     34.440000  35.009211\n",
      "...         ...        ...\n",
      "1019  43.740000  43.043528\n",
      "1020  46.815000  46.360301\n",
      "1021  27.110000  27.289749\n",
      "1022  24.680001  24.723626\n",
      "1023  33.285000  33.835052\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0123\n",
      "Epoch 14/25, Validation Loss: 0.0065\n",
      "         actual  predicted\n",
      "0     35.590000  36.303364\n",
      "1     62.500000  64.132598\n",
      "2     42.475000  43.116726\n",
      "3     40.870000  41.615204\n",
      "4     34.440000  35.184891\n",
      "...         ...        ...\n",
      "1019  43.740000  43.536082\n",
      "1020  46.815000  46.616231\n",
      "1021  27.110000  26.619666\n",
      "1022  24.680001  23.756896\n",
      "1023  33.285000  34.093651\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0119\n",
      "Epoch 15/25, Validation Loss: 0.0085\n",
      "         actual  predicted\n",
      "0     35.590000  35.790217\n",
      "1     62.500000  64.128754\n",
      "2     42.475000  42.817513\n",
      "3     40.870000  41.146341\n",
      "4     34.440000  34.936975\n",
      "...         ...        ...\n",
      "1019  43.740000  42.904159\n",
      "1020  46.815000  46.280503\n",
      "1021  27.110000  27.350537\n",
      "1022  24.680001  24.886265\n",
      "1023  33.285000  33.694639\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0112\n",
      "Epoch 16/25, Validation Loss: 0.0058\n",
      "         actual  predicted\n",
      "0     35.590000  36.033975\n",
      "1     62.500000  63.630576\n",
      "2     42.475000  42.666352\n",
      "3     40.870000  41.152919\n",
      "4     34.440000  34.576531\n",
      "...         ...        ...\n",
      "1019  43.740000  42.935554\n",
      "1020  46.815000  46.095037\n",
      "1021  27.110000  26.996293\n",
      "1022  24.680001  24.726087\n",
      "1023  33.285000  33.629153\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0110\n",
      "Epoch 17/25, Validation Loss: 0.0052\n",
      "         actual  predicted\n",
      "0     35.590000  36.396318\n",
      "1     62.500000  66.016787\n",
      "2     42.475000  44.016945\n",
      "3     40.870000  42.162355\n",
      "4     34.440000  35.007498\n",
      "...         ...        ...\n",
      "1019  43.740000  44.366474\n",
      "1020  46.815000  47.723159\n",
      "1021  27.110000  27.000639\n",
      "1022  24.680001  24.679962\n",
      "1023  33.285000  33.570634\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0107\n",
      "Epoch 18/25, Validation Loss: 0.0146\n",
      "         actual  predicted\n",
      "0     35.590000  35.526108\n",
      "1     62.500000  64.674091\n",
      "2     42.475000  42.667020\n",
      "3     40.870000  40.986205\n",
      "4     34.440000  34.350900\n",
      "...         ...        ...\n",
      "1019  43.740000  42.868819\n",
      "1020  46.815000  46.365865\n",
      "1021  27.110000  26.605705\n",
      "1022  24.680001  24.037524\n",
      "1023  33.285000  32.930128\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0113\n",
      "Epoch 19/25, Validation Loss: 0.0065\n",
      "         actual  predicted\n",
      "0     35.590000  35.362151\n",
      "1     62.500000  64.134626\n",
      "2     42.475000  42.227056\n",
      "3     40.870000  40.700889\n",
      "4     34.440000  34.368367\n",
      "...         ...        ...\n",
      "1019  43.740000  43.093340\n",
      "1020  46.815000  46.569260\n",
      "1021  27.110000  26.918310\n",
      "1022  24.680001  24.450920\n",
      "1023  33.285000  33.143413\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0101\n",
      "Epoch 20/25, Validation Loss: 0.0059\n",
      "         actual  predicted\n",
      "0     35.590000  36.084188\n",
      "1     62.500000  64.895904\n",
      "2     42.475000  43.203247\n",
      "3     40.870000  41.493875\n",
      "4     34.440000  34.977268\n",
      "...         ...        ...\n",
      "1019  43.740000  43.194398\n",
      "1020  46.815000  46.410586\n",
      "1021  27.110000  27.133664\n",
      "1022  24.680001  24.579253\n",
      "1023  33.285000  33.697390\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0106\n",
      "Epoch 21/25, Validation Loss: 0.0064\n",
      "         actual  predicted\n",
      "0     35.590000  36.032951\n",
      "1     62.500000  64.209920\n",
      "2     42.475000  42.986215\n",
      "3     40.870000  41.033305\n",
      "4     34.440000  34.649773\n",
      "...         ...        ...\n",
      "1019  43.740000  43.191846\n",
      "1020  46.815000  46.567717\n",
      "1021  27.110000  27.029742\n",
      "1022  24.680001  24.362894\n",
      "1023  33.285000  33.567467\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0094\n",
      "Epoch 22/25, Validation Loss: 0.0051\n",
      "         actual  predicted\n",
      "0     35.590000  35.881719\n",
      "1     62.500000  64.568435\n",
      "2     42.475000  42.402963\n",
      "3     40.870000  40.729039\n",
      "4     34.440000  34.320517\n",
      "...         ...        ...\n",
      "1019  43.740000  43.102480\n",
      "1020  46.815000  46.047910\n",
      "1021  27.110000  27.263590\n",
      "1022  24.680001  24.927966\n",
      "1023  33.285000  33.671339\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0092\n",
      "Epoch 23/25, Validation Loss: 0.0046\n",
      "         actual  predicted\n",
      "0     35.590000  35.799116\n",
      "1     62.500000  63.894685\n",
      "2     42.475000  42.521423\n",
      "3     40.870000  40.743024\n",
      "4     34.440000  34.499614\n",
      "...         ...        ...\n",
      "1019  43.740000  43.296816\n",
      "1020  46.815000  46.608225\n",
      "1021  27.110000  26.912667\n",
      "1022  24.680001  24.461519\n",
      "1023  33.285000  33.446094\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0091\n",
      "Epoch 24/25, Validation Loss: 0.0058\n",
      "         actual  predicted\n",
      "0     35.590000  35.889643\n",
      "1     62.500000  63.988893\n",
      "2     42.475000  42.880831\n",
      "3     40.870000  41.055312\n",
      "4     34.440000  34.704939\n",
      "...         ...        ...\n",
      "1019  43.740000  43.119260\n",
      "1020  46.815000  46.133077\n",
      "1021  27.110000  26.849097\n",
      "1022  24.680001  24.359539\n",
      "1023  33.285000  33.627722\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0090\n",
      "Epoch 25/25, Validation Loss: 0.0051\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4369,  1.6490,  1.2161, -0.7247, -0.1103, -0.1009, -1.7508,\n",
      "          -0.5681, -1.9687, -1.9687, -0.8792,  4.1508,  0.0000, -0.9065,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -0.9411,\n",
      "          -1.0386, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4441,  1.6207,  1.1753, -0.6836, -0.1333, -0.1076, -1.6197,\n",
      "          -0.5681, -2.0056, -2.0056, -0.0644,  4.1508,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0007,\n",
      "          -0.9729, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4514,  1.5924,  1.1345, -0.6426, -0.1562, -0.1142, -1.4886,\n",
      "          -0.5681, -2.0425, -2.0425,  0.7504,  4.1508,  0.0000, -0.9062,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0603,\n",
      "          -0.9071, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4586,  1.5641,  1.0937, -0.6016, -0.1791, -0.1209, -1.3575,\n",
      "          -0.5681, -2.0794, -2.0794,  1.5652,  4.1508,  0.0000, -0.9061,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1199,\n",
      "          -0.8414, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4659,  1.5358,  1.0529, -0.5605, -0.2021, -0.1275, -1.2264,\n",
      "          -0.5681, -2.1163, -2.1163,  2.3800,  4.1508,  0.0000, -0.9059,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1795,\n",
      "          -0.7757, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.6707,  2.1820,  1.2569, -1.2558,  0.3838,  0.0256, -0.5708,\n",
      "          -0.3964,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9077,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.7766,\n",
      "          -1.1809, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.1923,  1.7008,  3.6011, -1.2260,  0.0849, -0.0560, -1.2592,\n",
      "          -0.4694,  0.8735,  0.8735, -0.3360, -0.7989,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.8472,\n",
      "          -1.1308, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.0926,  1.2596,  2.2197, -1.1127,  0.0272, -0.0676, -0.7894,\n",
      "          -0.3392,  1.2980,  1.2980,  1.7010, -0.7989,  0.0000, -0.9046,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.9160,\n",
      "          -1.0749, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2006,  1.1195,  1.8444, -1.1076,  0.0108, -0.0809, -0.5708,\n",
      "          -0.4822,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9043,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1995,\n",
      "          -0.7444, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2369,  0.9374,  0.9469, -1.1114,  0.0096, -0.0809, -1.8819,\n",
      "          -0.5681, -1.6549, -1.6549,  2.3800, -0.7989,  0.0000, -0.9042,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0351,\n",
      "          -0.9609, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2872,  0.8395,  0.8979, -1.0392, -0.0206, -0.0876, -1.8819,\n",
      "          -0.4822,  1.7594,  1.7594, -1.0150, -0.7989,  0.0000, -0.9041,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0958,\n",
      "          -0.8907, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3347,  0.7615,  0.8653, -1.0354, -0.0282, -0.0942, -1.2264,\n",
      "          -0.3105,  1.5748,  1.5748, -1.6940, -0.7989,  0.0000, -0.9039,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1517,\n",
      "          -0.8166, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3542,  0.8125,  0.8000, -0.9823, -0.0332, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -0.3360, -0.7989,  0.0000, -0.9038,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3532,\n",
      "          -0.4007, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3514,  0.8187,  0.7511, -0.8189, -0.0345, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -2.3730, -0.7989,  0.0000, -0.9037,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.2486,\n",
      "          -0.6584, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3682,  0.9183,  0.6178, -0.8569, -0.0887, -0.1153, -1.2264,\n",
      "          -0.3964,  1.5287,  1.5287,  0.1166, -0.7989,  0.0000, -0.9034,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3050,\n",
      "          -0.5368, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3919,  1.0263,  1.1713, -0.6042, -0.0465, -0.1042, -1.5541,\n",
      "          -0.5681,  1.6210,  1.6210,  1.7010, -0.7989,  0.0000, -0.9031,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3402,\n",
      "          -0.4420, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4268,  0.9811,  0.7021, -0.7657, -0.0358, -0.1042, -1.8819,\n",
      "          -0.5681,  1.7132,  1.7132, -1.0150, -0.7989,  0.0000, -0.9029,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3679,\n",
      "          -0.3469, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4156,  1.0019,  0.7103, -0.7429, -0.0383, -0.1042, -1.8819,\n",
      "          -0.5681,  1.5287,  1.5287, -0.3360, -0.7989,  0.0000, -0.9027,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3878,\n",
      "          -0.2562, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4184,  1.1060,  0.7755, -0.6783, -0.0358, -0.1009, -0.5708,\n",
      "          -0.5681,  1.5287,  1.5287,  0.3430, -0.7989,  0.0000, -0.9026,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4017,\n",
      "          -0.1644, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4017,  1.1528,  0.8000, -0.6099, -0.0358, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4825,  1.4825, -2.3730, -0.7989,  0.0000, -0.9025,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4096,\n",
      "          -0.0719, -1.1552,  1.1552,  0.0000]]])\n",
      "predicted: tensor([[1.1456]], device='cuda:0')\n",
      "[46.52]\n",
      "        actual  predicted\n",
      "0      35.5900  35.889643\n",
      "1      62.5000  63.988893\n",
      "2      42.4750  42.880831\n",
      "3      40.8700  41.055312\n",
      "4      34.4400  34.704939\n",
      "...        ...        ...\n",
      "38802  48.5800  48.904463\n",
      "38803  49.3450  48.894560\n",
      "38804  32.6800  32.855287\n",
      "38805  38.6175  38.150807\n",
      "38806  38.5400  39.129776\n",
      "\n",
      "[38807 rows x 2 columns]\n",
      "Score (RMSE): 0.6877\n",
      "Score (MAE): 0.4326\n",
      "Score (ME): -0.0093\n",
      "Score (MAPE): 1.2758%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (199269, 27) to (200228, 27)\n",
      "training data cutoff:  2023-07-14 02:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([154985, 20, 25]) torch.Size([154985]) torch.Size([154985, 1])\n",
      "Testing data shape: torch.Size([39048, 20, 25]) torch.Size([39048]) torch.Size([39048, 1])\n",
      "Shuffled Training data shape: torch.Size([155226, 20, 25]) torch.Size([155226]) torch.Size([155226, 1])\n",
      "Shuffled Testing data shape: torch.Size([38807, 20, 25]) torch.Size([38807]) torch.Size([38807, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     22.510000  22.347036\n",
      "1     28.520000  29.374486\n",
      "2     24.355000  24.345254\n",
      "3     21.735000  21.511544\n",
      "4     29.563333  30.268106\n",
      "...         ...        ...\n",
      "1019  19.600000  18.935364\n",
      "1020  21.970000  22.133239\n",
      "1021  28.680000  28.924386\n",
      "1022  23.265000  23.349317\n",
      "1023  22.240000  22.237049\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.1167\n",
      "Epoch 1/25, Validation Loss: 0.0300\n",
      "         actual  predicted\n",
      "0     22.510000  22.230445\n",
      "1     28.520000  29.172851\n",
      "2     24.355000  23.984168\n",
      "3     21.735000  21.025864\n",
      "4     29.563333  30.466455\n",
      "...         ...        ...\n",
      "1019  19.600000  19.240807\n",
      "1020  21.970000  22.413546\n",
      "1021  28.680000  29.431874\n",
      "1022  23.265000  23.500276\n",
      "1023  22.240000  22.630139\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0379\n",
      "Epoch 2/25, Validation Loss: 0.0343\n",
      "         actual  predicted\n",
      "0     22.510000  22.475553\n",
      "1     28.520000  29.504095\n",
      "2     24.355000  24.398667\n",
      "3     21.735000  21.586542\n",
      "4     29.563333  31.083151\n",
      "...         ...        ...\n",
      "1019  19.600000  19.209102\n",
      "1020  21.970000  22.452845\n",
      "1021  28.680000  29.450790\n",
      "1022  23.265000  23.662994\n",
      "1023  22.240000  22.741130\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0301\n",
      "Epoch 3/25, Validation Loss: 0.0514\n",
      "         actual  predicted\n",
      "0     22.510000  22.205374\n",
      "1     28.520000  28.524835\n",
      "2     24.355000  24.007426\n",
      "3     21.735000  21.346388\n",
      "4     29.563333  30.141475\n",
      "...         ...        ...\n",
      "1019  19.600000  19.253761\n",
      "1020  21.970000  22.126665\n",
      "1021  28.680000  28.542920\n",
      "1022  23.265000  23.368840\n",
      "1023  22.240000  22.425186\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0253\n",
      "Epoch 4/25, Validation Loss: 0.0153\n",
      "         actual  predicted\n",
      "0     22.510000  22.350784\n",
      "1     28.520000  28.654052\n",
      "2     24.355000  24.265797\n",
      "3     21.735000  21.456297\n",
      "4     29.563333  29.994442\n",
      "...         ...        ...\n",
      "1019  19.600000  19.593806\n",
      "1020  21.970000  22.212191\n",
      "1021  28.680000  28.733410\n",
      "1022  23.265000  23.443433\n",
      "1023  22.240000  22.612203\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0225\n",
      "Epoch 5/25, Validation Loss: 0.0134\n",
      "         actual  predicted\n",
      "0     22.510000  22.416799\n",
      "1     28.520000  28.507588\n",
      "2     24.355000  24.233352\n",
      "3     21.735000  21.569912\n",
      "4     29.563333  30.209814\n",
      "...         ...        ...\n",
      "1019  19.600000  19.458421\n",
      "1020  21.970000  22.175850\n",
      "1021  28.680000  28.926544\n",
      "1022  23.265000  23.559765\n",
      "1023  22.240000  22.831998\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0198\n",
      "Epoch 6/25, Validation Loss: 0.0142\n",
      "         actual  predicted\n",
      "0     22.510000  22.000361\n",
      "1     28.520000  28.521703\n",
      "2     24.355000  24.099685\n",
      "3     21.735000  21.145273\n",
      "4     29.563333  30.116635\n",
      "...         ...        ...\n",
      "1019  19.600000  19.025458\n",
      "1020  21.970000  21.937843\n",
      "1021  28.680000  28.503388\n",
      "1022  23.265000  23.303377\n",
      "1023  22.240000  22.362296\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0182\n",
      "Epoch 7/25, Validation Loss: 0.0144\n",
      "         actual  predicted\n",
      "0     22.510000  22.099728\n",
      "1     28.520000  28.578664\n",
      "2     24.355000  24.080068\n",
      "3     21.735000  21.368040\n",
      "4     29.563333  30.429732\n",
      "...         ...        ...\n",
      "1019  19.600000  19.151966\n",
      "1020  21.970000  21.759992\n",
      "1021  28.680000  28.550734\n",
      "1022  23.265000  23.125106\n",
      "1023  22.240000  22.330227\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0169\n",
      "Epoch 8/25, Validation Loss: 0.0258\n",
      "         actual  predicted\n",
      "0     22.510000  22.223463\n",
      "1     28.520000  28.277780\n",
      "2     24.355000  24.196189\n",
      "3     21.735000  21.339901\n",
      "4     29.563333  29.799998\n",
      "...         ...        ...\n",
      "1019  19.600000  19.111248\n",
      "1020  21.970000  21.900111\n",
      "1021  28.680000  28.407977\n",
      "1022  23.265000  23.226554\n",
      "1023  22.240000  22.252122\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0155\n",
      "Epoch 9/25, Validation Loss: 0.0096\n",
      "         actual  predicted\n",
      "0     22.510000  22.571768\n",
      "1     28.520000  28.542291\n",
      "2     24.355000  24.525946\n",
      "3     21.735000  21.668462\n",
      "4     29.563333  30.203931\n",
      "...         ...        ...\n",
      "1019  19.600000  19.595114\n",
      "1020  21.970000  22.384450\n",
      "1021  28.680000  28.467373\n",
      "1022  23.265000  23.440875\n",
      "1023  22.240000  22.586148\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0147\n",
      "Epoch 10/25, Validation Loss: 0.0063\n",
      "         actual  predicted\n",
      "0     22.510000  22.443090\n",
      "1     28.520000  28.624363\n",
      "2     24.355000  24.503044\n",
      "3     21.735000  21.644227\n",
      "4     29.563333  30.357046\n",
      "...         ...        ...\n",
      "1019  19.600000  19.587703\n",
      "1020  21.970000  22.158457\n",
      "1021  28.680000  28.576250\n",
      "1022  23.265000  23.331492\n",
      "1023  22.240000  22.528812\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0137\n",
      "Epoch 11/25, Validation Loss: 0.0072\n",
      "         actual  predicted\n",
      "0     22.510000  22.321814\n",
      "1     28.520000  28.207307\n",
      "2     24.355000  24.309995\n",
      "3     21.735000  21.375146\n",
      "4     29.563333  29.902087\n",
      "...         ...        ...\n",
      "1019  19.600000  19.225442\n",
      "1020  21.970000  22.005824\n",
      "1021  28.680000  28.436814\n",
      "1022  23.265000  23.268833\n",
      "1023  22.240000  22.274600\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0131\n",
      "Epoch 12/25, Validation Loss: 0.0055\n",
      "         actual  predicted\n",
      "0     22.510000  22.418382\n",
      "1     28.520000  28.463075\n",
      "2     24.355000  24.253532\n",
      "3     21.735000  21.596649\n",
      "4     29.563333  30.105427\n",
      "...         ...        ...\n",
      "1019  19.600000  19.745129\n",
      "1020  21.970000  22.094976\n",
      "1021  28.680000  28.604440\n",
      "1022  23.265000  23.210994\n",
      "1023  22.240000  22.538651\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0125\n",
      "Epoch 13/25, Validation Loss: 0.0050\n",
      "         actual  predicted\n",
      "0     22.510000  22.350656\n",
      "1     28.520000  28.086645\n",
      "2     24.355000  24.325645\n",
      "3     21.735000  21.596547\n",
      "4     29.563333  29.794090\n",
      "...         ...        ...\n",
      "1019  19.600000  19.484136\n",
      "1020  21.970000  22.029735\n",
      "1021  28.680000  28.251746\n",
      "1022  23.265000  23.283281\n",
      "1023  22.240000  22.285863\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0123\n",
      "Epoch 14/25, Validation Loss: 0.0075\n",
      "         actual  predicted\n",
      "0     22.510000  22.325538\n",
      "1     28.520000  28.413815\n",
      "2     24.355000  24.440961\n",
      "3     21.735000  21.547045\n",
      "4     29.563333  30.172395\n",
      "...         ...        ...\n",
      "1019  19.600000  19.631102\n",
      "1020  21.970000  22.272833\n",
      "1021  28.680000  28.310301\n",
      "1022  23.265000  23.353740\n",
      "1023  22.240000  22.545393\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0116\n",
      "Epoch 15/25, Validation Loss: 0.0042\n",
      "         actual  predicted\n",
      "0     22.510000  22.404654\n",
      "1     28.520000  28.616767\n",
      "2     24.355000  24.359629\n",
      "3     21.735000  21.541230\n",
      "4     29.563333  30.307829\n",
      "...         ...        ...\n",
      "1019  19.600000  19.430375\n",
      "1020  21.970000  22.286042\n",
      "1021  28.680000  28.274965\n",
      "1022  23.265000  23.228284\n",
      "1023  22.240000  22.462621\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0114\n",
      "Epoch 16/25, Validation Loss: 0.0046\n",
      "         actual  predicted\n",
      "0     22.510000  22.355850\n",
      "1     28.520000  28.251893\n",
      "2     24.355000  24.325065\n",
      "3     21.735000  21.467211\n",
      "4     29.563333  29.997636\n",
      "...         ...        ...\n",
      "1019  19.600000  19.746949\n",
      "1020  21.970000  22.282807\n",
      "1021  28.680000  28.406770\n",
      "1022  23.265000  23.389305\n",
      "1023  22.240000  22.585249\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0109\n",
      "Epoch 17/25, Validation Loss: 0.0044\n",
      "         actual  predicted\n",
      "0     22.510000  22.352392\n",
      "1     28.520000  28.435863\n",
      "2     24.355000  24.412368\n",
      "3     21.735000  21.613460\n",
      "4     29.563333  30.393805\n",
      "...         ...        ...\n",
      "1019  19.600000  19.623254\n",
      "1020  21.970000  22.056233\n",
      "1021  28.680000  28.578041\n",
      "1022  23.265000  23.306181\n",
      "1023  22.240000  22.269049\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0107\n",
      "Epoch 18/25, Validation Loss: 0.0056\n",
      "         actual  predicted\n",
      "0     22.510000  22.232904\n",
      "1     28.520000  28.205383\n",
      "2     24.355000  24.337289\n",
      "3     21.735000  21.407130\n",
      "4     29.563333  30.151266\n",
      "...         ...        ...\n",
      "1019  19.600000  19.370702\n",
      "1020  21.970000  22.199692\n",
      "1021  28.680000  28.746387\n",
      "1022  23.265000  23.329759\n",
      "1023  22.240000  22.480898\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0104\n",
      "Epoch 19/25, Validation Loss: 0.0063\n",
      "         actual  predicted\n",
      "0     22.510000  22.324087\n",
      "1     28.520000  28.296011\n",
      "2     24.355000  24.405723\n",
      "3     21.735000  21.567715\n",
      "4     29.563333  30.097571\n",
      "...         ...        ...\n",
      "1019  19.600000  19.476536\n",
      "1020  21.970000  22.195629\n",
      "1021  28.680000  28.395664\n",
      "1022  23.265000  23.311495\n",
      "1023  22.240000  22.480112\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4369,  1.6490,  1.2161, -0.7247, -0.1103, -0.1009, -1.7508,\n",
      "          -0.5681, -1.9687, -1.9687, -0.8792,  4.1508,  0.0000, -0.9065,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -0.9411,\n",
      "          -1.0386, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4441,  1.6207,  1.1753, -0.6836, -0.1333, -0.1076, -1.6197,\n",
      "          -0.5681, -2.0056, -2.0056, -0.0644,  4.1508,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0007,\n",
      "          -0.9729, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4514,  1.5924,  1.1345, -0.6426, -0.1562, -0.1142, -1.4886,\n",
      "          -0.5681, -2.0425, -2.0425,  0.7504,  4.1508,  0.0000, -0.9062,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0603,\n",
      "          -0.9071, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4586,  1.5641,  1.0937, -0.6016, -0.1791, -0.1209, -1.3575,\n",
      "          -0.5681, -2.0794, -2.0794,  1.5652,  4.1508,  0.0000, -0.9061,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1199,\n",
      "          -0.8414, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4659,  1.5358,  1.0529, -0.5605, -0.2021, -0.1275, -1.2264,\n",
      "          -0.5681, -2.1163, -2.1163,  2.3800,  4.1508,  0.0000, -0.9059,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1795,\n",
      "          -0.7757, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.6707,  2.1820,  1.2569, -1.2558,  0.3838,  0.0256, -0.5708,\n",
      "          -0.3964,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9077,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.7766,\n",
      "          -1.1809, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.1923,  1.7008,  3.6011, -1.2260,  0.0849, -0.0560, -1.2592,\n",
      "          -0.4694,  0.8735,  0.8735, -0.3360, -0.7989,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.8472,\n",
      "          -1.1308, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.0926,  1.2596,  2.2197, -1.1127,  0.0272, -0.0676, -0.7894,\n",
      "          -0.3392,  1.2980,  1.2980,  1.7010, -0.7989,  0.0000, -0.9046,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.9160,\n",
      "          -1.0749, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2006,  1.1195,  1.8444, -1.1076,  0.0108, -0.0809, -0.5708,\n",
      "          -0.4822,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9043,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1995,\n",
      "          -0.7444, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2369,  0.9374,  0.9469, -1.1114,  0.0096, -0.0809, -1.8819,\n",
      "          -0.5681, -1.6549, -1.6549,  2.3800, -0.7989,  0.0000, -0.9042,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0351,\n",
      "          -0.9609, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2872,  0.8395,  0.8979, -1.0392, -0.0206, -0.0876, -1.8819,\n",
      "          -0.4822,  1.7594,  1.7594, -1.0150, -0.7989,  0.0000, -0.9041,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0958,\n",
      "          -0.8907, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3347,  0.7615,  0.8653, -1.0354, -0.0282, -0.0942, -1.2264,\n",
      "          -0.3105,  1.5748,  1.5748, -1.6940, -0.7989,  0.0000, -0.9039,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1517,\n",
      "          -0.8166, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3542,  0.8125,  0.8000, -0.9823, -0.0332, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -0.3360, -0.7989,  0.0000, -0.9038,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3532,\n",
      "          -0.4007, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3514,  0.8187,  0.7511, -0.8189, -0.0345, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -2.3730, -0.7989,  0.0000, -0.9037,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.2486,\n",
      "          -0.6584, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3682,  0.9183,  0.6178, -0.8569, -0.0887, -0.1153, -1.2264,\n",
      "          -0.3964,  1.5287,  1.5287,  0.1166, -0.7989,  0.0000, -0.9034,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3050,\n",
      "          -0.5368, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3919,  1.0263,  1.1713, -0.6042, -0.0465, -0.1042, -1.5541,\n",
      "          -0.5681,  1.6210,  1.6210,  1.7010, -0.7989,  0.0000, -0.9031,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3402,\n",
      "          -0.4420, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4268,  0.9811,  0.7021, -0.7657, -0.0358, -0.1042, -1.8819,\n",
      "          -0.5681,  1.7132,  1.7132, -1.0150, -0.7989,  0.0000, -0.9029,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3679,\n",
      "          -0.3469, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4156,  1.0019,  0.7103, -0.7429, -0.0383, -0.1042, -1.8819,\n",
      "          -0.5681,  1.5287,  1.5287, -0.3360, -0.7989,  0.0000, -0.9027,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3878,\n",
      "          -0.2562, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4184,  1.1060,  0.7755, -0.6783, -0.0358, -0.1009, -0.5708,\n",
      "          -0.5681,  1.5287,  1.5287,  0.3430, -0.7989,  0.0000, -0.9026,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4017,\n",
      "          -0.1644, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4017,  1.1528,  0.8000, -0.6099, -0.0358, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4825,  1.4825, -2.3730, -0.7989,  0.0000, -0.9025,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4096,\n",
      "          -0.0719, -1.1552,  1.1552,  0.0000]]])\n",
      "predicted: tensor([[0.2836]], device='cuda:0')\n",
      "[25.06]\n",
      "          actual  predicted\n",
      "0      22.510000  22.324087\n",
      "1      28.520000  28.296011\n",
      "2      24.355000  24.405723\n",
      "3      21.735000  21.567715\n",
      "4      29.563333  30.097571\n",
      "...          ...        ...\n",
      "38802  22.735000  22.797732\n",
      "38803  21.870000  21.935123\n",
      "38804  23.635000  23.860444\n",
      "38805  25.130000  24.945004\n",
      "38806  28.505000  28.496948\n",
      "\n",
      "[38807 rows x 2 columns]\n",
      "Score (RMSE): 0.2375\n",
      "Score (MAE): 0.1367\n",
      "Score (ME): -0.0368\n",
      "Score (MAPE): 0.5543%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (199269, 27) to (200228, 27)\n",
      "training data cutoff:  2023-07-14 02:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([154985, 20, 25]) torch.Size([154985]) torch.Size([154985, 1])\n",
      "Testing data shape: torch.Size([39048, 20, 25]) torch.Size([39048]) torch.Size([39048, 1])\n",
      "Shuffled Training data shape: torch.Size([155226, 20, 25]) torch.Size([155226]) torch.Size([155226, 1])\n",
      "Shuffled Testing data shape: torch.Size([38807, 20, 25]) torch.Size([38807]) torch.Size([38807, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual    predicted\n",
      "0     771.200006  1226.760842\n",
      "1     320.999995   538.009923\n",
      "2     293.666664   177.745848\n",
      "3       3.999999    73.405022\n",
      "4      88.000002   125.618843\n",
      "...          ...          ...\n",
      "1019    5.500000    66.125658\n",
      "1020   58.500004    88.112368\n",
      "1021   46.000000   103.554452\n",
      "1022    5.999996    59.353317\n",
      "1023    5.500000    78.495103\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.7130\n",
      "Epoch 1/25, Validation Loss: 0.6750\n",
      "          actual    predicted\n",
      "0     771.200006  1460.563832\n",
      "1     320.999995   758.522439\n",
      "2     293.666664   287.030795\n",
      "3       3.999999    23.158475\n",
      "4      88.000002   155.948474\n",
      "...          ...          ...\n",
      "1019    5.500000     6.881294\n",
      "1020   58.500004    24.970370\n",
      "1021   46.000000    44.128870\n",
      "1022    5.999996     0.715721\n",
      "1023    5.500000    13.788322\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.5713\n",
      "Epoch 2/25, Validation Loss: 0.6779\n",
      "          actual    predicted\n",
      "0     771.200006  1383.009325\n",
      "1     320.999995   928.029436\n",
      "2     293.666664    88.571978\n",
      "3       3.999999   -23.821690\n",
      "4      88.000002    15.547901\n",
      "...          ...          ...\n",
      "1019    5.500000   -28.669828\n",
      "1020   58.500004   -13.835176\n",
      "1021   46.000000    11.580470\n",
      "1022    5.999996   -41.891013\n",
      "1023    5.500000   -33.580987\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.5632\n",
      "Epoch 3/25, Validation Loss: 0.6384\n",
      "          actual    predicted\n",
      "0     771.200006  1163.061174\n",
      "1     320.999995   491.389497\n",
      "2     293.666664   128.575022\n",
      "3       3.999999    15.405858\n",
      "4      88.000002    72.747132\n",
      "...          ...          ...\n",
      "1019    5.500000    21.828479\n",
      "1020   58.500004    28.768578\n",
      "1021   46.000000    62.675669\n",
      "1022    5.999996    11.017004\n",
      "1023    5.500000    17.112318\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.5304\n",
      "Epoch 4/25, Validation Loss: 0.6517\n",
      "          actual    predicted\n",
      "0     771.200006  1335.190502\n",
      "1     320.999995   563.363992\n",
      "2     293.666664   221.506055\n",
      "3       3.999999    30.872875\n",
      "4      88.000002   121.400893\n",
      "...          ...          ...\n",
      "1019    5.500000    22.494210\n",
      "1020   58.500004    28.098730\n",
      "1021   46.000000    82.237175\n",
      "1022    5.999996    15.594823\n",
      "1023    5.500000    25.383241\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.5272\n",
      "Epoch 5/25, Validation Loss: 0.6498\n",
      "          actual    predicted\n",
      "0     771.200006  1077.403774\n",
      "1     320.999995   596.732551\n",
      "2     293.666664   343.052624\n",
      "3       3.999999    69.163498\n",
      "4      88.000002   235.542454\n",
      "...          ...          ...\n",
      "1019    5.500000    71.698029\n",
      "1020   58.500004   101.798935\n",
      "1021   46.000000   170.145870\n",
      "1022    5.999996    60.165148\n",
      "1023    5.500000    72.512440\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.5286\n",
      "Epoch 6/25, Validation Loss: 0.5952\n",
      "          actual    predicted\n",
      "0     771.200006  1355.557216\n",
      "1     320.999995   619.661935\n",
      "2     293.666664    27.653817\n",
      "3       3.999999     1.261421\n",
      "4      88.000002    17.976383\n",
      "...          ...          ...\n",
      "1019    5.500000    -0.138881\n",
      "1020   58.500004     2.856413\n",
      "1021   46.000000     7.028885\n",
      "1022    5.999996    -1.913033\n",
      "1023    5.500000     0.705644\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.5366\n",
      "Epoch 7/25, Validation Loss: 0.6205\n",
      "          actual   predicted\n",
      "0     771.200006  894.925495\n",
      "1     320.999995  265.801791\n",
      "2     293.666664   99.174055\n",
      "3       3.999999   20.176783\n",
      "4      88.000002   48.394275\n",
      "...          ...         ...\n",
      "1019    5.500000   16.214331\n",
      "1020   58.500004   20.371804\n",
      "1021   46.000000   32.514813\n",
      "1022    5.999996    5.027646\n",
      "1023    5.500000   20.177398\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.5176\n",
      "Epoch 8/25, Validation Loss: 0.5796\n",
      "          actual   predicted\n",
      "0     771.200006  854.282907\n",
      "1     320.999995  459.785616\n",
      "2     293.666664  240.941160\n",
      "3       3.999999   39.419948\n",
      "4      88.000002   97.585231\n",
      "...          ...         ...\n",
      "1019    5.500000   35.143993\n",
      "1020   58.500004   48.431533\n",
      "1021   46.000000   59.687826\n",
      "1022    5.999996   26.776541\n",
      "1023    5.500000   39.048641\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.5029\n",
      "Epoch 9/25, Validation Loss: 0.5784\n",
      "          actual   predicted\n",
      "0     771.200006  982.045271\n",
      "1     320.999995  395.899821\n",
      "2     293.666664  176.826856\n",
      "3       3.999999   38.460029\n",
      "4      88.000002  160.790409\n",
      "...          ...         ...\n",
      "1019    5.500000   27.601454\n",
      "1020   58.500004   41.023776\n",
      "1021   46.000000   63.376719\n",
      "1022    5.999996   20.282680\n",
      "1023    5.500000   35.033353\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.5040\n",
      "Epoch 10/25, Validation Loss: 0.6129\n",
      "          actual   predicted\n",
      "0     771.200006  801.835522\n",
      "1     320.999995  372.861204\n",
      "2     293.666664   84.412973\n",
      "3       3.999999   25.739834\n",
      "4      88.000002   56.823800\n",
      "...          ...         ...\n",
      "1019    5.500000   20.498329\n",
      "1020   58.500004   35.796169\n",
      "1021   46.000000   51.612031\n",
      "1022    5.999996   15.154594\n",
      "1023    5.500000   25.554334\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.4796\n",
      "Epoch 11/25, Validation Loss: 0.5496\n",
      "          actual   predicted\n",
      "0     771.200006  985.031339\n",
      "1     320.999995  340.372406\n",
      "2     293.666664   83.139361\n",
      "3       3.999999    8.319434\n",
      "4      88.000002   52.003600\n",
      "...          ...         ...\n",
      "1019    5.500000    5.602549\n",
      "1020   58.500004   25.842419\n",
      "1021   46.000000   39.922807\n",
      "1022    5.999996   -0.698680\n",
      "1023    5.500000   11.347031\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.5015\n",
      "Epoch 12/25, Validation Loss: 0.5856\n",
      "          actual    predicted\n",
      "0     771.200006  1114.688135\n",
      "1     320.999995   298.900410\n",
      "2     293.666664    93.307259\n",
      "3       3.999999    42.521671\n",
      "4      88.000002    71.613719\n",
      "...          ...          ...\n",
      "1019    5.500000    39.824137\n",
      "1020   58.500004    55.955478\n",
      "1021   46.000000    58.738410\n",
      "1022    5.999996    34.468148\n",
      "1023    5.500000    43.120964\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.4855\n",
      "Epoch 13/25, Validation Loss: 0.6236\n",
      "          actual   predicted\n",
      "0     771.200006  943.845606\n",
      "1     320.999995  488.817009\n",
      "2     293.666664  212.170803\n",
      "3       3.999999   25.002992\n",
      "4      88.000002  149.827815\n",
      "...          ...         ...\n",
      "1019    5.500000   23.288595\n",
      "1020   58.500004   48.372688\n",
      "1021   46.000000   55.768050\n",
      "1022    5.999996   16.783024\n",
      "1023    5.500000   25.738296\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.4821\n",
      "Epoch 14/25, Validation Loss: 0.6222\n",
      "          actual   predicted\n",
      "0     771.200006  919.823071\n",
      "1     320.999995  516.892410\n",
      "2     293.666664  331.822530\n",
      "3       3.999999   41.375531\n",
      "4      88.000002  225.599816\n",
      "...          ...         ...\n",
      "1019    5.500000   38.938959\n",
      "1020   58.500004   82.007899\n",
      "1021   46.000000  169.488535\n",
      "1022    5.999996   23.517788\n",
      "1023    5.500000   47.469473\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.4881\n",
      "Epoch 15/25, Validation Loss: 0.5693\n",
      "          actual   predicted\n",
      "0     771.200006  899.704936\n",
      "1     320.999995  281.378980\n",
      "2     293.666664  143.705917\n",
      "3       3.999999   39.146471\n",
      "4      88.000002   84.972133\n",
      "...          ...         ...\n",
      "1019    5.500000   35.193978\n",
      "1020   58.500004   50.420719\n",
      "1021   46.000000   64.283163\n",
      "1022    5.999996   27.195799\n",
      "1023    5.500000   38.148217\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.4522\n",
      "Epoch 16/25, Validation Loss: 0.5451\n",
      "          actual    predicted\n",
      "0     771.200006  1169.496025\n",
      "1     320.999995   431.985194\n",
      "2     293.666664   212.102325\n",
      "3       3.999999    23.003314\n",
      "4      88.000002   103.629649\n",
      "...          ...          ...\n",
      "1019    5.500000    21.541482\n",
      "1020   58.500004    42.013513\n",
      "1021   46.000000    63.544701\n",
      "1022    5.999996    10.191073\n",
      "1023    5.500000    21.001625\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.4367\n",
      "Epoch 17/25, Validation Loss: 0.5142\n",
      "          actual    predicted\n",
      "0     771.200006  1096.192812\n",
      "1     320.999995   478.294305\n",
      "2     293.666664   234.143492\n",
      "3       3.999999    -6.715633\n",
      "4      88.000002   105.634898\n",
      "...          ...          ...\n",
      "1019    5.500000   -10.871782\n",
      "1020   58.500004    17.025429\n",
      "1021   46.000000    46.373188\n",
      "1022    5.999996   -23.079739\n",
      "1023    5.500000   -10.680629\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.4652\n",
      "Epoch 18/25, Validation Loss: 0.5465\n",
      "          actual    predicted\n",
      "0     771.200006  1042.798803\n",
      "1     320.999995   342.254406\n",
      "2     293.666664   212.162533\n",
      "3       3.999999    18.580076\n",
      "4      88.000002    80.418301\n",
      "...          ...          ...\n",
      "1019    5.500000    21.585022\n",
      "1020   58.500004    52.625094\n",
      "1021   46.000000    50.562254\n",
      "1022    5.999996     7.322032\n",
      "1023    5.500000    13.639572\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.4389\n",
      "Epoch 19/25, Validation Loss: 0.5445\n",
      "          actual    predicted\n",
      "0     771.200006  1227.793386\n",
      "1     320.999995   502.064287\n",
      "2     293.666664   228.554537\n",
      "3       3.999999    16.917533\n",
      "4      88.000002   121.357868\n",
      "...          ...          ...\n",
      "1019    5.500000    15.308986\n",
      "1020   58.500004    34.272772\n",
      "1021   46.000000    49.916867\n",
      "1022    5.999996     2.851374\n",
      "1023    5.500000    17.123247\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.4343\n",
      "Epoch 20/25, Validation Loss: 0.5294\n",
      "          actual    predicted\n",
      "0     771.200006  1026.383044\n",
      "1     320.999995   294.511295\n",
      "2     293.666664   103.797808\n",
      "3       3.999999     7.726067\n",
      "4      88.000002    47.940171\n",
      "...          ...          ...\n",
      "1019    5.500000     6.092858\n",
      "1020   58.500004    23.121914\n",
      "1021   46.000000    26.376693\n",
      "1022    5.999996    -1.248567\n",
      "1023    5.500000     4.249855\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.4031\n",
      "Epoch 21/25, Validation Loss: 0.5004\n",
      "          actual   predicted\n",
      "0     771.200006  936.304026\n",
      "1     320.999995  411.296047\n",
      "2     293.666664  253.582409\n",
      "3       3.999999   13.481206\n",
      "4      88.000002  157.245408\n",
      "...          ...         ...\n",
      "1019    5.500000   10.021447\n",
      "1020   58.500004   39.304707\n",
      "1021   46.000000   84.968171\n",
      "1022    5.999996   -1.440513\n",
      "1023    5.500000    8.249447\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.4078\n",
      "Epoch 22/25, Validation Loss: 0.5035\n",
      "          actual    predicted\n",
      "0     771.200006  1026.911144\n",
      "1     320.999995   306.712474\n",
      "2     293.666664   155.561028\n",
      "3       3.999999    18.636294\n",
      "4      88.000002    86.832334\n",
      "...          ...          ...\n",
      "1019    5.500000    19.124155\n",
      "1020   58.500004    41.323464\n",
      "1021   46.000000    46.655962\n",
      "1022    5.999996     9.606743\n",
      "1023    5.500000    13.497588\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.3912\n",
      "Epoch 23/25, Validation Loss: 0.4528\n",
      "          actual   predicted\n",
      "0     771.200006  777.928051\n",
      "1     320.999995  326.750415\n",
      "2     293.666664  202.182596\n",
      "3       3.999999    8.683112\n",
      "4      88.000002   88.978810\n",
      "...          ...         ...\n",
      "1019    5.500000    1.683435\n",
      "1020   58.500004   32.223109\n",
      "1021   46.000000   57.824290\n",
      "1022    5.999996   -5.493876\n",
      "1023    5.500000   -0.849204\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.3994\n",
      "Epoch 24/25, Validation Loss: 0.4949\n",
      "          actual   predicted\n",
      "0     771.200006  848.043125\n",
      "1     320.999995  378.400599\n",
      "2     293.666664  274.593344\n",
      "3       3.999999   30.889505\n",
      "4      88.000002  156.812355\n",
      "...          ...         ...\n",
      "1019    5.500000   24.279622\n",
      "1020   58.500004   71.829191\n",
      "1021   46.000000  104.946646\n",
      "1022    5.999996   13.997004\n",
      "1023    5.500000   20.902187\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.3725\n",
      "Epoch 25/25, Validation Loss: 0.4550\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4369,  1.6490,  1.2161, -0.7247, -0.1103, -0.1009, -1.7508,\n",
      "          -0.5681, -1.9687, -1.9687, -0.8792,  4.1508,  0.0000, -0.9065,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -0.9411,\n",
      "          -1.0386, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4441,  1.6207,  1.1753, -0.6836, -0.1333, -0.1076, -1.6197,\n",
      "          -0.5681, -2.0056, -2.0056, -0.0644,  4.1508,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0007,\n",
      "          -0.9729, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4514,  1.5924,  1.1345, -0.6426, -0.1562, -0.1142, -1.4886,\n",
      "          -0.5681, -2.0425, -2.0425,  0.7504,  4.1508,  0.0000, -0.9062,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0603,\n",
      "          -0.9071, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4586,  1.5641,  1.0937, -0.6016, -0.1791, -0.1209, -1.3575,\n",
      "          -0.5681, -2.0794, -2.0794,  1.5652,  4.1508,  0.0000, -0.9061,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1199,\n",
      "          -0.8414, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4659,  1.5358,  1.0529, -0.5605, -0.2021, -0.1275, -1.2264,\n",
      "          -0.5681, -2.1163, -2.1163,  2.3800,  4.1508,  0.0000, -0.9059,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1795,\n",
      "          -0.7757, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.6707,  2.1820,  1.2569, -1.2558,  0.3838,  0.0256, -0.5708,\n",
      "          -0.3964,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9077,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.7766,\n",
      "          -1.1809, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.1923,  1.7008,  3.6011, -1.2260,  0.0849, -0.0560, -1.2592,\n",
      "          -0.4694,  0.8735,  0.8735, -0.3360, -0.7989,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.8472,\n",
      "          -1.1308, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.0926,  1.2596,  2.2197, -1.1127,  0.0272, -0.0676, -0.7894,\n",
      "          -0.3392,  1.2980,  1.2980,  1.7010, -0.7989,  0.0000, -0.9046,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.9160,\n",
      "          -1.0749, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2006,  1.1195,  1.8444, -1.1076,  0.0108, -0.0809, -0.5708,\n",
      "          -0.4822,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9043,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1995,\n",
      "          -0.7444, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2369,  0.9374,  0.9469, -1.1114,  0.0096, -0.0809, -1.8819,\n",
      "          -0.5681, -1.6549, -1.6549,  2.3800, -0.7989,  0.0000, -0.9042,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0351,\n",
      "          -0.9609, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2872,  0.8395,  0.8979, -1.0392, -0.0206, -0.0876, -1.8819,\n",
      "          -0.4822,  1.7594,  1.7594, -1.0150, -0.7989,  0.0000, -0.9041,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0958,\n",
      "          -0.8907, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3347,  0.7615,  0.8653, -1.0354, -0.0282, -0.0942, -1.2264,\n",
      "          -0.3105,  1.5748,  1.5748, -1.6940, -0.7989,  0.0000, -0.9039,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1517,\n",
      "          -0.8166, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3542,  0.8125,  0.8000, -0.9823, -0.0332, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -0.3360, -0.7989,  0.0000, -0.9038,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3532,\n",
      "          -0.4007, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3514,  0.8187,  0.7511, -0.8189, -0.0345, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -2.3730, -0.7989,  0.0000, -0.9037,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.2486,\n",
      "          -0.6584, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3682,  0.9183,  0.6178, -0.8569, -0.0887, -0.1153, -1.2264,\n",
      "          -0.3964,  1.5287,  1.5287,  0.1166, -0.7989,  0.0000, -0.9034,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3050,\n",
      "          -0.5368, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3919,  1.0263,  1.1713, -0.6042, -0.0465, -0.1042, -1.5541,\n",
      "          -0.5681,  1.6210,  1.6210,  1.7010, -0.7989,  0.0000, -0.9031,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3402,\n",
      "          -0.4420, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4268,  0.9811,  0.7021, -0.7657, -0.0358, -0.1042, -1.8819,\n",
      "          -0.5681,  1.7132,  1.7132, -1.0150, -0.7989,  0.0000, -0.9029,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3679,\n",
      "          -0.3469, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4156,  1.0019,  0.7103, -0.7429, -0.0383, -0.1042, -1.8819,\n",
      "          -0.5681,  1.5287,  1.5287, -0.3360, -0.7989,  0.0000, -0.9027,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3878,\n",
      "          -0.2562, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4184,  1.1060,  0.7755, -0.6783, -0.0358, -0.1009, -0.5708,\n",
      "          -0.5681,  1.5287,  1.5287,  0.3430, -0.7989,  0.0000, -0.9026,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4017,\n",
      "          -0.1644, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4017,  1.1528,  0.8000, -0.6099, -0.0358, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4825,  1.4825, -2.3730, -0.7989,  0.0000, -0.9025,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4096,\n",
      "          -0.0719, -1.1552,  1.1552,  0.0000]]])\n",
      "predicted: tensor([[-0.0674]], device='cuda:0')\n",
      "[131.9]\n",
      "           actual   predicted\n",
      "0      771.200006  848.043125\n",
      "1      320.999995  378.400599\n",
      "2      293.666664  274.593344\n",
      "3        3.999999   30.889505\n",
      "4       88.000002  156.812355\n",
      "...           ...         ...\n",
      "38802    8.000005   26.717743\n",
      "38803    4.499995   14.246695\n",
      "38804  641.000018  645.567878\n",
      "38805    5.000003   12.670746\n",
      "38806    4.499995   20.363383\n",
      "\n",
      "[38807 rows x 2 columns]\n",
      "Score (RMSE): 535.9650\n",
      "Score (MAE): 79.0261\n",
      "Score (ME): 11.9116\n",
      "Score (MAPE): 179546.9023%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100108, 27) to (100414, 27)\n",
      "training data cutoff:  2023-07-14 01:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([75809, 20, 25]) torch.Size([75809]) torch.Size([75809, 1])\n",
      "Testing data shape: torch.Size([19182, 20, 25]) torch.Size([19182]) torch.Size([19182, 1])\n",
      "Shuffled Training data shape: torch.Size([75992, 20, 25]) torch.Size([75992]) torch.Size([75992, 1])\n",
      "Shuffled Testing data shape: torch.Size([18999, 20, 25]) torch.Size([18999]) torch.Size([18999, 1])\n",
      "Index(['device_id', 'date_time_rounded', 'tmp', 'hum', 'CO2', 'VOC', 'vis',\n",
      "       'IR', 'WIFI', 'BLE', 'rssi', 'channel_rssi', 'channel_index',\n",
      "       'spreading_factor', 'bandwidth', 'f_cnt', 'isHoliday', 'isExamTime',\n",
      "       'weekday_sin', 'weekday_cos', 'month_sin', 'month_cos', 'time_sin',\n",
      "       'time_cos', 'semester_SS23', 'semester_WS22/23', 'semester_WS23/24',\n",
      "       'group'],\n",
      "      dtype='object')\n",
      "Index(['date_time_rounded', 'device_id', 'tmp', 'hum', 'CO2', 'VOC', 'vis',\n",
      "       'IR', 'WIFI', 'BLE', 'rssi', 'channel_rssi', 'channel_index',\n",
      "       'spreading_factor', 'bandwidth', 'f_cnt', 'isHoliday', 'isExamTime',\n",
      "       'weekday_sin', 'weekday_cos', 'month_sin', 'month_cos', 'time_sin',\n",
      "       'time_cos', 'semester_SS23', 'semester_WS22/23', 'semester_WS23/24'],\n",
      "      dtype='object')\n",
      "device_id                    object\n",
      "date_time_rounded    datetime64[ns]\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "dtype: object\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "dtype: object\n",
      "1446  rows differ from the last saved dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     434.999999  409.372929\n",
      "1     469.999999  452.877493\n",
      "2     554.263158  573.502654\n",
      "3     426.499999  406.960760\n",
      "4     483.187500  493.281358\n",
      "...          ...         ...\n",
      "1019  517.749999  477.648940\n",
      "1020  445.750000  432.332728\n",
      "1021  615.499995  578.950495\n",
      "1022  477.750000  447.414932\n",
      "1023  423.750001  438.722152\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.3254\n",
      "Epoch 1/25, Validation Loss: 0.1803\n",
      "          actual   predicted\n",
      "0     434.999999  427.669788\n",
      "1     469.999999  464.627739\n",
      "2     554.263158  544.749738\n",
      "3     426.499999  423.070686\n",
      "4     483.187500  485.537314\n",
      "...          ...         ...\n",
      "1019  517.749999  493.197186\n",
      "1020  445.750000  455.155123\n",
      "1021  615.499995  609.518949\n",
      "1022  477.750000  467.249274\n",
      "1023  423.750001  443.545461\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.1873\n",
      "Epoch 2/25, Validation Loss: 0.1540\n",
      "          actual   predicted\n",
      "0     434.999999  406.082902\n",
      "1     469.999999  464.381286\n",
      "2     554.263158  591.356471\n",
      "3     426.499999  406.610569\n",
      "4     483.187500  514.753027\n",
      "...          ...         ...\n",
      "1019  517.749999  494.738186\n",
      "1020  445.750000  434.681616\n",
      "1021  615.499995  640.451125\n",
      "1022  477.750000  448.617034\n",
      "1023  423.750001  435.883877\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1762\n",
      "Epoch 3/25, Validation Loss: 0.1806\n",
      "          actual   predicted\n",
      "0     434.999999  405.516633\n",
      "1     469.999999  472.664699\n",
      "2     554.263158  575.474920\n",
      "3     426.499999  401.555191\n",
      "4     483.187500  515.221491\n",
      "...          ...         ...\n",
      "1019  517.749999  507.367396\n",
      "1020  445.750000  445.723362\n",
      "1021  615.499995  607.776394\n",
      "1022  477.750000  461.912328\n",
      "1023  423.750001  435.366359\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1706\n",
      "Epoch 4/25, Validation Loss: 0.1408\n",
      "          actual   predicted\n",
      "0     434.999999  426.424211\n",
      "1     469.999999  487.823092\n",
      "2     554.263158  550.514981\n",
      "3     426.499999  421.895941\n",
      "4     483.187500  516.667402\n",
      "...          ...         ...\n",
      "1019  517.749999  496.931411\n",
      "1020  445.750000  452.472592\n",
      "1021  615.499995  572.948477\n",
      "1022  477.750000  465.169985\n",
      "1023  423.750001  438.451114\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.1609\n",
      "Epoch 5/25, Validation Loss: 0.1407\n",
      "          actual   predicted\n",
      "0     434.999999  398.005724\n",
      "1     469.999999  459.717791\n",
      "2     554.263158  555.185272\n",
      "3     426.499999  396.429221\n",
      "4     483.187500  526.867742\n",
      "...          ...         ...\n",
      "1019  517.749999  488.468578\n",
      "1020  445.750000  444.216234\n",
      "1021  615.499995  640.035058\n",
      "1022  477.750000  449.100517\n",
      "1023  423.750001  434.325556\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1621\n",
      "Epoch 6/25, Validation Loss: 0.1731\n",
      "          actual   predicted\n",
      "0     434.999999  408.904009\n",
      "1     469.999999  474.205413\n",
      "2     554.263158  598.171525\n",
      "3     426.499999  403.087416\n",
      "4     483.187500  537.022260\n",
      "...          ...         ...\n",
      "1019  517.749999  516.708288\n",
      "1020  445.750000  444.328310\n",
      "1021  615.499995  629.866733\n",
      "1022  477.750000  472.597457\n",
      "1023  423.750001  436.063355\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.1558\n",
      "Epoch 7/25, Validation Loss: 0.1489\n",
      "          actual   predicted\n",
      "0     434.999999  415.103792\n",
      "1     469.999999  485.069487\n",
      "2     554.263158  577.429352\n",
      "3     426.499999  416.193738\n",
      "4     483.187500  550.084272\n",
      "...          ...         ...\n",
      "1019  517.749999  508.178359\n",
      "1020  445.750000  451.650712\n",
      "1021  615.499995  618.731194\n",
      "1022  477.750000  466.221287\n",
      "1023  423.750001  435.237874\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1543\n",
      "Epoch 8/25, Validation Loss: 0.1574\n",
      "          actual   predicted\n",
      "0     434.999999  405.778087\n",
      "1     469.999999  473.015808\n",
      "2     554.263158  573.974837\n",
      "3     426.499999  408.349934\n",
      "4     483.187500  527.881392\n",
      "...          ...         ...\n",
      "1019  517.749999  504.543350\n",
      "1020  445.750000  440.625737\n",
      "1021  615.499995  608.160137\n",
      "1022  477.750000  455.608110\n",
      "1023  423.750001  424.423328\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1549\n",
      "Epoch 9/25, Validation Loss: 0.1469\n",
      "          actual   predicted\n",
      "0     434.999999  411.119081\n",
      "1     469.999999  486.857826\n",
      "2     554.263158  561.253210\n",
      "3     426.499999  412.854572\n",
      "4     483.187500  532.276498\n",
      "...          ...         ...\n",
      "1019  517.749999  512.082728\n",
      "1020  445.750000  448.542783\n",
      "1021  615.499995  606.560249\n",
      "1022  477.750000  465.230434\n",
      "1023  423.750001  432.195727\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.1468\n",
      "Epoch 10/25, Validation Loss: 0.1298\n",
      "          actual   predicted\n",
      "0     434.999999  416.118074\n",
      "1     469.999999  486.812328\n",
      "2     554.263158  559.654796\n",
      "3     426.499999  415.866795\n",
      "4     483.187500  534.885894\n",
      "...          ...         ...\n",
      "1019  517.749999  499.404268\n",
      "1020  445.750000  448.712730\n",
      "1021  615.499995  589.101809\n",
      "1022  477.750000  463.311664\n",
      "1023  423.750001  432.401852\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.1424\n",
      "Epoch 11/25, Validation Loss: 0.1312\n",
      "          actual   predicted\n",
      "0     434.999999  417.680595\n",
      "1     469.999999  486.533061\n",
      "2     554.263158  566.476005\n",
      "3     426.499999  412.364745\n",
      "4     483.187500  533.189793\n",
      "...          ...         ...\n",
      "1019  517.749999  505.624807\n",
      "1020  445.750000  446.761931\n",
      "1021  615.499995  587.252450\n",
      "1022  477.750000  464.936951\n",
      "1023  423.750001  425.285380\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1415\n",
      "Epoch 12/25, Validation Loss: 0.1284\n",
      "          actual   predicted\n",
      "0     434.999999  415.442959\n",
      "1     469.999999  489.346492\n",
      "2     554.263158  560.613965\n",
      "3     426.499999  415.667532\n",
      "4     483.187500  537.422452\n",
      "...          ...         ...\n",
      "1019  517.749999  511.351948\n",
      "1020  445.750000  446.750169\n",
      "1021  615.499995  607.804060\n",
      "1022  477.750000  466.145168\n",
      "1023  423.750001  425.889845\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1431\n",
      "Epoch 13/25, Validation Loss: 0.1275\n",
      "          actual   predicted\n",
      "0     434.999999  420.910858\n",
      "1     469.999999  488.195799\n",
      "2     554.263158  561.076795\n",
      "3     426.499999  419.441231\n",
      "4     483.187500  527.165960\n",
      "...          ...         ...\n",
      "1019  517.749999  511.232750\n",
      "1020  445.750000  449.686214\n",
      "1021  615.499995  591.119937\n",
      "1022  477.750000  471.053193\n",
      "1023  423.750001  432.380945\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.1398\n",
      "Epoch 14/25, Validation Loss: 0.1276\n",
      "          actual   predicted\n",
      "0     434.999999  413.289105\n",
      "1     469.999999  481.811285\n",
      "2     554.263158  580.210606\n",
      "3     426.499999  413.842838\n",
      "4     483.187500  548.972455\n",
      "...          ...         ...\n",
      "1019  517.749999  493.696566\n",
      "1020  445.750000  437.070268\n",
      "1021  615.499995  609.866342\n",
      "1022  477.750000  461.446938\n",
      "1023  423.750001  426.459893\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.1403\n",
      "Epoch 15/25, Validation Loss: 0.1305\n",
      "          actual   predicted\n",
      "0     434.999999  419.935818\n",
      "1     469.999999  487.886237\n",
      "2     554.263158  571.028904\n",
      "3     426.499999  417.164040\n",
      "4     483.187500  546.311413\n",
      "...          ...         ...\n",
      "1019  517.749999  509.350021\n",
      "1020  445.750000  453.313322\n",
      "1021  615.499995  596.394939\n",
      "1022  477.750000  472.283852\n",
      "1023  423.750001  433.601088\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.1385\n",
      "Epoch 16/25, Validation Loss: 0.1253\n",
      "          actual   predicted\n",
      "0     434.999999  415.552067\n",
      "1     469.999999  494.086433\n",
      "2     554.263158  585.343901\n",
      "3     426.499999  415.252250\n",
      "4     483.187500  558.278522\n",
      "...          ...         ...\n",
      "1019  517.749999  512.571776\n",
      "1020  445.750000  448.653104\n",
      "1021  615.499995  615.621136\n",
      "1022  477.750000  471.299343\n",
      "1023  423.750001  438.590673\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.1372\n",
      "Epoch 17/25, Validation Loss: 0.1332\n",
      "          actual   predicted\n",
      "0     434.999999  414.053568\n",
      "1     469.999999  480.176217\n",
      "2     554.263158  552.488061\n",
      "3     426.499999  414.189854\n",
      "4     483.187500  532.277276\n",
      "...          ...         ...\n",
      "1019  517.749999  506.802352\n",
      "1020  445.750000  443.063137\n",
      "1021  615.499995  598.053313\n",
      "1022  477.750000  462.408087\n",
      "1023  423.750001  429.171434\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.1369\n",
      "Epoch 18/25, Validation Loss: 0.1391\n",
      "          actual   predicted\n",
      "0     434.999999  420.254056\n",
      "1     469.999999  495.103056\n",
      "2     554.263158  579.836172\n",
      "3     426.499999  423.071558\n",
      "4     483.187500  577.142893\n",
      "...          ...         ...\n",
      "1019  517.749999  517.767264\n",
      "1020  445.750000  450.834718\n",
      "1021  615.499995  609.973684\n",
      "1022  477.750000  469.435680\n",
      "1023  423.750001  429.570539\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.1359\n",
      "Epoch 19/25, Validation Loss: 0.1259\n",
      "          actual   predicted\n",
      "0     434.999999  416.105531\n",
      "1     469.999999  481.783978\n",
      "2     554.263158  565.195416\n",
      "3     426.499999  417.910292\n",
      "4     483.187500  559.710629\n",
      "...          ...         ...\n",
      "1019  517.749999  498.349020\n",
      "1020  445.750000  446.431949\n",
      "1021  615.499995  617.783646\n",
      "1022  477.750000  462.989097\n",
      "1023  423.750001  434.249223\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.1358\n",
      "Epoch 20/25, Validation Loss: 0.1405\n",
      "          actual   predicted\n",
      "0     434.999999  419.725459\n",
      "1     469.999999  485.819707\n",
      "2     554.263158  566.251054\n",
      "3     426.499999  418.274167\n",
      "4     483.187500  537.414440\n",
      "...          ...         ...\n",
      "1019  517.749999  510.878573\n",
      "1020  445.750000  452.870771\n",
      "1021  615.499995  604.424736\n",
      "1022  477.750000  469.261950\n",
      "1023  423.750001  432.228508\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.1341\n",
      "Epoch 21/25, Validation Loss: 0.1227\n",
      "          actual   predicted\n",
      "0     434.999999  417.717120\n",
      "1     469.999999  482.226894\n",
      "2     554.263158  561.256829\n",
      "3     426.499999  417.505714\n",
      "4     483.187500  544.342987\n",
      "...          ...         ...\n",
      "1019  517.749999  506.595379\n",
      "1020  445.750000  450.597631\n",
      "1021  615.499995  609.268948\n",
      "1022  477.750000  464.727092\n",
      "1023  423.750001  431.303807\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.1329\n",
      "Epoch 22/25, Validation Loss: 0.1247\n",
      "          actual   predicted\n",
      "0     434.999999  414.700203\n",
      "1     469.999999  483.347314\n",
      "2     554.263158  564.565727\n",
      "3     426.499999  414.656775\n",
      "4     483.187500  552.849073\n",
      "...          ...         ...\n",
      "1019  517.749999  504.378714\n",
      "1020  445.750000  448.515146\n",
      "1021  615.499995  626.345898\n",
      "1022  477.750000  464.777400\n",
      "1023  423.750001  426.754309\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.1313\n",
      "Epoch 23/25, Validation Loss: 0.1240\n",
      "          actual   predicted\n",
      "0     434.999999  421.197034\n",
      "1     469.999999  482.763734\n",
      "2     554.263158  563.795167\n",
      "3     426.499999  422.258936\n",
      "4     483.187500  552.490648\n",
      "...          ...         ...\n",
      "1019  517.749999  499.068890\n",
      "1020  445.750000  446.766385\n",
      "1021  615.499995  603.633886\n",
      "1022  477.750000  465.421502\n",
      "1023  423.750001  428.935644\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.1316\n",
      "Epoch 24/25, Validation Loss: 0.1301\n",
      "          actual   predicted\n",
      "0     434.999999  419.319246\n",
      "1     469.999999  487.119687\n",
      "2     554.263158  566.149519\n",
      "3     426.499999  419.025004\n",
      "4     483.187500  560.384210\n",
      "...          ...         ...\n",
      "1019  517.749999  506.513890\n",
      "1020  445.750000  451.269876\n",
      "1021  615.499995  614.988730\n",
      "1022  477.750000  466.942249\n",
      "1023  423.750001  427.809119\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.1311\n",
      "Epoch 25/25, Validation Loss: 0.1266\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3896e-01,  1.6481e+00,  1.2189e+00, -7.3040e-01, -9.9768e-02,\n",
      "          -8.3099e-02, -1.8816e+00, -5.9109e-01, -2.0547e+00, -2.0547e+00,\n",
      "          -1.4280e+00,  4.1557e+00,  0.0000e+00, -9.0600e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.3963e-01, -1.0391e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.4623e-01,  1.6198e+00,  1.1779e+00, -6.8921e-01, -1.1689e-01,\n",
      "          -8.7525e-02, -1.7402e+00, -5.9109e-01, -2.0932e+00, -2.0932e+00,\n",
      "          -1.0415e-01,  4.1557e+00,  0.0000e+00, -9.0586e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.9923e-01, -9.7336e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.5349e-01,  1.5915e+00,  1.1368e+00, -6.4801e-01, -1.3400e-01,\n",
      "          -9.1950e-02, -1.5988e+00, -5.9109e-01, -2.1317e+00, -2.1317e+00,\n",
      "           1.2197e+00,  4.1557e+00,  0.0000e+00, -9.0573e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.0588e+00, -9.0760e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6076e-01,  1.5632e+00,  1.0958e+00, -6.0681e-01, -1.5112e-01,\n",
      "          -9.6376e-02, -1.4575e+00, -5.9109e-01, -2.1702e+00, -2.1702e+00,\n",
      "           2.5435e+00,  4.1557e+00,  0.0000e+00, -9.0560e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1184e+00, -8.4183e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6802e-01,  1.5349e+00,  1.0548e+00, -5.6561e-01, -1.6824e-01,\n",
      "          -1.0080e-01, -1.3161e+00, -5.9109e-01, -2.2087e+00, -2.2087e+00,\n",
      "           3.8674e+00,  4.1557e+00,  0.0000e+00, -9.0546e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1780e+00, -7.7606e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-6.6942e-01,  2.1811e+00,  1.2599e+00, -1.2637e+00,  2.6910e-01,\n",
      "           9.8832e-04, -6.0919e-01, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0720e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -7.7521e-01, -1.1816e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-1.9069e-01,  1.6999e+00,  3.6163e+00, -1.2338e+00,  4.5965e-02,\n",
      "          -5.3226e-02, -1.3514e+00, -4.8829e-01,  9.1100e-01,  9.1100e-01,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0578e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -8.4576e-01, -1.1314e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 9.4465e-02,  1.2588e+00,  2.2277e+00, -1.1200e+00,  2.9364e-03,\n",
      "          -6.0971e-02, -8.4482e-01, -3.5272e-01,  1.3539e+00,  1.3539e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0413e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -9.1458e-01, -1.0755e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.0253e-01,  1.1187e+00,  1.8504e+00, -1.1149e+00, -9.2903e-03,\n",
      "          -6.9822e-02, -6.0919e-01, -5.0170e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0386e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1980e+00, -7.4471e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.3886e-01,  9.3658e-01,  9.4820e-01, -1.1187e+00, -1.0231e-02,\n",
      "          -6.9822e-02, -2.0230e+00, -5.9109e-01, -1.7273e+00, -1.7273e+00,\n",
      "           3.8674e+00, -7.9865e-01,  0.0000e+00, -9.0372e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0336e+00, -9.6137e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.8916e-01,  8.3876e-01,  8.9899e-01, -1.0463e+00, -3.2803e-02,\n",
      "          -7.4248e-02, -2.0230e+00, -5.0170e-01,  1.8354e+00,  1.8354e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0359e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0943e+00, -8.9111e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.3667e-01,  7.6072e-01,  8.6618e-01, -1.0424e+00, -3.8446e-02,\n",
      "          -7.8673e-02, -1.3161e+00, -3.2292e-01,  1.6428e+00,  1.6428e+00,\n",
      "          -2.7518e+00, -7.9865e-01,  0.0000e+00, -9.0346e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1502e+00, -8.1703e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5624e-01,  8.1171e-01,  8.0056e-01, -9.8904e-01, -4.2208e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0332e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3516e+00, -4.0081e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5344e-01,  8.1795e-01,  7.5135e-01, -8.2501e-01, -4.3149e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0319e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.2470e+00, -6.5866e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.7021e-01,  9.1750e-01,  6.1738e-01, -8.6316e-01, -8.3591e-02,\n",
      "          -9.2688e-02, -1.3161e+00, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           1.9004e-01, -7.9865e-01,  0.0000e+00, -9.0292e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3034e+00, -5.3698e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.9396e-01,  1.0256e+00,  1.1738e+00, -6.0948e-01, -5.2084e-02,\n",
      "          -8.5312e-02, -1.6695e+00, -5.9109e-01,  1.6909e+00,  1.6909e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0259e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3387e+00, -4.4220e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2890e-01,  9.8028e-01,  7.0214e-01, -7.7160e-01, -4.4090e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.7872e+00,  1.7872e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0239e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3664e+00, -3.4705e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.1772e-01,  1.0011e+00,  7.1034e-01, -7.4872e-01, -4.5971e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0225e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3862e+00, -2.5629e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2051e-01,  1.1052e+00,  7.7596e-01, -6.8387e-01, -4.4090e-02,\n",
      "          -8.3099e-02, -6.0919e-01, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "           5.5777e-01, -7.9865e-01,  0.0000e+00, -9.0212e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4001e+00, -1.6443e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.0375e-01,  1.1520e+00,  8.0056e-01, -6.1520e-01, -4.4090e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.5465e+00,  1.5465e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0199e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4080e+00, -7.1849e-02, -1.1547e+00,  1.1547e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.1590]], device='cuda:0')\n",
      "[507.78]\n",
      "           actual   predicted\n",
      "0      434.999999  419.319246\n",
      "1      469.999999  487.119687\n",
      "2      554.263158  566.149519\n",
      "3      426.499999  419.025004\n",
      "4      483.187500  560.384210\n",
      "...           ...         ...\n",
      "18994  424.250002  421.480666\n",
      "18995  497.000000  527.474020\n",
      "18996  413.500001  420.223716\n",
      "18997  432.250001  434.709779\n",
      "18998  422.999999  419.171057\n",
      "\n",
      "[18999 rows x 2 columns]\n",
      "Score (RMSE): 43.1613\n",
      "Score (MAE): 16.8535\n",
      "Score (ME): -3.6071\n",
      "Score (MAPE): 2.9325%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100108, 27) to (100414, 27)\n",
      "training data cutoff:  2023-07-14 01:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([75809, 20, 25]) torch.Size([75809]) torch.Size([75809, 1])\n",
      "Testing data shape: torch.Size([19182, 20, 25]) torch.Size([19182]) torch.Size([19182, 1])\n",
      "Shuffled Training data shape: torch.Size([75992, 20, 25]) torch.Size([75992]) torch.Size([75992, 1])\n",
      "Shuffled Testing data shape: torch.Size([18999, 20, 25]) torch.Size([18999]) torch.Size([18999, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     511.750009  591.392304\n",
      "1     783.250000  799.474445\n",
      "2     946.199995  929.854171\n",
      "3     958.499996  908.497907\n",
      "4     711.250002  746.719568\n",
      "...          ...         ...\n",
      "1019  626.499994  630.374928\n",
      "1020  549.750001  606.469452\n",
      "1021  656.249997  618.285192\n",
      "1022  603.434776  621.076589\n",
      "1023  643.250002  611.289003\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2760\n",
      "Epoch 1/25, Validation Loss: 0.1181\n",
      "          actual    predicted\n",
      "0     511.750009   599.255547\n",
      "1     783.250000   943.032726\n",
      "2     946.199995  1053.351501\n",
      "3     958.499996  1033.493020\n",
      "4     711.250002   852.927234\n",
      "...          ...          ...\n",
      "1019  626.499994   677.212288\n",
      "1020  549.750001   617.400418\n",
      "1021  656.249997   665.809982\n",
      "1022  603.434776   661.393810\n",
      "1023  643.250002   640.473136\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.1390\n",
      "Epoch 2/25, Validation Loss: 0.1776\n",
      "          actual   predicted\n",
      "0     511.750009  580.550650\n",
      "1     783.250000  876.448487\n",
      "2     946.199995  943.611987\n",
      "3     958.499996  959.406503\n",
      "4     711.250002  780.393925\n",
      "...          ...         ...\n",
      "1019  626.499994  638.587200\n",
      "1020  549.750001  593.170101\n",
      "1021  656.249997  646.287178\n",
      "1022  603.434776  622.159564\n",
      "1023  643.250002  608.057423\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1315\n",
      "Epoch 3/25, Validation Loss: 0.1060\n",
      "          actual   predicted\n",
      "0     511.750009  561.402554\n",
      "1     783.250000  861.838335\n",
      "2     946.199995  941.644114\n",
      "3     958.499996  930.166871\n",
      "4     711.250002  769.165593\n",
      "...          ...         ...\n",
      "1019  626.499994  626.402651\n",
      "1020  549.750001  570.512989\n",
      "1021  656.249997  614.998096\n",
      "1022  603.434776  581.736702\n",
      "1023  643.250002  581.071958\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1221\n",
      "Epoch 4/25, Validation Loss: 0.1151\n",
      "          actual   predicted\n",
      "0     511.750009  566.706429\n",
      "1     783.250000  861.836022\n",
      "2     946.199995  931.299549\n",
      "3     958.499996  959.110428\n",
      "4     711.250002  801.121458\n",
      "...          ...         ...\n",
      "1019  626.499994  632.760907\n",
      "1020  549.750001  579.500472\n",
      "1021  656.249997  632.942937\n",
      "1022  603.434776  594.286482\n",
      "1023  643.250002  601.507402\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.1196\n",
      "Epoch 5/25, Validation Loss: 0.1048\n",
      "          actual   predicted\n",
      "0     511.750009  543.505603\n",
      "1     783.250000  808.479877\n",
      "2     946.199995  875.272888\n",
      "3     958.499996  861.273941\n",
      "4     711.250002  714.286640\n",
      "...          ...         ...\n",
      "1019  626.499994  618.622751\n",
      "1020  549.750001  557.838195\n",
      "1021  656.249997  609.090960\n",
      "1022  603.434776  579.871375\n",
      "1023  643.250002  575.000293\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1169\n",
      "Epoch 6/25, Validation Loss: 0.1180\n",
      "          actual   predicted\n",
      "0     511.750009  557.651634\n",
      "1     783.250000  866.121266\n",
      "2     946.199995  914.718727\n",
      "3     958.499996  922.341925\n",
      "4     711.250002  790.476103\n",
      "...          ...         ...\n",
      "1019  626.499994  649.035420\n",
      "1020  549.750001  575.725662\n",
      "1021  656.249997  646.682316\n",
      "1022  603.434776  587.548026\n",
      "1023  643.250002  597.718686\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.1184\n",
      "Epoch 7/25, Validation Loss: 0.1018\n",
      "          actual   predicted\n",
      "0     511.750009  567.684452\n",
      "1     783.250000  836.231565\n",
      "2     946.199995  918.368382\n",
      "3     958.499996  918.854706\n",
      "4     711.250002  784.915448\n",
      "...          ...         ...\n",
      "1019  626.499994  643.296689\n",
      "1020  549.750001  575.990003\n",
      "1021  656.249997  647.001953\n",
      "1022  603.434776  600.919564\n",
      "1023  643.250002  616.861907\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1114\n",
      "Epoch 8/25, Validation Loss: 0.0993\n",
      "          actual   predicted\n",
      "0     511.750009  550.143122\n",
      "1     783.250000  872.654060\n",
      "2     946.199995  903.877143\n",
      "3     958.499996  936.403347\n",
      "4     711.250002  822.610955\n",
      "...          ...         ...\n",
      "1019  626.499994  642.620039\n",
      "1020  549.750001  565.496299\n",
      "1021  656.249997  634.592579\n",
      "1022  603.434776  583.221283\n",
      "1023  643.250002  589.491759\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1112\n",
      "Epoch 9/25, Validation Loss: 0.0995\n",
      "          actual   predicted\n",
      "0     511.750009  554.232977\n",
      "1     783.250000  851.074836\n",
      "2     946.199995  891.004921\n",
      "3     958.499996  921.113811\n",
      "4     711.250002  773.243377\n",
      "...          ...         ...\n",
      "1019  626.499994  631.853869\n",
      "1020  549.750001  563.066680\n",
      "1021  656.249997  645.966541\n",
      "1022  603.434776  581.694703\n",
      "1023  643.250002  595.406925\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.1086\n",
      "Epoch 10/25, Validation Loss: 0.0974\n",
      "          actual   predicted\n",
      "0     511.750009  548.305529\n",
      "1     783.250000  827.613489\n",
      "2     946.199995  856.072214\n",
      "3     958.499996  916.513086\n",
      "4     711.250002  748.607535\n",
      "...          ...         ...\n",
      "1019  626.499994  627.099207\n",
      "1020  549.750001  571.207780\n",
      "1021  656.249997  627.709186\n",
      "1022  603.434776  593.903970\n",
      "1023  643.250002  599.417968\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.1075\n",
      "Epoch 11/25, Validation Loss: 0.1027\n",
      "          actual   predicted\n",
      "0     511.750009  565.327941\n",
      "1     783.250000  856.280329\n",
      "2     946.199995  897.287076\n",
      "3     958.499996  934.416646\n",
      "4     711.250002  789.505855\n",
      "...          ...         ...\n",
      "1019  626.499994  660.318539\n",
      "1020  549.750001  588.290269\n",
      "1021  656.249997  650.215792\n",
      "1022  603.434776  607.162915\n",
      "1023  643.250002  619.540649\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1075\n",
      "Epoch 12/25, Validation Loss: 0.1015\n",
      "          actual   predicted\n",
      "0     511.750009  567.512406\n",
      "1     783.250000  856.947464\n",
      "2     946.199995  908.578851\n",
      "3     958.499996  928.588635\n",
      "4     711.250002  795.425637\n",
      "...          ...         ...\n",
      "1019  626.499994  668.114306\n",
      "1020  549.750001  584.752348\n",
      "1021  656.249997  663.034522\n",
      "1022  603.434776  624.351232\n",
      "1023  643.250002  635.140855\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1071\n",
      "Epoch 13/25, Validation Loss: 0.1106\n",
      "          actual   predicted\n",
      "0     511.750009  541.610323\n",
      "1     783.250000  862.199254\n",
      "2     946.199995  897.439247\n",
      "3     958.499996  961.974465\n",
      "4     711.250002  766.331883\n",
      "...          ...         ...\n",
      "1019  626.499994  667.602232\n",
      "1020  549.750001  562.546153\n",
      "1021  656.249997  640.539276\n",
      "1022  603.434776  607.391429\n",
      "1023  643.250002  610.181623\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.1050\n",
      "Epoch 14/25, Validation Loss: 0.0983\n",
      "          actual   predicted\n",
      "0     511.750009  561.542537\n",
      "1     783.250000  858.649988\n",
      "2     946.199995  891.507643\n",
      "3     958.499996  932.399633\n",
      "4     711.250002  797.859778\n",
      "...          ...         ...\n",
      "1019  626.499994  646.619066\n",
      "1020  549.750001  573.890615\n",
      "1021  656.249997  645.796698\n",
      "1022  603.434776  599.201985\n",
      "1023  643.250002  603.955443\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Early stopping!\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3896e-01,  1.6481e+00,  1.2189e+00, -7.3040e-01, -9.9768e-02,\n",
      "          -8.3099e-02, -1.8816e+00, -5.9109e-01, -2.0547e+00, -2.0547e+00,\n",
      "          -1.4280e+00,  4.1557e+00,  0.0000e+00, -9.0600e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.3963e-01, -1.0391e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.4623e-01,  1.6198e+00,  1.1779e+00, -6.8921e-01, -1.1689e-01,\n",
      "          -8.7525e-02, -1.7402e+00, -5.9109e-01, -2.0932e+00, -2.0932e+00,\n",
      "          -1.0415e-01,  4.1557e+00,  0.0000e+00, -9.0586e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.9923e-01, -9.7336e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.5349e-01,  1.5915e+00,  1.1368e+00, -6.4801e-01, -1.3400e-01,\n",
      "          -9.1950e-02, -1.5988e+00, -5.9109e-01, -2.1317e+00, -2.1317e+00,\n",
      "           1.2197e+00,  4.1557e+00,  0.0000e+00, -9.0573e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.0588e+00, -9.0760e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6076e-01,  1.5632e+00,  1.0958e+00, -6.0681e-01, -1.5112e-01,\n",
      "          -9.6376e-02, -1.4575e+00, -5.9109e-01, -2.1702e+00, -2.1702e+00,\n",
      "           2.5435e+00,  4.1557e+00,  0.0000e+00, -9.0560e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1184e+00, -8.4183e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6802e-01,  1.5349e+00,  1.0548e+00, -5.6561e-01, -1.6824e-01,\n",
      "          -1.0080e-01, -1.3161e+00, -5.9109e-01, -2.2087e+00, -2.2087e+00,\n",
      "           3.8674e+00,  4.1557e+00,  0.0000e+00, -9.0546e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1780e+00, -7.7606e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-6.6942e-01,  2.1811e+00,  1.2599e+00, -1.2637e+00,  2.6910e-01,\n",
      "           9.8832e-04, -6.0919e-01, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0720e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -7.7521e-01, -1.1816e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-1.9069e-01,  1.6999e+00,  3.6163e+00, -1.2338e+00,  4.5965e-02,\n",
      "          -5.3226e-02, -1.3514e+00, -4.8829e-01,  9.1100e-01,  9.1100e-01,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0578e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -8.4576e-01, -1.1314e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 9.4465e-02,  1.2588e+00,  2.2277e+00, -1.1200e+00,  2.9364e-03,\n",
      "          -6.0971e-02, -8.4482e-01, -3.5272e-01,  1.3539e+00,  1.3539e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0413e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -9.1458e-01, -1.0755e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.0253e-01,  1.1187e+00,  1.8504e+00, -1.1149e+00, -9.2903e-03,\n",
      "          -6.9822e-02, -6.0919e-01, -5.0170e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0386e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1980e+00, -7.4471e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.3886e-01,  9.3658e-01,  9.4820e-01, -1.1187e+00, -1.0231e-02,\n",
      "          -6.9822e-02, -2.0230e+00, -5.9109e-01, -1.7273e+00, -1.7273e+00,\n",
      "           3.8674e+00, -7.9865e-01,  0.0000e+00, -9.0372e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0336e+00, -9.6137e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.8916e-01,  8.3876e-01,  8.9899e-01, -1.0463e+00, -3.2803e-02,\n",
      "          -7.4248e-02, -2.0230e+00, -5.0170e-01,  1.8354e+00,  1.8354e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0359e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0943e+00, -8.9111e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.3667e-01,  7.6072e-01,  8.6618e-01, -1.0424e+00, -3.8446e-02,\n",
      "          -7.8673e-02, -1.3161e+00, -3.2292e-01,  1.6428e+00,  1.6428e+00,\n",
      "          -2.7518e+00, -7.9865e-01,  0.0000e+00, -9.0346e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1502e+00, -8.1703e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5624e-01,  8.1171e-01,  8.0056e-01, -9.8904e-01, -4.2208e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0332e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3516e+00, -4.0081e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5344e-01,  8.1795e-01,  7.5135e-01, -8.2501e-01, -4.3149e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0319e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.2470e+00, -6.5866e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.7021e-01,  9.1750e-01,  6.1738e-01, -8.6316e-01, -8.3591e-02,\n",
      "          -9.2688e-02, -1.3161e+00, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           1.9004e-01, -7.9865e-01,  0.0000e+00, -9.0292e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3034e+00, -5.3698e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.9396e-01,  1.0256e+00,  1.1738e+00, -6.0948e-01, -5.2084e-02,\n",
      "          -8.5312e-02, -1.6695e+00, -5.9109e-01,  1.6909e+00,  1.6909e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0259e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3387e+00, -4.4220e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2890e-01,  9.8028e-01,  7.0214e-01, -7.7160e-01, -4.4090e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.7872e+00,  1.7872e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0239e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3664e+00, -3.4705e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.1772e-01,  1.0011e+00,  7.1034e-01, -7.4872e-01, -4.5971e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0225e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3862e+00, -2.5629e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2051e-01,  1.1052e+00,  7.7596e-01, -6.8387e-01, -4.4090e-02,\n",
      "          -8.3099e-02, -6.0919e-01, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "           5.5777e-01, -7.9865e-01,  0.0000e+00, -9.0212e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4001e+00, -1.6443e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.0375e-01,  1.1520e+00,  8.0056e-01, -6.1520e-01, -4.4090e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.5465e+00,  1.5465e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0199e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4080e+00, -7.1849e-02, -1.1547e+00,  1.1547e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.3891]], device='cuda:0')\n",
      "[679.26]\n",
      "            actual    predicted\n",
      "0       511.750009   561.542537\n",
      "1       783.250000   858.649988\n",
      "2       946.199995   891.507643\n",
      "3       958.499996   932.399633\n",
      "4       711.250002   797.859778\n",
      "...            ...          ...\n",
      "18994   678.750001   674.216455\n",
      "18995   648.750000   656.794093\n",
      "18996  1196.499985  1233.137036\n",
      "18997   590.749998   621.491070\n",
      "18998   583.499999   610.718946\n",
      "\n",
      "[18999 rows x 2 columns]\n",
      "Score (RMSE): 82.5849\n",
      "Score (MAE): 42.0179\n",
      "Score (ME): -5.6383\n",
      "Score (MAPE): 5.0652%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100108, 27) to (100414, 27)\n",
      "training data cutoff:  2023-07-14 01:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([75809, 20, 25]) torch.Size([75809]) torch.Size([75809, 1])\n",
      "Testing data shape: torch.Size([19182, 20, 25]) torch.Size([19182]) torch.Size([19182, 1])\n",
      "Shuffled Training data shape: torch.Size([75992, 20, 25]) torch.Size([75992]) torch.Size([75992, 1])\n",
      "Shuffled Testing data shape: torch.Size([18999, 20, 25]) torch.Size([18999]) torch.Size([18999, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     26.115000  22.108273\n",
      "1     32.987500  30.565413\n",
      "2     37.017500  33.302206\n",
      "3     15.945251  18.253378\n",
      "4     37.537500  36.104490\n",
      "...         ...        ...\n",
      "1019  33.045000  31.788237\n",
      "1020  34.680000  33.307949\n",
      "1021  36.748000  34.925572\n",
      "1022  40.260556  39.680747\n",
      "1023  42.928000  41.458456\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.3727\n",
      "Epoch 1/25, Validation Loss: 0.0329\n",
      "         actual  predicted\n",
      "0     26.115000  23.288619\n",
      "1     32.987500  31.532543\n",
      "2     37.017500  34.326979\n",
      "3     15.945251  17.760892\n",
      "4     37.537500  36.173306\n",
      "...         ...        ...\n",
      "1019  33.045000  31.626650\n",
      "1020  34.680000  32.986176\n",
      "1021  36.748000  34.112402\n",
      "1022  40.260556  37.215966\n",
      "1023  42.928000  39.848950\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0593\n",
      "Epoch 2/25, Validation Loss: 0.0376\n",
      "         actual  predicted\n",
      "0     26.115000  19.632082\n",
      "1     32.987500  31.837320\n",
      "2     37.017500  36.038020\n",
      "3     15.945251  15.348094\n",
      "4     37.537500  38.461476\n",
      "...         ...        ...\n",
      "1019  33.045000  30.965345\n",
      "1020  34.680000  33.759310\n",
      "1021  36.748000  35.310103\n",
      "1022  40.260556  41.135451\n",
      "1023  42.928000  43.047037\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0473\n",
      "Epoch 3/25, Validation Loss: 0.0723\n",
      "         actual  predicted\n",
      "0     26.115000  24.240532\n",
      "1     32.987500  32.414949\n",
      "2     37.017500  35.380748\n",
      "3     15.945251  17.311706\n",
      "4     37.537500  37.593497\n",
      "...         ...        ...\n",
      "1019  33.045000  32.540379\n",
      "1020  34.680000  34.055493\n",
      "1021  36.748000  35.143563\n",
      "1022  40.260556  39.344574\n",
      "1023  42.928000  41.635752\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0431\n",
      "Epoch 4/25, Validation Loss: 0.0166\n",
      "         actual  predicted\n",
      "0     26.115000  25.254062\n",
      "1     32.987500  32.969422\n",
      "2     37.017500  36.029234\n",
      "3     15.945251  17.639789\n",
      "4     37.537500  37.312436\n",
      "...         ...        ...\n",
      "1019  33.045000  33.247674\n",
      "1020  34.680000  34.379396\n",
      "1021  36.748000  35.553008\n",
      "1022  40.260556  39.610256\n",
      "1023  42.928000  42.087295\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0366\n",
      "Epoch 5/25, Validation Loss: 0.0160\n",
      "         actual  predicted\n",
      "0     26.115000  23.995463\n",
      "1     32.987500  33.485871\n",
      "2     37.017500  37.096738\n",
      "3     15.945251  15.177443\n",
      "4     37.537500  38.523163\n",
      "...         ...        ...\n",
      "1019  33.045000  33.172735\n",
      "1020  34.680000  34.565262\n",
      "1021  36.748000  36.183880\n",
      "1022  40.260556  40.213974\n",
      "1023  42.928000  42.439357\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0322\n",
      "Epoch 6/25, Validation Loss: 0.0197\n",
      "         actual  predicted\n",
      "0     26.115000  23.355455\n",
      "1     32.987500  32.390327\n",
      "2     37.017500  36.302584\n",
      "3     15.945251  14.671392\n",
      "4     37.537500  37.547676\n",
      "...         ...        ...\n",
      "1019  33.045000  32.665161\n",
      "1020  34.680000  34.105227\n",
      "1021  36.748000  35.403681\n",
      "1022  40.260556  39.477849\n",
      "1023  42.928000  41.435216\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0304\n",
      "Epoch 7/25, Validation Loss: 0.0181\n",
      "         actual  predicted\n",
      "0     26.115000  25.373685\n",
      "1     32.987500  32.782259\n",
      "2     37.017500  36.164506\n",
      "3     15.945251  15.481080\n",
      "4     37.537500  37.422424\n",
      "...         ...        ...\n",
      "1019  33.045000  33.442533\n",
      "1020  34.680000  34.646650\n",
      "1021  36.748000  35.909488\n",
      "1022  40.260556  40.396019\n",
      "1023  42.928000  42.978816\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0285\n",
      "Epoch 8/25, Validation Loss: 0.0265\n",
      "         actual  predicted\n",
      "0     26.115000  25.408378\n",
      "1     32.987500  32.330312\n",
      "2     37.017500  36.041963\n",
      "3     15.945251  15.998511\n",
      "4     37.537500  37.009086\n",
      "...         ...        ...\n",
      "1019  33.045000  33.647130\n",
      "1020  34.680000  34.697862\n",
      "1021  36.748000  36.215153\n",
      "1022  40.260556  40.040820\n",
      "1023  42.928000  41.914660\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0276\n",
      "Epoch 9/25, Validation Loss: 0.0118\n",
      "         actual  predicted\n",
      "0     26.115000  25.554424\n",
      "1     32.987500  33.235933\n",
      "2     37.017500  36.656621\n",
      "3     15.945251  16.380510\n",
      "4     37.537500  38.007426\n",
      "...         ...        ...\n",
      "1019  33.045000  33.909845\n",
      "1020  34.680000  35.125759\n",
      "1021  36.748000  36.694965\n",
      "1022  40.260556  40.079181\n",
      "1023  42.928000  42.211537\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0265\n",
      "Epoch 10/25, Validation Loss: 0.0182\n",
      "         actual  predicted\n",
      "0     26.115000  25.375739\n",
      "1     32.987500  32.460320\n",
      "2     37.017500  35.679443\n",
      "3     15.945251  16.355621\n",
      "4     37.537500  36.895249\n",
      "...         ...        ...\n",
      "1019  33.045000  32.801950\n",
      "1020  34.680000  34.070991\n",
      "1021  36.748000  35.900319\n",
      "1022  40.260556  39.430250\n",
      "1023  42.928000  41.655605\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0243\n",
      "Epoch 11/25, Validation Loss: 0.0125\n",
      "         actual  predicted\n",
      "0     26.115000  25.907383\n",
      "1     32.987500  33.170487\n",
      "2     37.017500  37.211178\n",
      "3     15.945251  17.316032\n",
      "4     37.537500  38.430255\n",
      "...         ...        ...\n",
      "1019  33.045000  33.652260\n",
      "1020  34.680000  35.044786\n",
      "1021  36.748000  36.575842\n",
      "1022  40.260556  40.402108\n",
      "1023  42.928000  42.508124\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0241\n",
      "Epoch 12/25, Validation Loss: 0.0171\n",
      "         actual  predicted\n",
      "0     26.115000  25.483326\n",
      "1     32.987500  33.226114\n",
      "2     37.017500  36.501989\n",
      "3     15.945251  16.134805\n",
      "4     37.537500  37.667521\n",
      "...         ...        ...\n",
      "1019  33.045000  33.753338\n",
      "1020  34.680000  35.013954\n",
      "1021  36.748000  36.532726\n",
      "1022  40.260556  39.554089\n",
      "1023  42.928000  41.708942\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0228\n",
      "Epoch 13/25, Validation Loss: 0.0109\n",
      "         actual  predicted\n",
      "0     26.115000  25.716855\n",
      "1     32.987500  33.478032\n",
      "2     37.017500  37.434457\n",
      "3     15.945251  15.833742\n",
      "4     37.537500  38.610759\n",
      "...         ...        ...\n",
      "1019  33.045000  34.055571\n",
      "1020  34.680000  35.513076\n",
      "1021  36.748000  37.038453\n",
      "1022  40.260556  41.881369\n",
      "1023  42.928000  43.709272\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0223\n",
      "Epoch 14/25, Validation Loss: 0.0293\n",
      "         actual  predicted\n",
      "0     26.115000  25.059406\n",
      "1     32.987500  32.948171\n",
      "2     37.017500  36.996523\n",
      "3     15.945251  14.773677\n",
      "4     37.537500  38.097771\n",
      "...         ...        ...\n",
      "1019  33.045000  33.650988\n",
      "1020  34.680000  35.204687\n",
      "1021  36.748000  36.640794\n",
      "1022  40.260556  40.201386\n",
      "1023  42.928000  42.215387\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0232\n",
      "Epoch 15/25, Validation Loss: 0.0155\n",
      "         actual  predicted\n",
      "0     26.115000  25.836562\n",
      "1     32.987500  32.906417\n",
      "2     37.017500  36.916163\n",
      "3     15.945251  15.491062\n",
      "4     37.537500  38.311467\n",
      "...         ...        ...\n",
      "1019  33.045000  33.720810\n",
      "1020  34.680000  35.056845\n",
      "1021  36.748000  36.617340\n",
      "1022  40.260556  40.347434\n",
      "1023  42.928000  42.378207\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0206\n",
      "Epoch 16/25, Validation Loss: 0.0147\n",
      "         actual  predicted\n",
      "0     26.115000  25.769744\n",
      "1     32.987500  33.073658\n",
      "2     37.017500  36.839198\n",
      "3     15.945251  15.378944\n",
      "4     37.537500  38.188097\n",
      "...         ...        ...\n",
      "1019  33.045000  33.499508\n",
      "1020  34.680000  34.893530\n",
      "1021  36.748000  36.341576\n",
      "1022  40.260556  40.094179\n",
      "1023  42.928000  42.382571\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0200\n",
      "Epoch 17/25, Validation Loss: 0.0143\n",
      "         actual  predicted\n",
      "0     26.115000  24.866271\n",
      "1     32.987500  32.430404\n",
      "2     37.017500  36.406398\n",
      "3     15.945251  15.533960\n",
      "4     37.537500  37.800005\n",
      "...         ...        ...\n",
      "1019  33.045000  33.240864\n",
      "1020  34.680000  34.556634\n",
      "1021  36.748000  36.117941\n",
      "1022  40.260556  39.628791\n",
      "1023  42.928000  41.453824\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0189\n",
      "Epoch 18/25, Validation Loss: 0.0106\n",
      "         actual  predicted\n",
      "0     26.115000  25.540206\n",
      "1     32.987500  32.709148\n",
      "2     37.017500  36.707505\n",
      "3     15.945251  15.707944\n",
      "4     37.537500  37.907053\n",
      "...         ...        ...\n",
      "1019  33.045000  33.136732\n",
      "1020  34.680000  34.518755\n",
      "1021  36.748000  36.140106\n",
      "1022  40.260556  39.987579\n",
      "1023  42.928000  41.922864\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0184\n",
      "Epoch 19/25, Validation Loss: 0.0103\n",
      "         actual  predicted\n",
      "0     26.115000  25.471763\n",
      "1     32.987500  32.955719\n",
      "2     37.017500  36.891847\n",
      "3     15.945251  15.109611\n",
      "4     37.537500  38.154367\n",
      "...         ...        ...\n",
      "1019  33.045000  33.690078\n",
      "1020  34.680000  35.174163\n",
      "1021  36.748000  36.618747\n",
      "1022  40.260556  40.067125\n",
      "1023  42.928000  41.801513\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0182\n",
      "Epoch 20/25, Validation Loss: 0.0098\n",
      "         actual  predicted\n",
      "0     26.115000  25.349188\n",
      "1     32.987500  33.103761\n",
      "2     37.017500  37.049816\n",
      "3     15.945251  15.646000\n",
      "4     37.537500  37.941490\n",
      "...         ...        ...\n",
      "1019  33.045000  33.591081\n",
      "1020  34.680000  35.167674\n",
      "1021  36.748000  36.875731\n",
      "1022  40.260556  40.092225\n",
      "1023  42.928000  42.364179\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0180\n",
      "Epoch 21/25, Validation Loss: 0.0110\n",
      "         actual  predicted\n",
      "0     26.115000  25.544259\n",
      "1     32.987500  33.116576\n",
      "2     37.017500  36.898605\n",
      "3     15.945251  15.843706\n",
      "4     37.537500  37.973279\n",
      "...         ...        ...\n",
      "1019  33.045000  33.189426\n",
      "1020  34.680000  34.578394\n",
      "1021  36.748000  36.129110\n",
      "1022  40.260556  39.267839\n",
      "1023  42.928000  41.380178\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0174\n",
      "Epoch 22/25, Validation Loss: 0.0096\n",
      "         actual  predicted\n",
      "0     26.115000  25.221706\n",
      "1     32.987500  32.586474\n",
      "2     37.017500  36.260088\n",
      "3     15.945251  15.855822\n",
      "4     37.537500  37.483166\n",
      "...         ...        ...\n",
      "1019  33.045000  32.801071\n",
      "1020  34.680000  34.191679\n",
      "1021  36.748000  35.921723\n",
      "1022  40.260556  38.860997\n",
      "1023  42.928000  41.114411\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0175\n",
      "Epoch 23/25, Validation Loss: 0.0104\n",
      "         actual  predicted\n",
      "0     26.115000  25.444403\n",
      "1     32.987500  32.848794\n",
      "2     37.017500  37.203485\n",
      "3     15.945251  14.662450\n",
      "4     37.537500  38.330836\n",
      "...         ...        ...\n",
      "1019  33.045000  33.470991\n",
      "1020  34.680000  35.047558\n",
      "1021  36.748000  36.726582\n",
      "1022  40.260556  40.362902\n",
      "1023  42.928000  42.577578\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0176\n",
      "Epoch 24/25, Validation Loss: 0.0124\n",
      "         actual  predicted\n",
      "0     26.115000  24.993363\n",
      "1     32.987500  32.734394\n",
      "2     37.017500  36.700801\n",
      "3     15.945251  14.609705\n",
      "4     37.537500  37.836840\n",
      "...         ...        ...\n",
      "1019  33.045000  32.997983\n",
      "1020  34.680000  34.586539\n",
      "1021  36.748000  36.402343\n",
      "1022  40.260556  39.370144\n",
      "1023  42.928000  41.353529\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0174\n",
      "Epoch 25/25, Validation Loss: 0.0131\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3896e-01,  1.6481e+00,  1.2189e+00, -7.3040e-01, -9.9768e-02,\n",
      "          -8.3099e-02, -1.8816e+00, -5.9109e-01, -2.0547e+00, -2.0547e+00,\n",
      "          -1.4280e+00,  4.1557e+00,  0.0000e+00, -9.0600e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.3963e-01, -1.0391e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.4623e-01,  1.6198e+00,  1.1779e+00, -6.8921e-01, -1.1689e-01,\n",
      "          -8.7525e-02, -1.7402e+00, -5.9109e-01, -2.0932e+00, -2.0932e+00,\n",
      "          -1.0415e-01,  4.1557e+00,  0.0000e+00, -9.0586e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.9923e-01, -9.7336e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.5349e-01,  1.5915e+00,  1.1368e+00, -6.4801e-01, -1.3400e-01,\n",
      "          -9.1950e-02, -1.5988e+00, -5.9109e-01, -2.1317e+00, -2.1317e+00,\n",
      "           1.2197e+00,  4.1557e+00,  0.0000e+00, -9.0573e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.0588e+00, -9.0760e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6076e-01,  1.5632e+00,  1.0958e+00, -6.0681e-01, -1.5112e-01,\n",
      "          -9.6376e-02, -1.4575e+00, -5.9109e-01, -2.1702e+00, -2.1702e+00,\n",
      "           2.5435e+00,  4.1557e+00,  0.0000e+00, -9.0560e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1184e+00, -8.4183e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6802e-01,  1.5349e+00,  1.0548e+00, -5.6561e-01, -1.6824e-01,\n",
      "          -1.0080e-01, -1.3161e+00, -5.9109e-01, -2.2087e+00, -2.2087e+00,\n",
      "           3.8674e+00,  4.1557e+00,  0.0000e+00, -9.0546e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1780e+00, -7.7606e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-6.6942e-01,  2.1811e+00,  1.2599e+00, -1.2637e+00,  2.6910e-01,\n",
      "           9.8832e-04, -6.0919e-01, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0720e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -7.7521e-01, -1.1816e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-1.9069e-01,  1.6999e+00,  3.6163e+00, -1.2338e+00,  4.5965e-02,\n",
      "          -5.3226e-02, -1.3514e+00, -4.8829e-01,  9.1100e-01,  9.1100e-01,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0578e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -8.4576e-01, -1.1314e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 9.4465e-02,  1.2588e+00,  2.2277e+00, -1.1200e+00,  2.9364e-03,\n",
      "          -6.0971e-02, -8.4482e-01, -3.5272e-01,  1.3539e+00,  1.3539e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0413e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -9.1458e-01, -1.0755e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.0253e-01,  1.1187e+00,  1.8504e+00, -1.1149e+00, -9.2903e-03,\n",
      "          -6.9822e-02, -6.0919e-01, -5.0170e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0386e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1980e+00, -7.4471e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.3886e-01,  9.3658e-01,  9.4820e-01, -1.1187e+00, -1.0231e-02,\n",
      "          -6.9822e-02, -2.0230e+00, -5.9109e-01, -1.7273e+00, -1.7273e+00,\n",
      "           3.8674e+00, -7.9865e-01,  0.0000e+00, -9.0372e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0336e+00, -9.6137e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.8916e-01,  8.3876e-01,  8.9899e-01, -1.0463e+00, -3.2803e-02,\n",
      "          -7.4248e-02, -2.0230e+00, -5.0170e-01,  1.8354e+00,  1.8354e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0359e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0943e+00, -8.9111e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.3667e-01,  7.6072e-01,  8.6618e-01, -1.0424e+00, -3.8446e-02,\n",
      "          -7.8673e-02, -1.3161e+00, -3.2292e-01,  1.6428e+00,  1.6428e+00,\n",
      "          -2.7518e+00, -7.9865e-01,  0.0000e+00, -9.0346e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1502e+00, -8.1703e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5624e-01,  8.1171e-01,  8.0056e-01, -9.8904e-01, -4.2208e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0332e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3516e+00, -4.0081e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5344e-01,  8.1795e-01,  7.5135e-01, -8.2501e-01, -4.3149e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0319e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.2470e+00, -6.5866e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.7021e-01,  9.1750e-01,  6.1738e-01, -8.6316e-01, -8.3591e-02,\n",
      "          -9.2688e-02, -1.3161e+00, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           1.9004e-01, -7.9865e-01,  0.0000e+00, -9.0292e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3034e+00, -5.3698e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.9396e-01,  1.0256e+00,  1.1738e+00, -6.0948e-01, -5.2084e-02,\n",
      "          -8.5312e-02, -1.6695e+00, -5.9109e-01,  1.6909e+00,  1.6909e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0259e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3387e+00, -4.4220e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2890e-01,  9.8028e-01,  7.0214e-01, -7.7160e-01, -4.4090e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.7872e+00,  1.7872e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0239e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3664e+00, -3.4705e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.1772e-01,  1.0011e+00,  7.1034e-01, -7.4872e-01, -4.5971e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0225e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3862e+00, -2.5629e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2051e-01,  1.1052e+00,  7.7596e-01, -6.8387e-01, -4.4090e-02,\n",
      "          -8.3099e-02, -6.0919e-01, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "           5.5777e-01, -7.9865e-01,  0.0000e+00, -9.0212e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4001e+00, -1.6443e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.0375e-01,  1.1520e+00,  8.0056e-01, -6.1520e-01, -4.4090e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.5465e+00,  1.5465e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0199e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4080e+00, -7.1849e-02, -1.1547e+00,  1.1547e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.6141]], device='cuda:0')\n",
      "[41.42]\n",
      "          actual  predicted\n",
      "0      26.115000  24.993363\n",
      "1      32.987500  32.734394\n",
      "2      37.017500  36.700801\n",
      "3      15.945251  14.609705\n",
      "4      37.537500  37.836840\n",
      "...          ...        ...\n",
      "18994  49.497500  49.461098\n",
      "18995  30.566522  30.105755\n",
      "18996  35.530000  35.487456\n",
      "18997  50.475000  49.805347\n",
      "18998  33.566774  33.659740\n",
      "\n",
      "[18999 rows x 2 columns]\n",
      "Score (RMSE): 1.1049\n",
      "Score (MAE): 0.7766\n",
      "Score (ME): 0.4431\n",
      "Score (MAPE): 2.3710%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100108, 27) to (100414, 27)\n",
      "training data cutoff:  2023-07-14 01:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([75809, 20, 25]) torch.Size([75809]) torch.Size([75809, 1])\n",
      "Testing data shape: torch.Size([19182, 20, 25]) torch.Size([19182]) torch.Size([19182, 1])\n",
      "Shuffled Training data shape: torch.Size([75992, 20, 25]) torch.Size([75992]) torch.Size([75992, 1])\n",
      "Shuffled Testing data shape: torch.Size([18999, 20, 25]) torch.Size([18999]) torch.Size([18999, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     27.867500  29.122315\n",
      "1     24.752500  25.052488\n",
      "2     19.026666  17.011632\n",
      "3     23.090000  23.248099\n",
      "4     25.920000  26.477419\n",
      "...         ...        ...\n",
      "1019  30.410000  31.652680\n",
      "1020  19.020000  18.302702\n",
      "1021  24.137273  23.747358\n",
      "1022  25.667500  24.097082\n",
      "1023  26.255000  26.222331\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2876\n",
      "Epoch 1/25, Validation Loss: 0.0544\n",
      "         actual  predicted\n",
      "0     27.867500  28.495506\n",
      "1     24.752500  24.633708\n",
      "2     19.026666  17.726212\n",
      "3     23.090000  23.313789\n",
      "4     25.920000  26.033175\n",
      "...         ...        ...\n",
      "1019  30.410000  30.714590\n",
      "1020  19.020000  18.488815\n",
      "1021  24.137273  23.898506\n",
      "1022  25.667500  24.204507\n",
      "1023  26.255000  26.000004\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0626\n",
      "Epoch 2/25, Validation Loss: 0.0414\n",
      "         actual  predicted\n",
      "0     27.867500  28.271247\n",
      "1     24.752500  24.752881\n",
      "2     19.026666  19.222373\n",
      "3     23.090000  23.673721\n",
      "4     25.920000  26.380780\n",
      "...         ...        ...\n",
      "1019  30.410000  30.300147\n",
      "1020  19.020000  19.945218\n",
      "1021  24.137273  24.222431\n",
      "1022  25.667500  24.661951\n",
      "1023  26.255000  26.059907\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0477\n",
      "Epoch 3/25, Validation Loss: 0.0237\n",
      "         actual  predicted\n",
      "0     27.867500  28.072912\n",
      "1     24.752500  24.283023\n",
      "2     19.026666  18.349087\n",
      "3     23.090000  23.207819\n",
      "4     25.920000  25.730751\n",
      "...         ...        ...\n",
      "1019  30.410000  30.574183\n",
      "1020  19.020000  18.919503\n",
      "1021  24.137273  23.900319\n",
      "1022  25.667500  24.083096\n",
      "1023  26.255000  25.669799\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0428\n",
      "Epoch 4/25, Validation Loss: 0.0250\n",
      "         actual  predicted\n",
      "0     27.867500  28.389509\n",
      "1     24.752500  24.613729\n",
      "2     19.026666  18.547941\n",
      "3     23.090000  23.215215\n",
      "4     25.920000  26.161700\n",
      "...         ...        ...\n",
      "1019  30.410000  30.879450\n",
      "1020  19.020000  18.786857\n",
      "1021  24.137273  24.059001\n",
      "1022  25.667500  24.618239\n",
      "1023  26.255000  26.406932\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0406\n",
      "Epoch 5/25, Validation Loss: 0.0223\n",
      "         actual  predicted\n",
      "0     27.867500  27.928410\n",
      "1     24.752500  24.664149\n",
      "2     19.026666  18.911023\n",
      "3     23.090000  23.217182\n",
      "4     25.920000  25.946760\n",
      "...         ...        ...\n",
      "1019  30.410000  30.084521\n",
      "1020  19.020000  19.700965\n",
      "1021  24.137273  24.395073\n",
      "1022  25.667500  24.689268\n",
      "1023  26.255000  26.439799\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0362\n",
      "Epoch 6/25, Validation Loss: 0.0152\n",
      "         actual  predicted\n",
      "0     27.867500  28.566447\n",
      "1     24.752500  24.849179\n",
      "2     19.026666  18.392670\n",
      "3     23.090000  23.432599\n",
      "4     25.920000  26.476824\n",
      "...         ...        ...\n",
      "1019  30.410000  31.568774\n",
      "1020  19.020000  18.827606\n",
      "1021  24.137273  24.429466\n",
      "1022  25.667500  24.851123\n",
      "1023  26.255000  26.906096\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0330\n",
      "Epoch 7/25, Validation Loss: 0.0349\n",
      "         actual  predicted\n",
      "0     27.867500  27.931392\n",
      "1     24.752500  24.535575\n",
      "2     19.026666  19.039080\n",
      "3     23.090000  23.322497\n",
      "4     25.920000  25.919426\n",
      "...         ...        ...\n",
      "1019  30.410000  30.088525\n",
      "1020  19.020000  19.624782\n",
      "1021  24.137273  24.327931\n",
      "1022  25.667500  24.602981\n",
      "1023  26.255000  26.149998\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0332\n",
      "Epoch 8/25, Validation Loss: 0.0143\n",
      "         actual  predicted\n",
      "0     27.867500  28.945739\n",
      "1     24.752500  24.785839\n",
      "2     19.026666  17.908907\n",
      "3     23.090000  23.057446\n",
      "4     25.920000  26.577741\n",
      "...         ...        ...\n",
      "1019  30.410000  31.605737\n",
      "1020  19.020000  18.251848\n",
      "1021  24.137273  24.204885\n",
      "1022  25.667500  24.810117\n",
      "1023  26.255000  26.831680\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0301\n",
      "Epoch 9/25, Validation Loss: 0.0562\n",
      "         actual  predicted\n",
      "0     27.867500  28.367401\n",
      "1     24.752500  24.550133\n",
      "2     19.026666  18.672598\n",
      "3     23.090000  23.487826\n",
      "4     25.920000  26.269138\n",
      "...         ...        ...\n",
      "1019  30.410000  30.858344\n",
      "1020  19.020000  18.778368\n",
      "1021  24.137273  24.320048\n",
      "1022  25.667500  24.617804\n",
      "1023  26.255000  26.147176\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0295\n",
      "Epoch 10/25, Validation Loss: 0.0169\n",
      "         actual  predicted\n",
      "0     27.867500  27.861348\n",
      "1     24.752500  24.239117\n",
      "2     19.026666  18.415386\n",
      "3     23.090000  23.092428\n",
      "4     25.920000  25.630123\n",
      "...         ...        ...\n",
      "1019  30.410000  30.326047\n",
      "1020  19.020000  18.899642\n",
      "1021  24.137273  24.075643\n",
      "1022  25.667500  24.436491\n",
      "1023  26.255000  26.059937\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0270\n",
      "Epoch 11/25, Validation Loss: 0.0186\n",
      "         actual  predicted\n",
      "0     27.867500  28.247925\n",
      "1     24.752500  24.838113\n",
      "2     19.026666  18.329244\n",
      "3     23.090000  23.199845\n",
      "4     25.920000  26.194677\n",
      "...         ...        ...\n",
      "1019  30.410000  30.522750\n",
      "1020  19.020000  18.313778\n",
      "1021  24.137273  24.170262\n",
      "1022  25.667500  24.692010\n",
      "1023  26.255000  26.132321\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0262\n",
      "Epoch 12/25, Validation Loss: 0.0180\n",
      "         actual  predicted\n",
      "0     27.867500  28.196198\n",
      "1     24.752500  24.533004\n",
      "2     19.026666  18.649738\n",
      "3     23.090000  23.023982\n",
      "4     25.920000  26.014444\n",
      "...         ...        ...\n",
      "1019  30.410000  30.582258\n",
      "1020  19.020000  18.864774\n",
      "1021  24.137273  24.125619\n",
      "1022  25.667500  24.435294\n",
      "1023  26.255000  26.228246\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0238\n",
      "Epoch 13/25, Validation Loss: 0.0141\n",
      "         actual  predicted\n",
      "0     27.867500  28.155665\n",
      "1     24.752500  24.706806\n",
      "2     19.026666  18.888555\n",
      "3     23.090000  23.283079\n",
      "4     25.920000  26.170088\n",
      "...         ...        ...\n",
      "1019  30.410000  30.643223\n",
      "1020  19.020000  18.911722\n",
      "1021  24.137273  24.304114\n",
      "1022  25.667500  24.638698\n",
      "1023  26.255000  26.397186\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0228\n",
      "Epoch 14/25, Validation Loss: 0.0139\n",
      "         actual  predicted\n",
      "0     27.867500  28.245646\n",
      "1     24.752500  24.559551\n",
      "2     19.026666  18.406101\n",
      "3     23.090000  22.958533\n",
      "4     25.920000  26.099505\n",
      "...         ...        ...\n",
      "1019  30.410000  30.853247\n",
      "1020  19.020000  18.547514\n",
      "1021  24.137273  24.191291\n",
      "1022  25.667500  24.462605\n",
      "1023  26.255000  26.375403\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0229\n",
      "Epoch 15/25, Validation Loss: 0.0194\n",
      "         actual  predicted\n",
      "0     27.867500  28.243475\n",
      "1     24.752500  24.536075\n",
      "2     19.026666  18.325581\n",
      "3     23.090000  22.993758\n",
      "4     25.920000  25.980764\n",
      "...         ...        ...\n",
      "1019  30.410000  30.964446\n",
      "1020  19.020000  18.552433\n",
      "1021  24.137273  24.139982\n",
      "1022  25.667500  24.320881\n",
      "1023  26.255000  26.155565\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0224\n",
      "Epoch 16/25, Validation Loss: 0.0179\n",
      "         actual  predicted\n",
      "0     27.867500  28.160302\n",
      "1     24.752500  24.579037\n",
      "2     19.026666  18.466899\n",
      "3     23.090000  23.073302\n",
      "4     25.920000  25.942094\n",
      "...         ...        ...\n",
      "1019  30.410000  31.116853\n",
      "1020  19.020000  18.704834\n",
      "1021  24.137273  24.095703\n",
      "1022  25.667500  24.359169\n",
      "1023  26.255000  26.190730\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0216\n",
      "Epoch 17/25, Validation Loss: 0.0173\n",
      "         actual  predicted\n",
      "0     27.867500  28.192380\n",
      "1     24.752500  24.539518\n",
      "2     19.026666  18.518327\n",
      "3     23.090000  23.129806\n",
      "4     25.920000  25.784743\n",
      "...         ...        ...\n",
      "1019  30.410000  30.536055\n",
      "1020  19.020000  18.820856\n",
      "1021  24.137273  24.142255\n",
      "1022  25.667500  24.506237\n",
      "1023  26.255000  26.217925\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0223\n",
      "Epoch 18/25, Validation Loss: 0.0138\n",
      "         actual  predicted\n",
      "0     27.867500  28.269027\n",
      "1     24.752500  24.553382\n",
      "2     19.026666  18.535253\n",
      "3     23.090000  23.031849\n",
      "4     25.920000  25.974210\n",
      "...         ...        ...\n",
      "1019  30.410000  31.026351\n",
      "1020  19.020000  18.558104\n",
      "1021  24.137273  24.289254\n",
      "1022  25.667500  24.447212\n",
      "1023  26.255000  26.418249\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0206\n",
      "Epoch 19/25, Validation Loss: 0.0148\n",
      "         actual  predicted\n",
      "0     27.867500  28.531525\n",
      "1     24.752500  24.653320\n",
      "2     19.026666  18.614487\n",
      "3     23.090000  23.181800\n",
      "4     25.920000  26.185112\n",
      "...         ...        ...\n",
      "1019  30.410000  30.869143\n",
      "1020  19.020000  18.817513\n",
      "1021  24.137273  24.254681\n",
      "1022  25.667500  24.540875\n",
      "1023  26.255000  26.421376\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0205\n",
      "Epoch 20/25, Validation Loss: 0.0167\n",
      "         actual  predicted\n",
      "0     27.867500  28.292988\n",
      "1     24.752500  24.610195\n",
      "2     19.026666  18.642740\n",
      "3     23.090000  23.123226\n",
      "4     25.920000  25.951908\n",
      "...         ...        ...\n",
      "1019  30.410000  30.676089\n",
      "1020  19.020000  18.856427\n",
      "1021  24.137273  24.197549\n",
      "1022  25.667500  24.289040\n",
      "1023  26.255000  26.269059\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0204\n",
      "Epoch 21/25, Validation Loss: 0.0163\n",
      "         actual  predicted\n",
      "0     27.867500  28.285094\n",
      "1     24.752500  24.552092\n",
      "2     19.026666  18.512117\n",
      "3     23.090000  23.126917\n",
      "4     25.920000  26.188114\n",
      "...         ...        ...\n",
      "1019  30.410000  30.777229\n",
      "1020  19.020000  18.503971\n",
      "1021  24.137273  24.203840\n",
      "1022  25.667500  24.453037\n",
      "1023  26.255000  26.327622\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0199\n",
      "Epoch 22/25, Validation Loss: 0.0173\n",
      "         actual  predicted\n",
      "0     27.867500  28.230369\n",
      "1     24.752500  24.527806\n",
      "2     19.026666  18.700663\n",
      "3     23.090000  23.059667\n",
      "4     25.920000  26.057765\n",
      "...         ...        ...\n",
      "1019  30.410000  30.777264\n",
      "1020  19.020000  18.831786\n",
      "1021  24.137273  24.244009\n",
      "1022  25.667500  24.496267\n",
      "1023  26.255000  26.393453\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0191\n",
      "Epoch 23/25, Validation Loss: 0.0137\n",
      "         actual  predicted\n",
      "0     27.867500  28.257891\n",
      "1     24.752500  24.610197\n",
      "2     19.026666  18.736604\n",
      "3     23.090000  23.204077\n",
      "4     25.920000  26.086047\n",
      "...         ...        ...\n",
      "1019  30.410000  30.841244\n",
      "1020  19.020000  18.900063\n",
      "1021  24.137273  24.228831\n",
      "1022  25.667500  24.441360\n",
      "1023  26.255000  26.327219\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0189\n",
      "Epoch 24/25, Validation Loss: 0.0130\n",
      "         actual  predicted\n",
      "0     27.867500  28.283003\n",
      "1     24.752500  24.630015\n",
      "2     19.026666  18.537137\n",
      "3     23.090000  23.129170\n",
      "4     25.920000  26.091420\n",
      "...         ...        ...\n",
      "1019  30.410000  30.585944\n",
      "1020  19.020000  18.784696\n",
      "1021  24.137273  24.177723\n",
      "1022  25.667500  24.432375\n",
      "1023  26.255000  26.254564\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0190\n",
      "Epoch 25/25, Validation Loss: 0.0134\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3896e-01,  1.6481e+00,  1.2189e+00, -7.3040e-01, -9.9768e-02,\n",
      "          -8.3099e-02, -1.8816e+00, -5.9109e-01, -2.0547e+00, -2.0547e+00,\n",
      "          -1.4280e+00,  4.1557e+00,  0.0000e+00, -9.0600e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.3963e-01, -1.0391e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.4623e-01,  1.6198e+00,  1.1779e+00, -6.8921e-01, -1.1689e-01,\n",
      "          -8.7525e-02, -1.7402e+00, -5.9109e-01, -2.0932e+00, -2.0932e+00,\n",
      "          -1.0415e-01,  4.1557e+00,  0.0000e+00, -9.0586e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.9923e-01, -9.7336e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.5349e-01,  1.5915e+00,  1.1368e+00, -6.4801e-01, -1.3400e-01,\n",
      "          -9.1950e-02, -1.5988e+00, -5.9109e-01, -2.1317e+00, -2.1317e+00,\n",
      "           1.2197e+00,  4.1557e+00,  0.0000e+00, -9.0573e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.0588e+00, -9.0760e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6076e-01,  1.5632e+00,  1.0958e+00, -6.0681e-01, -1.5112e-01,\n",
      "          -9.6376e-02, -1.4575e+00, -5.9109e-01, -2.1702e+00, -2.1702e+00,\n",
      "           2.5435e+00,  4.1557e+00,  0.0000e+00, -9.0560e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1184e+00, -8.4183e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6802e-01,  1.5349e+00,  1.0548e+00, -5.6561e-01, -1.6824e-01,\n",
      "          -1.0080e-01, -1.3161e+00, -5.9109e-01, -2.2087e+00, -2.2087e+00,\n",
      "           3.8674e+00,  4.1557e+00,  0.0000e+00, -9.0546e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1780e+00, -7.7606e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-6.6942e-01,  2.1811e+00,  1.2599e+00, -1.2637e+00,  2.6910e-01,\n",
      "           9.8832e-04, -6.0919e-01, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0720e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -7.7521e-01, -1.1816e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-1.9069e-01,  1.6999e+00,  3.6163e+00, -1.2338e+00,  4.5965e-02,\n",
      "          -5.3226e-02, -1.3514e+00, -4.8829e-01,  9.1100e-01,  9.1100e-01,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0578e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -8.4576e-01, -1.1314e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 9.4465e-02,  1.2588e+00,  2.2277e+00, -1.1200e+00,  2.9364e-03,\n",
      "          -6.0971e-02, -8.4482e-01, -3.5272e-01,  1.3539e+00,  1.3539e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0413e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -9.1458e-01, -1.0755e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.0253e-01,  1.1187e+00,  1.8504e+00, -1.1149e+00, -9.2903e-03,\n",
      "          -6.9822e-02, -6.0919e-01, -5.0170e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0386e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1980e+00, -7.4471e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.3886e-01,  9.3658e-01,  9.4820e-01, -1.1187e+00, -1.0231e-02,\n",
      "          -6.9822e-02, -2.0230e+00, -5.9109e-01, -1.7273e+00, -1.7273e+00,\n",
      "           3.8674e+00, -7.9865e-01,  0.0000e+00, -9.0372e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0336e+00, -9.6137e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.8916e-01,  8.3876e-01,  8.9899e-01, -1.0463e+00, -3.2803e-02,\n",
      "          -7.4248e-02, -2.0230e+00, -5.0170e-01,  1.8354e+00,  1.8354e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0359e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0943e+00, -8.9111e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.3667e-01,  7.6072e-01,  8.6618e-01, -1.0424e+00, -3.8446e-02,\n",
      "          -7.8673e-02, -1.3161e+00, -3.2292e-01,  1.6428e+00,  1.6428e+00,\n",
      "          -2.7518e+00, -7.9865e-01,  0.0000e+00, -9.0346e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1502e+00, -8.1703e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5624e-01,  8.1171e-01,  8.0056e-01, -9.8904e-01, -4.2208e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0332e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3516e+00, -4.0081e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5344e-01,  8.1795e-01,  7.5135e-01, -8.2501e-01, -4.3149e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0319e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.2470e+00, -6.5866e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.7021e-01,  9.1750e-01,  6.1738e-01, -8.6316e-01, -8.3591e-02,\n",
      "          -9.2688e-02, -1.3161e+00, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           1.9004e-01, -7.9865e-01,  0.0000e+00, -9.0292e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3034e+00, -5.3698e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.9396e-01,  1.0256e+00,  1.1738e+00, -6.0948e-01, -5.2084e-02,\n",
      "          -8.5312e-02, -1.6695e+00, -5.9109e-01,  1.6909e+00,  1.6909e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0259e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3387e+00, -4.4220e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2890e-01,  9.8028e-01,  7.0214e-01, -7.7160e-01, -4.4090e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.7872e+00,  1.7872e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0239e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3664e+00, -3.4705e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.1772e-01,  1.0011e+00,  7.1034e-01, -7.4872e-01, -4.5971e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0225e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3862e+00, -2.5629e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2051e-01,  1.1052e+00,  7.7596e-01, -6.8387e-01, -4.4090e-02,\n",
      "          -8.3099e-02, -6.0919e-01, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "           5.5777e-01, -7.9865e-01,  0.0000e+00, -9.0212e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4001e+00, -1.6443e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.0375e-01,  1.1520e+00,  8.0056e-01, -6.1520e-01, -4.4090e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.5465e+00,  1.5465e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0199e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4080e+00, -7.1849e-02, -1.1547e+00,  1.1547e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.1663]], device='cuda:0')\n",
      "[24.63]\n",
      "          actual  predicted\n",
      "0      27.867500  28.283003\n",
      "1      24.752500  24.630015\n",
      "2      19.026666  18.537137\n",
      "3      23.090000  23.129170\n",
      "4      25.920000  26.091420\n",
      "...          ...        ...\n",
      "18994  23.260000  23.302592\n",
      "18995  19.742500  19.023234\n",
      "18996  28.312500  28.683460\n",
      "18997  28.656923  29.153024\n",
      "18998  27.540000  27.641087\n",
      "\n",
      "[18999 rows x 2 columns]\n",
      "Score (RMSE): 0.4150\n",
      "Score (MAE): 0.2615\n",
      "Score (ME): 0.0417\n",
      "Score (MAPE): 1.1047%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100108, 27) to (100414, 27)\n",
      "training data cutoff:  2023-07-14 01:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([75809, 20, 25]) torch.Size([75809]) torch.Size([75809, 1])\n",
      "Testing data shape: torch.Size([19182, 20, 25]) torch.Size([19182]) torch.Size([19182, 1])\n",
      "Shuffled Training data shape: torch.Size([75992, 20, 25]) torch.Size([75992]) torch.Size([75992, 1])\n",
      "Shuffled Testing data shape: torch.Size([18999, 20, 25]) torch.Size([18999]) torch.Size([18999, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0      34.499996  -48.179938\n",
      "1      64.399999   43.507279\n",
      "2       7.666672  -90.036515\n",
      "3     281.666668  446.830955\n",
      "4      14.000006  -93.298196\n",
      "...          ...         ...\n",
      "1019  146.250000  -81.809021\n",
      "1020  205.250000  132.464904\n",
      "1021  741.625021  308.692404\n",
      "1022   10.249996  194.912097\n",
      "1023  431.500004  316.694286\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.6778\n",
      "Epoch 1/25, Validation Loss: 1.2411\n",
      "          actual   predicted\n",
      "0      34.499996   89.677311\n",
      "1      64.399999  108.592653\n",
      "2       7.666672   71.838618\n",
      "3     281.666668  184.869619\n",
      "4      14.000006   69.845729\n",
      "...          ...         ...\n",
      "1019  146.250000   90.831403\n",
      "1020  205.250000  106.034513\n",
      "1021  741.625021  167.506607\n",
      "1022   10.249996  125.173027\n",
      "1023  431.500004  162.463532\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.5705\n",
      "Epoch 2/25, Validation Loss: 1.2447\n",
      "          actual  predicted\n",
      "0      34.499996   0.360624\n",
      "1      64.399999  30.745851\n",
      "2       7.666672 -27.516347\n",
      "3     281.666668  37.250754\n",
      "4      14.000006 -26.826883\n",
      "...          ...        ...\n",
      "1019  146.250000  -3.813677\n",
      "1020  205.250000  13.338839\n",
      "1021  741.625021  55.836463\n",
      "1022   10.249996  31.185889\n",
      "1023  431.500004  90.540231\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.5452\n",
      "Epoch 3/25, Validation Loss: 1.2012\n",
      "          actual   predicted\n",
      "0      34.499996   10.951564\n",
      "1      64.399999   83.589553\n",
      "2       7.666672  -35.325999\n",
      "3     281.666668  155.717902\n",
      "4      14.000006  -26.250273\n",
      "...          ...         ...\n",
      "1019  146.250000    4.393473\n",
      "1020  205.250000   42.242155\n",
      "1021  741.625021  150.851126\n",
      "1022   10.249996  115.356258\n",
      "1023  431.500004  251.754847\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.5464\n",
      "Epoch 4/25, Validation Loss: 1.1951\n",
      "          actual   predicted\n",
      "0      34.499996   72.672948\n",
      "1      64.399999   93.122310\n",
      "2       7.666672   13.079401\n",
      "3     281.666668  187.285854\n",
      "4      14.000006   18.141701\n",
      "...          ...         ...\n",
      "1019  146.250000   54.270264\n",
      "1020  205.250000   77.651151\n",
      "1021  741.625021  562.090940\n",
      "1022   10.249996  482.919085\n",
      "1023  431.500004  547.106994\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.5274\n",
      "Epoch 5/25, Validation Loss: 1.1902\n",
      "          actual   predicted\n",
      "0      34.499996  115.857390\n",
      "1      64.399999  203.305831\n",
      "2       7.666672   -9.287720\n",
      "3     281.666668  334.644351\n",
      "4      14.000006    5.145076\n",
      "...          ...         ...\n",
      "1019  146.250000   46.818122\n",
      "1020  205.250000   82.609597\n",
      "1021  741.625021  515.662863\n",
      "1022   10.249996  527.783688\n",
      "1023  431.500004  596.855309\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.5288\n",
      "Epoch 6/25, Validation Loss: 1.2512\n",
      "          actual   predicted\n",
      "0      34.499996  118.644794\n",
      "1      64.399999  187.336046\n",
      "2       7.666672   54.791517\n",
      "3     281.666668  269.128373\n",
      "4      14.000006   67.208490\n",
      "...          ...         ...\n",
      "1019  146.250000  131.338633\n",
      "1020  205.250000  231.329963\n",
      "1021  741.625021  526.841250\n",
      "1022   10.249996  616.883937\n",
      "1023  431.500004  559.765011\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.5220\n",
      "Epoch 7/25, Validation Loss: 1.1828\n",
      "          actual   predicted\n",
      "0      34.499996   45.804927\n",
      "1      64.399999   99.327430\n",
      "2       7.666672   -0.844421\n",
      "3     281.666668  155.580696\n",
      "4      14.000006   10.054201\n",
      "...          ...         ...\n",
      "1019  146.250000   28.796095\n",
      "1020  205.250000   48.897430\n",
      "1021  741.625021  234.889837\n",
      "1022   10.249996  420.921643\n",
      "1023  431.500004  544.212852\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.5336\n",
      "Epoch 8/25, Validation Loss: 1.1959\n",
      "          actual   predicted\n",
      "0      34.499996   22.766989\n",
      "1      64.399999   31.555345\n",
      "2       7.666672  -26.288361\n",
      "3     281.666668   98.507566\n",
      "4      14.000006   -7.711730\n",
      "...          ...         ...\n",
      "1019  146.250000  -13.189544\n",
      "1020  205.250000   14.845276\n",
      "1021  741.625021   67.013028\n",
      "1022   10.249996  134.042146\n",
      "1023  431.500004   81.522753\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.5224\n",
      "Epoch 9/25, Validation Loss: 1.1875\n",
      "          actual   predicted\n",
      "0      34.499996   54.354219\n",
      "1      64.399999  224.328486\n",
      "2       7.666672   -3.182597\n",
      "3     281.666668  325.839035\n",
      "4      14.000006   30.322622\n",
      "...          ...         ...\n",
      "1019  146.250000   48.704598\n",
      "1020  205.250000  108.920836\n",
      "1021  741.625021  275.833642\n",
      "1022   10.249996  498.469502\n",
      "1023  431.500004  341.456968\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.5012\n",
      "Epoch 10/25, Validation Loss: 1.1754\n",
      "          actual   predicted\n",
      "0      34.499996  148.943127\n",
      "1      64.399999  189.870997\n",
      "2       7.666672   28.398470\n",
      "3     281.666668  442.373973\n",
      "4      14.000006   50.814785\n",
      "...          ...         ...\n",
      "1019  146.250000   75.034184\n",
      "1020  205.250000  143.328258\n",
      "1021  741.625021  306.817977\n",
      "1022   10.249996  487.107709\n",
      "1023  431.500004  274.429919\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.4884\n",
      "Epoch 11/25, Validation Loss: 1.1319\n",
      "          actual   predicted\n",
      "0      34.499996   58.316280\n",
      "1      64.399999  168.635143\n",
      "2       7.666672   22.986026\n",
      "3     281.666668  262.274111\n",
      "4      14.000006   38.160349\n",
      "...          ...         ...\n",
      "1019  146.250000   57.664254\n",
      "1020  205.250000  103.162394\n",
      "1021  741.625021  305.577023\n",
      "1022   10.249996  357.478952\n",
      "1023  431.500004  432.075664\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.4995\n",
      "Epoch 12/25, Validation Loss: 1.1536\n",
      "          actual   predicted\n",
      "0      34.499996  157.891708\n",
      "1      64.399999  113.325186\n",
      "2       7.666672   55.230558\n",
      "3     281.666668  305.407726\n",
      "4      14.000006   63.491640\n",
      "...          ...         ...\n",
      "1019  146.250000   84.973035\n",
      "1020  205.250000   89.818557\n",
      "1021  741.625021  493.800470\n",
      "1022   10.249996  789.118482\n",
      "1023  431.500004  395.575665\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.4935\n",
      "Epoch 13/25, Validation Loss: 1.1572\n",
      "          actual   predicted\n",
      "0      34.499996   23.798452\n",
      "1      64.399999   89.141720\n",
      "2       7.666672   -7.213419\n",
      "3     281.666668  248.850668\n",
      "4      14.000006    9.900027\n",
      "...          ...         ...\n",
      "1019  146.250000   11.697494\n",
      "1020  205.250000   27.508426\n",
      "1021  741.625021  112.901838\n",
      "1022   10.249996  326.208990\n",
      "1023  431.500004  296.523747\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.4884\n",
      "Epoch 14/25, Validation Loss: 1.1353\n",
      "          actual   predicted\n",
      "0      34.499996   78.739888\n",
      "1      64.399999   70.709742\n",
      "2       7.666672   18.588791\n",
      "3     281.666668  114.208995\n",
      "4      14.000006   28.921038\n",
      "...          ...         ...\n",
      "1019  146.250000   62.794714\n",
      "1020  205.250000   66.081966\n",
      "1021  741.625021  252.913379\n",
      "1022   10.249996  821.286093\n",
      "1023  431.500004  150.421703\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.4858\n",
      "Epoch 15/25, Validation Loss: 1.1791\n",
      "          actual   predicted\n",
      "0      34.499996   50.519461\n",
      "1      64.399999   67.043860\n",
      "2       7.666672    6.890006\n",
      "3     281.666668  105.392762\n",
      "4      14.000006   18.184241\n",
      "...          ...         ...\n",
      "1019  146.250000   36.685646\n",
      "1020  205.250000   36.615190\n",
      "1021  741.625021  137.942187\n",
      "1022   10.249996  321.749037\n",
      "1023  431.500004  207.243283\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.4781\n",
      "Epoch 16/25, Validation Loss: 1.1287\n",
      "          actual   predicted\n",
      "0      34.499996   76.691301\n",
      "1      64.399999  104.604054\n",
      "2       7.666672   10.005070\n",
      "3     281.666668  210.208558\n",
      "4      14.000006   29.036521\n",
      "...          ...         ...\n",
      "1019  146.250000   52.317372\n",
      "1020  205.250000   67.377525\n",
      "1021  741.625021  392.924927\n",
      "1022   10.249996  890.092048\n",
      "1023  431.500004  491.718310\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.4603\n",
      "Epoch 17/25, Validation Loss: 1.1471\n",
      "          actual   predicted\n",
      "0      34.499996   57.735487\n",
      "1      64.399999   91.175066\n",
      "2       7.666672  -12.582229\n",
      "3     281.666668  170.319661\n",
      "4      14.000006    2.673339\n",
      "...          ...         ...\n",
      "1019  146.250000   24.850670\n",
      "1020  205.250000   32.890751\n",
      "1021  741.625021  295.631889\n",
      "1022   10.249996  747.489608\n",
      "1023  431.500004  359.867114\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.4573\n",
      "Epoch 18/25, Validation Loss: 1.1256\n",
      "          actual   predicted\n",
      "0      34.499996   47.144927\n",
      "1      64.399999   66.643684\n",
      "2       7.666672   -8.531444\n",
      "3     281.666668  152.797432\n",
      "4      14.000006    2.500391\n",
      "...          ...         ...\n",
      "1019  146.250000   21.684700\n",
      "1020  205.250000   20.591590\n",
      "1021  741.625021  213.772367\n",
      "1022   10.249996  696.083708\n",
      "1023  431.500004  258.947733\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.4451\n",
      "Epoch 19/25, Validation Loss: 1.1100\n",
      "          actual   predicted\n",
      "0      34.499996   65.444960\n",
      "1      64.399999  108.877425\n",
      "2       7.666672    5.415605\n",
      "3     281.666668  170.749649\n",
      "4      14.000006   14.690373\n",
      "...          ...         ...\n",
      "1019  146.250000   37.772704\n",
      "1020  205.250000   38.021923\n",
      "1021  741.625021  313.876225\n",
      "1022   10.249996  675.967496\n",
      "1023  431.500004  468.960883\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.4425\n",
      "Epoch 20/25, Validation Loss: 1.1032\n",
      "          actual   predicted\n",
      "0      34.499996  173.616220\n",
      "1      64.399999  183.031869\n",
      "2       7.666672    2.363439\n",
      "3     281.666668  274.320400\n",
      "4      14.000006   19.189483\n",
      "...          ...         ...\n",
      "1019  146.250000   58.207245\n",
      "1020  205.250000   49.816182\n",
      "1021  741.625021  414.763571\n",
      "1022   10.249996  697.859549\n",
      "1023  431.500004  504.451394\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.4439\n",
      "Epoch 21/25, Validation Loss: 1.1497\n",
      "          actual   predicted\n",
      "0      34.499996   59.760388\n",
      "1      64.399999  131.744569\n",
      "2       7.666672  -12.632295\n",
      "3     281.666668  221.486238\n",
      "4      14.000006    3.379961\n",
      "...          ...         ...\n",
      "1019  146.250000   17.716587\n",
      "1020  205.250000   32.452756\n",
      "1021  741.625021  241.821517\n",
      "1022   10.249996  616.023759\n",
      "1023  431.500004  381.023709\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.4255\n",
      "Epoch 22/25, Validation Loss: 1.1759\n",
      "          actual   predicted\n",
      "0      34.499996   90.329139\n",
      "1      64.399999  112.073102\n",
      "2       7.666672    4.133782\n",
      "3     281.666668  180.679482\n",
      "4      14.000006   16.985106\n",
      "...          ...         ...\n",
      "1019  146.250000   42.052967\n",
      "1020  205.250000   51.888122\n",
      "1021  741.625021  294.789408\n",
      "1022   10.249996  722.462908\n",
      "1023  431.500004  345.262558\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.4325\n",
      "Epoch 23/25, Validation Loss: 1.0977\n",
      "          actual   predicted\n",
      "0      34.499996  195.457848\n",
      "1      64.399999  299.492338\n",
      "2       7.666672   39.329872\n",
      "3     281.666668  364.188666\n",
      "4      14.000006   60.457885\n",
      "...          ...         ...\n",
      "1019  146.250000   87.827157\n",
      "1020  205.250000   86.155979\n",
      "1021  741.625021  510.611653\n",
      "1022   10.249996  834.269885\n",
      "1023  431.500004  651.853956\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.4495\n",
      "Epoch 24/25, Validation Loss: 1.1631\n",
      "          actual   predicted\n",
      "0      34.499996   44.217593\n",
      "1      64.399999   60.206495\n",
      "2       7.666672  -22.540378\n",
      "3     281.666668  114.615620\n",
      "4      14.000006   -7.820892\n",
      "...          ...         ...\n",
      "1019  146.250000    0.660479\n",
      "1020  205.250000   -1.976126\n",
      "1021  741.625021  210.381298\n",
      "1022   10.249996  538.060863\n",
      "1023  431.500004  262.187708\n",
      "\n",
      "[1024 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.4275\n",
      "Epoch 25/25, Validation Loss: 1.1434\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3896e-01,  1.6481e+00,  1.2189e+00, -7.3040e-01, -9.9768e-02,\n",
      "          -8.3099e-02, -1.8816e+00, -5.9109e-01, -2.0547e+00, -2.0547e+00,\n",
      "          -1.4280e+00,  4.1557e+00,  0.0000e+00, -9.0600e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.3963e-01, -1.0391e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.4623e-01,  1.6198e+00,  1.1779e+00, -6.8921e-01, -1.1689e-01,\n",
      "          -8.7525e-02, -1.7402e+00, -5.9109e-01, -2.0932e+00, -2.0932e+00,\n",
      "          -1.0415e-01,  4.1557e+00,  0.0000e+00, -9.0586e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.9923e-01, -9.7336e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.5349e-01,  1.5915e+00,  1.1368e+00, -6.4801e-01, -1.3400e-01,\n",
      "          -9.1950e-02, -1.5988e+00, -5.9109e-01, -2.1317e+00, -2.1317e+00,\n",
      "           1.2197e+00,  4.1557e+00,  0.0000e+00, -9.0573e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.0588e+00, -9.0760e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6076e-01,  1.5632e+00,  1.0958e+00, -6.0681e-01, -1.5112e-01,\n",
      "          -9.6376e-02, -1.4575e+00, -5.9109e-01, -2.1702e+00, -2.1702e+00,\n",
      "           2.5435e+00,  4.1557e+00,  0.0000e+00, -9.0560e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1184e+00, -8.4183e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6802e-01,  1.5349e+00,  1.0548e+00, -5.6561e-01, -1.6824e-01,\n",
      "          -1.0080e-01, -1.3161e+00, -5.9109e-01, -2.2087e+00, -2.2087e+00,\n",
      "           3.8674e+00,  4.1557e+00,  0.0000e+00, -9.0546e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1780e+00, -7.7606e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-6.6942e-01,  2.1811e+00,  1.2599e+00, -1.2637e+00,  2.6910e-01,\n",
      "           9.8832e-04, -6.0919e-01, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0720e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -7.7521e-01, -1.1816e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-1.9069e-01,  1.6999e+00,  3.6163e+00, -1.2338e+00,  4.5965e-02,\n",
      "          -5.3226e-02, -1.3514e+00, -4.8829e-01,  9.1100e-01,  9.1100e-01,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0578e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -8.4576e-01, -1.1314e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 9.4465e-02,  1.2588e+00,  2.2277e+00, -1.1200e+00,  2.9364e-03,\n",
      "          -6.0971e-02, -8.4482e-01, -3.5272e-01,  1.3539e+00,  1.3539e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0413e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -9.1458e-01, -1.0755e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.0253e-01,  1.1187e+00,  1.8504e+00, -1.1149e+00, -9.2903e-03,\n",
      "          -6.9822e-02, -6.0919e-01, -5.0170e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0386e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1980e+00, -7.4471e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.3886e-01,  9.3658e-01,  9.4820e-01, -1.1187e+00, -1.0231e-02,\n",
      "          -6.9822e-02, -2.0230e+00, -5.9109e-01, -1.7273e+00, -1.7273e+00,\n",
      "           3.8674e+00, -7.9865e-01,  0.0000e+00, -9.0372e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0336e+00, -9.6137e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.8916e-01,  8.3876e-01,  8.9899e-01, -1.0463e+00, -3.2803e-02,\n",
      "          -7.4248e-02, -2.0230e+00, -5.0170e-01,  1.8354e+00,  1.8354e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0359e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0943e+00, -8.9111e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.3667e-01,  7.6072e-01,  8.6618e-01, -1.0424e+00, -3.8446e-02,\n",
      "          -7.8673e-02, -1.3161e+00, -3.2292e-01,  1.6428e+00,  1.6428e+00,\n",
      "          -2.7518e+00, -7.9865e-01,  0.0000e+00, -9.0346e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1502e+00, -8.1703e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5624e-01,  8.1171e-01,  8.0056e-01, -9.8904e-01, -4.2208e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0332e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3516e+00, -4.0081e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5344e-01,  8.1795e-01,  7.5135e-01, -8.2501e-01, -4.3149e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0319e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.2470e+00, -6.5866e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.7021e-01,  9.1750e-01,  6.1738e-01, -8.6316e-01, -8.3591e-02,\n",
      "          -9.2688e-02, -1.3161e+00, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           1.9004e-01, -7.9865e-01,  0.0000e+00, -9.0292e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3034e+00, -5.3698e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.9396e-01,  1.0256e+00,  1.1738e+00, -6.0948e-01, -5.2084e-02,\n",
      "          -8.5312e-02, -1.6695e+00, -5.9109e-01,  1.6909e+00,  1.6909e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0259e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3387e+00, -4.4220e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2890e-01,  9.8028e-01,  7.0214e-01, -7.7160e-01, -4.4090e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.7872e+00,  1.7872e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0239e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3664e+00, -3.4705e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.1772e-01,  1.0011e+00,  7.1034e-01, -7.4872e-01, -4.5971e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0225e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3862e+00, -2.5629e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2051e-01,  1.1052e+00,  7.7596e-01, -6.8387e-01, -4.4090e-02,\n",
      "          -8.3099e-02, -6.0919e-01, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "           5.5777e-01, -7.9865e-01,  0.0000e+00, -9.0212e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4001e+00, -1.6443e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.0375e-01,  1.1520e+00,  8.0056e-01, -6.1520e-01, -4.4090e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.5465e+00,  1.5465e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0199e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4080e+00, -7.1849e-02, -1.1547e+00,  1.1547e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.1638]], device='cuda:0')\n",
      "[29.76]\n",
      "           actual   predicted\n",
      "0       34.499996   44.217593\n",
      "1       64.399999   60.206495\n",
      "2        7.666672  -22.540378\n",
      "3      281.666668  114.615620\n",
      "4       14.000006   -7.820892\n",
      "...           ...         ...\n",
      "18994    9.000003   -5.577160\n",
      "18995    5.999994   14.916873\n",
      "18996    7.749994  196.826442\n",
      "18997  115.999998   80.288002\n",
      "18998    6.500001  -14.913640\n",
      "\n",
      "[18999 rows x 2 columns]\n",
      "Score (RMSE): 1139.5475\n",
      "Score (MAE): 139.4792\n",
      "Score (ME): 95.7959\n",
      "Score (MAPE): 578.6226%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\2361498595.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utils)\n",
    "performance_df = pd.read_csv('model_performances.csv')\n",
    "d_model=256\n",
    "nhead=8\n",
    "num_layers=5 \n",
    "dropout_pe=0.15\n",
    "dropout_encoder=0.15\n",
    "batch_size=1024\n",
    "learning_rate=0.00031\n",
    "epochs=25\n",
    "y_feature = 'CO2'\n",
    "aggregation_level = 'quarter_hour'\n",
    "freq = 15 if aggregation_level == 'quarter_hour' else 30 if aggregation_level == 'half_hour' else 60\n",
    "window_size = 5 * 60 // freq\n",
    "for aggregation_level in ['quarter_hour', 'half_hour', 'hour']:\n",
    "    for y_feature in ['CO2', 'VOC', 'hum', 'tmp', 'vis']:\n",
    "        \n",
    "        model, scaler, rmse, mae, me, mape, train_loss, val_loss, n_features = utils.create_multivariate_transformer_model_for_feature(df, d_model=d_model, nhead=nhead, num_layers=num_layers, dropout_pe=dropout_pe, dropout_encoder=dropout_encoder, batch_size=batch_size, learning_rate=learning_rate, epochs=epochs, y_feature=y_feature, aggregation_level=aggregation_level, window_size=window_size)\n",
    "        performance_df = performance_df.append({'model_name': 'multivariate_transformer','aggregation_level': aggregation_level, 'y_feature': y_feature, 'n_features': n_features, 'rmse': rmse, 'mae': mae, 'me': me, 'mape': mape, 'd_model': d_model, 'nhead': nhead, 'num_layers': num_layers, 'dropout_pe': dropout_pe,'dropout_encoder': dropout_encoder, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'train_loss': train_loss, 'val_loss': val_loss, 'note': '2 decoder Linear layers with LeakyReLu, shuffled training data'}, ignore_index=True)\n",
    "        performance_df.to_csv('model_performances.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (392384, 27) to (399291, 27)\n",
      "training data cutoff:  2023-07-14 04:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([312832, 20, 25]) torch.Size([312832]) torch.Size([312832, 1])\n",
      "Testing data shape: torch.Size([78609, 20, 25]) torch.Size([78609]) torch.Size([78609, 1])\n",
      "Shuffled Training data shape: torch.Size([313152, 20, 25]) torch.Size([313152]) torch.Size([313152, 1])\n",
      "Shuffled Testing data shape: torch.Size([78289, 20, 25]) torch.Size([78289]) torch.Size([78289, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     448.999999  452.671162\n",
      "1     694.000001  692.941092\n",
      "2     427.000001  441.003520\n",
      "3     471.000001  501.871488\n",
      "4     655.999998  677.827931\n",
      "...          ...         ...\n",
      "4091  455.999999  461.012605\n",
      "4092  470.000001  495.623072\n",
      "4093  425.000001  424.863443\n",
      "4094  407.999999  431.743032\n",
      "4095  429.000001  434.778908\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.3456\n",
      "Epoch 1/25, Validation Loss: 0.1174\n",
      "          actual   predicted\n",
      "0     448.999999  454.273958\n",
      "1     694.000001  678.068329\n",
      "2     427.000001  440.606113\n",
      "3     471.000001  526.152003\n",
      "4     655.999998  657.926496\n",
      "...          ...         ...\n",
      "4091  455.999999  462.387797\n",
      "4092  470.000001  488.588632\n",
      "4093  425.000001  429.513564\n",
      "4094  407.999999  426.266525\n",
      "4095  429.000001  436.736703\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0783\n",
      "Epoch 2/25, Validation Loss: 0.0535\n",
      "          actual   predicted\n",
      "0     448.999999  447.891144\n",
      "1     694.000001  701.100418\n",
      "2     427.000001  430.570957\n",
      "3     471.000001  533.690370\n",
      "4     655.999998  682.105127\n",
      "...          ...         ...\n",
      "4091  455.999999  460.094041\n",
      "4092  470.000001  481.962970\n",
      "4093  425.000001  424.383625\n",
      "4094  407.999999  421.818832\n",
      "4095  429.000001  432.438655\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0561\n",
      "Epoch 3/25, Validation Loss: 0.0441\n",
      "          actual   predicted\n",
      "0     448.999999  445.676160\n",
      "1     694.000001  699.220066\n",
      "2     427.000001  426.108468\n",
      "3     471.000001  525.883741\n",
      "4     655.999998  681.998043\n",
      "...          ...         ...\n",
      "4091  455.999999  459.286779\n",
      "4092  470.000001  480.832168\n",
      "4093  425.000001  418.596640\n",
      "4094  407.999999  421.208320\n",
      "4095  429.000001  430.589007\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0496\n",
      "Epoch 4/25, Validation Loss: 0.0404\n",
      "          actual   predicted\n",
      "0     448.999999  456.078156\n",
      "1     694.000001  694.664324\n",
      "2     427.000001  431.827105\n",
      "3     471.000001  520.523285\n",
      "4     655.999998  675.230433\n",
      "...          ...         ...\n",
      "4091  455.999999  460.287350\n",
      "4092  470.000001  485.859648\n",
      "4093  425.000001  425.602268\n",
      "4094  407.999999  424.703615\n",
      "4095  429.000001  437.719175\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0456\n",
      "Epoch 5/25, Validation Loss: 0.0371\n",
      "          actual   predicted\n",
      "0     448.999999  453.011045\n",
      "1     694.000001  692.129607\n",
      "2     427.000001  430.574139\n",
      "3     471.000001  510.594895\n",
      "4     655.999998  669.366600\n",
      "...          ...         ...\n",
      "4091  455.999999  459.084198\n",
      "4092  470.000001  481.465012\n",
      "4093  425.000001  424.804443\n",
      "4094  407.999999  421.199886\n",
      "4095  429.000001  435.606266\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0422\n",
      "Epoch 6/25, Validation Loss: 0.0345\n",
      "          actual   predicted\n",
      "0     448.999999  448.011017\n",
      "1     694.000001  680.131074\n",
      "2     427.000001  431.871023\n",
      "3     471.000001  502.782160\n",
      "4     655.999998  658.525250\n",
      "...          ...         ...\n",
      "4091  455.999999  454.684582\n",
      "4092  470.000001  473.823853\n",
      "4093  425.000001  427.403624\n",
      "4094  407.999999  419.904563\n",
      "4095  429.000001  433.315182\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0408\n",
      "Epoch 7/25, Validation Loss: 0.0372\n",
      "          actual   predicted\n",
      "0     448.999999  451.133049\n",
      "1     694.000001  706.694762\n",
      "2     427.000001  427.989622\n",
      "3     471.000001  511.658081\n",
      "4     655.999998  681.061184\n",
      "...          ...         ...\n",
      "4091  455.999999  461.149302\n",
      "4092  470.000001  480.891888\n",
      "4093  425.000001  423.327126\n",
      "4094  407.999999  420.385658\n",
      "4095  429.000001  433.911294\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0401\n",
      "Epoch 8/25, Validation Loss: 0.0359\n",
      "          actual   predicted\n",
      "0     448.999999  451.351307\n",
      "1     694.000001  703.623072\n",
      "2     427.000001  428.457322\n",
      "3     471.000001  506.157454\n",
      "4     655.999998  676.707619\n",
      "...          ...         ...\n",
      "4091  455.999999  460.339708\n",
      "4092  470.000001  479.896046\n",
      "4093  425.000001  423.133919\n",
      "4094  407.999999  419.990258\n",
      "4095  429.000001  433.621309\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0384\n",
      "Epoch 9/25, Validation Loss: 0.0331\n",
      "          actual   predicted\n",
      "0     448.999999  450.015509\n",
      "1     694.000001  691.085112\n",
      "2     427.000001  430.822086\n",
      "3     471.000001  505.345415\n",
      "4     655.999998  665.365396\n",
      "...          ...         ...\n",
      "4091  455.999999  459.781341\n",
      "4092  470.000001  477.556318\n",
      "4093  425.000001  426.944891\n",
      "4094  407.999999  420.844565\n",
      "4095  429.000001  433.875062\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0372\n",
      "Epoch 10/25, Validation Loss: 0.0324\n",
      "          actual   predicted\n",
      "0     448.999999  449.371981\n",
      "1     694.000001  694.744016\n",
      "2     427.000001  430.217370\n",
      "3     471.000001  503.887974\n",
      "4     655.999998  672.354258\n",
      "...          ...         ...\n",
      "4091  455.999999  457.584864\n",
      "4092  470.000001  477.784469\n",
      "4093  425.000001  427.259318\n",
      "4094  407.999999  419.780604\n",
      "4095  429.000001  432.827845\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0371\n",
      "Epoch 11/25, Validation Loss: 0.0315\n",
      "          actual   predicted\n",
      "0     448.999999  451.810911\n",
      "1     694.000001  699.022591\n",
      "2     427.000001  427.887942\n",
      "3     471.000001  503.139960\n",
      "4     655.999998  677.765062\n",
      "...          ...         ...\n",
      "4091  455.999999  455.660314\n",
      "4092  470.000001  477.511533\n",
      "4093  425.000001  424.198141\n",
      "4094  407.999999  418.553107\n",
      "4095  429.000001  434.574948\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0357\n",
      "Epoch 12/25, Validation Loss: 0.0320\n",
      "          actual   predicted\n",
      "0     448.999999  451.314476\n",
      "1     694.000001  681.183959\n",
      "2     427.000001  431.074464\n",
      "3     471.000001  495.608105\n",
      "4     655.999998  657.595968\n",
      "...          ...         ...\n",
      "4091  455.999999  460.195905\n",
      "4092  470.000001  470.939037\n",
      "4093  425.000001  428.553008\n",
      "4094  407.999999  418.905982\n",
      "4095  429.000001  434.745267\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0354\n",
      "Epoch 13/25, Validation Loss: 0.0325\n",
      "          actual   predicted\n",
      "0     448.999999  450.067509\n",
      "1     694.000001  703.179189\n",
      "2     427.000001  427.906907\n",
      "3     471.000001  503.016702\n",
      "4     655.999998  673.072115\n",
      "...          ...         ...\n",
      "4091  455.999999  461.026490\n",
      "4092  470.000001  480.258930\n",
      "4093  425.000001  424.404557\n",
      "4094  407.999999  422.448461\n",
      "4095  429.000001  429.149370\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0353\n",
      "Epoch 14/25, Validation Loss: 0.0319\n",
      "          actual   predicted\n",
      "0     448.999999  448.997557\n",
      "1     694.000001  696.487381\n",
      "2     427.000001  427.228794\n",
      "3     471.000001  497.244942\n",
      "4     655.999998  669.603529\n",
      "...          ...         ...\n",
      "4091  455.999999  460.130379\n",
      "4092  470.000001  475.915736\n",
      "4093  425.000001  424.778989\n",
      "4094  407.999999  420.852803\n",
      "4095  429.000001  431.822982\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0346\n",
      "Epoch 15/25, Validation Loss: 0.0304\n",
      "          actual   predicted\n",
      "0     448.999999  451.389984\n",
      "1     694.000001  697.558397\n",
      "2     427.000001  425.925923\n",
      "3     471.000001  502.996866\n",
      "4     655.999998  668.088428\n",
      "...          ...         ...\n",
      "4091  455.999999  458.414770\n",
      "4092  470.000001  478.652032\n",
      "4093  425.000001  423.361645\n",
      "4094  407.999999  416.156031\n",
      "4095  429.000001  430.763703\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0338\n",
      "Epoch 16/25, Validation Loss: 0.0312\n",
      "          actual   predicted\n",
      "0     448.999999  449.975380\n",
      "1     694.000001  690.644495\n",
      "2     427.000001  429.080325\n",
      "3     471.000001  498.691527\n",
      "4     655.999998  660.835782\n",
      "...          ...         ...\n",
      "4091  455.999999  462.850562\n",
      "4092  470.000001  476.797860\n",
      "4093  425.000001  424.917050\n",
      "4094  407.999999  420.195027\n",
      "4095  429.000001  432.337674\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0333\n",
      "Epoch 17/25, Validation Loss: 0.0306\n",
      "          actual   predicted\n",
      "0     448.999999  452.224695\n",
      "1     694.000001  693.029160\n",
      "2     427.000001  429.548740\n",
      "3     471.000001  498.182503\n",
      "4     655.999998  666.562918\n",
      "...          ...         ...\n",
      "4091  455.999999  457.181411\n",
      "4092  470.000001  478.484374\n",
      "4093  425.000001  426.226613\n",
      "4094  407.999999  416.595007\n",
      "4095  429.000001  433.956718\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0333\n",
      "Epoch 18/25, Validation Loss: 0.0300\n",
      "          actual   predicted\n",
      "0     448.999999  447.556374\n",
      "1     694.000001  699.619385\n",
      "2     427.000001  426.253816\n",
      "3     471.000001  504.463152\n",
      "4     655.999998  672.018489\n",
      "...          ...         ...\n",
      "4091  455.999999  458.672462\n",
      "4092  470.000001  471.623134\n",
      "4093  425.000001  424.308752\n",
      "4094  407.999999  415.834183\n",
      "4095  429.000001  430.258835\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0326\n",
      "Epoch 19/25, Validation Loss: 0.0311\n",
      "          actual   predicted\n",
      "0     448.999999  449.754622\n",
      "1     694.000001  680.221639\n",
      "2     427.000001  431.386263\n",
      "3     471.000001  494.008133\n",
      "4     655.999998  647.648974\n",
      "...          ...         ...\n",
      "4091  455.999999  459.436605\n",
      "4092  470.000001  471.704861\n",
      "4093  425.000001  429.496577\n",
      "4094  407.999999  420.864750\n",
      "4095  429.000001  431.965765\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0325\n",
      "Epoch 20/25, Validation Loss: 0.0318\n",
      "          actual   predicted\n",
      "0     448.999999  451.661055\n",
      "1     694.000001  688.132815\n",
      "2     427.000001  430.127063\n",
      "3     471.000001  501.726537\n",
      "4     655.999998  662.658578\n",
      "...          ...         ...\n",
      "4091  455.999999  460.960605\n",
      "4092  470.000001  472.425132\n",
      "4093  425.000001  428.849449\n",
      "4094  407.999999  419.734081\n",
      "4095  429.000001  431.626647\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0332\n",
      "Epoch 21/25, Validation Loss: 0.0300\n",
      "          actual   predicted\n",
      "0     448.999999  451.361250\n",
      "1     694.000001  699.978176\n",
      "2     427.000001  427.977073\n",
      "3     471.000001  505.099412\n",
      "4     655.999998  676.496238\n",
      "...          ...         ...\n",
      "4091  455.999999  455.261645\n",
      "4092  470.000001  474.469069\n",
      "4093  425.000001  424.522550\n",
      "4094  407.999999  416.050855\n",
      "4095  429.000001  432.208533\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0324\n",
      "Epoch 22/25, Validation Loss: 0.0315\n",
      "          actual   predicted\n",
      "0     448.999999  450.538254\n",
      "1     694.000001  686.059967\n",
      "2     427.000001  429.528476\n",
      "3     471.000001  500.581294\n",
      "4     655.999998  662.229618\n",
      "...          ...         ...\n",
      "4091  455.999999  457.320383\n",
      "4092  470.000001  472.062651\n",
      "4093  425.000001  428.032952\n",
      "4094  407.999999  416.604835\n",
      "4095  429.000001  431.693050\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0313\n",
      "Epoch 23/25, Validation Loss: 0.0294\n",
      "          actual   predicted\n",
      "0     448.999999  451.374931\n",
      "1     694.000001  693.227245\n",
      "2     427.000001  429.903259\n",
      "3     471.000001  502.876839\n",
      "4     655.999998  668.208387\n",
      "...          ...         ...\n",
      "4091  455.999999  456.554431\n",
      "4092  470.000001  474.642904\n",
      "4093  425.000001  427.761141\n",
      "4094  407.999999  417.630502\n",
      "4095  429.000001  432.083573\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0312\n",
      "Epoch 24/25, Validation Loss: 0.0296\n",
      "          actual   predicted\n",
      "0     448.999999  453.390503\n",
      "1     694.000001  691.168840\n",
      "2     427.000001  429.902120\n",
      "3     471.000001  501.726269\n",
      "4     655.999998  661.821661\n",
      "...          ...         ...\n",
      "4091  455.999999  460.538747\n",
      "4092  470.000001  474.841308\n",
      "4093  425.000001  428.234753\n",
      "4094  407.999999  417.169084\n",
      "4095  429.000001  432.746806\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0311\n",
      "Epoch 25/25, Validation Loss: 0.0296\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4371,  1.6471,  1.2275, -0.7223, -0.1084, -0.1014, -1.5026,\n",
      "          -0.5233, -1.8361, -1.8361, -0.5868,  4.1524,  0.0000, -0.9068,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9400,\n",
      "          -1.0373, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4443,  1.6188,  1.1864, -0.6811, -0.1326, -0.1086, -1.3903,\n",
      "          -0.5233, -1.8705, -1.8705, -0.0432,  4.1524,  0.0000, -0.9066,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9995,\n",
      "          -0.9717, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4516,  1.5905,  1.1454, -0.6400, -0.1568, -0.1157, -1.2780,\n",
      "          -0.5233, -1.9049, -1.9049,  0.5005,  4.1524,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.0590,\n",
      "          -0.9061, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4589,  1.5622,  1.1043, -0.5989, -0.1810, -0.1229, -1.1657,\n",
      "          -0.5233, -1.9393, -1.9393,  1.0442,  4.1524,  0.0000, -0.9064,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1184,\n",
      "          -0.8406, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4661,  1.5339,  1.0632, -0.5578, -0.2052, -0.1300, -1.0534,\n",
      "          -0.5233, -1.9737, -1.9737,  1.5879,  4.1524,  0.0000, -0.9062,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1779,\n",
      "          -0.7750, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.6714,  2.1800,  1.2685, -1.2546,  0.4127,  0.0343, -0.4919,\n",
      "          -0.3653,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9080,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.7760,\n",
      "          -1.1793, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.1926,  1.6989,  3.6279, -1.2248,  0.0975, -0.0532, -1.0815,\n",
      "          -0.4325,  0.8111,  0.8111, -0.2244, -0.7996,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.8464,\n",
      "          -1.1292, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.0926,  1.2578,  2.2376, -1.1112,  0.0367, -0.0657, -0.6791,\n",
      "          -0.3127,  1.2064,  1.2064,  1.1348, -0.7996,  0.0000, -0.9049,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.9151,\n",
      "          -1.0735, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2006,  1.1177,  1.8598, -1.1061,  0.0194, -0.0800, -0.4919,\n",
      "          -0.4443,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9046,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1978,\n",
      "          -0.7437, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2370,  0.9357,  0.9565, -1.1099,  0.0181, -0.0800, -1.6149,\n",
      "          -0.5233, -1.5439, -1.5439,  1.5879, -0.7996,  0.0000, -0.9045,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0338,\n",
      "          -0.9597, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2873,  0.8379,  0.9072, -1.0376, -0.0138, -0.0871, -1.6149,\n",
      "          -0.4443,  1.6362,  1.6362, -0.6774, -0.7996,  0.0000, -0.9044,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0943,\n",
      "          -0.8897, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3348,  0.7598,  0.8744, -1.0338, -0.0218, -0.0943, -1.0534,\n",
      "          -0.2864,  1.4643,  1.4643, -1.1305, -0.7996,  0.0000, -0.9042,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1501,\n",
      "          -0.8158, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3543,  0.8108,  0.8087, -0.9805, -0.0271, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -0.2244, -0.7996,  0.0000, -0.9041,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3511,\n",
      "          -0.4009, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3516,  0.8170,  0.7594, -0.8167, -0.0284, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -1.5836, -0.7996,  0.0000, -0.9039,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.2467,\n",
      "          -0.6580, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3683,  0.9166,  0.6252, -0.8548, -0.0856, -0.1169, -1.0534,\n",
      "          -0.3653,  1.4213,  1.4213,  0.0777, -0.7996,  0.0000, -0.9037,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3030,\n",
      "          -0.5367, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3921,  1.0246,  1.1823, -0.6016, -0.0411, -0.1050, -1.3342,\n",
      "          -0.5233,  1.5072,  1.5072,  1.1348, -0.7996,  0.0000, -0.9033,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3382,\n",
      "          -0.4422, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4270,  0.9794,  0.7101, -0.7634, -0.0298, -0.1050, -1.6149,\n",
      "          -0.5233,  1.5932,  1.5932, -0.6774, -0.7996,  0.0000, -0.9031,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3658,\n",
      "          -0.3473, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4158,  1.0002,  0.7183, -0.7405, -0.0324, -0.1050, -1.6149,\n",
      "          -0.5233,  1.4213,  1.4213, -0.2244, -0.7996,  0.0000, -0.9030,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3856,\n",
      "          -0.2568, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4186,  1.1042,  0.7840, -0.6758, -0.0298, -0.1014, -0.4919,\n",
      "          -0.5233,  1.4213,  1.4213,  0.2287, -0.7996,  0.0000, -0.9029,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3995,\n",
      "          -0.1653, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4019,  1.1510,  0.8087, -0.6073, -0.0298, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3783,  1.3783, -1.5836, -0.7996,  0.0000, -0.9027,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.4074,\n",
      "          -0.0730, -1.1564,  1.1564,  0.0000]]])\n",
      "predicted: tensor([[0.7907]], device='cuda:0')\n",
      "[583.81]\n",
      "           actual   predicted\n",
      "0      448.999999  453.390503\n",
      "1      694.000001  691.168840\n",
      "2      427.000001  429.902120\n",
      "3      471.000001  501.726269\n",
      "4      655.999998  661.821661\n",
      "...           ...         ...\n",
      "78284  489.000000  494.752802\n",
      "78285  453.999999  458.671366\n",
      "78286  445.000000  451.768930\n",
      "78287  470.000001  477.759923\n",
      "78288  471.666667  473.751745\n",
      "\n",
      "[78289 rows x 2 columns]\n",
      "Score (RMSE): 21.1947\n",
      "Score (MAE): 7.4227\n",
      "Score (ME): -0.9524\n",
      "Score (MAPE): 1.3606%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (392384, 27) to (399291, 27)\n",
      "training data cutoff:  2023-07-14 04:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([312832, 20, 25]) torch.Size([312832]) torch.Size([312832, 1])\n",
      "Testing data shape: torch.Size([78609, 20, 25]) torch.Size([78609]) torch.Size([78609, 1])\n",
      "Shuffled Training data shape: torch.Size([313152, 20, 25]) torch.Size([313152]) torch.Size([313152, 1])\n",
      "Shuffled Testing data shape: torch.Size([78289, 20, 25]) torch.Size([78289]) torch.Size([78289, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      574.000006   585.340392\n",
      "1      567.999995   581.982176\n",
      "2      675.000001   699.212901\n",
      "3      699.000003   662.118800\n",
      "4      647.999993   659.922944\n",
      "...           ...          ...\n",
      "4091   598.000000   648.873762\n",
      "4092   591.294119   595.759427\n",
      "4093   638.999993   623.250506\n",
      "4094   603.999995   614.905289\n",
      "4095  1139.000015  1147.972202\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.3337\n",
      "Epoch 1/25, Validation Loss: 0.1258\n",
      "           actual    predicted\n",
      "0      574.000006   581.187622\n",
      "1      567.999995   588.753827\n",
      "2      675.000001   695.838314\n",
      "3      699.000003   674.646064\n",
      "4      647.999993   663.083796\n",
      "...           ...          ...\n",
      "4091   598.000000   640.977223\n",
      "4092   591.294119   599.420507\n",
      "4093   638.999993   649.879312\n",
      "4094   603.999995   611.788802\n",
      "4095  1139.000015  1143.731124\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0990\n",
      "Epoch 2/25, Validation Loss: 0.0611\n",
      "           actual    predicted\n",
      "0      574.000006   581.298516\n",
      "1      567.999995   585.441738\n",
      "2      675.000001   704.824321\n",
      "3      699.000003   676.412251\n",
      "4      647.999993   653.415435\n",
      "...           ...          ...\n",
      "4091   598.000000   624.706621\n",
      "4092   591.294119   609.271127\n",
      "4093   638.999993   647.970912\n",
      "4094   603.999995   614.595881\n",
      "4095  1139.000015  1110.739581\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0670\n",
      "Epoch 3/25, Validation Loss: 0.0467\n",
      "           actual    predicted\n",
      "0      574.000006   585.266890\n",
      "1      567.999995   590.353814\n",
      "2      675.000001   717.302203\n",
      "3      699.000003   693.961428\n",
      "4      647.999993   659.136771\n",
      "...           ...          ...\n",
      "4091   598.000000   637.557684\n",
      "4092   591.294119   613.545856\n",
      "4093   638.999993   661.718431\n",
      "4094   603.999995   613.331268\n",
      "4095  1139.000015  1147.934856\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0560\n",
      "Epoch 4/25, Validation Loss: 0.0438\n",
      "           actual    predicted\n",
      "0      574.000006   571.306403\n",
      "1      567.999995   575.941596\n",
      "2      675.000001   699.138507\n",
      "3      699.000003   677.135887\n",
      "4      647.999993   638.033408\n",
      "...           ...          ...\n",
      "4091   598.000000   620.443176\n",
      "4092   591.294119   598.373064\n",
      "4093   638.999993   644.418660\n",
      "4094   603.999995   603.766703\n",
      "4095  1139.000015  1125.079753\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0512\n",
      "Epoch 5/25, Validation Loss: 0.0392\n",
      "           actual    predicted\n",
      "0      574.000006   580.005385\n",
      "1      567.999995   582.525831\n",
      "2      675.000001   702.792591\n",
      "3      699.000003   679.252669\n",
      "4      647.999993   641.760555\n",
      "...           ...          ...\n",
      "4091   598.000000   620.909337\n",
      "4092   591.294119   607.466616\n",
      "4093   638.999993   655.170737\n",
      "4094   603.999995   612.075185\n",
      "4095  1139.000015  1116.852473\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0485\n",
      "Epoch 6/25, Validation Loss: 0.0378\n",
      "           actual    predicted\n",
      "0      574.000006   578.254513\n",
      "1      567.999995   579.316449\n",
      "2      675.000001   709.074727\n",
      "3      699.000003   682.430136\n",
      "4      647.999993   638.650861\n",
      "...           ...          ...\n",
      "4091   598.000000   618.116266\n",
      "4092   591.294119   603.148687\n",
      "4093   638.999993   654.758700\n",
      "4094   603.999995   610.283775\n",
      "4095  1139.000015  1115.151891\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0462\n",
      "Epoch 7/25, Validation Loss: 0.0369\n",
      "           actual    predicted\n",
      "0      574.000006   574.026160\n",
      "1      567.999995   571.945878\n",
      "2      675.000001   707.734014\n",
      "3      699.000003   686.024852\n",
      "4      647.999993   640.254793\n",
      "...           ...          ...\n",
      "4091   598.000000   616.570497\n",
      "4092   591.294119   608.160779\n",
      "4093   638.999993   659.301570\n",
      "4094   603.999995   601.398598\n",
      "4095  1139.000015  1140.010879\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0442\n",
      "Epoch 8/25, Validation Loss: 0.0365\n",
      "           actual    predicted\n",
      "0      574.000006   573.542312\n",
      "1      567.999995   572.370202\n",
      "2      675.000001   704.131958\n",
      "3      699.000003   675.726055\n",
      "4      647.999993   634.689045\n",
      "...           ...          ...\n",
      "4091   598.000000   614.427107\n",
      "4092   591.294119   604.701984\n",
      "4093   638.999993   650.669117\n",
      "4094   603.999995   606.681828\n",
      "4095  1139.000015  1140.968683\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0429\n",
      "Epoch 9/25, Validation Loss: 0.0369\n",
      "           actual    predicted\n",
      "0      574.000006   575.386798\n",
      "1      567.999995   574.471927\n",
      "2      675.000001   704.419359\n",
      "3      699.000003   684.251559\n",
      "4      647.999993   644.047850\n",
      "...           ...          ...\n",
      "4091   598.000000   616.885383\n",
      "4092   591.294119   607.954252\n",
      "4093   638.999993   656.653060\n",
      "4094   603.999995   602.835523\n",
      "4095  1139.000015  1120.831726\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0423\n",
      "Epoch 10/25, Validation Loss: 0.0356\n",
      "           actual    predicted\n",
      "0      574.000006   568.522879\n",
      "1      567.999995   573.760077\n",
      "2      675.000001   694.267713\n",
      "3      699.000003   676.229718\n",
      "4      647.999993   636.381926\n",
      "...           ...          ...\n",
      "4091   598.000000   611.222218\n",
      "4092   591.294119   603.941817\n",
      "4093   638.999993   651.687189\n",
      "4094   603.999995   601.922468\n",
      "4095  1139.000015  1110.162087\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0417\n",
      "Epoch 11/25, Validation Loss: 0.0356\n",
      "           actual    predicted\n",
      "0      574.000006   573.700162\n",
      "1      567.999995   580.276382\n",
      "2      675.000001   698.561781\n",
      "3      699.000003   674.992895\n",
      "4      647.999993   640.075249\n",
      "...           ...          ...\n",
      "4091   598.000000   611.726476\n",
      "4092   591.294119   598.583536\n",
      "4093   638.999993   654.228991\n",
      "4094   603.999995   602.546698\n",
      "4095  1139.000015  1118.749643\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0409\n",
      "Epoch 12/25, Validation Loss: 0.0353\n",
      "           actual    predicted\n",
      "0      574.000006   574.367873\n",
      "1      567.999995   572.947523\n",
      "2      675.000001   709.555789\n",
      "3      699.000003   694.697139\n",
      "4      647.999993   641.434526\n",
      "...           ...          ...\n",
      "4091   598.000000   618.761188\n",
      "4092   591.294119   605.191623\n",
      "4093   638.999993   657.515607\n",
      "4094   603.999995   604.705913\n",
      "4095  1139.000015  1137.412440\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0405\n",
      "Epoch 13/25, Validation Loss: 0.0359\n",
      "           actual    predicted\n",
      "0      574.000006   570.089043\n",
      "1      567.999995   571.145297\n",
      "2      675.000001   700.194433\n",
      "3      699.000003   687.221543\n",
      "4      647.999993   637.345960\n",
      "...           ...          ...\n",
      "4091   598.000000   608.920304\n",
      "4092   591.294119   597.808499\n",
      "4093   638.999993   654.407744\n",
      "4094   603.999995   602.836869\n",
      "4095  1139.000015  1150.143187\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0404\n",
      "Epoch 14/25, Validation Loss: 0.0368\n",
      "           actual    predicted\n",
      "0      574.000006   572.521399\n",
      "1      567.999995   570.859524\n",
      "2      675.000001   702.018846\n",
      "3      699.000003   681.116329\n",
      "4      647.999993   642.188135\n",
      "...           ...          ...\n",
      "4091   598.000000   610.276231\n",
      "4092   591.294119   601.757606\n",
      "4093   638.999993   655.219203\n",
      "4094   603.999995   600.167292\n",
      "4095  1139.000015  1133.129039\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0398\n",
      "Epoch 15/25, Validation Loss: 0.0354\n",
      "           actual    predicted\n",
      "0      574.000006   575.793623\n",
      "1      567.999995   569.838878\n",
      "2      675.000001   696.059914\n",
      "3      699.000003   680.479045\n",
      "4      647.999993   642.251087\n",
      "...           ...          ...\n",
      "4091   598.000000   610.925598\n",
      "4092   591.294119   604.379727\n",
      "4093   638.999993   653.678043\n",
      "4094   603.999995   604.587678\n",
      "4095  1139.000015  1130.318579\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0396\n",
      "Epoch 16/25, Validation Loss: 0.0350\n",
      "           actual    predicted\n",
      "0      574.000006   574.430637\n",
      "1      567.999995   576.768988\n",
      "2      675.000001   704.374907\n",
      "3      699.000003   691.354490\n",
      "4      647.999993   646.944098\n",
      "...           ...          ...\n",
      "4091   598.000000   616.570091\n",
      "4092   591.294119   604.657987\n",
      "4093   638.999993   656.922500\n",
      "4094   603.999995   606.085865\n",
      "4095  1139.000015  1135.812656\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0395\n",
      "Epoch 17/25, Validation Loss: 0.0350\n",
      "           actual    predicted\n",
      "0      574.000006   575.597739\n",
      "1      567.999995   575.301918\n",
      "2      675.000001   695.409795\n",
      "3      699.000003   684.073573\n",
      "4      647.999993   633.090858\n",
      "...           ...          ...\n",
      "4091   598.000000   610.419211\n",
      "4092   591.294119   599.065537\n",
      "4093   638.999993   655.189769\n",
      "4094   603.999995   600.279782\n",
      "4095  1139.000015  1119.070070\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0392\n",
      "Epoch 18/25, Validation Loss: 0.0348\n",
      "           actual    predicted\n",
      "0      574.000006   573.872756\n",
      "1      567.999995   570.723353\n",
      "2      675.000001   700.977053\n",
      "3      699.000003   682.845601\n",
      "4      647.999993   636.868388\n",
      "...           ...          ...\n",
      "4091   598.000000   605.374735\n",
      "4092   591.294119   600.975917\n",
      "4093   638.999993   657.114276\n",
      "4094   603.999995   599.493038\n",
      "4095  1139.000015  1124.729150\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0390\n",
      "Epoch 19/25, Validation Loss: 0.0350\n",
      "           actual    predicted\n",
      "0      574.000006   574.927054\n",
      "1      567.999995   573.046302\n",
      "2      675.000001   698.243655\n",
      "3      699.000003   682.426919\n",
      "4      647.999993   635.107703\n",
      "...           ...          ...\n",
      "4091   598.000000   608.475868\n",
      "4092   591.294119   601.863929\n",
      "4093   638.999993   652.292261\n",
      "4094   603.999995   606.373250\n",
      "4095  1139.000015  1129.667459\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0388\n",
      "Epoch 20/25, Validation Loss: 0.0346\n",
      "           actual    predicted\n",
      "0      574.000006   575.877517\n",
      "1      567.999995   568.373654\n",
      "2      675.000001   698.555669\n",
      "3      699.000003   681.060123\n",
      "4      647.999993   629.262114\n",
      "...           ...          ...\n",
      "4091   598.000000   596.943746\n",
      "4092   591.294119   602.668032\n",
      "4093   638.999993   652.573322\n",
      "4094   603.999995   601.876561\n",
      "4095  1139.000015  1127.167313\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0387\n",
      "Epoch 21/25, Validation Loss: 0.0346\n",
      "           actual    predicted\n",
      "0      574.000006   575.245117\n",
      "1      567.999995   570.166754\n",
      "2      675.000001   696.624487\n",
      "3      699.000003   681.169342\n",
      "4      647.999993   639.715192\n",
      "...           ...          ...\n",
      "4091   598.000000   609.243423\n",
      "4092   591.294119   602.635116\n",
      "4093   638.999993   653.404276\n",
      "4094   603.999995   602.782588\n",
      "4095  1139.000015  1134.908131\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0387\n",
      "Epoch 22/25, Validation Loss: 0.0356\n",
      "           actual    predicted\n",
      "0      574.000006   573.435315\n",
      "1      567.999995   566.984342\n",
      "2      675.000001   703.116813\n",
      "3      699.000003   689.822440\n",
      "4      647.999993   640.990480\n",
      "...           ...          ...\n",
      "4091   598.000000   617.737724\n",
      "4092   591.294119   601.099129\n",
      "4093   638.999993   653.137089\n",
      "4094   603.999995   600.278890\n",
      "4095  1139.000015  1133.850562\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0388\n",
      "Epoch 23/25, Validation Loss: 0.0350\n",
      "           actual    predicted\n",
      "0      574.000006   575.121826\n",
      "1      567.999995   568.125023\n",
      "2      675.000001   695.029336\n",
      "3      699.000003   685.184148\n",
      "4      647.999993   636.464490\n",
      "...           ...          ...\n",
      "4091   598.000000   609.033640\n",
      "4092   591.294119   599.957305\n",
      "4093   638.999993   652.804728\n",
      "4094   603.999995   601.195170\n",
      "4095  1139.000015  1125.332907\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0382\n",
      "Epoch 24/25, Validation Loss: 0.0343\n",
      "           actual    predicted\n",
      "0      574.000006   574.547401\n",
      "1      567.999995   566.823049\n",
      "2      675.000001   696.709899\n",
      "3      699.000003   680.209659\n",
      "4      647.999993   635.531745\n",
      "...           ...          ...\n",
      "4091   598.000000   607.853171\n",
      "4092   591.294119   596.503942\n",
      "4093   638.999993   652.985782\n",
      "4094   603.999995   599.449619\n",
      "4095  1139.000015  1116.699460\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0382\n",
      "Epoch 25/25, Validation Loss: 0.0345\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4371,  1.6471,  1.2275, -0.7223, -0.1084, -0.1014, -1.5026,\n",
      "          -0.5233, -1.8361, -1.8361, -0.5868,  4.1524,  0.0000, -0.9068,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9400,\n",
      "          -1.0373, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4443,  1.6188,  1.1864, -0.6811, -0.1326, -0.1086, -1.3903,\n",
      "          -0.5233, -1.8705, -1.8705, -0.0432,  4.1524,  0.0000, -0.9066,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9995,\n",
      "          -0.9717, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4516,  1.5905,  1.1454, -0.6400, -0.1568, -0.1157, -1.2780,\n",
      "          -0.5233, -1.9049, -1.9049,  0.5005,  4.1524,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.0590,\n",
      "          -0.9061, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4589,  1.5622,  1.1043, -0.5989, -0.1810, -0.1229, -1.1657,\n",
      "          -0.5233, -1.9393, -1.9393,  1.0442,  4.1524,  0.0000, -0.9064,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1184,\n",
      "          -0.8406, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4661,  1.5339,  1.0632, -0.5578, -0.2052, -0.1300, -1.0534,\n",
      "          -0.5233, -1.9737, -1.9737,  1.5879,  4.1524,  0.0000, -0.9062,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1779,\n",
      "          -0.7750, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.6714,  2.1800,  1.2685, -1.2546,  0.4127,  0.0343, -0.4919,\n",
      "          -0.3653,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9080,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.7760,\n",
      "          -1.1793, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.1926,  1.6989,  3.6279, -1.2248,  0.0975, -0.0532, -1.0815,\n",
      "          -0.4325,  0.8111,  0.8111, -0.2244, -0.7996,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.8464,\n",
      "          -1.1292, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.0926,  1.2578,  2.2376, -1.1112,  0.0367, -0.0657, -0.6791,\n",
      "          -0.3127,  1.2064,  1.2064,  1.1348, -0.7996,  0.0000, -0.9049,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.9151,\n",
      "          -1.0735, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2006,  1.1177,  1.8598, -1.1061,  0.0194, -0.0800, -0.4919,\n",
      "          -0.4443,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9046,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1978,\n",
      "          -0.7437, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2370,  0.9357,  0.9565, -1.1099,  0.0181, -0.0800, -1.6149,\n",
      "          -0.5233, -1.5439, -1.5439,  1.5879, -0.7996,  0.0000, -0.9045,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0338,\n",
      "          -0.9597, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2873,  0.8379,  0.9072, -1.0376, -0.0138, -0.0871, -1.6149,\n",
      "          -0.4443,  1.6362,  1.6362, -0.6774, -0.7996,  0.0000, -0.9044,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0943,\n",
      "          -0.8897, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3348,  0.7598,  0.8744, -1.0338, -0.0218, -0.0943, -1.0534,\n",
      "          -0.2864,  1.4643,  1.4643, -1.1305, -0.7996,  0.0000, -0.9042,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1501,\n",
      "          -0.8158, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3543,  0.8108,  0.8087, -0.9805, -0.0271, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -0.2244, -0.7996,  0.0000, -0.9041,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3511,\n",
      "          -0.4009, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3516,  0.8170,  0.7594, -0.8167, -0.0284, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -1.5836, -0.7996,  0.0000, -0.9039,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.2467,\n",
      "          -0.6580, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3683,  0.9166,  0.6252, -0.8548, -0.0856, -0.1169, -1.0534,\n",
      "          -0.3653,  1.4213,  1.4213,  0.0777, -0.7996,  0.0000, -0.9037,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3030,\n",
      "          -0.5367, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3921,  1.0246,  1.1823, -0.6016, -0.0411, -0.1050, -1.3342,\n",
      "          -0.5233,  1.5072,  1.5072,  1.1348, -0.7996,  0.0000, -0.9033,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3382,\n",
      "          -0.4422, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4270,  0.9794,  0.7101, -0.7634, -0.0298, -0.1050, -1.6149,\n",
      "          -0.5233,  1.5932,  1.5932, -0.6774, -0.7996,  0.0000, -0.9031,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3658,\n",
      "          -0.3473, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4158,  1.0002,  0.7183, -0.7405, -0.0324, -0.1050, -1.6149,\n",
      "          -0.5233,  1.4213,  1.4213, -0.2244, -0.7996,  0.0000, -0.9030,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3856,\n",
      "          -0.2568, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4186,  1.1042,  0.7840, -0.6758, -0.0298, -0.1014, -0.4919,\n",
      "          -0.5233,  1.4213,  1.4213,  0.2287, -0.7996,  0.0000, -0.9029,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3995,\n",
      "          -0.1653, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4019,  1.1510,  0.8087, -0.6073, -0.0298, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3783,  1.3783, -1.5836, -0.7996,  0.0000, -0.9027,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.4074,\n",
      "          -0.0730, -1.1564,  1.1564,  0.0000]]])\n",
      "predicted: tensor([[-0.6018]], device='cuda:0')\n",
      "[621.42]\n",
      "            actual    predicted\n",
      "0       574.000006   574.547401\n",
      "1       567.999995   566.823049\n",
      "2       675.000001   696.709899\n",
      "3       699.000003   680.209659\n",
      "4       647.999993   635.531745\n",
      "...            ...          ...\n",
      "78284   713.999998   733.286023\n",
      "78285   617.000002   636.122918\n",
      "78286   641.999998   660.303841\n",
      "78287   866.000003   923.405949\n",
      "78288  1082.000010  1189.554368\n",
      "\n",
      "[78289 rows x 2 columns]\n",
      "Score (RMSE): 49.1065\n",
      "Score (MAE): 26.4403\n",
      "Score (ME): 0.0952\n",
      "Score (MAPE): 3.3907%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (392384, 27) to (399291, 27)\n",
      "training data cutoff:  2023-07-14 04:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([312832, 20, 25]) torch.Size([312832]) torch.Size([312832, 1])\n",
      "Testing data shape: torch.Size([78609, 20, 25]) torch.Size([78609]) torch.Size([78609, 1])\n",
      "Shuffled Training data shape: torch.Size([313152, 20, 25]) torch.Size([313152]) torch.Size([313152, 1])\n",
      "Shuffled Testing data shape: torch.Size([78289, 20, 25]) torch.Size([78289]) torch.Size([78289, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual  predicted\n",
      "0      32.58  33.998987\n",
      "1      43.61  42.644821\n",
      "2      39.82  40.808243\n",
      "3      28.60  28.881912\n",
      "4      26.59  27.313765\n",
      "...      ...        ...\n",
      "4091   39.59  39.769257\n",
      "4092   29.35  30.471621\n",
      "4093   35.12  34.596632\n",
      "4094   35.76  37.674065\n",
      "4095   34.49  34.521746\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2437\n",
      "Epoch 1/25, Validation Loss: 0.0264\n",
      "      actual  predicted\n",
      "0      32.58  33.256083\n",
      "1      43.61  43.026213\n",
      "2      39.82  40.405527\n",
      "3      28.60  28.709188\n",
      "4      26.59  26.758243\n",
      "...      ...        ...\n",
      "4091   39.59  39.217778\n",
      "4092   29.35  29.528297\n",
      "4093   35.12  34.778517\n",
      "4094   35.76  37.719985\n",
      "4095   34.49  32.837973\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0223\n",
      "Epoch 2/25, Validation Loss: 0.0157\n",
      "      actual  predicted\n",
      "0      32.58  32.975569\n",
      "1      43.61  43.212152\n",
      "2      39.82  39.885146\n",
      "3      28.60  28.697730\n",
      "4      26.59  26.805058\n",
      "...      ...        ...\n",
      "4091   39.59  38.667086\n",
      "4092   29.35  29.287579\n",
      "4093   35.12  34.869687\n",
      "4094   35.76  37.583246\n",
      "4095   34.49  32.527821\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0166\n",
      "Epoch 3/25, Validation Loss: 0.0118\n",
      "      actual  predicted\n",
      "0      32.58  32.963012\n",
      "1      43.61  43.244637\n",
      "2      39.82  40.089849\n",
      "3      28.60  28.759542\n",
      "4      26.59  26.953323\n",
      "...      ...        ...\n",
      "4091   39.59  38.697590\n",
      "4092   29.35  29.423805\n",
      "4093   35.12  35.022442\n",
      "4094   35.76  37.323474\n",
      "4095   34.49  32.609284\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0140\n",
      "Epoch 4/25, Validation Loss: 0.0097\n",
      "      actual  predicted\n",
      "0      32.58  32.962356\n",
      "1      43.61  43.512094\n",
      "2      39.82  40.133052\n",
      "3      28.60  28.607081\n",
      "4      26.59  26.728341\n",
      "...      ...        ...\n",
      "4091   39.59  39.043362\n",
      "4092   29.35  29.249760\n",
      "4093   35.12  35.048215\n",
      "4094   35.76  37.309266\n",
      "4095   34.49  32.652561\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0123\n",
      "Epoch 5/25, Validation Loss: 0.0097\n",
      "      actual  predicted\n",
      "0      32.58  32.937056\n",
      "1      43.61  43.066271\n",
      "2      39.82  39.920013\n",
      "3      28.60  28.641602\n",
      "4      26.59  26.887616\n",
      "...      ...        ...\n",
      "4091   39.59  38.560382\n",
      "4092   29.35  29.329021\n",
      "4093   35.12  34.951981\n",
      "4094   35.76  37.167115\n",
      "4095   34.49  32.938417\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0110\n",
      "Epoch 6/25, Validation Loss: 0.0072\n",
      "      actual  predicted\n",
      "0      32.58  32.841830\n",
      "1      43.61  43.186598\n",
      "2      39.82  39.834390\n",
      "3      28.60  28.670462\n",
      "4      26.59  26.629009\n",
      "...      ...        ...\n",
      "4091   39.59  38.650843\n",
      "4092   29.35  29.669174\n",
      "4093   35.12  34.940800\n",
      "4094   35.76  37.096132\n",
      "4095   34.49  33.122774\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0099\n",
      "Epoch 7/25, Validation Loss: 0.0064\n",
      "      actual  predicted\n",
      "0      32.58  32.910615\n",
      "1      43.61  43.373305\n",
      "2      39.82  40.142969\n",
      "3      28.60  28.532994\n",
      "4      26.59  26.527105\n",
      "...      ...        ...\n",
      "4091   39.59  39.064559\n",
      "4092   29.35  29.326070\n",
      "4093   35.12  35.031038\n",
      "4094   35.76  36.952586\n",
      "4095   34.49  33.376513\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0090\n",
      "Epoch 8/25, Validation Loss: 0.0063\n",
      "      actual  predicted\n",
      "0      32.58  32.894156\n",
      "1      43.61  43.160737\n",
      "2      39.82  39.923145\n",
      "3      28.60  28.712934\n",
      "4      26.59  26.783034\n",
      "...      ...        ...\n",
      "4091   39.59  38.835951\n",
      "4092   29.35  29.590472\n",
      "4093   35.12  34.960881\n",
      "4094   35.76  36.922744\n",
      "4095   34.49  33.860583\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0083\n",
      "Epoch 9/25, Validation Loss: 0.0053\n",
      "      actual  predicted\n",
      "0      32.58  32.781946\n",
      "1      43.61  43.291523\n",
      "2      39.82  39.998368\n",
      "3      28.60  28.549162\n",
      "4      26.59  26.509515\n",
      "...      ...        ...\n",
      "4091   39.59  38.604354\n",
      "4092   29.35  29.442196\n",
      "4093   35.12  34.856325\n",
      "4094   35.76  36.983332\n",
      "4095   34.49  33.917445\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0078\n",
      "Epoch 10/25, Validation Loss: 0.0053\n",
      "      actual  predicted\n",
      "0      32.58  32.780536\n",
      "1      43.61  43.183541\n",
      "2      39.82  39.960484\n",
      "3      28.60  28.624265\n",
      "4      26.59  26.624050\n",
      "...      ...        ...\n",
      "4091   39.59  38.710591\n",
      "4092   29.35  29.496188\n",
      "4093   35.12  34.998503\n",
      "4094   35.76  36.903780\n",
      "4095   34.49  33.960844\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0072\n",
      "Epoch 11/25, Validation Loss: 0.0048\n",
      "      actual  predicted\n",
      "0      32.58  32.835912\n",
      "1      43.61  43.419820\n",
      "2      39.82  40.147006\n",
      "3      28.60  28.555828\n",
      "4      26.59  26.444652\n",
      "...      ...        ...\n",
      "4091   39.59  38.968386\n",
      "4092   29.35  29.358931\n",
      "4093   35.12  34.985284\n",
      "4094   35.76  36.901592\n",
      "4095   34.49  34.073774\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0070\n",
      "Epoch 12/25, Validation Loss: 0.0048\n",
      "      actual  predicted\n",
      "0      32.58  32.825536\n",
      "1      43.61  43.156839\n",
      "2      39.82  39.891291\n",
      "3      28.60  28.570854\n",
      "4      26.59  26.576765\n",
      "...      ...        ...\n",
      "4091   39.59  38.750435\n",
      "4092   29.35  29.292323\n",
      "4093   35.12  34.868465\n",
      "4094   35.76  36.796315\n",
      "4095   34.49  34.165008\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0068\n",
      "Epoch 13/25, Validation Loss: 0.0042\n",
      "      actual  predicted\n",
      "0      32.58  32.873437\n",
      "1      43.61  43.310758\n",
      "2      39.82  40.030758\n",
      "3      28.60  28.613614\n",
      "4      26.59  26.611356\n",
      "...      ...        ...\n",
      "4091   39.59  38.817266\n",
      "4092   29.35  29.541240\n",
      "4093   35.12  34.977376\n",
      "4094   35.76  36.799911\n",
      "4095   34.49  34.407552\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0062\n",
      "Epoch 14/25, Validation Loss: 0.0041\n",
      "      actual  predicted\n",
      "0      32.58  32.821978\n",
      "1      43.61  43.135691\n",
      "2      39.82  39.906993\n",
      "3      28.60  28.708540\n",
      "4      26.59  26.867939\n",
      "...      ...        ...\n",
      "4091   39.59  38.634936\n",
      "4092   29.35  29.663392\n",
      "4093   35.12  35.017488\n",
      "4094   35.76  36.656530\n",
      "4095   34.49  34.573028\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0059\n",
      "Epoch 15/25, Validation Loss: 0.0039\n",
      "      actual  predicted\n",
      "0      32.58  32.736642\n",
      "1      43.61  43.213825\n",
      "2      39.82  39.967829\n",
      "3      28.60  28.509191\n",
      "4      26.59  26.481497\n",
      "...      ...        ...\n",
      "4091   39.59  39.083704\n",
      "4092   29.35  29.362325\n",
      "4093   35.12  34.874417\n",
      "4094   35.76  36.709569\n",
      "4095   34.49  34.271094\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0057\n",
      "Epoch 16/25, Validation Loss: 0.0037\n",
      "      actual  predicted\n",
      "0      32.58  32.832904\n",
      "1      43.61  43.497022\n",
      "2      39.82  40.185056\n",
      "3      28.60  28.551354\n",
      "4      26.59  26.509889\n",
      "...      ...        ...\n",
      "4091   39.59  39.023187\n",
      "4092   29.35  29.506192\n",
      "4093   35.12  34.992577\n",
      "4094   35.76  36.726629\n",
      "4095   34.49  34.604155\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0055\n",
      "Epoch 17/25, Validation Loss: 0.0041\n",
      "      actual  predicted\n",
      "0      32.58  32.698447\n",
      "1      43.61  43.144852\n",
      "2      39.82  39.846722\n",
      "3      28.60  28.507369\n",
      "4      26.59  26.407293\n",
      "...      ...        ...\n",
      "4091   39.59  38.734538\n",
      "4092   29.35  29.436193\n",
      "4093   35.12  34.887007\n",
      "4094   35.76  36.503713\n",
      "4095   34.49  34.428771\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0053\n",
      "Epoch 18/25, Validation Loss: 0.0035\n",
      "      actual  predicted\n",
      "0      32.58  32.816533\n",
      "1      43.61  43.264864\n",
      "2      39.82  39.929778\n",
      "3      28.60  28.718232\n",
      "4      26.59  26.758234\n",
      "...      ...        ...\n",
      "4091   39.59  39.271104\n",
      "4092   29.35  29.583974\n",
      "4093   35.12  35.058339\n",
      "4094   35.76  36.534962\n",
      "4095   34.49  34.532234\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0051\n",
      "Epoch 19/25, Validation Loss: 0.0032\n",
      "      actual  predicted\n",
      "0      32.58  32.728327\n",
      "1      43.61  43.233667\n",
      "2      39.82  39.773419\n",
      "3      28.60  28.596075\n",
      "4      26.59  26.483209\n",
      "...      ...        ...\n",
      "4091   39.59  39.207821\n",
      "4092   29.35  29.549462\n",
      "4093   35.12  34.995101\n",
      "4094   35.76  36.425570\n",
      "4095   34.49  34.548194\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0050\n",
      "Epoch 20/25, Validation Loss: 0.0031\n",
      "      actual  predicted\n",
      "0      32.58  32.672620\n",
      "1      43.61  43.207224\n",
      "2      39.82  39.805653\n",
      "3      28.60  28.692387\n",
      "4      26.59  26.554993\n",
      "...      ...        ...\n",
      "4091   39.59  39.138717\n",
      "4092   29.35  29.511017\n",
      "4093   35.12  34.997080\n",
      "4094   35.76  36.454249\n",
      "4095   34.49  34.758927\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0047\n",
      "Epoch 21/25, Validation Loss: 0.0030\n",
      "      actual  predicted\n",
      "0      32.58  32.848457\n",
      "1      43.61  43.548185\n",
      "2      39.82  40.154118\n",
      "3      28.60  28.522920\n",
      "4      26.59  26.348304\n",
      "...      ...        ...\n",
      "4091   39.59  39.098646\n",
      "4092   29.35  29.388548\n",
      "4093   35.12  34.985065\n",
      "4094   35.76  36.636243\n",
      "4095   34.49  34.668806\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0045\n",
      "Epoch 22/25, Validation Loss: 0.0034\n",
      "      actual  predicted\n",
      "0      32.58  32.728696\n",
      "1      43.61  43.286460\n",
      "2      39.82  39.895147\n",
      "3      28.60  28.603900\n",
      "4      26.59  26.503152\n",
      "...      ...        ...\n",
      "4091   39.59  39.128218\n",
      "4092   29.35  29.549071\n",
      "4093   35.12  34.914955\n",
      "4094   35.76  36.452862\n",
      "4095   34.49  34.456205\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0044\n",
      "Epoch 23/25, Validation Loss: 0.0029\n",
      "      actual  predicted\n",
      "0      32.58  32.842133\n",
      "1      43.61  43.361399\n",
      "2      39.82  39.971387\n",
      "3      28.60  28.677016\n",
      "4      26.59  26.757272\n",
      "...      ...        ...\n",
      "4091   39.59  39.366907\n",
      "4092   29.35  29.560411\n",
      "4093   35.12  35.025833\n",
      "4094   35.76  36.529304\n",
      "4095   34.49  34.534391\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0043\n",
      "Epoch 24/25, Validation Loss: 0.0027\n",
      "      actual  predicted\n",
      "0      32.58  32.765497\n",
      "1      43.61  43.175910\n",
      "2      39.82  39.815888\n",
      "3      28.60  28.718027\n",
      "4      26.59  26.681290\n",
      "...      ...        ...\n",
      "4091   39.59  39.036115\n",
      "4092   29.35  29.539547\n",
      "4093   35.12  34.948285\n",
      "4094   35.76  36.381248\n",
      "4095   34.49  34.737729\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0042\n",
      "Epoch 25/25, Validation Loss: 0.0027\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4371,  1.6471,  1.2275, -0.7223, -0.1084, -0.1014, -1.5026,\n",
      "          -0.5233, -1.8361, -1.8361, -0.5868,  4.1524,  0.0000, -0.9068,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9400,\n",
      "          -1.0373, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4443,  1.6188,  1.1864, -0.6811, -0.1326, -0.1086, -1.3903,\n",
      "          -0.5233, -1.8705, -1.8705, -0.0432,  4.1524,  0.0000, -0.9066,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9995,\n",
      "          -0.9717, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4516,  1.5905,  1.1454, -0.6400, -0.1568, -0.1157, -1.2780,\n",
      "          -0.5233, -1.9049, -1.9049,  0.5005,  4.1524,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.0590,\n",
      "          -0.9061, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4589,  1.5622,  1.1043, -0.5989, -0.1810, -0.1229, -1.1657,\n",
      "          -0.5233, -1.9393, -1.9393,  1.0442,  4.1524,  0.0000, -0.9064,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1184,\n",
      "          -0.8406, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4661,  1.5339,  1.0632, -0.5578, -0.2052, -0.1300, -1.0534,\n",
      "          -0.5233, -1.9737, -1.9737,  1.5879,  4.1524,  0.0000, -0.9062,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1779,\n",
      "          -0.7750, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.6714,  2.1800,  1.2685, -1.2546,  0.4127,  0.0343, -0.4919,\n",
      "          -0.3653,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9080,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.7760,\n",
      "          -1.1793, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.1926,  1.6989,  3.6279, -1.2248,  0.0975, -0.0532, -1.0815,\n",
      "          -0.4325,  0.8111,  0.8111, -0.2244, -0.7996,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.8464,\n",
      "          -1.1292, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.0926,  1.2578,  2.2376, -1.1112,  0.0367, -0.0657, -0.6791,\n",
      "          -0.3127,  1.2064,  1.2064,  1.1348, -0.7996,  0.0000, -0.9049,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.9151,\n",
      "          -1.0735, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2006,  1.1177,  1.8598, -1.1061,  0.0194, -0.0800, -0.4919,\n",
      "          -0.4443,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9046,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1978,\n",
      "          -0.7437, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2370,  0.9357,  0.9565, -1.1099,  0.0181, -0.0800, -1.6149,\n",
      "          -0.5233, -1.5439, -1.5439,  1.5879, -0.7996,  0.0000, -0.9045,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0338,\n",
      "          -0.9597, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2873,  0.8379,  0.9072, -1.0376, -0.0138, -0.0871, -1.6149,\n",
      "          -0.4443,  1.6362,  1.6362, -0.6774, -0.7996,  0.0000, -0.9044,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0943,\n",
      "          -0.8897, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3348,  0.7598,  0.8744, -1.0338, -0.0218, -0.0943, -1.0534,\n",
      "          -0.2864,  1.4643,  1.4643, -1.1305, -0.7996,  0.0000, -0.9042,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1501,\n",
      "          -0.8158, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3543,  0.8108,  0.8087, -0.9805, -0.0271, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -0.2244, -0.7996,  0.0000, -0.9041,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3511,\n",
      "          -0.4009, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3516,  0.8170,  0.7594, -0.8167, -0.0284, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -1.5836, -0.7996,  0.0000, -0.9039,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.2467,\n",
      "          -0.6580, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3683,  0.9166,  0.6252, -0.8548, -0.0856, -0.1169, -1.0534,\n",
      "          -0.3653,  1.4213,  1.4213,  0.0777, -0.7996,  0.0000, -0.9037,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3030,\n",
      "          -0.5367, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3921,  1.0246,  1.1823, -0.6016, -0.0411, -0.1050, -1.3342,\n",
      "          -0.5233,  1.5072,  1.5072,  1.1348, -0.7996,  0.0000, -0.9033,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3382,\n",
      "          -0.4422, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4270,  0.9794,  0.7101, -0.7634, -0.0298, -0.1050, -1.6149,\n",
      "          -0.5233,  1.5932,  1.5932, -0.6774, -0.7996,  0.0000, -0.9031,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3658,\n",
      "          -0.3473, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4158,  1.0002,  0.7183, -0.7405, -0.0324, -0.1050, -1.6149,\n",
      "          -0.5233,  1.4213,  1.4213, -0.2244, -0.7996,  0.0000, -0.9030,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3856,\n",
      "          -0.2568, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4186,  1.1042,  0.7840, -0.6758, -0.0298, -0.1014, -0.4919,\n",
      "          -0.5233,  1.4213,  1.4213,  0.2287, -0.7996,  0.0000, -0.9029,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3995,\n",
      "          -0.1653, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4019,  1.1510,  0.8087, -0.6073, -0.0298, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3783,  1.3783, -1.5836, -0.7996,  0.0000, -0.9027,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.4074,\n",
      "          -0.0730, -1.1564,  1.1564,  0.0000]]])\n",
      "predicted: tensor([[1.1125]], device='cuda:0')\n",
      "[46.22]\n",
      "          actual  predicted\n",
      "0      32.580000  32.765497\n",
      "1      43.610000  43.175910\n",
      "2      39.820000  39.815888\n",
      "3      28.600000  28.718027\n",
      "4      26.590000  26.681290\n",
      "...          ...        ...\n",
      "78284  33.550000  33.819992\n",
      "78285  31.440000  31.777386\n",
      "78286  41.200000  41.123384\n",
      "78287  24.300001  24.552916\n",
      "78288  34.720000  34.467265\n",
      "\n",
      "[78289 rows x 2 columns]\n",
      "Score (RMSE): 0.5043\n",
      "Score (MAE): 0.2742\n",
      "Score (ME): 0.0358\n",
      "Score (MAPE): 0.7627%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (392384, 27) to (399291, 27)\n",
      "training data cutoff:  2023-07-14 04:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([312832, 20, 25]) torch.Size([312832]) torch.Size([312832, 1])\n",
      "Testing data shape: torch.Size([78609, 20, 25]) torch.Size([78609]) torch.Size([78609, 1])\n",
      "Shuffled Training data shape: torch.Size([313152, 20, 25]) torch.Size([313152]) torch.Size([313152, 1])\n",
      "Shuffled Testing data shape: torch.Size([78289, 20, 25]) torch.Size([78289]) torch.Size([78289, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     26.138571  27.617521\n",
      "1     22.330000  22.192136\n",
      "2     23.220000  22.934391\n",
      "3     27.960000  27.735146\n",
      "4     31.230000  31.823772\n",
      "...         ...        ...\n",
      "4091  24.110000  24.146521\n",
      "4092  24.970000  25.163222\n",
      "4093  30.910000  29.925175\n",
      "4094  29.930000  30.482494\n",
      "4095  25.400000  25.672650\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.2523\n",
      "Epoch 1/25, Validation Loss: 0.0260\n",
      "         actual  predicted\n",
      "0     26.138571  27.099535\n",
      "1     22.330000  22.247442\n",
      "2     23.220000  23.039679\n",
      "3     27.960000  27.802396\n",
      "4     31.230000  31.251912\n",
      "...         ...        ...\n",
      "4091  24.110000  24.112959\n",
      "4092  24.970000  25.029242\n",
      "4093  30.910000  30.565088\n",
      "4094  29.930000  30.218029\n",
      "4095  25.400000  25.332769\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0211\n",
      "Epoch 2/25, Validation Loss: 0.0131\n",
      "         actual  predicted\n",
      "0     26.138571  26.848202\n",
      "1     22.330000  22.197391\n",
      "2     23.220000  23.004035\n",
      "3     27.960000  27.940210\n",
      "4     31.230000  31.394792\n",
      "...         ...        ...\n",
      "4091  24.110000  24.142792\n",
      "4092  24.970000  25.109901\n",
      "4093  30.910000  31.010992\n",
      "4094  29.930000  30.274010\n",
      "4095  25.400000  25.386786\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0156\n",
      "Epoch 3/25, Validation Loss: 0.0104\n",
      "         actual  predicted\n",
      "0     26.138571  26.574639\n",
      "1     22.330000  22.275023\n",
      "2     23.220000  23.168281\n",
      "3     27.960000  27.901294\n",
      "4     31.230000  31.396573\n",
      "...         ...        ...\n",
      "4091  24.110000  24.172889\n",
      "4092  24.970000  25.165994\n",
      "4093  30.910000  31.260496\n",
      "4094  29.930000  30.207918\n",
      "4095  25.400000  25.348160\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0130\n",
      "Epoch 4/25, Validation Loss: 0.0085\n",
      "         actual  predicted\n",
      "0     26.138571  26.338436\n",
      "1     22.330000  22.275731\n",
      "2     23.220000  23.177276\n",
      "3     27.960000  27.872577\n",
      "4     31.230000  31.289644\n",
      "...         ...        ...\n",
      "4091  24.110000  24.174011\n",
      "4092  24.970000  25.081546\n",
      "4093  30.910000  31.191109\n",
      "4094  29.930000  30.131315\n",
      "4095  25.400000  25.355622\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0113\n",
      "Epoch 5/25, Validation Loss: 0.0070\n",
      "         actual  predicted\n",
      "0     26.138571  26.097657\n",
      "1     22.330000  22.236485\n",
      "2     23.220000  23.095959\n",
      "3     27.960000  27.812114\n",
      "4     31.230000  31.127722\n",
      "...         ...        ...\n",
      "4091  24.110000  24.130092\n",
      "4092  24.970000  25.065135\n",
      "4093  30.910000  31.152347\n",
      "4094  29.930000  29.949084\n",
      "4095  25.400000  25.334440\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0100\n",
      "Epoch 6/25, Validation Loss: 0.0065\n",
      "         actual  predicted\n",
      "0     26.138571  26.127521\n",
      "1     22.330000  22.263713\n",
      "2     23.220000  23.113443\n",
      "3     27.960000  27.943402\n",
      "4     31.230000  31.430667\n",
      "...         ...        ...\n",
      "4091  24.110000  24.122397\n",
      "4092  24.970000  25.120514\n",
      "4093  30.910000  31.469765\n",
      "4094  29.930000  30.239803\n",
      "4095  25.400000  25.377144\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0089\n",
      "Epoch 7/25, Validation Loss: 0.0058\n",
      "         actual  predicted\n",
      "0     26.138571  26.067153\n",
      "1     22.330000  22.292866\n",
      "2     23.220000  23.148131\n",
      "3     27.960000  27.966928\n",
      "4     31.230000  31.557136\n",
      "...         ...        ...\n",
      "4091  24.110000  24.121907\n",
      "4092  24.970000  25.173315\n",
      "4093  30.910000  31.489501\n",
      "4094  29.930000  30.322701\n",
      "4095  25.400000  25.417176\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0081\n",
      "Epoch 8/25, Validation Loss: 0.0057\n",
      "         actual  predicted\n",
      "0     26.138571  25.933990\n",
      "1     22.330000  22.281047\n",
      "2     23.220000  23.170616\n",
      "3     27.960000  27.881947\n",
      "4     31.230000  31.304028\n",
      "...         ...        ...\n",
      "4091  24.110000  24.164576\n",
      "4092  24.970000  25.221255\n",
      "4093  30.910000  31.290052\n",
      "4094  29.930000  30.108299\n",
      "4095  25.400000  25.339755\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0075\n",
      "Epoch 9/25, Validation Loss: 0.0045\n",
      "         actual  predicted\n",
      "0     26.138571  25.928824\n",
      "1     22.330000  22.252465\n",
      "2     23.220000  23.150677\n",
      "3     27.960000  27.909832\n",
      "4     31.230000  31.361684\n",
      "...         ...        ...\n",
      "4091  24.110000  24.132834\n",
      "4092  24.970000  25.280040\n",
      "4093  30.910000  31.354391\n",
      "4094  29.930000  30.159027\n",
      "4095  25.400000  25.340423\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0069\n",
      "Epoch 10/25, Validation Loss: 0.0043\n",
      "         actual  predicted\n",
      "0     26.138571  25.845335\n",
      "1     22.330000  22.249824\n",
      "2     23.220000  23.140185\n",
      "3     27.960000  27.903996\n",
      "4     31.230000  31.359080\n",
      "...         ...        ...\n",
      "4091  24.110000  24.127551\n",
      "4092  24.970000  25.205226\n",
      "4093  30.910000  31.334167\n",
      "4094  29.930000  30.125384\n",
      "4095  25.400000  25.338470\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0065\n",
      "Epoch 11/25, Validation Loss: 0.0041\n",
      "         actual  predicted\n",
      "0     26.138571  25.789735\n",
      "1     22.330000  22.292934\n",
      "2     23.220000  23.179271\n",
      "3     27.960000  27.938782\n",
      "4     31.230000  31.302221\n",
      "...         ...        ...\n",
      "4091  24.110000  24.112340\n",
      "4092  24.970000  25.200890\n",
      "4093  30.910000  31.271255\n",
      "4094  29.930000  30.070701\n",
      "4095  25.400000  25.384222\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0059\n",
      "Epoch 12/25, Validation Loss: 0.0037\n",
      "         actual  predicted\n",
      "0     26.138571  25.781183\n",
      "1     22.330000  22.339962\n",
      "2     23.220000  23.211560\n",
      "3     27.960000  27.859819\n",
      "4     31.230000  31.171764\n",
      "...         ...        ...\n",
      "4091  24.110000  24.097253\n",
      "4092  24.970000  25.191893\n",
      "4093  30.910000  31.126473\n",
      "4094  29.930000  30.014013\n",
      "4095  25.400000  25.330824\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0056\n",
      "Epoch 13/25, Validation Loss: 0.0036\n",
      "         actual  predicted\n",
      "0     26.138571  25.627736\n",
      "1     22.330000  22.285321\n",
      "2     23.220000  23.173175\n",
      "3     27.960000  27.828574\n",
      "4     31.230000  31.161237\n",
      "...         ...        ...\n",
      "4091  24.110000  24.113244\n",
      "4092  24.970000  25.119552\n",
      "4093  30.910000  31.100184\n",
      "4094  29.930000  29.951794\n",
      "4095  25.400000  25.290279\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0055\n",
      "Epoch 14/25, Validation Loss: 0.0034\n",
      "         actual  predicted\n",
      "0     26.138571  25.687552\n",
      "1     22.330000  22.270542\n",
      "2     23.220000  23.146666\n",
      "3     27.960000  27.889550\n",
      "4     31.230000  31.260115\n",
      "...         ...        ...\n",
      "4091  24.110000  24.094968\n",
      "4092  24.970000  25.151925\n",
      "4093  30.910000  31.172606\n",
      "4094  29.930000  29.974022\n",
      "4095  25.400000  25.383425\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0052\n",
      "Epoch 15/25, Validation Loss: 0.0031\n",
      "         actual  predicted\n",
      "0     26.138571  25.753724\n",
      "1     22.330000  22.236791\n",
      "2     23.220000  23.197135\n",
      "3     27.960000  27.946116\n",
      "4     31.230000  31.233804\n",
      "...         ...        ...\n",
      "4091  24.110000  24.106654\n",
      "4092  24.970000  25.206340\n",
      "4093  30.910000  31.157259\n",
      "4094  29.930000  30.063909\n",
      "4095  25.400000  25.373930\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0048\n",
      "Epoch 16/25, Validation Loss: 0.0029\n",
      "         actual  predicted\n",
      "0     26.138571  25.652579\n",
      "1     22.330000  22.297412\n",
      "2     23.220000  23.224876\n",
      "3     27.960000  27.851027\n",
      "4     31.230000  31.127738\n",
      "...         ...        ...\n",
      "4091  24.110000  24.076077\n",
      "4092  24.970000  25.128404\n",
      "4093  30.910000  30.999559\n",
      "4094  29.930000  29.892470\n",
      "4095  25.400000  25.354579\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0047\n",
      "Epoch 17/25, Validation Loss: 0.0029\n",
      "         actual  predicted\n",
      "0     26.138571  25.714346\n",
      "1     22.330000  22.292632\n",
      "2     23.220000  23.196911\n",
      "3     27.960000  28.003672\n",
      "4     31.230000  31.410178\n",
      "...         ...        ...\n",
      "4091  24.110000  24.123108\n",
      "4092  24.970000  25.083790\n",
      "4093  30.910000  31.149192\n",
      "4094  29.930000  30.169938\n",
      "4095  25.400000  25.457943\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0045\n",
      "Epoch 18/25, Validation Loss: 0.0030\n",
      "         actual  predicted\n",
      "0     26.138571  25.704711\n",
      "1     22.330000  22.301701\n",
      "2     23.220000  23.240962\n",
      "3     27.960000  27.813784\n",
      "4     31.230000  31.143027\n",
      "...         ...        ...\n",
      "4091  24.110000  24.068881\n",
      "4092  24.970000  25.142222\n",
      "4093  30.910000  30.925705\n",
      "4094  29.930000  29.920324\n",
      "4095  25.400000  25.339411\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0043\n",
      "Epoch 19/25, Validation Loss: 0.0028\n",
      "         actual  predicted\n",
      "0     26.138571  25.740709\n",
      "1     22.330000  22.316566\n",
      "2     23.220000  23.190648\n",
      "3     27.960000  27.988037\n",
      "4     31.230000  31.432128\n",
      "...         ...        ...\n",
      "4091  24.110000  24.127069\n",
      "4092  24.970000  25.066175\n",
      "4093  30.910000  31.187530\n",
      "4094  29.930000  30.226980\n",
      "4095  25.400000  25.387679\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0042\n",
      "Epoch 20/25, Validation Loss: 0.0029\n",
      "         actual  predicted\n",
      "0     26.138571  25.613567\n",
      "1     22.330000  22.298491\n",
      "2     23.220000  23.197975\n",
      "3     27.960000  27.873745\n",
      "4     31.230000  31.234576\n",
      "...         ...        ...\n",
      "4091  24.110000  24.136658\n",
      "4092  24.970000  24.965622\n",
      "4093  30.910000  30.989296\n",
      "4094  29.930000  29.995755\n",
      "4095  25.400000  25.361060\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0039\n",
      "Epoch 21/25, Validation Loss: 0.0024\n",
      "         actual  predicted\n",
      "0     26.138571  25.707110\n",
      "1     22.330000  22.276277\n",
      "2     23.220000  23.166338\n",
      "3     27.960000  27.880159\n",
      "4     31.230000  31.283098\n",
      "...         ...        ...\n",
      "4091  24.110000  24.100465\n",
      "4092  24.970000  25.005230\n",
      "4093  30.910000  31.032183\n",
      "4094  29.930000  30.060575\n",
      "4095  25.400000  25.330612\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0038\n",
      "Epoch 22/25, Validation Loss: 0.0023\n",
      "         actual  predicted\n",
      "0     26.138571  25.718442\n",
      "1     22.330000  22.259497\n",
      "2     23.220000  23.177915\n",
      "3     27.960000  27.931422\n",
      "4     31.230000  31.203073\n",
      "...         ...        ...\n",
      "4091  24.110000  24.060646\n",
      "4092  24.970000  24.994267\n",
      "4093  30.910000  30.983668\n",
      "4094  29.930000  30.009743\n",
      "4095  25.400000  25.384051\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0036\n",
      "Epoch 23/25, Validation Loss: 0.0022\n",
      "         actual  predicted\n",
      "0     26.138571  25.740118\n",
      "1     22.330000  22.320289\n",
      "2     23.220000  23.193375\n",
      "3     27.960000  27.810614\n",
      "4     31.230000  31.134344\n",
      "...         ...        ...\n",
      "4091  24.110000  24.065058\n",
      "4092  24.970000  24.990510\n",
      "4093  30.910000  30.849248\n",
      "4094  29.930000  29.909293\n",
      "4095  25.400000  25.354475\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0035\n",
      "Epoch 24/25, Validation Loss: 0.0024\n",
      "         actual  predicted\n",
      "0     26.138571  25.840966\n",
      "1     22.330000  22.377495\n",
      "2     23.220000  23.235297\n",
      "3     27.960000  27.960824\n",
      "4     31.230000  31.244952\n",
      "...         ...        ...\n",
      "4091  24.110000  24.134801\n",
      "4092  24.970000  25.035599\n",
      "4093  30.910000  31.050770\n",
      "4094  29.930000  30.037347\n",
      "4095  25.400000  25.398742\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0035\n",
      "Epoch 25/25, Validation Loss: 0.0023\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4371,  1.6471,  1.2275, -0.7223, -0.1084, -0.1014, -1.5026,\n",
      "          -0.5233, -1.8361, -1.8361, -0.5868,  4.1524,  0.0000, -0.9068,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9400,\n",
      "          -1.0373, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4443,  1.6188,  1.1864, -0.6811, -0.1326, -0.1086, -1.3903,\n",
      "          -0.5233, -1.8705, -1.8705, -0.0432,  4.1524,  0.0000, -0.9066,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9995,\n",
      "          -0.9717, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4516,  1.5905,  1.1454, -0.6400, -0.1568, -0.1157, -1.2780,\n",
      "          -0.5233, -1.9049, -1.9049,  0.5005,  4.1524,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.0590,\n",
      "          -0.9061, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4589,  1.5622,  1.1043, -0.5989, -0.1810, -0.1229, -1.1657,\n",
      "          -0.5233, -1.9393, -1.9393,  1.0442,  4.1524,  0.0000, -0.9064,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1184,\n",
      "          -0.8406, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4661,  1.5339,  1.0632, -0.5578, -0.2052, -0.1300, -1.0534,\n",
      "          -0.5233, -1.9737, -1.9737,  1.5879,  4.1524,  0.0000, -0.9062,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1779,\n",
      "          -0.7750, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.6714,  2.1800,  1.2685, -1.2546,  0.4127,  0.0343, -0.4919,\n",
      "          -0.3653,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9080,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.7760,\n",
      "          -1.1793, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.1926,  1.6989,  3.6279, -1.2248,  0.0975, -0.0532, -1.0815,\n",
      "          -0.4325,  0.8111,  0.8111, -0.2244, -0.7996,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.8464,\n",
      "          -1.1292, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.0926,  1.2578,  2.2376, -1.1112,  0.0367, -0.0657, -0.6791,\n",
      "          -0.3127,  1.2064,  1.2064,  1.1348, -0.7996,  0.0000, -0.9049,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.9151,\n",
      "          -1.0735, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2006,  1.1177,  1.8598, -1.1061,  0.0194, -0.0800, -0.4919,\n",
      "          -0.4443,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9046,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1978,\n",
      "          -0.7437, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2370,  0.9357,  0.9565, -1.1099,  0.0181, -0.0800, -1.6149,\n",
      "          -0.5233, -1.5439, -1.5439,  1.5879, -0.7996,  0.0000, -0.9045,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0338,\n",
      "          -0.9597, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2873,  0.8379,  0.9072, -1.0376, -0.0138, -0.0871, -1.6149,\n",
      "          -0.4443,  1.6362,  1.6362, -0.6774, -0.7996,  0.0000, -0.9044,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0943,\n",
      "          -0.8897, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3348,  0.7598,  0.8744, -1.0338, -0.0218, -0.0943, -1.0534,\n",
      "          -0.2864,  1.4643,  1.4643, -1.1305, -0.7996,  0.0000, -0.9042,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1501,\n",
      "          -0.8158, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3543,  0.8108,  0.8087, -0.9805, -0.0271, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -0.2244, -0.7996,  0.0000, -0.9041,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3511,\n",
      "          -0.4009, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3516,  0.8170,  0.7594, -0.8167, -0.0284, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -1.5836, -0.7996,  0.0000, -0.9039,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.2467,\n",
      "          -0.6580, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3683,  0.9166,  0.6252, -0.8548, -0.0856, -0.1169, -1.0534,\n",
      "          -0.3653,  1.4213,  1.4213,  0.0777, -0.7996,  0.0000, -0.9037,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3030,\n",
      "          -0.5367, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3921,  1.0246,  1.1823, -0.6016, -0.0411, -0.1050, -1.3342,\n",
      "          -0.5233,  1.5072,  1.5072,  1.1348, -0.7996,  0.0000, -0.9033,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3382,\n",
      "          -0.4422, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4270,  0.9794,  0.7101, -0.7634, -0.0298, -0.1050, -1.6149,\n",
      "          -0.5233,  1.5932,  1.5932, -0.6774, -0.7996,  0.0000, -0.9031,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3658,\n",
      "          -0.3473, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4158,  1.0002,  0.7183, -0.7405, -0.0324, -0.1050, -1.6149,\n",
      "          -0.5233,  1.4213,  1.4213, -0.2244, -0.7996,  0.0000, -0.9030,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3856,\n",
      "          -0.2568, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4186,  1.1042,  0.7840, -0.6758, -0.0298, -0.1014, -0.4919,\n",
      "          -0.5233,  1.4213,  1.4213,  0.2287, -0.7996,  0.0000, -0.9029,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3995,\n",
      "          -0.1653, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4019,  1.1510,  0.8087, -0.6073, -0.0298, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3783,  1.3783, -1.5836, -0.7996,  0.0000, -0.9027,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.4074,\n",
      "          -0.0730, -1.1564,  1.1564,  0.0000]]])\n",
      "predicted: tensor([[0.3985]], device='cuda:0')\n",
      "[25.47]\n",
      "          actual  predicted\n",
      "0      26.138571  25.840966\n",
      "1      22.330000  22.377495\n",
      "2      23.220000  23.235297\n",
      "3      27.960000  27.960824\n",
      "4      31.230000  31.244952\n",
      "...          ...        ...\n",
      "78284  28.760000  28.768685\n",
      "78285  26.390000  26.322161\n",
      "78286  21.600000  21.734093\n",
      "78287  23.860000  23.901935\n",
      "78288  19.570000  19.795019\n",
      "\n",
      "[78289 rows x 2 columns]\n",
      "Score (RMSE): 0.1716\n",
      "Score (MAE): 0.1027\n",
      "Score (ME): -0.0541\n",
      "Score (MAPE): 0.4396%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (392384, 27) to (399291, 27)\n",
      "training data cutoff:  2023-07-14 04:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([312832, 20, 25]) torch.Size([312832]) torch.Size([312832, 1])\n",
      "Testing data shape: torch.Size([78609, 20, 25]) torch.Size([78609]) torch.Size([78609, 1])\n",
      "Shuffled Training data shape: torch.Size([313152, 20, 25]) torch.Size([313152]) torch.Size([313152, 1])\n",
      "Shuffled Testing data shape: torch.Size([78289, 20, 25]) torch.Size([78289]) torch.Size([78289, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0       4.999997    6.779450\n",
      "1      13.999995   43.832589\n",
      "2     290.000001  195.651438\n",
      "3       9.000000   75.641337\n",
      "4     450.999989  347.505175\n",
      "...          ...         ...\n",
      "4091   25.000000   83.284611\n",
      "4092  371.999995  336.005028\n",
      "4093    7.000004   33.245187\n",
      "4094  193.000000  120.983893\n",
      "4095   16.000002   42.309174\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.7629\n",
      "Epoch 1/25, Validation Loss: 0.3903\n",
      "          actual   predicted\n",
      "0       4.999997   -2.686923\n",
      "1      13.999995  -48.540521\n",
      "2     290.000001  167.127109\n",
      "3       9.000000  -40.591250\n",
      "4     450.999989  312.717623\n",
      "...          ...         ...\n",
      "4091   25.000000   81.979462\n",
      "4092  371.999995  344.864060\n",
      "4093    7.000004  -23.752533\n",
      "4094  193.000000   61.554948\n",
      "4095   16.000002   -4.766878\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.5157\n",
      "Epoch 2/25, Validation Loss: 0.3249\n",
      "          actual   predicted\n",
      "0       4.999997   32.937877\n",
      "1      13.999995   -3.367068\n",
      "2     290.000001  274.244252\n",
      "3       9.000000  -45.372148\n",
      "4     450.999989  403.351709\n",
      "...          ...         ...\n",
      "4091   25.000000   73.708179\n",
      "4092  371.999995  391.455670\n",
      "4093    7.000004    7.833361\n",
      "4094  193.000000  121.390198\n",
      "4095   16.000002   28.177322\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.4578\n",
      "Epoch 3/25, Validation Loss: 0.2996\n",
      "          actual   predicted\n",
      "0       4.999997   82.292872\n",
      "1      13.999995   -6.212588\n",
      "2     290.000001  312.653914\n",
      "3       9.000000 -134.975492\n",
      "4     450.999989  501.504207\n",
      "...          ...         ...\n",
      "4091   25.000000  107.360190\n",
      "4092  371.999995  453.511689\n",
      "4093    7.000004  -17.047482\n",
      "4094  193.000000  137.658035\n",
      "4095   16.000002   68.533250\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.4295\n",
      "Epoch 4/25, Validation Loss: 0.3057\n",
      "          actual   predicted\n",
      "0       4.999997   62.289797\n",
      "1      13.999995   34.256651\n",
      "2     290.000001  225.140362\n",
      "3       9.000000  -89.679151\n",
      "4     450.999989  446.677627\n",
      "...          ...         ...\n",
      "4091   25.000000   52.738341\n",
      "4092  371.999995  400.558930\n",
      "4093    7.000004   14.172731\n",
      "4094  193.000000  130.756431\n",
      "4095   16.000002   38.952409\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.3997\n",
      "Epoch 5/25, Validation Loss: 0.2726\n",
      "          actual   predicted\n",
      "0       4.999997   54.019703\n",
      "1      13.999995   16.048639\n",
      "2     290.000001  228.418303\n",
      "3       9.000000  -44.761499\n",
      "4     450.999989  489.087302\n",
      "...          ...         ...\n",
      "4091   25.000000  108.852104\n",
      "4092  371.999995  401.746630\n",
      "4093    7.000004    9.722255\n",
      "4094  193.000000  111.984944\n",
      "4095   16.000002   45.620239\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.3827\n",
      "Epoch 6/25, Validation Loss: 0.2670\n",
      "          actual   predicted\n",
      "0       4.999997   45.894814\n",
      "1      13.999995   57.864947\n",
      "2     290.000001  224.597791\n",
      "3       9.000000   -4.975780\n",
      "4     450.999989  405.536979\n",
      "...          ...         ...\n",
      "4091   25.000000   54.725294\n",
      "4092  371.999995  382.397873\n",
      "4093    7.000004   43.617440\n",
      "4094  193.000000  157.008592\n",
      "4095   16.000002   13.261176\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.3641\n",
      "Epoch 7/25, Validation Loss: 0.2709\n",
      "          actual   predicted\n",
      "0       4.999997   42.206988\n",
      "1      13.999995    1.331068\n",
      "2     290.000001  239.419564\n",
      "3       9.000000  -60.291109\n",
      "4     450.999989  474.077168\n",
      "...          ...         ...\n",
      "4091   25.000000   85.360798\n",
      "4092  371.999995  418.796829\n",
      "4093    7.000004  -17.722413\n",
      "4094  193.000000  128.937482\n",
      "4095   16.000002   48.482032\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.3441\n",
      "Epoch 8/25, Validation Loss: 0.2517\n",
      "          actual   predicted\n",
      "0       4.999997   36.047856\n",
      "1      13.999995   15.704131\n",
      "2     290.000001  273.491460\n",
      "3       9.000000  -47.010444\n",
      "4     450.999989  441.130737\n",
      "...          ...         ...\n",
      "4091   25.000000   95.175654\n",
      "4092  371.999995  386.806474\n",
      "4093    7.000004   -4.112447\n",
      "4094  193.000000  130.356603\n",
      "4095   16.000002   16.447268\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.3312\n",
      "Epoch 9/25, Validation Loss: 0.2591\n",
      "          actual   predicted\n",
      "0       4.999997   26.569888\n",
      "1      13.999995    8.273181\n",
      "2     290.000001  184.636249\n",
      "3       9.000000  -43.655149\n",
      "4     450.999989  435.396409\n",
      "...          ...         ...\n",
      "4091   25.000000   62.981549\n",
      "4092  371.999995  400.046138\n",
      "4093    7.000004  -10.041683\n",
      "4094  193.000000   96.295730\n",
      "4095   16.000002   -3.249092\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.3441\n",
      "Epoch 10/25, Validation Loss: 0.2425\n",
      "          actual   predicted\n",
      "0       4.999997   47.299154\n",
      "1      13.999995   11.842593\n",
      "2     290.000001  244.000879\n",
      "3       9.000000  -39.319285\n",
      "4     450.999989  476.480334\n",
      "...          ...         ...\n",
      "4091   25.000000   93.885493\n",
      "4092  371.999995  453.876539\n",
      "4093    7.000004   12.523131\n",
      "4094  193.000000  149.605873\n",
      "4095   16.000002    0.146306\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.3116\n",
      "Epoch 11/25, Validation Loss: 0.2349\n",
      "          actual   predicted\n",
      "0       4.999997   59.891734\n",
      "1      13.999995   65.166718\n",
      "2     290.000001  227.756488\n",
      "3       9.000000    7.734943\n",
      "4     450.999989  395.529744\n",
      "...          ...         ...\n",
      "4091   25.000000  112.649792\n",
      "4092  371.999995  411.369702\n",
      "4093    7.000004   10.068053\n",
      "4094  193.000000  144.691767\n",
      "4095   16.000002   37.362459\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.3076\n",
      "Epoch 12/25, Validation Loss: 0.2457\n",
      "          actual   predicted\n",
      "0       4.999997   61.270113\n",
      "1      13.999995   50.897523\n",
      "2     290.000001  216.636758\n",
      "3       9.000000   13.495524\n",
      "4     450.999989  413.732564\n",
      "...          ...         ...\n",
      "4091   25.000000   55.909854\n",
      "4092  371.999995  391.208975\n",
      "4093    7.000004   23.365236\n",
      "4094  193.000000  122.736229\n",
      "4095   16.000002   52.932552\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.2965\n",
      "Epoch 13/25, Validation Loss: 0.2498\n",
      "          actual   predicted\n",
      "0       4.999997   69.467918\n",
      "1      13.999995   31.115339\n",
      "2     290.000001  257.606425\n",
      "3       9.000000  -47.407593\n",
      "4     450.999989  461.083482\n",
      "...          ...         ...\n",
      "4091   25.000000   89.782116\n",
      "4092  371.999995  455.778442\n",
      "4093    7.000004   -2.487967\n",
      "4094  193.000000  163.355307\n",
      "4095   16.000002   45.185253\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.2812\n",
      "Epoch 14/25, Validation Loss: 0.2492\n",
      "          actual   predicted\n",
      "0       4.999997   37.515278\n",
      "1      13.999995   21.871348\n",
      "2     290.000001  209.968827\n",
      "3       9.000000  -28.220055\n",
      "4     450.999989  469.806751\n",
      "...          ...         ...\n",
      "4091   25.000000   68.765311\n",
      "4092  371.999995  427.467615\n",
      "4093    7.000004  -10.083849\n",
      "4094  193.000000  138.165051\n",
      "4095   16.000002   10.050738\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.2668\n",
      "Epoch 15/25, Validation Loss: 0.2596\n",
      "          actual   predicted\n",
      "0       4.999997   27.525045\n",
      "1      13.999995   35.971464\n",
      "2     290.000001  262.558731\n",
      "3       9.000000  -17.438283\n",
      "4     450.999989  399.069741\n",
      "...          ...         ...\n",
      "4091   25.000000   69.720905\n",
      "4092  371.999995  383.251629\n",
      "4093    7.000004  -12.725434\n",
      "4094  193.000000  138.188958\n",
      "4095   16.000002   19.153146\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.2671\n",
      "Epoch 16/25, Validation Loss: 0.2316\n",
      "          actual   predicted\n",
      "0       4.999997   42.287250\n",
      "1      13.999995   33.189316\n",
      "2     290.000001  289.958541\n",
      "3       9.000000   -8.680238\n",
      "4     450.999989  401.025586\n",
      "...          ...         ...\n",
      "4091   25.000000   64.512434\n",
      "4092  371.999995  379.615703\n",
      "4093    7.000004    9.820404\n",
      "4094  193.000000  162.460345\n",
      "4095   16.000002   36.949879\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.2482\n",
      "Epoch 17/25, Validation Loss: 0.2479\n",
      "          actual   predicted\n",
      "0       4.999997   38.700421\n",
      "1      13.999995   29.528999\n",
      "2     290.000001  228.237801\n",
      "3       9.000000    3.856584\n",
      "4     450.999989  396.128371\n",
      "...          ...         ...\n",
      "4091   25.000000   72.162672\n",
      "4092  371.999995  378.943689\n",
      "4093    7.000004    2.021317\n",
      "4094  193.000000  122.384802\n",
      "4095   16.000002   25.805915\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.2438\n",
      "Epoch 18/25, Validation Loss: 0.2458\n",
      "          actual   predicted\n",
      "0       4.999997   34.662379\n",
      "1      13.999995   36.435809\n",
      "2     290.000001  230.969971\n",
      "3       9.000000  -14.268363\n",
      "4     450.999989  430.193062\n",
      "...          ...         ...\n",
      "4091   25.000000   67.558289\n",
      "4092  371.999995  399.580559\n",
      "4093    7.000004  -11.163172\n",
      "4094  193.000000  129.896132\n",
      "4095   16.000002   21.139157\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.2550\n",
      "Epoch 19/25, Validation Loss: 0.2331\n",
      "          actual   predicted\n",
      "0       4.999997   36.052342\n",
      "1      13.999995   29.724254\n",
      "2     290.000001  210.492545\n",
      "3       9.000000  -14.112639\n",
      "4     450.999989  450.776350\n",
      "...          ...         ...\n",
      "4091   25.000000   63.999530\n",
      "4092  371.999995  407.407683\n",
      "4093    7.000004   -5.777726\n",
      "4094  193.000000  128.041105\n",
      "4095   16.000002   22.379352\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.2408\n",
      "Epoch 20/25, Validation Loss: 0.2493\n",
      "          actual   predicted\n",
      "0       4.999997   38.722816\n",
      "1      13.999995   36.302626\n",
      "2     290.000001  242.878845\n",
      "3       9.000000   -8.652515\n",
      "4     450.999989  433.509399\n",
      "...          ...         ...\n",
      "4091   25.000000   69.772637\n",
      "4092  371.999995  397.375642\n",
      "4093    7.000004   -0.677855\n",
      "4094  193.000000  139.084544\n",
      "4095   16.000002   33.264531\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.2317\n",
      "Epoch 21/25, Validation Loss: 0.2307\n",
      "          actual   predicted\n",
      "0       4.999997   33.947099\n",
      "1      13.999995   30.723786\n",
      "2     290.000001  230.104014\n",
      "3       9.000000  -10.946195\n",
      "4     450.999989  424.026933\n",
      "...          ...         ...\n",
      "4091   25.000000   57.588174\n",
      "4092  371.999995  392.974779\n",
      "4093    7.000004   -3.296305\n",
      "4094  193.000000  124.071146\n",
      "4095   16.000002   30.618640\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.2218\n",
      "Epoch 22/25, Validation Loss: 0.2359\n",
      "          actual   predicted\n",
      "0       4.999997   35.912240\n",
      "1      13.999995   35.256767\n",
      "2     290.000001  219.624349\n",
      "3       9.000000   -8.451553\n",
      "4     450.999989  448.994442\n",
      "...          ...         ...\n",
      "4091   25.000000   65.084707\n",
      "4092  371.999995  404.575429\n",
      "4093    7.000004   -0.431922\n",
      "4094  193.000000  132.402477\n",
      "4095   16.000002   23.969000\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.2214\n",
      "Epoch 23/25, Validation Loss: 0.2288\n",
      "          actual   predicted\n",
      "0       4.999997   38.007637\n",
      "1      13.999995   33.865772\n",
      "2     290.000001  221.603469\n",
      "3       9.000000   -6.225563\n",
      "4     450.999989  419.524467\n",
      "...          ...         ...\n",
      "4091   25.000000   59.732253\n",
      "4092  371.999995  385.629451\n",
      "4093    7.000004    4.311498\n",
      "4094  193.000000  136.060428\n",
      "4095   16.000002   24.245077\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.2149\n",
      "Epoch 24/25, Validation Loss: 0.2361\n",
      "          actual   predicted\n",
      "0       4.999997   36.874384\n",
      "1      13.999995   25.281381\n",
      "2     290.000001  211.160177\n",
      "3       9.000000  -18.635538\n",
      "4     450.999989  454.894744\n",
      "...          ...         ...\n",
      "4091   25.000000   58.098689\n",
      "4092  371.999995  409.078658\n",
      "4093    7.000004   -1.674090\n",
      "4094  193.000000  126.142253\n",
      "4095   16.000002   15.875511\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.2162\n",
      "Epoch 25/25, Validation Loss: 0.2539\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4371,  1.6471,  1.2275, -0.7223, -0.1084, -0.1014, -1.5026,\n",
      "          -0.5233, -1.8361, -1.8361, -0.5868,  4.1524,  0.0000, -0.9068,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9400,\n",
      "          -1.0373, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4443,  1.6188,  1.1864, -0.6811, -0.1326, -0.1086, -1.3903,\n",
      "          -0.5233, -1.8705, -1.8705, -0.0432,  4.1524,  0.0000, -0.9066,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -0.9995,\n",
      "          -0.9717, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4516,  1.5905,  1.1454, -0.6400, -0.1568, -0.1157, -1.2780,\n",
      "          -0.5233, -1.9049, -1.9049,  0.5005,  4.1524,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.0590,\n",
      "          -0.9061, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4589,  1.5622,  1.1043, -0.5989, -0.1810, -0.1229, -1.1657,\n",
      "          -0.5233, -1.9393, -1.9393,  1.0442,  4.1524,  0.0000, -0.9064,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1184,\n",
      "          -0.8406, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4661,  1.5339,  1.0632, -0.5578, -0.2052, -0.1300, -1.0534,\n",
      "          -0.5233, -1.9737, -1.9737,  1.5879,  4.1524,  0.0000, -0.9062,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.9697,  0.1371, -1.1779,\n",
      "          -0.7750, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.6714,  2.1800,  1.2685, -1.2546,  0.4127,  0.0343, -0.4919,\n",
      "          -0.3653,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9080,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.7760,\n",
      "          -1.1793, -1.1564,  1.1564,  0.0000],\n",
      "         [-0.1926,  1.6989,  3.6279, -1.2248,  0.0975, -0.0532, -1.0815,\n",
      "          -0.4325,  0.8111,  0.8111, -0.2244, -0.7996,  0.0000, -0.9065,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.8464,\n",
      "          -1.1292, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.0926,  1.2578,  2.2376, -1.1112,  0.0367, -0.0657, -0.6791,\n",
      "          -0.3127,  1.2064,  1.2064,  1.1348, -0.7996,  0.0000, -0.9049,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -0.9151,\n",
      "          -1.0735, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2006,  1.1177,  1.8598, -1.1061,  0.0194, -0.0800, -0.4919,\n",
      "          -0.4443,  1.4213,  1.4213,  1.1348, -0.7996,  0.0000, -0.9046,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1978,\n",
      "          -0.7437, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2370,  0.9357,  0.9565, -1.1099,  0.0181, -0.0800, -1.6149,\n",
      "          -0.5233, -1.5439, -1.5439,  1.5879, -0.7996,  0.0000, -0.9045,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0338,\n",
      "          -0.9597, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.2873,  0.8379,  0.9072, -1.0376, -0.0138, -0.0871, -1.6149,\n",
      "          -0.4443,  1.6362,  1.6362, -0.6774, -0.7996,  0.0000, -0.9044,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.0943,\n",
      "          -0.8897, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3348,  0.7598,  0.8744, -1.0338, -0.0218, -0.0943, -1.0534,\n",
      "          -0.2864,  1.4643,  1.4643, -1.1305, -0.7996,  0.0000, -0.9042,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.1501,\n",
      "          -0.8158, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3543,  0.8108,  0.8087, -0.9805, -0.0271, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -0.2244, -0.7996,  0.0000, -0.9041,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3511,\n",
      "          -0.4009, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3516,  0.8170,  0.7594, -0.8167, -0.0284, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3353,  1.3353, -1.5836, -0.7996,  0.0000, -0.9039,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.2467,\n",
      "          -0.6580, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3683,  0.9166,  0.6252, -0.8548, -0.0856, -0.1169, -1.0534,\n",
      "          -0.3653,  1.4213,  1.4213,  0.0777, -0.7996,  0.0000, -0.9037,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3030,\n",
      "          -0.5367, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.3921,  1.0246,  1.1823, -0.6016, -0.0411, -0.1050, -1.3342,\n",
      "          -0.5233,  1.5072,  1.5072,  1.1348, -0.7996,  0.0000, -0.9033,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3382,\n",
      "          -0.4422, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4270,  0.9794,  0.7101, -0.7634, -0.0298, -0.1050, -1.6149,\n",
      "          -0.5233,  1.5932,  1.5932, -0.6774, -0.7996,  0.0000, -0.9031,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3658,\n",
      "          -0.3473, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4158,  1.0002,  0.7183, -0.7405, -0.0324, -0.1050, -1.6149,\n",
      "          -0.5233,  1.4213,  1.4213, -0.2244, -0.7996,  0.0000, -0.9030,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3856,\n",
      "          -0.2568, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4186,  1.1042,  0.7840, -0.6758, -0.0298, -0.1014, -0.4919,\n",
      "          -0.5233,  1.4213,  1.4213,  0.2287, -0.7996,  0.0000, -0.9029,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.3995,\n",
      "          -0.1653, -1.1564,  1.1564,  0.0000],\n",
      "         [ 0.4019,  1.1510,  0.8087, -0.6073, -0.0298, -0.0978, -1.6149,\n",
      "          -0.5233,  1.3783,  1.3783, -1.5836, -0.7996,  0.0000, -0.9027,\n",
      "           0.0000, -0.4485, -0.0395,  1.4107, -1.7643,  0.8555, -1.4074,\n",
      "          -0.0730, -1.1564,  1.1564,  0.0000]]])\n",
      "predicted: tensor([[-0.1759]], device='cuda:0')\n",
      "[46.98]\n",
      "           actual   predicted\n",
      "0        4.999997   36.874384\n",
      "1       13.999995   25.281381\n",
      "2      290.000001  211.160177\n",
      "3        9.000000  -18.635538\n",
      "4      450.999989  454.894744\n",
      "...           ...         ...\n",
      "78284   25.000000   61.512233\n",
      "78285   10.000004  -11.645236\n",
      "78286  184.000000  219.790294\n",
      "78287    4.000005   -6.212218\n",
      "78288   16.000002   38.755473\n",
      "\n",
      "[78289 rows x 2 columns]\n",
      "Score (RMSE): 384.3276\n",
      "Score (MAE): 57.4954\n",
      "Score (ME): 0.8010\n",
      "Score (MAPE): 141998.3526%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (199269, 27) to (200228, 27)\n",
      "training data cutoff:  2023-07-14 02:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([154985, 20, 25]) torch.Size([154985]) torch.Size([154985, 1])\n",
      "Testing data shape: torch.Size([39048, 20, 25]) torch.Size([39048]) torch.Size([39048, 1])\n",
      "Shuffled Training data shape: torch.Size([155226, 20, 25]) torch.Size([155226]) torch.Size([155226, 1])\n",
      "Shuffled Testing data shape: torch.Size([38807, 20, 25]) torch.Size([38807]) torch.Size([38807, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     434.499998  476.042225\n",
      "1     398.000001  423.375814\n",
      "2     506.000000  503.168638\n",
      "3     413.999996  417.901243\n",
      "4     397.000004  411.008843\n",
      "...          ...         ...\n",
      "4091  415.500000  484.110888\n",
      "4092  476.500000  478.646514\n",
      "4093  411.500003  418.330119\n",
      "4094  415.999999  444.494512\n",
      "4095  404.000000  412.212515\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.6134\n",
      "Epoch 1/25, Validation Loss: 0.3322\n",
      "          actual   predicted\n",
      "0     434.499998  444.870850\n",
      "1     398.000001  416.803160\n",
      "2     506.000000  499.892265\n",
      "3     413.999996  417.760597\n",
      "4     397.000004  414.658144\n",
      "...          ...         ...\n",
      "4091  415.500000  440.837746\n",
      "4092  476.500000  482.832436\n",
      "4093  411.500003  409.431669\n",
      "4094  415.999999  418.695893\n",
      "4095  404.000000  397.678254\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2435\n",
      "Epoch 2/25, Validation Loss: 0.1434\n",
      "          actual   predicted\n",
      "0     434.499998  432.364658\n",
      "1     398.000001  408.591317\n",
      "2     506.000000  512.102688\n",
      "3     413.999996  412.434444\n",
      "4     397.000004  411.683610\n",
      "...          ...         ...\n",
      "4091  415.500000  432.040639\n",
      "4092  476.500000  482.869072\n",
      "4093  411.500003  409.673672\n",
      "4094  415.999999  412.180840\n",
      "4095  404.000000  399.635566\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1302\n",
      "Epoch 3/25, Validation Loss: 0.0935\n",
      "          actual   predicted\n",
      "0     434.499998  429.279868\n",
      "1     398.000001  412.288780\n",
      "2     506.000000  510.973587\n",
      "3     413.999996  415.647885\n",
      "4     397.000004  412.912812\n",
      "...          ...         ...\n",
      "4091  415.500000  432.756826\n",
      "4092  476.500000  482.547066\n",
      "4093  411.500003  409.110368\n",
      "4094  415.999999  416.224156\n",
      "4095  404.000000  407.737795\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1041\n",
      "Epoch 4/25, Validation Loss: 0.0811\n",
      "          actual   predicted\n",
      "0     434.499998  427.070625\n",
      "1     398.000001  407.096004\n",
      "2     506.000000  510.057684\n",
      "3     413.999996  415.292103\n",
      "4     397.000004  405.125960\n",
      "...          ...         ...\n",
      "4091  415.500000  428.703596\n",
      "4092  476.500000  478.420212\n",
      "4093  411.500003  405.234872\n",
      "4094  415.999999  412.464658\n",
      "4095  404.000000  401.819854\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0942\n",
      "Epoch 5/25, Validation Loss: 0.0754\n",
      "          actual   predicted\n",
      "0     434.499998  431.201249\n",
      "1     398.000001  405.418530\n",
      "2     506.000000  516.044792\n",
      "3     413.999996  413.842001\n",
      "4     397.000004  401.569719\n",
      "...          ...         ...\n",
      "4091  415.500000  429.365572\n",
      "4092  476.500000  481.794716\n",
      "4093  411.500003  406.010427\n",
      "4094  415.999999  415.321229\n",
      "4095  404.000000  400.765200\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0886\n",
      "Epoch 6/25, Validation Loss: 0.0720\n",
      "          actual   predicted\n",
      "0     434.499998  430.670192\n",
      "1     398.000001  404.206728\n",
      "2     506.000000  514.468207\n",
      "3     413.999996  413.247328\n",
      "4     397.000004  402.237319\n",
      "...          ...         ...\n",
      "4091  415.500000  423.956768\n",
      "4092  476.500000  482.450412\n",
      "4093  411.500003  407.243421\n",
      "4094  415.999999  417.396795\n",
      "4095  404.000000  400.874806\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0854\n",
      "Epoch 7/25, Validation Loss: 0.0707\n",
      "          actual   predicted\n",
      "0     434.499998  430.916309\n",
      "1     398.000001  403.423532\n",
      "2     506.000000  515.756802\n",
      "3     413.999996  413.186951\n",
      "4     397.000004  403.292974\n",
      "...          ...         ...\n",
      "4091  415.500000  427.171166\n",
      "4092  476.500000  479.470651\n",
      "4093  411.500003  408.367350\n",
      "4094  415.999999  412.904616\n",
      "4095  404.000000  403.530515\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0813\n",
      "Epoch 8/25, Validation Loss: 0.0721\n",
      "          actual   predicted\n",
      "0     434.499998  432.856051\n",
      "1     398.000001  403.276165\n",
      "2     506.000000  515.765705\n",
      "3     413.999996  413.912255\n",
      "4     397.000004  400.507709\n",
      "...          ...         ...\n",
      "4091  415.500000  425.843497\n",
      "4092  476.500000  479.421394\n",
      "4093  411.500003  407.647956\n",
      "4094  415.999999  416.414929\n",
      "4095  404.000000  400.146588\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0798\n",
      "Epoch 9/25, Validation Loss: 0.0705\n",
      "          actual   predicted\n",
      "0     434.499998  431.489224\n",
      "1     398.000001  402.454939\n",
      "2     506.000000  517.685941\n",
      "3     413.999996  411.937674\n",
      "4     397.000004  399.513541\n",
      "...          ...         ...\n",
      "4091  415.500000  423.420719\n",
      "4092  476.500000  480.704191\n",
      "4093  411.500003  409.039983\n",
      "4094  415.999999  416.528012\n",
      "4095  404.000000  402.902947\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0786\n",
      "Epoch 10/25, Validation Loss: 0.0714\n",
      "          actual   predicted\n",
      "0     434.499998  437.237101\n",
      "1     398.000001  401.695360\n",
      "2     506.000000  522.755907\n",
      "3     413.999996  413.773866\n",
      "4     397.000004  400.903588\n",
      "...          ...         ...\n",
      "4091  415.500000  425.673142\n",
      "4092  476.500000  483.025334\n",
      "4093  411.500003  407.403607\n",
      "4094  415.999999  417.572980\n",
      "4095  404.000000  401.344488\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0775\n",
      "Epoch 11/25, Validation Loss: 0.0710\n",
      "          actual   predicted\n",
      "0     434.499998  436.020211\n",
      "1     398.000001  404.384008\n",
      "2     506.000000  514.700349\n",
      "3     413.999996  416.217267\n",
      "4     397.000004  402.073488\n",
      "...          ...         ...\n",
      "4091  415.500000  423.546155\n",
      "4092  476.500000  475.574498\n",
      "4093  411.500003  408.118501\n",
      "4094  415.999999  417.660802\n",
      "4095  404.000000  403.718366\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0777\n",
      "Epoch 12/25, Validation Loss: 0.0665\n",
      "          actual   predicted\n",
      "0     434.499998  435.382822\n",
      "1     398.000001  406.413574\n",
      "2     506.000000  515.146686\n",
      "3     413.999996  416.254436\n",
      "4     397.000004  403.594574\n",
      "...          ...         ...\n",
      "4091  415.500000  425.004526\n",
      "4092  476.500000  476.990167\n",
      "4093  411.500003  407.428248\n",
      "4094  415.999999  416.985298\n",
      "4095  404.000000  402.659153\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0755\n",
      "Epoch 13/25, Validation Loss: 0.0665\n",
      "          actual   predicted\n",
      "0     434.499998  439.381212\n",
      "1     398.000001  404.953150\n",
      "2     506.000000  512.661854\n",
      "3     413.999996  415.380415\n",
      "4     397.000004  404.284191\n",
      "...          ...         ...\n",
      "4091  415.500000  426.497371\n",
      "4092  476.500000  478.545020\n",
      "4093  411.500003  409.232969\n",
      "4094  415.999999  422.391786\n",
      "4095  404.000000  405.698082\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0742\n",
      "Epoch 14/25, Validation Loss: 0.0646\n",
      "          actual   predicted\n",
      "0     434.499998  441.379278\n",
      "1     398.000001  397.959297\n",
      "2     506.000000  513.978112\n",
      "3     413.999996  415.397436\n",
      "4     397.000004  400.968216\n",
      "...          ...         ...\n",
      "4091  415.500000  421.428533\n",
      "4092  476.500000  481.067611\n",
      "4093  411.500003  404.924492\n",
      "4094  415.999999  418.489005\n",
      "4095  404.000000  401.069852\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0736\n",
      "Epoch 15/25, Validation Loss: 0.0646\n",
      "          actual   predicted\n",
      "0     434.499998  440.129014\n",
      "1     398.000001  403.428484\n",
      "2     506.000000  515.797331\n",
      "3     413.999996  416.493686\n",
      "4     397.000004  402.847852\n",
      "...          ...         ...\n",
      "4091  415.500000  423.919819\n",
      "4092  476.500000  477.738714\n",
      "4093  411.500003  410.697981\n",
      "4094  415.999999  418.947818\n",
      "4095  404.000000  406.757368\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0726\n",
      "Epoch 16/25, Validation Loss: 0.0636\n",
      "          actual   predicted\n",
      "0     434.499998  440.407149\n",
      "1     398.000001  399.904518\n",
      "2     506.000000  522.065899\n",
      "3     413.999996  411.667443\n",
      "4     397.000004  397.034542\n",
      "...          ...         ...\n",
      "4091  415.500000  420.696772\n",
      "4092  476.500000  480.921164\n",
      "4093  411.500003  406.537338\n",
      "4094  415.999999  418.778609\n",
      "4095  404.000000  401.083425\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0716\n",
      "Epoch 17/25, Validation Loss: 0.0649\n",
      "          actual   predicted\n",
      "0     434.499998  442.839092\n",
      "1     398.000001  403.071914\n",
      "2     506.000000  515.132560\n",
      "3     413.999996  417.282748\n",
      "4     397.000004  402.860526\n",
      "...          ...         ...\n",
      "4091  415.500000  424.725676\n",
      "4092  476.500000  476.609655\n",
      "4093  411.500003  408.980424\n",
      "4094  415.999999  421.301344\n",
      "4095  404.000000  407.999135\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0711\n",
      "Epoch 18/25, Validation Loss: 0.0632\n",
      "          actual   predicted\n",
      "0     434.499998  442.419570\n",
      "1     398.000001  404.039988\n",
      "2     506.000000  516.759374\n",
      "3     413.999996  417.191200\n",
      "4     397.000004  399.084526\n",
      "...          ...         ...\n",
      "4091  415.500000  423.013919\n",
      "4092  476.500000  478.873565\n",
      "4093  411.500003  407.526962\n",
      "4094  415.999999  420.424547\n",
      "4095  404.000000  404.968681\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0708\n",
      "Epoch 19/25, Validation Loss: 0.0621\n",
      "          actual   predicted\n",
      "0     434.499998  445.538037\n",
      "1     398.000001  404.434004\n",
      "2     506.000000  514.827639\n",
      "3     413.999996  416.675101\n",
      "4     397.000004  400.063894\n",
      "...          ...         ...\n",
      "4091  415.500000  422.116098\n",
      "4092  476.500000  483.070641\n",
      "4093  411.500003  410.432674\n",
      "4094  415.999999  423.175793\n",
      "4095  404.000000  406.705560\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0702\n",
      "Epoch 20/25, Validation Loss: 0.0610\n",
      "          actual   predicted\n",
      "0     434.499998  442.948143\n",
      "1     398.000001  401.244897\n",
      "2     506.000000  519.453900\n",
      "3     413.999996  416.939298\n",
      "4     397.000004  398.329973\n",
      "...          ...         ...\n",
      "4091  415.500000  424.474876\n",
      "4092  476.500000  480.554329\n",
      "4093  411.500003  408.607191\n",
      "4094  415.999999  420.881789\n",
      "4095  404.000000  408.489089\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0696\n",
      "Epoch 21/25, Validation Loss: 0.0654\n",
      "          actual   predicted\n",
      "0     434.499998  446.024328\n",
      "1     398.000001  399.570316\n",
      "2     506.000000  521.031049\n",
      "3     413.999996  413.788557\n",
      "4     397.000004  399.121877\n",
      "...          ...         ...\n",
      "4091  415.500000  423.590300\n",
      "4092  476.500000  480.696922\n",
      "4093  411.500003  406.680116\n",
      "4094  415.999999  419.652199\n",
      "4095  404.000000  405.268227\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0689\n",
      "Epoch 22/25, Validation Loss: 0.0612\n",
      "          actual   predicted\n",
      "0     434.499998  443.261193\n",
      "1     398.000001  401.844202\n",
      "2     506.000000  520.398060\n",
      "3     413.999996  415.461962\n",
      "4     397.000004  398.272745\n",
      "...          ...         ...\n",
      "4091  415.500000  424.349842\n",
      "4092  476.500000  480.358931\n",
      "4093  411.500003  406.622610\n",
      "4094  415.999999  419.043464\n",
      "4095  404.000000  405.173735\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0677\n",
      "Epoch 23/25, Validation Loss: 0.0607\n",
      "          actual   predicted\n",
      "0     434.499998  444.463714\n",
      "1     398.000001  397.662198\n",
      "2     506.000000  521.128126\n",
      "3     413.999996  410.845903\n",
      "4     397.000004  395.026402\n",
      "...          ...         ...\n",
      "4091  415.500000  423.448785\n",
      "4092  476.500000  481.431098\n",
      "4093  411.500003  405.327786\n",
      "4094  415.999999  418.090248\n",
      "4095  404.000000  403.961751\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0679\n",
      "Epoch 24/25, Validation Loss: 0.0674\n",
      "          actual   predicted\n",
      "0     434.499998  439.158259\n",
      "1     398.000001  405.168899\n",
      "2     506.000000  513.276014\n",
      "3     413.999996  418.668492\n",
      "4     397.000004  400.853482\n",
      "...          ...         ...\n",
      "4091  415.500000  422.166839\n",
      "4092  476.500000  479.243910\n",
      "4093  411.500003  410.987563\n",
      "4094  415.999999  421.425070\n",
      "4095  404.000000  407.569251\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0671\n",
      "Epoch 25/25, Validation Loss: 0.0615\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4369,  1.6490,  1.2161, -0.7247, -0.1103, -0.1009, -1.7508,\n",
      "          -0.5681, -1.9687, -1.9687, -0.8792,  4.1508,  0.0000, -0.9065,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -0.9411,\n",
      "          -1.0386, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4441,  1.6207,  1.1753, -0.6836, -0.1333, -0.1076, -1.6197,\n",
      "          -0.5681, -2.0056, -2.0056, -0.0644,  4.1508,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0007,\n",
      "          -0.9729, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4514,  1.5924,  1.1345, -0.6426, -0.1562, -0.1142, -1.4886,\n",
      "          -0.5681, -2.0425, -2.0425,  0.7504,  4.1508,  0.0000, -0.9062,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0603,\n",
      "          -0.9071, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4586,  1.5641,  1.0937, -0.6016, -0.1791, -0.1209, -1.3575,\n",
      "          -0.5681, -2.0794, -2.0794,  1.5652,  4.1508,  0.0000, -0.9061,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1199,\n",
      "          -0.8414, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4659,  1.5358,  1.0529, -0.5605, -0.2021, -0.1275, -1.2264,\n",
      "          -0.5681, -2.1163, -2.1163,  2.3800,  4.1508,  0.0000, -0.9059,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1795,\n",
      "          -0.7757, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.6707,  2.1820,  1.2569, -1.2558,  0.3838,  0.0256, -0.5708,\n",
      "          -0.3964,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9077,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.7766,\n",
      "          -1.1809, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.1923,  1.7008,  3.6011, -1.2260,  0.0849, -0.0560, -1.2592,\n",
      "          -0.4694,  0.8735,  0.8735, -0.3360, -0.7989,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.8472,\n",
      "          -1.1308, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.0926,  1.2596,  2.2197, -1.1127,  0.0272, -0.0676, -0.7894,\n",
      "          -0.3392,  1.2980,  1.2980,  1.7010, -0.7989,  0.0000, -0.9046,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.9160,\n",
      "          -1.0749, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2006,  1.1195,  1.8444, -1.1076,  0.0108, -0.0809, -0.5708,\n",
      "          -0.4822,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9043,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1995,\n",
      "          -0.7444, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2369,  0.9374,  0.9469, -1.1114,  0.0096, -0.0809, -1.8819,\n",
      "          -0.5681, -1.6549, -1.6549,  2.3800, -0.7989,  0.0000, -0.9042,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0351,\n",
      "          -0.9609, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2872,  0.8395,  0.8979, -1.0392, -0.0206, -0.0876, -1.8819,\n",
      "          -0.4822,  1.7594,  1.7594, -1.0150, -0.7989,  0.0000, -0.9041,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0958,\n",
      "          -0.8907, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3347,  0.7615,  0.8653, -1.0354, -0.0282, -0.0942, -1.2264,\n",
      "          -0.3105,  1.5748,  1.5748, -1.6940, -0.7989,  0.0000, -0.9039,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1517,\n",
      "          -0.8166, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3542,  0.8125,  0.8000, -0.9823, -0.0332, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -0.3360, -0.7989,  0.0000, -0.9038,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3532,\n",
      "          -0.4007, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3514,  0.8187,  0.7511, -0.8189, -0.0345, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -2.3730, -0.7989,  0.0000, -0.9037,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.2486,\n",
      "          -0.6584, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3682,  0.9183,  0.6178, -0.8569, -0.0887, -0.1153, -1.2264,\n",
      "          -0.3964,  1.5287,  1.5287,  0.1166, -0.7989,  0.0000, -0.9034,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3050,\n",
      "          -0.5368, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3919,  1.0263,  1.1713, -0.6042, -0.0465, -0.1042, -1.5541,\n",
      "          -0.5681,  1.6210,  1.6210,  1.7010, -0.7989,  0.0000, -0.9031,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3402,\n",
      "          -0.4420, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4268,  0.9811,  0.7021, -0.7657, -0.0358, -0.1042, -1.8819,\n",
      "          -0.5681,  1.7132,  1.7132, -1.0150, -0.7989,  0.0000, -0.9029,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3679,\n",
      "          -0.3469, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4156,  1.0019,  0.7103, -0.7429, -0.0383, -0.1042, -1.8819,\n",
      "          -0.5681,  1.5287,  1.5287, -0.3360, -0.7989,  0.0000, -0.9027,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3878,\n",
      "          -0.2562, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4184,  1.1060,  0.7755, -0.6783, -0.0358, -0.1009, -0.5708,\n",
      "          -0.5681,  1.5287,  1.5287,  0.3430, -0.7989,  0.0000, -0.9026,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4017,\n",
      "          -0.1644, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4017,  1.1528,  0.8000, -0.6099, -0.0358, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4825,  1.4825, -2.3730, -0.7989,  0.0000, -0.9025,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4096,\n",
      "          -0.0719, -1.1552,  1.1552,  0.0000]]])\n",
      "predicted: tensor([[0.7923]], device='cuda:0')\n",
      "[585.05]\n",
      "           actual   predicted\n",
      "0      434.499998  439.158259\n",
      "1      398.000001  405.168899\n",
      "2      506.000000  513.276014\n",
      "3      413.999996  418.668492\n",
      "4      397.000004  400.853482\n",
      "...           ...         ...\n",
      "38802  456.500000  460.715661\n",
      "38803  445.499999  438.447307\n",
      "38804  606.499997  585.398130\n",
      "38805  424.000000  429.360622\n",
      "38806  436.500000  448.059939\n",
      "\n",
      "[38807 rows x 2 columns]\n",
      "Score (RMSE): 30.6353\n",
      "Score (MAE): 10.5513\n",
      "Score (ME): 1.0681\n",
      "Score (MAPE): 1.8526%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (199269, 27) to (200228, 27)\n",
      "training data cutoff:  2023-07-14 02:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([154985, 20, 25]) torch.Size([154985]) torch.Size([154985, 1])\n",
      "Testing data shape: torch.Size([39048, 20, 25]) torch.Size([39048]) torch.Size([39048, 1])\n",
      "Shuffled Training data shape: torch.Size([155226, 20, 25]) torch.Size([155226]) torch.Size([155226, 1])\n",
      "Shuffled Testing data shape: torch.Size([38807, 20, 25]) torch.Size([38807]) torch.Size([38807, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual    predicted\n",
      "0      608.000001   674.047945\n",
      "1      596.499997   600.755571\n",
      "2      666.500002   629.212407\n",
      "3      846.000002   842.879302\n",
      "4      599.500006   619.240490\n",
      "...           ...          ...\n",
      "4091  1438.499972   920.170041\n",
      "4092   851.500003   768.926669\n",
      "4093   705.000004   840.908414\n",
      "4094   690.499998   704.871658\n",
      "4095  1004.500007  1038.955512\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.5982\n",
      "Epoch 1/25, Validation Loss: 0.2954\n",
      "           actual    predicted\n",
      "0      608.000001   637.973659\n",
      "1      596.499997   619.170504\n",
      "2      666.500002   648.753524\n",
      "3      846.000002   870.476004\n",
      "4      599.500006   590.532782\n",
      "...           ...          ...\n",
      "4091  1438.499972  1471.319816\n",
      "4092   851.500003   933.028981\n",
      "4093   705.000004   801.718029\n",
      "4094   690.499998   698.975535\n",
      "4095  1004.500007   999.548531\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2176\n",
      "Epoch 2/25, Validation Loss: 0.1590\n",
      "           actual    predicted\n",
      "0      608.000001   627.333138\n",
      "1      596.499997   608.826570\n",
      "2      666.500002   633.993650\n",
      "3      846.000002   873.493294\n",
      "4      599.500006   573.842150\n",
      "...           ...          ...\n",
      "4091  1438.499972  1387.316731\n",
      "4092   851.500003   955.017440\n",
      "4093   705.000004   750.860391\n",
      "4094   690.499998   699.273840\n",
      "4095  1004.500007   975.828461\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1370\n",
      "Epoch 3/25, Validation Loss: 0.0993\n",
      "           actual    predicted\n",
      "0      608.000001   625.280733\n",
      "1      596.499997   588.961835\n",
      "2      666.500002   650.725730\n",
      "3      846.000002   875.939091\n",
      "4      599.500006   571.351681\n",
      "...           ...          ...\n",
      "4091  1438.499972  1453.981241\n",
      "4092   851.500003   912.793971\n",
      "4093   705.000004   754.645552\n",
      "4094   690.499998   701.273156\n",
      "4095  1004.500007   991.030337\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.1021\n",
      "Epoch 4/25, Validation Loss: 0.0753\n",
      "           actual    predicted\n",
      "0      608.000001   628.446975\n",
      "1      596.499997   593.918598\n",
      "2      666.500002   658.954632\n",
      "3      846.000002   862.224300\n",
      "4      599.500006   570.005876\n",
      "...           ...          ...\n",
      "4091  1438.499972  1470.323797\n",
      "4092   851.500003   884.515323\n",
      "4093   705.000004   764.354924\n",
      "4094   690.499998   702.425861\n",
      "4095  1004.500007   994.152527\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0859\n",
      "Epoch 5/25, Validation Loss: 0.0662\n",
      "           actual    predicted\n",
      "0      608.000001   634.144791\n",
      "1      596.499997   606.892439\n",
      "2      666.500002   657.372742\n",
      "3      846.000002   855.368775\n",
      "4      599.500006   579.803073\n",
      "...           ...          ...\n",
      "4091  1438.499972  1429.585759\n",
      "4092   851.500003   874.064756\n",
      "4093   705.000004   746.679246\n",
      "4094   690.499998   696.110257\n",
      "4095  1004.500007   980.721467\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0785\n",
      "Epoch 6/25, Validation Loss: 0.0626\n",
      "           actual    predicted\n",
      "0      608.000001   630.740870\n",
      "1      596.499997   608.512585\n",
      "2      666.500002   653.077029\n",
      "3      846.000002   858.625421\n",
      "4      599.500006   575.173505\n",
      "...           ...          ...\n",
      "4091  1438.499972  1456.423343\n",
      "4092   851.500003   884.890901\n",
      "4093   705.000004   745.760045\n",
      "4094   690.499998   696.023963\n",
      "4095  1004.500007   992.696450\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0734\n",
      "Epoch 7/25, Validation Loss: 0.0586\n",
      "           actual    predicted\n",
      "0      608.000001   622.266102\n",
      "1      596.499997   598.726809\n",
      "2      666.500002   650.630063\n",
      "3      846.000002   862.796703\n",
      "4      599.500006   568.467357\n",
      "...           ...          ...\n",
      "4091  1438.499972  1470.030112\n",
      "4092   851.500003   886.816811\n",
      "4093   705.000004   750.839259\n",
      "4094   690.499998   691.270489\n",
      "4095  1004.500007  1002.861788\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0695\n",
      "Epoch 8/25, Validation Loss: 0.0587\n",
      "           actual    predicted\n",
      "0      608.000001   625.551686\n",
      "1      596.499997   599.372570\n",
      "2      666.500002   659.638422\n",
      "3      846.000002   844.542226\n",
      "4      599.500006   571.431236\n",
      "...           ...          ...\n",
      "4091  1438.499972  1429.572518\n",
      "4092   851.500003   881.014034\n",
      "4093   705.000004   750.944108\n",
      "4094   690.499998   690.636816\n",
      "4095  1004.500007   994.439497\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0669\n",
      "Epoch 9/25, Validation Loss: 0.0560\n",
      "           actual    predicted\n",
      "0      608.000001   631.691942\n",
      "1      596.499997   606.178983\n",
      "2      666.500002   654.226094\n",
      "3      846.000002   847.323372\n",
      "4      599.500006   570.925351\n",
      "...           ...          ...\n",
      "4091  1438.499972  1438.302990\n",
      "4092   851.500003   877.595147\n",
      "4093   705.000004   743.270549\n",
      "4094   690.499998   692.030650\n",
      "4095  1004.500007   996.289037\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0646\n",
      "Epoch 10/25, Validation Loss: 0.0553\n",
      "           actual    predicted\n",
      "0      608.000001   628.397918\n",
      "1      596.499997   598.399723\n",
      "2      666.500002   655.871655\n",
      "3      846.000002   856.372818\n",
      "4      599.500006   564.481793\n",
      "...           ...          ...\n",
      "4091  1438.499972  1470.379334\n",
      "4092   851.500003   886.078646\n",
      "4093   705.000004   749.121589\n",
      "4094   690.499998   689.324090\n",
      "4095  1004.500007  1010.321696\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0630\n",
      "Epoch 11/25, Validation Loss: 0.0558\n",
      "           actual    predicted\n",
      "0      608.000001   632.058059\n",
      "1      596.499997   601.600025\n",
      "2      666.500002   659.023998\n",
      "3      846.000002   835.758394\n",
      "4      599.500006   573.430458\n",
      "...           ...          ...\n",
      "4091  1438.499972  1410.771167\n",
      "4092   851.500003   881.415983\n",
      "4093   705.000004   744.977945\n",
      "4094   690.499998   690.614068\n",
      "4095  1004.500007   986.636439\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0624\n",
      "Epoch 12/25, Validation Loss: 0.0550\n",
      "           actual    predicted\n",
      "0      608.000001   623.332059\n",
      "1      596.499997   598.459684\n",
      "2      666.500002   653.159996\n",
      "3      846.000002   839.902735\n",
      "4      599.500006   568.997841\n",
      "...           ...          ...\n",
      "4091  1438.499972  1432.057104\n",
      "4092   851.500003   871.549154\n",
      "4093   705.000004   730.266782\n",
      "4094   690.499998   679.892846\n",
      "4095  1004.500007   988.732991\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0606\n",
      "Epoch 13/25, Validation Loss: 0.0544\n",
      "           actual    predicted\n",
      "0      608.000001   634.001619\n",
      "1      596.499997   600.083093\n",
      "2      666.500002   662.127401\n",
      "3      846.000002   849.339686\n",
      "4      599.500006   574.298648\n",
      "...           ...          ...\n",
      "4091  1438.499972  1442.522889\n",
      "4092   851.500003   885.871482\n",
      "4093   705.000004   735.738654\n",
      "4094   690.499998   690.074147\n",
      "4095  1004.500007   998.460124\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0597\n",
      "Epoch 14/25, Validation Loss: 0.0535\n",
      "           actual    predicted\n",
      "0      608.000001   623.885432\n",
      "1      596.499997   597.741866\n",
      "2      666.500002   651.518287\n",
      "3      846.000002   845.695531\n",
      "4      599.500006   561.220417\n",
      "...           ...          ...\n",
      "4091  1438.499972  1436.872782\n",
      "4092   851.500003   885.715822\n",
      "4093   705.000004   732.362903\n",
      "4094   690.499998   687.805181\n",
      "4095  1004.500007   997.775581\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0588\n",
      "Epoch 15/25, Validation Loss: 0.0539\n",
      "           actual    predicted\n",
      "0      608.000001   624.895240\n",
      "1      596.499997   599.087781\n",
      "2      666.500002   654.968370\n",
      "3      846.000002   842.392244\n",
      "4      599.500006   569.932392\n",
      "...           ...          ...\n",
      "4091  1438.499972  1428.776682\n",
      "4092   851.500003   880.406230\n",
      "4093   705.000004   728.054921\n",
      "4094   690.499998   683.397273\n",
      "4095  1004.500007   982.536492\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0580\n",
      "Epoch 16/25, Validation Loss: 0.0530\n",
      "           actual    predicted\n",
      "0      608.000001   631.323612\n",
      "1      596.499997   601.700603\n",
      "2      666.500002   655.890771\n",
      "3      846.000002   843.574247\n",
      "4      599.500006   576.024517\n",
      "...           ...          ...\n",
      "4091  1438.499972  1418.125587\n",
      "4092   851.500003   881.529590\n",
      "4093   705.000004   729.421285\n",
      "4094   690.499998   688.395343\n",
      "4095  1004.500007   984.213210\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0578\n",
      "Epoch 17/25, Validation Loss: 0.0530\n",
      "           actual    predicted\n",
      "0      608.000001   625.809036\n",
      "1      596.499997   595.080865\n",
      "2      666.500002   653.037369\n",
      "3      846.000002   841.463073\n",
      "4      599.500006   563.921298\n",
      "...           ...          ...\n",
      "4091  1438.499972  1418.518610\n",
      "4092   851.500003   887.223569\n",
      "4093   705.000004   732.572012\n",
      "4094   690.499998   688.382832\n",
      "4095  1004.500007   996.006287\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0570\n",
      "Epoch 18/25, Validation Loss: 0.0526\n",
      "           actual    predicted\n",
      "0      608.000001   627.323600\n",
      "1      596.499997   597.396740\n",
      "2      666.500002   657.775115\n",
      "3      846.000002   847.666647\n",
      "4      599.500006   565.187515\n",
      "...           ...          ...\n",
      "4091  1438.499972  1436.705294\n",
      "4092   851.500003   895.974349\n",
      "4093   705.000004   731.661824\n",
      "4094   690.499998   687.938508\n",
      "4095  1004.500007  1004.509482\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0570\n",
      "Epoch 19/25, Validation Loss: 0.0527\n",
      "           actual    predicted\n",
      "0      608.000001   631.482456\n",
      "1      596.499997   603.898674\n",
      "2      666.500002   656.208083\n",
      "3      846.000002   860.266385\n",
      "4      599.500006   567.389148\n",
      "...           ...          ...\n",
      "4091  1438.499972  1436.474990\n",
      "4092   851.500003   887.183148\n",
      "4093   705.000004   725.404796\n",
      "4094   690.499998   690.059337\n",
      "4095  1004.500007  1005.269470\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0563\n",
      "Epoch 20/25, Validation Loss: 0.0527\n",
      "           actual    predicted\n",
      "0      608.000001   626.629283\n",
      "1      596.499997   602.752424\n",
      "2      666.500002   655.101423\n",
      "3      846.000002   840.611646\n",
      "4      599.500006   571.595838\n",
      "...           ...          ...\n",
      "4091  1438.499972  1422.104075\n",
      "4092   851.500003   893.576104\n",
      "4093   705.000004   719.597387\n",
      "4094   690.499998   685.286951\n",
      "4095  1004.500007   995.929320\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0558\n",
      "Epoch 21/25, Validation Loss: 0.0521\n",
      "           actual    predicted\n",
      "0      608.000001   624.066632\n",
      "1      596.499997   598.813486\n",
      "2      666.500002   653.638999\n",
      "3      846.000002   845.965166\n",
      "4      599.500006   562.930441\n",
      "...           ...          ...\n",
      "4091  1438.499972  1412.022465\n",
      "4092   851.500003   889.334719\n",
      "4093   705.000004   728.176796\n",
      "4094   690.499998   690.553221\n",
      "4095  1004.500007   988.796780\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0554\n",
      "Epoch 22/25, Validation Loss: 0.0523\n",
      "           actual    predicted\n",
      "0      608.000001   621.031653\n",
      "1      596.499997   588.657420\n",
      "2      666.500002   649.717075\n",
      "3      846.000002   843.921385\n",
      "4      599.500006   560.821621\n",
      "...           ...          ...\n",
      "4091  1438.499972  1436.354504\n",
      "4092   851.500003   894.714948\n",
      "4093   705.000004   724.599246\n",
      "4094   690.499998   678.812134\n",
      "4095  1004.500007   996.715476\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0553\n",
      "Epoch 23/25, Validation Loss: 0.0527\n",
      "           actual    predicted\n",
      "0      608.000001   629.300575\n",
      "1      596.499997   603.493916\n",
      "2      666.500002   654.450500\n",
      "3      846.000002   845.259036\n",
      "4      599.500006   572.297904\n",
      "...           ...          ...\n",
      "4091  1438.499972  1388.087528\n",
      "4092   851.500003   892.805832\n",
      "4093   705.000004   718.741512\n",
      "4094   690.499998   684.394060\n",
      "4095  1004.500007   984.747114\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0553\n",
      "Epoch 24/25, Validation Loss: 0.0524\n",
      "           actual    predicted\n",
      "0      608.000001   631.887951\n",
      "1      596.499997   590.595410\n",
      "2      666.500002   654.829341\n",
      "3      846.000002   857.258391\n",
      "4      599.500006   563.198538\n",
      "...           ...          ...\n",
      "4091  1438.499972  1449.221978\n",
      "4092   851.500003   897.227483\n",
      "4093   705.000004   737.075654\n",
      "4094   690.499998   686.609584\n",
      "4095  1004.500007  1016.756971\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0550\n",
      "Epoch 25/25, Validation Loss: 0.0536\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4369,  1.6490,  1.2161, -0.7247, -0.1103, -0.1009, -1.7508,\n",
      "          -0.5681, -1.9687, -1.9687, -0.8792,  4.1508,  0.0000, -0.9065,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -0.9411,\n",
      "          -1.0386, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4441,  1.6207,  1.1753, -0.6836, -0.1333, -0.1076, -1.6197,\n",
      "          -0.5681, -2.0056, -2.0056, -0.0644,  4.1508,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0007,\n",
      "          -0.9729, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4514,  1.5924,  1.1345, -0.6426, -0.1562, -0.1142, -1.4886,\n",
      "          -0.5681, -2.0425, -2.0425,  0.7504,  4.1508,  0.0000, -0.9062,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0603,\n",
      "          -0.9071, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4586,  1.5641,  1.0937, -0.6016, -0.1791, -0.1209, -1.3575,\n",
      "          -0.5681, -2.0794, -2.0794,  1.5652,  4.1508,  0.0000, -0.9061,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1199,\n",
      "          -0.8414, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4659,  1.5358,  1.0529, -0.5605, -0.2021, -0.1275, -1.2264,\n",
      "          -0.5681, -2.1163, -2.1163,  2.3800,  4.1508,  0.0000, -0.9059,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1795,\n",
      "          -0.7757, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.6707,  2.1820,  1.2569, -1.2558,  0.3838,  0.0256, -0.5708,\n",
      "          -0.3964,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9077,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.7766,\n",
      "          -1.1809, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.1923,  1.7008,  3.6011, -1.2260,  0.0849, -0.0560, -1.2592,\n",
      "          -0.4694,  0.8735,  0.8735, -0.3360, -0.7989,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.8472,\n",
      "          -1.1308, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.0926,  1.2596,  2.2197, -1.1127,  0.0272, -0.0676, -0.7894,\n",
      "          -0.3392,  1.2980,  1.2980,  1.7010, -0.7989,  0.0000, -0.9046,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.9160,\n",
      "          -1.0749, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2006,  1.1195,  1.8444, -1.1076,  0.0108, -0.0809, -0.5708,\n",
      "          -0.4822,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9043,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1995,\n",
      "          -0.7444, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2369,  0.9374,  0.9469, -1.1114,  0.0096, -0.0809, -1.8819,\n",
      "          -0.5681, -1.6549, -1.6549,  2.3800, -0.7989,  0.0000, -0.9042,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0351,\n",
      "          -0.9609, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2872,  0.8395,  0.8979, -1.0392, -0.0206, -0.0876, -1.8819,\n",
      "          -0.4822,  1.7594,  1.7594, -1.0150, -0.7989,  0.0000, -0.9041,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0958,\n",
      "          -0.8907, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3347,  0.7615,  0.8653, -1.0354, -0.0282, -0.0942, -1.2264,\n",
      "          -0.3105,  1.5748,  1.5748, -1.6940, -0.7989,  0.0000, -0.9039,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1517,\n",
      "          -0.8166, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3542,  0.8125,  0.8000, -0.9823, -0.0332, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -0.3360, -0.7989,  0.0000, -0.9038,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3532,\n",
      "          -0.4007, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3514,  0.8187,  0.7511, -0.8189, -0.0345, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -2.3730, -0.7989,  0.0000, -0.9037,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.2486,\n",
      "          -0.6584, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3682,  0.9183,  0.6178, -0.8569, -0.0887, -0.1153, -1.2264,\n",
      "          -0.3964,  1.5287,  1.5287,  0.1166, -0.7989,  0.0000, -0.9034,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3050,\n",
      "          -0.5368, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3919,  1.0263,  1.1713, -0.6042, -0.0465, -0.1042, -1.5541,\n",
      "          -0.5681,  1.6210,  1.6210,  1.7010, -0.7989,  0.0000, -0.9031,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3402,\n",
      "          -0.4420, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4268,  0.9811,  0.7021, -0.7657, -0.0358, -0.1042, -1.8819,\n",
      "          -0.5681,  1.7132,  1.7132, -1.0150, -0.7989,  0.0000, -0.9029,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3679,\n",
      "          -0.3469, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4156,  1.0019,  0.7103, -0.7429, -0.0383, -0.1042, -1.8819,\n",
      "          -0.5681,  1.5287,  1.5287, -0.3360, -0.7989,  0.0000, -0.9027,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3878,\n",
      "          -0.2562, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4184,  1.1060,  0.7755, -0.6783, -0.0358, -0.1009, -0.5708,\n",
      "          -0.5681,  1.5287,  1.5287,  0.3430, -0.7989,  0.0000, -0.9026,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4017,\n",
      "          -0.1644, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4017,  1.1528,  0.8000, -0.6099, -0.0358, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4825,  1.4825, -2.3730, -0.7989,  0.0000, -0.9025,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4096,\n",
      "          -0.0719, -1.1552,  1.1552,  0.0000]]])\n",
      "predicted: tensor([[-0.5465]], device='cuda:0')\n",
      "[636.69]\n",
      "           actual   predicted\n",
      "0      608.000001  631.887951\n",
      "1      596.499997  590.595410\n",
      "2      666.500002  654.829341\n",
      "3      846.000002  857.258391\n",
      "4      599.500006  563.198538\n",
      "...           ...         ...\n",
      "38802  611.499996  624.377244\n",
      "38803  600.499994  656.705801\n",
      "38804  871.000001  823.813921\n",
      "38805  618.000001  604.876384\n",
      "38806  703.333335  738.462938\n",
      "\n",
      "[38807 rows x 2 columns]\n",
      "Score (RMSE): 61.1889\n",
      "Score (MAE): 30.5482\n",
      "Score (ME): -4.5201\n",
      "Score (MAPE): 3.7714%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (199269, 27) to (200228, 27)\n",
      "training data cutoff:  2023-07-14 02:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([154985, 20, 25]) torch.Size([154985]) torch.Size([154985, 1])\n",
      "Testing data shape: torch.Size([39048, 20, 25]) torch.Size([39048]) torch.Size([39048, 1])\n",
      "Shuffled Training data shape: torch.Size([155226, 20, 25]) torch.Size([155226]) torch.Size([155226, 1])\n",
      "Shuffled Testing data shape: torch.Size([38807, 20, 25]) torch.Size([38807]) torch.Size([38807, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     26.290000  26.070426\n",
      "1     36.965000  34.554300\n",
      "2     28.790000  28.126140\n",
      "3     53.515000  53.178973\n",
      "4     31.255000  30.786290\n",
      "...         ...        ...\n",
      "4091  27.040000  26.280767\n",
      "4092  43.640000  43.776032\n",
      "4093  31.880000  32.816238\n",
      "4094  47.294999  51.669613\n",
      "4095  48.965000  51.609335\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.4719\n",
      "Epoch 1/25, Validation Loss: 0.1070\n",
      "         actual  predicted\n",
      "0     26.290000  26.475270\n",
      "1     36.965000  37.597141\n",
      "2     28.790000  28.691707\n",
      "3     53.515000  58.968329\n",
      "4     31.255000  31.626118\n",
      "...         ...        ...\n",
      "4091  27.040000  26.952414\n",
      "4092  43.640000  43.177548\n",
      "4093  31.880000  33.759051\n",
      "4094  47.294999  51.592014\n",
      "4095  48.965000  50.032481\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0686\n",
      "Epoch 2/25, Validation Loss: 0.0469\n",
      "         actual  predicted\n",
      "0     26.290000  25.956229\n",
      "1     36.965000  37.637289\n",
      "2     28.790000  28.551317\n",
      "3     53.515000  57.773304\n",
      "4     31.255000  31.149993\n",
      "...         ...        ...\n",
      "4091  27.040000  26.801618\n",
      "4092  43.640000  42.369425\n",
      "4093  31.880000  33.595736\n",
      "4094  47.294999  48.471106\n",
      "4095  48.965000  48.807989\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0418\n",
      "Epoch 3/25, Validation Loss: 0.0314\n",
      "         actual  predicted\n",
      "0     26.290000  26.117478\n",
      "1     36.965000  37.444269\n",
      "2     28.790000  28.571667\n",
      "3     53.515000  56.779058\n",
      "4     31.255000  31.433058\n",
      "...         ...        ...\n",
      "4091  27.040000  27.398271\n",
      "4092  43.640000  42.319431\n",
      "4093  31.880000  33.302586\n",
      "4094  47.294999  48.185596\n",
      "4095  48.965000  49.395764\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0329\n",
      "Epoch 4/25, Validation Loss: 0.0251\n",
      "         actual  predicted\n",
      "0     26.290000  25.974716\n",
      "1     36.965000  37.463206\n",
      "2     28.790000  28.591231\n",
      "3     53.515000  56.942999\n",
      "4     31.255000  31.493767\n",
      "...         ...        ...\n",
      "4091  27.040000  27.251898\n",
      "4092  43.640000  42.707358\n",
      "4093  31.880000  33.297010\n",
      "4094  47.294999  48.278333\n",
      "4095  48.965000  49.849551\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0283\n",
      "Epoch 5/25, Validation Loss: 0.0211\n",
      "         actual  predicted\n",
      "0     26.290000  26.105120\n",
      "1     36.965000  37.395165\n",
      "2     28.790000  28.585682\n",
      "3     53.515000  56.115320\n",
      "4     31.255000  31.517059\n",
      "...         ...        ...\n",
      "4091  27.040000  27.430686\n",
      "4092  43.640000  42.740863\n",
      "4093  31.880000  33.065756\n",
      "4094  47.294999  47.943623\n",
      "4095  48.965000  49.504393\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0249\n",
      "Epoch 6/25, Validation Loss: 0.0178\n",
      "         actual  predicted\n",
      "0     26.290000  25.869594\n",
      "1     36.965000  37.463426\n",
      "2     28.790000  28.560206\n",
      "3     53.515000  56.709665\n",
      "4     31.255000  31.359466\n",
      "...         ...        ...\n",
      "4091  27.040000  27.333995\n",
      "4092  43.640000  43.615084\n",
      "4093  31.880000  32.878331\n",
      "4094  47.294999  48.747283\n",
      "4095  48.965000  50.200580\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0224\n",
      "Epoch 7/25, Validation Loss: 0.0169\n",
      "         actual  predicted\n",
      "0     26.290000  26.047900\n",
      "1     36.965000  37.278820\n",
      "2     28.790000  28.556739\n",
      "3     53.515000  56.281412\n",
      "4     31.255000  31.496605\n",
      "...         ...        ...\n",
      "4091  27.040000  27.282901\n",
      "4092  43.640000  43.299740\n",
      "4093  31.880000  32.678494\n",
      "4094  47.294999  48.382162\n",
      "4095  48.965000  49.755649\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0203\n",
      "Epoch 8/25, Validation Loss: 0.0139\n",
      "         actual  predicted\n",
      "0     26.290000  26.208601\n",
      "1     36.965000  37.266371\n",
      "2     28.790000  28.845027\n",
      "3     53.515000  55.811273\n",
      "4     31.255000  31.498352\n",
      "...         ...        ...\n",
      "4091  27.040000  27.412969\n",
      "4092  43.640000  43.646440\n",
      "4093  31.880000  32.849843\n",
      "4094  47.294999  48.213022\n",
      "4095  48.965000  49.564372\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0185\n",
      "Epoch 9/25, Validation Loss: 0.0124\n",
      "         actual  predicted\n",
      "0     26.290000  26.081619\n",
      "1     36.965000  37.184923\n",
      "2     28.790000  28.588122\n",
      "3     53.515000  55.341352\n",
      "4     31.255000  31.369882\n",
      "...         ...        ...\n",
      "4091  27.040000  27.219405\n",
      "4092  43.640000  43.463351\n",
      "4093  31.880000  32.603894\n",
      "4094  47.294999  47.905376\n",
      "4095  48.965000  49.337144\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0172\n",
      "Epoch 10/25, Validation Loss: 0.0112\n",
      "         actual  predicted\n",
      "0     26.290000  26.134758\n",
      "1     36.965000  37.221061\n",
      "2     28.790000  28.736514\n",
      "3     53.515000  55.155090\n",
      "4     31.255000  31.377391\n",
      "...         ...        ...\n",
      "4091  27.040000  27.259402\n",
      "4092  43.640000  43.700222\n",
      "4093  31.880000  32.701007\n",
      "4094  47.294999  47.835073\n",
      "4095  48.965000  49.239157\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0161\n",
      "Epoch 11/25, Validation Loss: 0.0102\n",
      "         actual  predicted\n",
      "0     26.290000  26.069419\n",
      "1     36.965000  37.187721\n",
      "2     28.790000  28.624965\n",
      "3     53.515000  55.405458\n",
      "4     31.255000  31.410095\n",
      "...         ...        ...\n",
      "4091  27.040000  27.181001\n",
      "4092  43.640000  43.940107\n",
      "4093  31.880000  32.602532\n",
      "4094  47.294999  47.926467\n",
      "4095  48.965000  49.318757\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0149\n",
      "Epoch 12/25, Validation Loss: 0.0096\n",
      "         actual  predicted\n",
      "0     26.290000  26.136993\n",
      "1     36.965000  37.282998\n",
      "2     28.790000  28.694799\n",
      "3     53.515000  55.007482\n",
      "4     31.255000  31.298674\n",
      "...         ...        ...\n",
      "4091  27.040000  27.291335\n",
      "4092  43.640000  43.796266\n",
      "4093  31.880000  32.309763\n",
      "4094  47.294999  48.017270\n",
      "4095  48.965000  49.446571\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0142\n",
      "Epoch 13/25, Validation Loss: 0.0090\n",
      "         actual  predicted\n",
      "0     26.290000  25.775118\n",
      "1     36.965000  37.282633\n",
      "2     28.790000  28.742004\n",
      "3     53.515000  55.253597\n",
      "4     31.255000  31.372817\n",
      "...         ...        ...\n",
      "4091  27.040000  26.994325\n",
      "4092  43.640000  44.158135\n",
      "4093  31.880000  32.613610\n",
      "4094  47.294999  48.073502\n",
      "4095  48.965000  49.545187\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0135\n",
      "Epoch 14/25, Validation Loss: 0.0088\n",
      "         actual  predicted\n",
      "0     26.290000  26.012993\n",
      "1     36.965000  37.108393\n",
      "2     28.790000  28.715338\n",
      "3     53.515000  54.491770\n",
      "4     31.255000  31.443766\n",
      "...         ...        ...\n",
      "4091  27.040000  27.224341\n",
      "4092  43.640000  43.629428\n",
      "4093  31.880000  32.338488\n",
      "4094  47.294999  47.617462\n",
      "4095  48.965000  49.021269\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0128\n",
      "Epoch 15/25, Validation Loss: 0.0080\n",
      "         actual  predicted\n",
      "0     26.290000  26.169264\n",
      "1     36.965000  37.276240\n",
      "2     28.790000  28.913147\n",
      "3     53.515000  55.123979\n",
      "4     31.255000  31.540853\n",
      "...         ...        ...\n",
      "4091  27.040000  27.312766\n",
      "4092  43.640000  44.005758\n",
      "4093  31.880000  32.555590\n",
      "4094  47.294999  48.104153\n",
      "4095  48.965000  49.568249\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0122\n",
      "Epoch 16/25, Validation Loss: 0.0078\n",
      "         actual  predicted\n",
      "0     26.290000  25.967061\n",
      "1     36.965000  37.122181\n",
      "2     28.790000  28.663594\n",
      "3     53.515000  55.165624\n",
      "4     31.255000  31.454477\n",
      "...         ...        ...\n",
      "4091  27.040000  27.081766\n",
      "4092  43.640000  43.897048\n",
      "4093  31.880000  32.358602\n",
      "4094  47.294999  48.067899\n",
      "4095  48.965000  49.479530\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0116\n",
      "Epoch 17/25, Validation Loss: 0.0074\n",
      "         actual  predicted\n",
      "0     26.290000  26.032313\n",
      "1     36.965000  37.065385\n",
      "2     28.790000  28.559323\n",
      "3     53.515000  54.835227\n",
      "4     31.255000  31.350605\n",
      "...         ...        ...\n",
      "4091  27.040000  27.074078\n",
      "4092  43.640000  43.935710\n",
      "4093  31.880000  32.389678\n",
      "4094  47.294999  47.981644\n",
      "4095  48.965000  49.475436\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0112\n",
      "Epoch 18/25, Validation Loss: 0.0070\n",
      "         actual  predicted\n",
      "0     26.290000  25.927982\n",
      "1     36.965000  37.101535\n",
      "2     28.790000  28.614953\n",
      "3     53.515000  54.477855\n",
      "4     31.255000  31.402358\n",
      "...         ...        ...\n",
      "4091  27.040000  26.921183\n",
      "4092  43.640000  43.821334\n",
      "4093  31.880000  32.366202\n",
      "4094  47.294999  47.678689\n",
      "4095  48.965000  49.126103\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0108\n",
      "Epoch 19/25, Validation Loss: 0.0066\n",
      "         actual  predicted\n",
      "0     26.290000  26.031700\n",
      "1     36.965000  37.112640\n",
      "2     28.790000  28.745906\n",
      "3     53.515000  54.769039\n",
      "4     31.255000  31.289703\n",
      "...         ...        ...\n",
      "4091  27.040000  27.081204\n",
      "4092  43.640000  43.872975\n",
      "4093  31.880000  32.389020\n",
      "4094  47.294999  47.789461\n",
      "4095  48.965000  49.308200\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0103\n",
      "Epoch 20/25, Validation Loss: 0.0064\n",
      "         actual  predicted\n",
      "0     26.290000  26.050293\n",
      "1     36.965000  36.974240\n",
      "2     28.790000  28.874272\n",
      "3     53.515000  54.122639\n",
      "4     31.255000  31.362597\n",
      "...         ...        ...\n",
      "4091  27.040000  27.170406\n",
      "4092  43.640000  43.587775\n",
      "4093  31.880000  32.313433\n",
      "4094  47.294999  47.397427\n",
      "4095  48.965000  48.933872\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0100\n",
      "Epoch 21/25, Validation Loss: 0.0062\n",
      "         actual  predicted\n",
      "0     26.290000  26.201950\n",
      "1     36.965000  37.179249\n",
      "2     28.790000  28.992065\n",
      "3     53.515000  53.886703\n",
      "4     31.255000  31.622059\n",
      "...         ...        ...\n",
      "4091  27.040000  27.295746\n",
      "4092  43.640000  43.636007\n",
      "4093  31.880000  32.326100\n",
      "4094  47.294999  47.278965\n",
      "4095  48.965000  48.777786\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0099\n",
      "Epoch 22/25, Validation Loss: 0.0063\n",
      "         actual  predicted\n",
      "0     26.290000  26.127908\n",
      "1     36.965000  36.976289\n",
      "2     28.790000  28.902808\n",
      "3     53.515000  53.879278\n",
      "4     31.255000  31.441526\n",
      "...         ...        ...\n",
      "4091  27.040000  27.153789\n",
      "4092  43.640000  43.553522\n",
      "4093  31.880000  32.203359\n",
      "4094  47.294999  47.400709\n",
      "4095  48.965000  48.938307\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0094\n",
      "Epoch 23/25, Validation Loss: 0.0057\n",
      "         actual  predicted\n",
      "0     26.290000  26.106621\n",
      "1     36.965000  36.947823\n",
      "2     28.790000  28.752907\n",
      "3     53.515000  54.389422\n",
      "4     31.255000  31.314912\n",
      "...         ...        ...\n",
      "4091  27.040000  27.056197\n",
      "4092  43.640000  43.708339\n",
      "4093  31.880000  32.254174\n",
      "4094  47.294999  47.749675\n",
      "4095  48.965000  49.298612\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0089\n",
      "Epoch 24/25, Validation Loss: 0.0055\n",
      "         actual  predicted\n",
      "0     26.290000  26.008683\n",
      "1     36.965000  36.954872\n",
      "2     28.790000  28.968626\n",
      "3     53.515000  54.069497\n",
      "4     31.255000  31.335605\n",
      "...         ...        ...\n",
      "4091  27.040000  27.091967\n",
      "4092  43.640000  43.654639\n",
      "4093  31.880000  32.248524\n",
      "4094  47.294999  47.509969\n",
      "4095  48.965000  49.078618\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0087\n",
      "Epoch 25/25, Validation Loss: 0.0053\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4369,  1.6490,  1.2161, -0.7247, -0.1103, -0.1009, -1.7508,\n",
      "          -0.5681, -1.9687, -1.9687, -0.8792,  4.1508,  0.0000, -0.9065,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -0.9411,\n",
      "          -1.0386, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4441,  1.6207,  1.1753, -0.6836, -0.1333, -0.1076, -1.6197,\n",
      "          -0.5681, -2.0056, -2.0056, -0.0644,  4.1508,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0007,\n",
      "          -0.9729, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4514,  1.5924,  1.1345, -0.6426, -0.1562, -0.1142, -1.4886,\n",
      "          -0.5681, -2.0425, -2.0425,  0.7504,  4.1508,  0.0000, -0.9062,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0603,\n",
      "          -0.9071, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4586,  1.5641,  1.0937, -0.6016, -0.1791, -0.1209, -1.3575,\n",
      "          -0.5681, -2.0794, -2.0794,  1.5652,  4.1508,  0.0000, -0.9061,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1199,\n",
      "          -0.8414, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4659,  1.5358,  1.0529, -0.5605, -0.2021, -0.1275, -1.2264,\n",
      "          -0.5681, -2.1163, -2.1163,  2.3800,  4.1508,  0.0000, -0.9059,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1795,\n",
      "          -0.7757, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.6707,  2.1820,  1.2569, -1.2558,  0.3838,  0.0256, -0.5708,\n",
      "          -0.3964,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9077,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.7766,\n",
      "          -1.1809, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.1923,  1.7008,  3.6011, -1.2260,  0.0849, -0.0560, -1.2592,\n",
      "          -0.4694,  0.8735,  0.8735, -0.3360, -0.7989,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.8472,\n",
      "          -1.1308, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.0926,  1.2596,  2.2197, -1.1127,  0.0272, -0.0676, -0.7894,\n",
      "          -0.3392,  1.2980,  1.2980,  1.7010, -0.7989,  0.0000, -0.9046,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.9160,\n",
      "          -1.0749, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2006,  1.1195,  1.8444, -1.1076,  0.0108, -0.0809, -0.5708,\n",
      "          -0.4822,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9043,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1995,\n",
      "          -0.7444, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2369,  0.9374,  0.9469, -1.1114,  0.0096, -0.0809, -1.8819,\n",
      "          -0.5681, -1.6549, -1.6549,  2.3800, -0.7989,  0.0000, -0.9042,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0351,\n",
      "          -0.9609, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2872,  0.8395,  0.8979, -1.0392, -0.0206, -0.0876, -1.8819,\n",
      "          -0.4822,  1.7594,  1.7594, -1.0150, -0.7989,  0.0000, -0.9041,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0958,\n",
      "          -0.8907, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3347,  0.7615,  0.8653, -1.0354, -0.0282, -0.0942, -1.2264,\n",
      "          -0.3105,  1.5748,  1.5748, -1.6940, -0.7989,  0.0000, -0.9039,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1517,\n",
      "          -0.8166, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3542,  0.8125,  0.8000, -0.9823, -0.0332, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -0.3360, -0.7989,  0.0000, -0.9038,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3532,\n",
      "          -0.4007, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3514,  0.8187,  0.7511, -0.8189, -0.0345, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -2.3730, -0.7989,  0.0000, -0.9037,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.2486,\n",
      "          -0.6584, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3682,  0.9183,  0.6178, -0.8569, -0.0887, -0.1153, -1.2264,\n",
      "          -0.3964,  1.5287,  1.5287,  0.1166, -0.7989,  0.0000, -0.9034,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3050,\n",
      "          -0.5368, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3919,  1.0263,  1.1713, -0.6042, -0.0465, -0.1042, -1.5541,\n",
      "          -0.5681,  1.6210,  1.6210,  1.7010, -0.7989,  0.0000, -0.9031,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3402,\n",
      "          -0.4420, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4268,  0.9811,  0.7021, -0.7657, -0.0358, -0.1042, -1.8819,\n",
      "          -0.5681,  1.7132,  1.7132, -1.0150, -0.7989,  0.0000, -0.9029,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3679,\n",
      "          -0.3469, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4156,  1.0019,  0.7103, -0.7429, -0.0383, -0.1042, -1.8819,\n",
      "          -0.5681,  1.5287,  1.5287, -0.3360, -0.7989,  0.0000, -0.9027,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3878,\n",
      "          -0.2562, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4184,  1.1060,  0.7755, -0.6783, -0.0358, -0.1009, -0.5708,\n",
      "          -0.5681,  1.5287,  1.5287,  0.3430, -0.7989,  0.0000, -0.9026,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4017,\n",
      "          -0.1644, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4017,  1.1528,  0.8000, -0.6099, -0.0358, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4825,  1.4825, -2.3730, -0.7989,  0.0000, -0.9025,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4096,\n",
      "          -0.0719, -1.1552,  1.1552,  0.0000]]])\n",
      "predicted: tensor([[1.1190]], device='cuda:0')\n",
      "[46.26]\n",
      "          actual  predicted\n",
      "0      26.290000  26.008683\n",
      "1      36.965000  36.954872\n",
      "2      28.790000  28.968626\n",
      "3      53.515000  54.069497\n",
      "4      31.255000  31.335605\n",
      "...          ...        ...\n",
      "38802  55.450001  55.860978\n",
      "38803  23.785000  23.964670\n",
      "38804  46.990000  47.029620\n",
      "38805  42.130000  41.873898\n",
      "38806  45.655001  45.483709\n",
      "\n",
      "[38807 rows x 2 columns]\n",
      "Score (RMSE): 0.7036\n",
      "Score (MAE): 0.4102\n",
      "Score (ME): 0.0867\n",
      "Score (MAPE): 1.1357%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (199269, 27) to (200228, 27)\n",
      "training data cutoff:  2023-07-14 02:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([154985, 20, 25]) torch.Size([154985]) torch.Size([154985, 1])\n",
      "Testing data shape: torch.Size([39048, 20, 25]) torch.Size([39048]) torch.Size([39048, 1])\n",
      "Shuffled Training data shape: torch.Size([155226, 20, 25]) torch.Size([155226]) torch.Size([155226, 1])\n",
      "Shuffled Testing data shape: torch.Size([38807, 20, 25]) torch.Size([38807]) torch.Size([38807, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual  predicted\n",
      "0     22.765  21.388666\n",
      "1     19.945  19.356305\n",
      "2     25.035  24.529508\n",
      "3     26.715  27.600684\n",
      "4     29.340  29.084450\n",
      "...      ...        ...\n",
      "4091  23.370  23.174994\n",
      "4092  23.810  21.938573\n",
      "4093  20.400  19.632373\n",
      "4094  20.785  20.474122\n",
      "4095  23.430  23.204627\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.4798\n",
      "Epoch 1/25, Validation Loss: 0.1217\n",
      "      actual  predicted\n",
      "0     22.765  23.073304\n",
      "1     19.945  20.477971\n",
      "2     25.035  25.154613\n",
      "3     26.715  26.412330\n",
      "4     29.340  29.317776\n",
      "...      ...        ...\n",
      "4091  23.370  23.281216\n",
      "4092  23.810  23.320644\n",
      "4093  20.400  20.348491\n",
      "4094  20.785  21.151840\n",
      "4095  23.430  23.789301\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.0757\n",
      "Epoch 2/25, Validation Loss: 0.0499\n",
      "      actual  predicted\n",
      "0     22.765  23.032317\n",
      "1     19.945  20.259536\n",
      "2     25.035  24.844553\n",
      "3     26.715  26.617728\n",
      "4     29.340  29.321241\n",
      "...      ...        ...\n",
      "4091  23.370  23.193943\n",
      "4092  23.810  23.362145\n",
      "4093  20.400  20.643900\n",
      "4094  20.785  21.117634\n",
      "4095  23.430  23.680031\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.0448\n",
      "Epoch 3/25, Validation Loss: 0.0345\n",
      "      actual  predicted\n",
      "0     22.765  22.989004\n",
      "1     19.945  20.182749\n",
      "2     25.035  24.884001\n",
      "3     26.715  26.592437\n",
      "4     29.340  29.303098\n",
      "...      ...        ...\n",
      "4091  23.370  23.302605\n",
      "4092  23.810  23.396593\n",
      "4093  20.400  20.795810\n",
      "4094  20.785  21.246803\n",
      "4095  23.430  23.550530\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0355\n",
      "Epoch 4/25, Validation Loss: 0.0272\n",
      "      actual  predicted\n",
      "0     22.765  22.928073\n",
      "1     19.945  20.096135\n",
      "2     25.035  24.946422\n",
      "3     26.715  26.622449\n",
      "4     29.340  29.380341\n",
      "...      ...        ...\n",
      "4091  23.370  23.302136\n",
      "4092  23.810  23.466922\n",
      "4093  20.400  20.941062\n",
      "4094  20.785  21.129245\n",
      "4095  23.430  23.584660\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0295\n",
      "Epoch 5/25, Validation Loss: 0.0218\n",
      "      actual  predicted\n",
      "0     22.765  22.942909\n",
      "1     19.945  19.952867\n",
      "2     25.035  24.893715\n",
      "3     26.715  26.603961\n",
      "4     29.340  29.260932\n",
      "...      ...        ...\n",
      "4091  23.370  23.278467\n",
      "4092  23.810  23.472373\n",
      "4093  20.400  20.923471\n",
      "4094  20.785  21.105407\n",
      "4095  23.430  23.497641\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0252\n",
      "Epoch 6/25, Validation Loss: 0.0178\n",
      "      actual  predicted\n",
      "0     22.765  22.899384\n",
      "1     19.945  19.909008\n",
      "2     25.035  24.903640\n",
      "3     26.715  26.596140\n",
      "4     29.340  29.294214\n",
      "...      ...        ...\n",
      "4091  23.370  23.269803\n",
      "4092  23.810  23.513033\n",
      "4093  20.400  20.961809\n",
      "4094  20.785  21.037166\n",
      "4095  23.430  23.516006\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0222\n",
      "Epoch 7/25, Validation Loss: 0.0154\n",
      "      actual  predicted\n",
      "0     22.765  22.953283\n",
      "1     19.945  19.886538\n",
      "2     25.035  24.890020\n",
      "3     26.715  26.667564\n",
      "4     29.340  29.287375\n",
      "...      ...        ...\n",
      "4091  23.370  23.254665\n",
      "4092  23.810  23.554486\n",
      "4093  20.400  20.983042\n",
      "4094  20.785  20.977627\n",
      "4095  23.430  23.532771\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0200\n",
      "Epoch 8/25, Validation Loss: 0.0134\n",
      "      actual  predicted\n",
      "0     22.765  22.967008\n",
      "1     19.945  19.913044\n",
      "2     25.035  24.932855\n",
      "3     26.715  26.727494\n",
      "4     29.340  29.459378\n",
      "...      ...        ...\n",
      "4091  23.370  23.298729\n",
      "4092  23.810  23.597906\n",
      "4093  20.400  20.911419\n",
      "4094  20.785  20.962105\n",
      "4095  23.430  23.491974\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0180\n",
      "Epoch 9/25, Validation Loss: 0.0122\n",
      "      actual  predicted\n",
      "0     22.765  22.966931\n",
      "1     19.945  19.882993\n",
      "2     25.035  25.022980\n",
      "3     26.715  26.727020\n",
      "4     29.340  29.336130\n",
      "...      ...        ...\n",
      "4091  23.370  23.268395\n",
      "4092  23.810  23.606144\n",
      "4093  20.400  20.852295\n",
      "4094  20.785  20.949680\n",
      "4095  23.430  23.483594\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0167\n",
      "Epoch 10/25, Validation Loss: 0.0107\n",
      "      actual  predicted\n",
      "0     22.765  22.895533\n",
      "1     19.945  19.854054\n",
      "2     25.035  25.024785\n",
      "3     26.715  26.594261\n",
      "4     29.340  29.259301\n",
      "...      ...        ...\n",
      "4091  23.370  23.253098\n",
      "4092  23.810  23.680426\n",
      "4093  20.400  20.828456\n",
      "4094  20.785  20.931835\n",
      "4095  23.430  23.436019\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0156\n",
      "Epoch 11/25, Validation Loss: 0.0099\n",
      "      actual  predicted\n",
      "0     22.765  23.001178\n",
      "1     19.945  19.986024\n",
      "2     25.035  25.014481\n",
      "3     26.715  26.675043\n",
      "4     29.340  29.282186\n",
      "...      ...        ...\n",
      "4091  23.370  23.250452\n",
      "4092  23.810  23.613850\n",
      "4093  20.400  20.762299\n",
      "4094  20.785  20.911715\n",
      "4095  23.430  23.542686\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0144\n",
      "Epoch 12/25, Validation Loss: 0.0090\n",
      "      actual  predicted\n",
      "0     22.765  22.983837\n",
      "1     19.945  19.818705\n",
      "2     25.035  24.877487\n",
      "3     26.715  26.671588\n",
      "4     29.340  29.351746\n",
      "...      ...        ...\n",
      "4091  23.370  23.225677\n",
      "4092  23.810  23.665483\n",
      "4093  20.400  20.647783\n",
      "4094  20.785  20.829735\n",
      "4095  23.430  23.424062\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0135\n",
      "Epoch 13/25, Validation Loss: 0.0087\n",
      "      actual  predicted\n",
      "0     22.765  23.030109\n",
      "1     19.945  19.937949\n",
      "2     25.035  24.921827\n",
      "3     26.715  26.674912\n",
      "4     29.340  29.302559\n",
      "...      ...        ...\n",
      "4091  23.370  23.259613\n",
      "4092  23.810  23.619554\n",
      "4093  20.400  20.596790\n",
      "4094  20.785  20.905600\n",
      "4095  23.430  23.415280\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0126\n",
      "Epoch 14/25, Validation Loss: 0.0079\n",
      "      actual  predicted\n",
      "0     22.765  22.993758\n",
      "1     19.945  19.973030\n",
      "2     25.035  25.015769\n",
      "3     26.715  26.671435\n",
      "4     29.340  29.290518\n",
      "...      ...        ...\n",
      "4091  23.370  23.237515\n",
      "4092  23.810  23.681264\n",
      "4093  20.400  20.623156\n",
      "4094  20.785  20.877926\n",
      "4095  23.430  23.457906\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0120\n",
      "Epoch 15/25, Validation Loss: 0.0074\n",
      "      actual  predicted\n",
      "0     22.765  22.926601\n",
      "1     19.945  19.867738\n",
      "2     25.035  25.023940\n",
      "3     26.715  26.765255\n",
      "4     29.340  29.370544\n",
      "...      ...        ...\n",
      "4091  23.370  23.244915\n",
      "4092  23.810  23.597577\n",
      "4093  20.400  20.572494\n",
      "4094  20.785  20.810635\n",
      "4095  23.430  23.451176\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0115\n",
      "Epoch 16/25, Validation Loss: 0.0072\n",
      "      actual  predicted\n",
      "0     22.765  22.992372\n",
      "1     19.945  20.030536\n",
      "2     25.035  24.968753\n",
      "3     26.715  26.663936\n",
      "4     29.340  29.275975\n",
      "...      ...        ...\n",
      "4091  23.370  23.255532\n",
      "4092  23.810  23.659660\n",
      "4093  20.400  20.656724\n",
      "4094  20.785  20.911811\n",
      "4095  23.430  23.429306\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0108\n",
      "Epoch 17/25, Validation Loss: 0.0067\n",
      "      actual  predicted\n",
      "0     22.765  23.064249\n",
      "1     19.945  20.005380\n",
      "2     25.035  25.000099\n",
      "3     26.715  26.782994\n",
      "4     29.340  29.383955\n",
      "...      ...        ...\n",
      "4091  23.370  23.284759\n",
      "4092  23.810  23.663551\n",
      "4093  20.400  20.580842\n",
      "4094  20.785  20.887908\n",
      "4095  23.430  23.409847\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0105\n",
      "Epoch 18/25, Validation Loss: 0.0069\n",
      "      actual  predicted\n",
      "0     22.765  22.913210\n",
      "1     19.945  19.981929\n",
      "2     25.035  25.087739\n",
      "3     26.715  26.631622\n",
      "4     29.340  29.302178\n",
      "...      ...        ...\n",
      "4091  23.370  23.259627\n",
      "4092  23.810  23.679927\n",
      "4093  20.400  20.630147\n",
      "4094  20.785  20.854285\n",
      "4095  23.430  23.448792\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0100\n",
      "Epoch 19/25, Validation Loss: 0.0061\n",
      "      actual  predicted\n",
      "0     22.765  23.056592\n",
      "1     19.945  20.054864\n",
      "2     25.035  24.929359\n",
      "3     26.715  26.770330\n",
      "4     29.340  29.330277\n",
      "...      ...        ...\n",
      "4091  23.370  23.289686\n",
      "4092  23.810  23.710103\n",
      "4093  20.400  20.607743\n",
      "4094  20.785  20.904731\n",
      "4095  23.430  23.402567\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0096\n",
      "Epoch 20/25, Validation Loss: 0.0059\n",
      "      actual  predicted\n",
      "0     22.765  22.977797\n",
      "1     19.945  20.027192\n",
      "2     25.035  25.024776\n",
      "3     26.715  26.733514\n",
      "4     29.340  29.302351\n",
      "...      ...        ...\n",
      "4091  23.370  23.254157\n",
      "4092  23.810  23.683104\n",
      "4093  20.400  20.608735\n",
      "4094  20.785  20.895927\n",
      "4095  23.430  23.438515\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0092\n",
      "Epoch 21/25, Validation Loss: 0.0057\n",
      "      actual  predicted\n",
      "0     22.765  22.973597\n",
      "1     19.945  19.933409\n",
      "2     25.035  24.972694\n",
      "3     26.715  26.679397\n",
      "4     29.340  29.311313\n",
      "...      ...        ...\n",
      "4091  23.370  23.325647\n",
      "4092  23.810  23.673037\n",
      "4093  20.400  20.557785\n",
      "4094  20.785  20.892711\n",
      "4095  23.430  23.389401\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0089\n",
      "Epoch 22/25, Validation Loss: 0.0055\n",
      "      actual  predicted\n",
      "0     22.765  22.965886\n",
      "1     19.945  20.086566\n",
      "2     25.035  25.082645\n",
      "3     26.715  26.697687\n",
      "4     29.340  29.284813\n",
      "...      ...        ...\n",
      "4091  23.370  23.312185\n",
      "4092  23.810  23.721795\n",
      "4093  20.400  20.634323\n",
      "4094  20.785  20.916483\n",
      "4095  23.430  23.463772\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0086\n",
      "Epoch 23/25, Validation Loss: 0.0055\n",
      "      actual  predicted\n",
      "0     22.765  22.976479\n",
      "1     19.945  19.916136\n",
      "2     25.035  25.131167\n",
      "3     26.715  26.871190\n",
      "4     29.340  29.472054\n",
      "...      ...        ...\n",
      "4091  23.370  23.279801\n",
      "4092  23.810  23.622326\n",
      "4093  20.400  20.449435\n",
      "4094  20.785  20.778889\n",
      "4095  23.430  23.440054\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0084\n",
      "Epoch 24/25, Validation Loss: 0.0061\n",
      "      actual  predicted\n",
      "0     22.765  22.954766\n",
      "1     19.945  19.921950\n",
      "2     25.035  25.012466\n",
      "3     26.715  26.782286\n",
      "4     29.340  29.377027\n",
      "...      ...        ...\n",
      "4091  23.370  23.303668\n",
      "4092  23.810  23.682784\n",
      "4093  20.400  20.428074\n",
      "4094  20.785  20.818805\n",
      "4095  23.430  23.408696\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0083\n",
      "Epoch 25/25, Validation Loss: 0.0051\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4369,  1.6490,  1.2161, -0.7247, -0.1103, -0.1009, -1.7508,\n",
      "          -0.5681, -1.9687, -1.9687, -0.8792,  4.1508,  0.0000, -0.9065,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -0.9411,\n",
      "          -1.0386, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4441,  1.6207,  1.1753, -0.6836, -0.1333, -0.1076, -1.6197,\n",
      "          -0.5681, -2.0056, -2.0056, -0.0644,  4.1508,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0007,\n",
      "          -0.9729, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4514,  1.5924,  1.1345, -0.6426, -0.1562, -0.1142, -1.4886,\n",
      "          -0.5681, -2.0425, -2.0425,  0.7504,  4.1508,  0.0000, -0.9062,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0603,\n",
      "          -0.9071, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4586,  1.5641,  1.0937, -0.6016, -0.1791, -0.1209, -1.3575,\n",
      "          -0.5681, -2.0794, -2.0794,  1.5652,  4.1508,  0.0000, -0.9061,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1199,\n",
      "          -0.8414, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4659,  1.5358,  1.0529, -0.5605, -0.2021, -0.1275, -1.2264,\n",
      "          -0.5681, -2.1163, -2.1163,  2.3800,  4.1508,  0.0000, -0.9059,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1795,\n",
      "          -0.7757, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.6707,  2.1820,  1.2569, -1.2558,  0.3838,  0.0256, -0.5708,\n",
      "          -0.3964,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9077,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.7766,\n",
      "          -1.1809, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.1923,  1.7008,  3.6011, -1.2260,  0.0849, -0.0560, -1.2592,\n",
      "          -0.4694,  0.8735,  0.8735, -0.3360, -0.7989,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.8472,\n",
      "          -1.1308, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.0926,  1.2596,  2.2197, -1.1127,  0.0272, -0.0676, -0.7894,\n",
      "          -0.3392,  1.2980,  1.2980,  1.7010, -0.7989,  0.0000, -0.9046,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.9160,\n",
      "          -1.0749, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2006,  1.1195,  1.8444, -1.1076,  0.0108, -0.0809, -0.5708,\n",
      "          -0.4822,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9043,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1995,\n",
      "          -0.7444, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2369,  0.9374,  0.9469, -1.1114,  0.0096, -0.0809, -1.8819,\n",
      "          -0.5681, -1.6549, -1.6549,  2.3800, -0.7989,  0.0000, -0.9042,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0351,\n",
      "          -0.9609, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2872,  0.8395,  0.8979, -1.0392, -0.0206, -0.0876, -1.8819,\n",
      "          -0.4822,  1.7594,  1.7594, -1.0150, -0.7989,  0.0000, -0.9041,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0958,\n",
      "          -0.8907, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3347,  0.7615,  0.8653, -1.0354, -0.0282, -0.0942, -1.2264,\n",
      "          -0.3105,  1.5748,  1.5748, -1.6940, -0.7989,  0.0000, -0.9039,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1517,\n",
      "          -0.8166, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3542,  0.8125,  0.8000, -0.9823, -0.0332, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -0.3360, -0.7989,  0.0000, -0.9038,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3532,\n",
      "          -0.4007, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3514,  0.8187,  0.7511, -0.8189, -0.0345, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -2.3730, -0.7989,  0.0000, -0.9037,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.2486,\n",
      "          -0.6584, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3682,  0.9183,  0.6178, -0.8569, -0.0887, -0.1153, -1.2264,\n",
      "          -0.3964,  1.5287,  1.5287,  0.1166, -0.7989,  0.0000, -0.9034,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3050,\n",
      "          -0.5368, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3919,  1.0263,  1.1713, -0.6042, -0.0465, -0.1042, -1.5541,\n",
      "          -0.5681,  1.6210,  1.6210,  1.7010, -0.7989,  0.0000, -0.9031,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3402,\n",
      "          -0.4420, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4268,  0.9811,  0.7021, -0.7657, -0.0358, -0.1042, -1.8819,\n",
      "          -0.5681,  1.7132,  1.7132, -1.0150, -0.7989,  0.0000, -0.9029,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3679,\n",
      "          -0.3469, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4156,  1.0019,  0.7103, -0.7429, -0.0383, -0.1042, -1.8819,\n",
      "          -0.5681,  1.5287,  1.5287, -0.3360, -0.7989,  0.0000, -0.9027,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3878,\n",
      "          -0.2562, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4184,  1.1060,  0.7755, -0.6783, -0.0358, -0.1009, -0.5708,\n",
      "          -0.5681,  1.5287,  1.5287,  0.3430, -0.7989,  0.0000, -0.9026,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4017,\n",
      "          -0.1644, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4017,  1.1528,  0.8000, -0.6099, -0.0358, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4825,  1.4825, -2.3730, -0.7989,  0.0000, -0.9025,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4096,\n",
      "          -0.0719, -1.1552,  1.1552,  0.0000]]])\n",
      "predicted: tensor([[0.3824]], device='cuda:0')\n",
      "[25.41]\n",
      "          actual  predicted\n",
      "0      22.765000  22.954766\n",
      "1      19.945000  19.921950\n",
      "2      25.035000  25.012466\n",
      "3      26.715000  26.782286\n",
      "4      29.340000  29.377027\n",
      "...          ...        ...\n",
      "38802  28.110000  28.157812\n",
      "38803  25.190000  25.148853\n",
      "38804  25.225000  25.345161\n",
      "38805  27.013333  26.645625\n",
      "38806  23.888182  24.108300\n",
      "\n",
      "[38807 rows x 2 columns]\n",
      "Score (RMSE): 0.2573\n",
      "Score (MAE): 0.1444\n",
      "Score (ME): -0.0184\n",
      "Score (MAPE): 0.5907%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (199269, 27) to (200228, 27)\n",
      "training data cutoff:  2023-07-14 02:30:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([154985, 20, 25]) torch.Size([154985]) torch.Size([154985, 1])\n",
      "Testing data shape: torch.Size([39048, 20, 25]) torch.Size([39048]) torch.Size([39048, 1])\n",
      "Shuffled Training data shape: torch.Size([155226, 20, 25]) torch.Size([155226]) torch.Size([155226, 1])\n",
      "Shuffled Testing data shape: torch.Size([38807, 20, 25]) torch.Size([38807]) torch.Size([38807, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0       5.999996   53.336186\n",
      "1      26.000004   51.029972\n",
      "2      10.000002   21.832642\n",
      "3       3.999999  -67.823688\n",
      "4       5.999996   67.436895\n",
      "...          ...         ...\n",
      "4091  118.499998  226.427087\n",
      "4092  236.000000  499.197422\n",
      "4093    5.999996   10.566425\n",
      "4094    8.000005  -59.088111\n",
      "4095    5.999996  -17.836826\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.9117\n",
      "Epoch 1/25, Validation Loss: 0.5982\n",
      "          actual   predicted\n",
      "0       5.999996   75.603636\n",
      "1      26.000004   61.763713\n",
      "2      10.000002   46.060714\n",
      "3       3.999999   25.327921\n",
      "4       5.999996  -27.046709\n",
      "...          ...         ...\n",
      "4091  118.499998  120.308919\n",
      "4092  236.000000  338.210175\n",
      "4093    5.999996   95.442096\n",
      "4094    8.000005   -7.663133\n",
      "4095    5.999996   13.472713\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.8004\n",
      "Epoch 2/25, Validation Loss: 0.4999\n",
      "          actual   predicted\n",
      "0       5.999996   43.199171\n",
      "1      26.000004   95.565333\n",
      "2      10.000002   27.121684\n",
      "3       3.999999   59.541986\n",
      "4       5.999996   -3.687362\n",
      "...          ...         ...\n",
      "4091  118.499998  181.012045\n",
      "4092  236.000000  202.082395\n",
      "4093    5.999996   54.367866\n",
      "4094    8.000005   -4.270652\n",
      "4095    5.999996   30.691089\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.6865\n",
      "Epoch 3/25, Validation Loss: 0.3943\n",
      "          actual   predicted\n",
      "0       5.999996   82.492567\n",
      "1      26.000004   84.099860\n",
      "2      10.000002    5.542214\n",
      "3       3.999999  -53.781824\n",
      "4       5.999996  -52.270149\n",
      "...          ...         ...\n",
      "4091  118.499998  146.865578\n",
      "4092  236.000000  153.835430\n",
      "4093    5.999996   65.946534\n",
      "4094    8.000005  -67.273824\n",
      "4095    5.999996  -18.246220\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.6047\n",
      "Epoch 4/25, Validation Loss: 0.3597\n",
      "          actual   predicted\n",
      "0       5.999996   62.763682\n",
      "1      26.000004   55.509760\n",
      "2      10.000002   34.426738\n",
      "3       3.999999  -73.537830\n",
      "4       5.999996  -34.871714\n",
      "...          ...         ...\n",
      "4091  118.499998  144.176314\n",
      "4092  236.000000  266.383815\n",
      "4093    5.999996   42.348591\n",
      "4094    8.000005  -88.348756\n",
      "4095    5.999996  -13.207011\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.5663\n",
      "Epoch 5/25, Validation Loss: 0.3536\n",
      "          actual   predicted\n",
      "0       5.999996   52.376787\n",
      "1      26.000004   54.784652\n",
      "2      10.000002   18.440717\n",
      "3       3.999999  -58.291443\n",
      "4       5.999996  -29.949721\n",
      "...          ...         ...\n",
      "4091  118.499998  163.996387\n",
      "4092  236.000000  222.694776\n",
      "4093    5.999996   36.352479\n",
      "4094    8.000005  -67.857350\n",
      "4095    5.999996  -22.004306\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.5494\n",
      "Epoch 6/25, Validation Loss: 0.3518\n",
      "          actual   predicted\n",
      "0       5.999996   81.770557\n",
      "1      26.000004   88.097790\n",
      "2      10.000002   42.481952\n",
      "3       3.999999  -10.057809\n",
      "4       5.999996   21.196718\n",
      "...          ...         ...\n",
      "4091  118.499998   97.853929\n",
      "4092  236.000000  231.761297\n",
      "4093    5.999996   62.949489\n",
      "4094    8.000005  -25.781839\n",
      "4095    5.999996    1.701047\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.5316\n",
      "Epoch 7/25, Validation Loss: 0.3579\n",
      "          actual   predicted\n",
      "0       5.999996   88.557039\n",
      "1      26.000004   78.086562\n",
      "2      10.000002   59.602404\n",
      "3       3.999999  -11.938663\n",
      "4       5.999996    3.837386\n",
      "...          ...         ...\n",
      "4091  118.499998   60.432167\n",
      "4092  236.000000  199.852778\n",
      "4093    5.999996   61.574664\n",
      "4094    8.000005  -34.664888\n",
      "4095    5.999996   16.458557\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.5306\n",
      "Epoch 8/25, Validation Loss: 0.3420\n",
      "          actual   predicted\n",
      "0       5.999996   72.005902\n",
      "1      26.000004   61.497593\n",
      "2      10.000002   31.257145\n",
      "3       3.999999    3.781132\n",
      "4       5.999996   43.585725\n",
      "...          ...         ...\n",
      "4091  118.499998   72.095654\n",
      "4092  236.000000  174.223656\n",
      "4093    5.999996   67.788579\n",
      "4094    8.000005   -0.262650\n",
      "4095    5.999996   25.334462\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.5238\n",
      "Epoch 9/25, Validation Loss: 0.3365\n",
      "          actual   predicted\n",
      "0       5.999996   75.886386\n",
      "1      26.000004   75.712454\n",
      "2      10.000002   57.075289\n",
      "3       3.999999  -10.085109\n",
      "4       5.999996   30.206186\n",
      "...          ...         ...\n",
      "4091  118.499998   98.607419\n",
      "4092  236.000000  252.718935\n",
      "4093    5.999996   54.062335\n",
      "4094    8.000005  -13.916955\n",
      "4095    5.999996    3.915191\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.4869\n",
      "Epoch 10/25, Validation Loss: 0.3323\n",
      "          actual   predicted\n",
      "0       5.999996   66.747696\n",
      "1      26.000004   56.555421\n",
      "2      10.000002   34.013051\n",
      "3       3.999999   -2.492408\n",
      "4       5.999996   40.909505\n",
      "...          ...         ...\n",
      "4091  118.499998  118.174809\n",
      "4092  236.000000  212.434587\n",
      "4093    5.999996   54.049431\n",
      "4094    8.000005    1.443361\n",
      "4095    5.999996   10.824230\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.5020\n",
      "Epoch 11/25, Validation Loss: 0.3369\n",
      "          actual   predicted\n",
      "0       5.999996   95.826970\n",
      "1      26.000004   62.949311\n",
      "2      10.000002   48.415400\n",
      "3       3.999999   -1.250483\n",
      "4       5.999996   50.661965\n",
      "...          ...         ...\n",
      "4091  118.499998  108.331645\n",
      "4092  236.000000  209.155416\n",
      "4093    5.999996   83.842528\n",
      "4094    8.000005   13.871497\n",
      "4095    5.999996   31.442125\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.4601\n",
      "Epoch 12/25, Validation Loss: 0.3317\n",
      "          actual   predicted\n",
      "0       5.999996   59.277250\n",
      "1      26.000004   56.342445\n",
      "2      10.000002   52.446798\n",
      "3       3.999999  -15.819028\n",
      "4       5.999996   25.937410\n",
      "...          ...         ...\n",
      "4091  118.499998   94.290497\n",
      "4092  236.000000  238.333767\n",
      "4093    5.999996   42.658711\n",
      "4094    8.000005  -20.394860\n",
      "4095    5.999996  -11.295783\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.4708\n",
      "Epoch 13/25, Validation Loss: 0.3238\n",
      "          actual   predicted\n",
      "0       5.999996   71.619160\n",
      "1      26.000004   44.402027\n",
      "2      10.000002   76.349491\n",
      "3       3.999999   -0.404705\n",
      "4       5.999996   35.847467\n",
      "...          ...         ...\n",
      "4091  118.499998  133.495329\n",
      "4092  236.000000  226.062427\n",
      "4093    5.999996   61.923593\n",
      "4094    8.000005   11.644649\n",
      "4095    5.999996   24.152766\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.5070\n",
      "Epoch 14/25, Validation Loss: 0.3286\n",
      "          actual   predicted\n",
      "0       5.999996   60.091897\n",
      "1      26.000004   35.259920\n",
      "2      10.000002   15.288618\n",
      "3       3.999999  -21.798711\n",
      "4       5.999996   20.292935\n",
      "...          ...         ...\n",
      "4091  118.499998   83.056458\n",
      "4092  236.000000  204.451731\n",
      "4093    5.999996   59.006789\n",
      "4094    8.000005  -11.031662\n",
      "4095    5.999996  -10.811494\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.5009\n",
      "Epoch 15/25, Validation Loss: 0.3187\n",
      "          actual   predicted\n",
      "0       5.999996   59.872143\n",
      "1      26.000004   36.831563\n",
      "2      10.000002   55.028937\n",
      "3       3.999999   20.519076\n",
      "4       5.999996   65.437063\n",
      "...          ...         ...\n",
      "4091  118.499998   83.367868\n",
      "4092  236.000000  204.449230\n",
      "4093    5.999996   89.806090\n",
      "4094    8.000005   42.373347\n",
      "4095    5.999996   42.126069\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.4692\n",
      "Epoch 16/25, Validation Loss: 0.3202\n",
      "          actual   predicted\n",
      "0       5.999996   54.187512\n",
      "1      26.000004   61.003321\n",
      "2      10.000002   66.693618\n",
      "3       3.999999    9.720056\n",
      "4       5.999996   52.572057\n",
      "...          ...         ...\n",
      "4091  118.499998  190.713243\n",
      "4092  236.000000  248.127851\n",
      "4093    5.999996   52.714703\n",
      "4094    8.000005   14.758602\n",
      "4095    5.999996   19.406373\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.4640\n",
      "Epoch 17/25, Validation Loss: 0.3063\n",
      "          actual   predicted\n",
      "0       5.999996   47.077159\n",
      "1      26.000004   31.085319\n",
      "2      10.000002   43.960793\n",
      "3       3.999999    3.041594\n",
      "4       5.999996   18.748543\n",
      "...          ...         ...\n",
      "4091  118.499998  108.094469\n",
      "4092  236.000000  261.543477\n",
      "4093    5.999996   46.053132\n",
      "4094    8.000005   17.760497\n",
      "4095    5.999996   -8.658312\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.4765\n",
      "Epoch 18/25, Validation Loss: 0.3013\n",
      "          actual   predicted\n",
      "0       5.999996   13.919247\n",
      "1      26.000004    9.580449\n",
      "2      10.000002   31.197366\n",
      "3       3.999999  -27.909709\n",
      "4       5.999996   10.507486\n",
      "...          ...         ...\n",
      "4091  118.499998  113.734556\n",
      "4092  236.000000  275.903511\n",
      "4093    5.999996   19.361438\n",
      "4094    8.000005  -16.680750\n",
      "4095    5.999996  -26.942646\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.4477\n",
      "Epoch 19/25, Validation Loss: 0.3263\n",
      "          actual   predicted\n",
      "0       5.999996    7.934655\n",
      "1      26.000004   22.122785\n",
      "2      10.000002   58.170119\n",
      "3       3.999999   -9.716759\n",
      "4       5.999996   25.589381\n",
      "...          ...         ...\n",
      "4091  118.499998  142.458130\n",
      "4092  236.000000  304.567997\n",
      "4093    5.999996   28.447753\n",
      "4094    8.000005    2.636861\n",
      "4095    5.999996  -13.123031\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.4561\n",
      "Epoch 20/25, Validation Loss: 0.3115\n",
      "          actual   predicted\n",
      "0       5.999996   35.616974\n",
      "1      26.000004   57.611018\n",
      "2      10.000002   33.384874\n",
      "3       3.999999   -9.109565\n",
      "4       5.999996  -26.336906\n",
      "...          ...         ...\n",
      "4091  118.499998   32.980425\n",
      "4092  236.000000  185.008252\n",
      "4093    5.999996   18.339847\n",
      "4094    8.000005  -18.114171\n",
      "4095    5.999996  -29.361676\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.4756\n",
      "Epoch 21/25, Validation Loss: 0.3109\n",
      "          actual   predicted\n",
      "0       5.999996   26.526070\n",
      "1      26.000004   28.003325\n",
      "2      10.000002   32.164063\n",
      "3       3.999999  -15.021555\n",
      "4       5.999996    5.288313\n",
      "...          ...         ...\n",
      "4091  118.499998  112.747196\n",
      "4092  236.000000  166.720946\n",
      "4093    5.999996   25.899394\n",
      "4094    8.000005   -8.133855\n",
      "4095    5.999996  -10.013998\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.4290\n",
      "Epoch 22/25, Validation Loss: 0.3095\n",
      "          actual   predicted\n",
      "0       5.999996   21.572638\n",
      "1      26.000004   40.956509\n",
      "2      10.000002   57.297680\n",
      "3       3.999999   10.484291\n",
      "4       5.999996   25.285234\n",
      "...          ...         ...\n",
      "4091  118.499998  109.802922\n",
      "4092  236.000000  213.269271\n",
      "4093    5.999996   36.332182\n",
      "4094    8.000005   18.934102\n",
      "4095    5.999996    7.986580\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.3989\n",
      "Epoch 23/25, Validation Loss: 0.2883\n",
      "          actual   predicted\n",
      "0       5.999996   12.015494\n",
      "1      26.000004   24.623654\n",
      "2      10.000002   48.467739\n",
      "3       3.999999   -0.000599\n",
      "4       5.999996   15.159420\n",
      "...          ...         ...\n",
      "4091  118.499998  119.754845\n",
      "4092  236.000000  257.108452\n",
      "4093    5.999996   43.221207\n",
      "4094    8.000005   17.027262\n",
      "4095    5.999996   -7.776493\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.3656\n",
      "Epoch 24/25, Validation Loss: 0.2889\n",
      "          actual   predicted\n",
      "0       5.999996   35.189590\n",
      "1      26.000004   38.984438\n",
      "2      10.000002   42.843111\n",
      "3       3.999999   11.781192\n",
      "4       5.999996   18.809363\n",
      "...          ...         ...\n",
      "4091  118.499998  142.300331\n",
      "4092  236.000000  214.561308\n",
      "4093    5.999996   43.491124\n",
      "4094    8.000005   23.961767\n",
      "4095    5.999996   -3.850767\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.3593\n",
      "Epoch 25/25, Validation Loss: 0.2948\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 0.4369,  1.6490,  1.2161, -0.7247, -0.1103, -0.1009, -1.7508,\n",
      "          -0.5681, -1.9687, -1.9687, -0.8792,  4.1508,  0.0000, -0.9065,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -0.9411,\n",
      "          -1.0386, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4441,  1.6207,  1.1753, -0.6836, -0.1333, -0.1076, -1.6197,\n",
      "          -0.5681, -2.0056, -2.0056, -0.0644,  4.1508,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0007,\n",
      "          -0.9729, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4514,  1.5924,  1.1345, -0.6426, -0.1562, -0.1142, -1.4886,\n",
      "          -0.5681, -2.0425, -2.0425,  0.7504,  4.1508,  0.0000, -0.9062,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.0603,\n",
      "          -0.9071, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4586,  1.5641,  1.0937, -0.6016, -0.1791, -0.1209, -1.3575,\n",
      "          -0.5681, -2.0794, -2.0794,  1.5652,  4.1508,  0.0000, -0.9061,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1199,\n",
      "          -0.8414, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4659,  1.5358,  1.0529, -0.5605, -0.2021, -0.1275, -1.2264,\n",
      "          -0.5681, -2.1163, -2.1163,  2.3800,  4.1508,  0.0000, -0.9059,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.9722,  0.1355, -1.1795,\n",
      "          -0.7757, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.6707,  2.1820,  1.2569, -1.2558,  0.3838,  0.0256, -0.5708,\n",
      "          -0.3964,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9077,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.7766,\n",
      "          -1.1809, -1.1552,  1.1552,  0.0000],\n",
      "         [-0.1923,  1.7008,  3.6011, -1.2260,  0.0849, -0.0560, -1.2592,\n",
      "          -0.4694,  0.8735,  0.8735, -0.3360, -0.7989,  0.0000, -0.9063,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.8472,\n",
      "          -1.1308, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.0926,  1.2596,  2.2197, -1.1127,  0.0272, -0.0676, -0.7894,\n",
      "          -0.3392,  1.2980,  1.2980,  1.7010, -0.7989,  0.0000, -0.9046,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -0.9160,\n",
      "          -1.0749, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2006,  1.1195,  1.8444, -1.1076,  0.0108, -0.0809, -0.5708,\n",
      "          -0.4822,  1.5287,  1.5287,  1.7010, -0.7989,  0.0000, -0.9043,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1995,\n",
      "          -0.7444, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2369,  0.9374,  0.9469, -1.1114,  0.0096, -0.0809, -1.8819,\n",
      "          -0.5681, -1.6549, -1.6549,  2.3800, -0.7989,  0.0000, -0.9042,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0351,\n",
      "          -0.9609, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.2872,  0.8395,  0.8979, -1.0392, -0.0206, -0.0876, -1.8819,\n",
      "          -0.4822,  1.7594,  1.7594, -1.0150, -0.7989,  0.0000, -0.9041,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.0958,\n",
      "          -0.8907, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3347,  0.7615,  0.8653, -1.0354, -0.0282, -0.0942, -1.2264,\n",
      "          -0.3105,  1.5748,  1.5748, -1.6940, -0.7989,  0.0000, -0.9039,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.1517,\n",
      "          -0.8166, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3542,  0.8125,  0.8000, -0.9823, -0.0332, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -0.3360, -0.7989,  0.0000, -0.9038,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3532,\n",
      "          -0.4007, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3514,  0.8187,  0.7511, -0.8189, -0.0345, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4364,  1.4364, -2.3730, -0.7989,  0.0000, -0.9037,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.2486,\n",
      "          -0.6584, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3682,  0.9183,  0.6178, -0.8569, -0.0887, -0.1153, -1.2264,\n",
      "          -0.3964,  1.5287,  1.5287,  0.1166, -0.7989,  0.0000, -0.9034,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3050,\n",
      "          -0.5368, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.3919,  1.0263,  1.1713, -0.6042, -0.0465, -0.1042, -1.5541,\n",
      "          -0.5681,  1.6210,  1.6210,  1.7010, -0.7989,  0.0000, -0.9031,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3402,\n",
      "          -0.4420, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4268,  0.9811,  0.7021, -0.7657, -0.0358, -0.1042, -1.8819,\n",
      "          -0.5681,  1.7132,  1.7132, -1.0150, -0.7989,  0.0000, -0.9029,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3679,\n",
      "          -0.3469, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4156,  1.0019,  0.7103, -0.7429, -0.0383, -0.1042, -1.8819,\n",
      "          -0.5681,  1.5287,  1.5287, -0.3360, -0.7989,  0.0000, -0.9027,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.3878,\n",
      "          -0.2562, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4184,  1.1060,  0.7755, -0.6783, -0.0358, -0.1009, -0.5708,\n",
      "          -0.5681,  1.5287,  1.5287,  0.3430, -0.7989,  0.0000, -0.9026,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4017,\n",
      "          -0.1644, -1.1552,  1.1552,  0.0000],\n",
      "         [ 0.4017,  1.1528,  0.8000, -0.6099, -0.0358, -0.0976, -1.8819,\n",
      "          -0.5681,  1.4825,  1.4825, -2.3730, -0.7989,  0.0000, -0.9025,\n",
      "           0.0000, -0.4481, -0.0399,  1.4107, -1.7667,  0.8535, -1.4096,\n",
      "          -0.0719, -1.1552,  1.1552,  0.0000]]])\n",
      "predicted: tensor([[-0.1520]], device='cuda:0')\n",
      "[64.77]\n",
      "           actual   predicted\n",
      "0        5.999996   35.189590\n",
      "1       26.000004   38.984438\n",
      "2       10.000002   42.843111\n",
      "3        3.999999   11.781192\n",
      "4        5.999996   18.809363\n",
      "...           ...         ...\n",
      "38802    7.000001   32.802637\n",
      "38803   42.500002   66.438321\n",
      "38804    5.999996   55.642057\n",
      "38805  266.000000  313.896267\n",
      "38806   65.000004   62.272805\n",
      "\n",
      "[38807 rows x 2 columns]\n",
      "Score (RMSE): 438.3356\n",
      "Score (MAE): 80.1184\n",
      "Score (ME): -4.8513\n",
      "Score (MAPE): 115272.2900%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100108, 27) to (100414, 27)\n",
      "training data cutoff:  2023-07-14 01:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([75809, 20, 25]) torch.Size([75809]) torch.Size([75809, 1])\n",
      "Testing data shape: torch.Size([19182, 20, 25]) torch.Size([19182]) torch.Size([19182, 1])\n",
      "Shuffled Training data shape: torch.Size([75992, 20, 25]) torch.Size([75992]) torch.Size([75992, 1])\n",
      "Shuffled Testing data shape: torch.Size([18999, 20, 25]) torch.Size([18999]) torch.Size([18999, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0     425.000004  495.203424\n",
      "1     498.500000  512.874742\n",
      "2     474.000000  458.693764\n",
      "3     389.027027  428.148781\n",
      "4     504.749999  491.834924\n",
      "...          ...         ...\n",
      "4091  537.333333  487.280802\n",
      "4092  447.514287  435.269555\n",
      "4093  411.999998  468.086963\n",
      "4094  398.499999  476.584063\n",
      "4095  415.749999  464.783737\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.7790\n",
      "Epoch 1/25, Validation Loss: 0.6404\n",
      "          actual   predicted\n",
      "0     425.000004  494.848063\n",
      "1     498.500000  505.199315\n",
      "2     474.000000  438.407584\n",
      "3     389.027027  424.964046\n",
      "4     504.749999  477.949285\n",
      "...          ...         ...\n",
      "4091  537.333333  593.147933\n",
      "4092  447.514287  469.886238\n",
      "4093  411.999998  436.315601\n",
      "4094  398.499999  467.656839\n",
      "4095  415.749999  416.818347\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.5717\n",
      "Epoch 2/25, Validation Loss: 0.5214\n",
      "          actual   predicted\n",
      "0     425.000004  479.473129\n",
      "1     498.500000  507.973828\n",
      "2     474.000000  447.735819\n",
      "3     389.027027  404.919479\n",
      "4     504.749999  486.573638\n",
      "...          ...         ...\n",
      "4091  537.333333  545.133993\n",
      "4092  447.514287  452.518570\n",
      "4093  411.999998  430.514426\n",
      "4094  398.499999  457.265012\n",
      "4095  415.749999  420.797993\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.4400\n",
      "Epoch 3/25, Validation Loss: 0.3680\n",
      "          actual   predicted\n",
      "0     425.000004  449.392739\n",
      "1     498.500000  509.050632\n",
      "2     474.000000  469.869705\n",
      "3     389.027027  384.364171\n",
      "4     504.749999  491.664331\n",
      "...          ...         ...\n",
      "4091  537.333333  552.131221\n",
      "4092  447.514287  465.796138\n",
      "4093  411.999998  416.172766\n",
      "4094  398.499999  429.731246\n",
      "4095  415.749999  416.127383\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.2974\n",
      "Epoch 4/25, Validation Loss: 0.2378\n",
      "          actual   predicted\n",
      "0     425.000004  434.090032\n",
      "1     498.500000  511.649178\n",
      "2     474.000000  474.162095\n",
      "3     389.027027  377.475167\n",
      "4     504.749999  494.211615\n",
      "...          ...         ...\n",
      "4091  537.333333  541.116319\n",
      "4092  447.514287  452.830511\n",
      "4093  411.999998  415.153804\n",
      "4094  398.499999  414.323211\n",
      "4095  415.749999  408.879475\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.2081\n",
      "Epoch 5/25, Validation Loss: 0.1753\n",
      "          actual   predicted\n",
      "0     425.000004  433.247946\n",
      "1     498.500000  501.064586\n",
      "2     474.000000  466.087817\n",
      "3     389.027027  383.849396\n",
      "4     504.749999  494.034039\n",
      "...          ...         ...\n",
      "4091  537.333333  530.920487\n",
      "4092  447.514287  450.636866\n",
      "4093  411.999998  411.183932\n",
      "4094  398.499999  406.547084\n",
      "4095  415.749999  415.719303\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1776\n",
      "Epoch 6/25, Validation Loss: 0.1518\n",
      "          actual   predicted\n",
      "0     425.000004  436.690024\n",
      "1     498.500000  511.413812\n",
      "2     474.000000  476.843541\n",
      "3     389.027027  380.378210\n",
      "4     504.749999  486.687258\n",
      "...          ...         ...\n",
      "4091  537.333333  539.933387\n",
      "4092  447.514287  448.963381\n",
      "4093  411.999998  407.386986\n",
      "4094  398.499999  400.485171\n",
      "4095  415.749999  419.567888\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.1593\n",
      "Epoch 7/25, Validation Loss: 0.1411\n",
      "          actual   predicted\n",
      "0     425.000004  430.871106\n",
      "1     498.500000  518.005592\n",
      "2     474.000000  475.401707\n",
      "3     389.027027  377.829857\n",
      "4     504.749999  493.526426\n",
      "...          ...         ...\n",
      "4091  537.333333  546.080564\n",
      "4092  447.514287  443.411304\n",
      "4093  411.999998  404.435677\n",
      "4094  398.499999  400.428343\n",
      "4095  415.749999  413.913256\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1534\n",
      "Epoch 8/25, Validation Loss: 0.1356\n",
      "          actual   predicted\n",
      "0     425.000004  434.195760\n",
      "1     498.500000  507.625058\n",
      "2     474.000000  473.273210\n",
      "3     389.027027  388.305338\n",
      "4     504.749999  490.234780\n",
      "...          ...         ...\n",
      "4091  537.333333  530.629119\n",
      "4092  447.514287  448.307229\n",
      "4093  411.999998  407.143751\n",
      "4094  398.499999  404.324142\n",
      "4095  415.749999  420.022913\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1464\n",
      "Epoch 9/25, Validation Loss: 0.1348\n",
      "          actual   predicted\n",
      "0     425.000004  437.824165\n",
      "1     498.500000  512.483140\n",
      "2     474.000000  479.203059\n",
      "3     389.027027  383.520272\n",
      "4     504.749999  495.302682\n",
      "...          ...         ...\n",
      "4091  537.333333  541.619056\n",
      "4092  447.514287  447.878387\n",
      "4093  411.999998  403.359509\n",
      "4094  398.499999  398.883466\n",
      "4095  415.749999  421.293511\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.1416\n",
      "Epoch 10/25, Validation Loss: 0.1275\n",
      "          actual   predicted\n",
      "0     425.000004  430.946698\n",
      "1     498.500000  510.180578\n",
      "2     474.000000  478.370183\n",
      "3     389.027027  382.163320\n",
      "4     504.749999  493.491062\n",
      "...          ...         ...\n",
      "4091  537.333333  538.182104\n",
      "4092  447.514287  443.059496\n",
      "4093  411.999998  402.052634\n",
      "4094  398.499999  399.808479\n",
      "4095  415.749999  415.805592\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.1409\n",
      "Epoch 11/25, Validation Loss: 0.1260\n",
      "          actual   predicted\n",
      "0     425.000004  432.256443\n",
      "1     498.500000  516.387452\n",
      "2     474.000000  477.191936\n",
      "3     389.027027  384.176376\n",
      "4     504.749999  494.056876\n",
      "...          ...         ...\n",
      "4091  537.333333  545.729432\n",
      "4092  447.514287  444.462568\n",
      "4093  411.999998  404.744140\n",
      "4094  398.499999  403.565914\n",
      "4095  415.749999  416.311421\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1371\n",
      "Epoch 12/25, Validation Loss: 0.1244\n",
      "          actual   predicted\n",
      "0     425.000004  430.696596\n",
      "1     498.500000  504.784828\n",
      "2     474.000000  472.010913\n",
      "3     389.027027  383.716707\n",
      "4     504.749999  491.130580\n",
      "...          ...         ...\n",
      "4091  537.333333  531.744017\n",
      "4092  447.514287  442.727782\n",
      "4093  411.999998  402.007288\n",
      "4094  398.499999  398.789816\n",
      "4095  415.749999  416.398276\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1354\n",
      "Epoch 13/25, Validation Loss: 0.1267\n",
      "          actual   predicted\n",
      "0     425.000004  432.415042\n",
      "1     498.500000  512.904054\n",
      "2     474.000000  474.774074\n",
      "3     389.027027  387.156979\n",
      "4     504.749999  492.591980\n",
      "...          ...         ...\n",
      "4091  537.333333  538.859763\n",
      "4092  447.514287  443.692451\n",
      "4093  411.999998  405.020469\n",
      "4094  398.499999  402.585344\n",
      "4095  415.749999  418.420460\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.1338\n",
      "Epoch 14/25, Validation Loss: 0.1232\n",
      "          actual   predicted\n",
      "0     425.000004  430.217843\n",
      "1     498.500000  507.085007\n",
      "2     474.000000  474.672145\n",
      "3     389.027027  386.526215\n",
      "4     504.749999  494.750747\n",
      "...          ...         ...\n",
      "4091  537.333333  531.833271\n",
      "4092  447.514287  443.621256\n",
      "4093  411.999998  405.278137\n",
      "4094  398.499999  402.402702\n",
      "4095  415.749999  416.655675\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.1305\n",
      "Epoch 15/25, Validation Loss: 0.1223\n",
      "          actual   predicted\n",
      "0     425.000004  433.588080\n",
      "1     498.500000  509.493698\n",
      "2     474.000000  472.941880\n",
      "3     389.027027  387.227302\n",
      "4     504.749999  497.259705\n",
      "...          ...         ...\n",
      "4091  537.333333  534.536313\n",
      "4092  447.514287  445.794081\n",
      "4093  411.999998  407.108426\n",
      "4094  398.499999  406.978085\n",
      "4095  415.749999  417.970368\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.1299\n",
      "Epoch 16/25, Validation Loss: 0.1187\n",
      "          actual   predicted\n",
      "0     425.000004  432.720338\n",
      "1     498.500000  509.956648\n",
      "2     474.000000  472.893264\n",
      "3     389.027027  385.070730\n",
      "4     504.749999  494.297755\n",
      "...          ...         ...\n",
      "4091  537.333333  533.693246\n",
      "4092  447.514287  441.713300\n",
      "4093  411.999998  402.060875\n",
      "4094  398.499999  402.359419\n",
      "4095  415.749999  415.512999\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.1276\n",
      "Epoch 17/25, Validation Loss: 0.1213\n",
      "          actual   predicted\n",
      "0     425.000004  427.371557\n",
      "1     498.500000  507.582384\n",
      "2     474.000000  471.500130\n",
      "3     389.027027  381.516525\n",
      "4     504.749999  486.480568\n",
      "...          ...         ...\n",
      "4091  537.333333  528.372952\n",
      "4092  447.514287  437.911892\n",
      "4093  411.999998  400.789595\n",
      "4094  398.499999  399.448434\n",
      "4095  415.749999  413.972337\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.1261\n",
      "Epoch 18/25, Validation Loss: 0.1202\n",
      "          actual   predicted\n",
      "0     425.000004  437.073320\n",
      "1     498.500000  506.289370\n",
      "2     474.000000  474.291754\n",
      "3     389.027027  388.856034\n",
      "4     504.749999  498.037152\n",
      "...          ...         ...\n",
      "4091  537.333333  532.892306\n",
      "4092  447.514287  446.755728\n",
      "4093  411.999998  405.137272\n",
      "4094  398.499999  402.702503\n",
      "4095  415.749999  418.402910\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.1302\n",
      "Epoch 19/25, Validation Loss: 0.1183\n",
      "          actual   predicted\n",
      "0     425.000004  432.164943\n",
      "1     498.500000  508.647561\n",
      "2     474.000000  473.122232\n",
      "3     389.027027  392.431553\n",
      "4     504.749999  498.892951\n",
      "...          ...         ...\n",
      "4091  537.333333  535.866744\n",
      "4092  447.514287  448.736108\n",
      "4093  411.999998  406.643830\n",
      "4094  398.499999  405.514381\n",
      "4095  415.749999  418.867985\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.1268\n",
      "Epoch 20/25, Validation Loss: 0.1175\n",
      "          actual   predicted\n",
      "0     425.000004  433.624462\n",
      "1     498.500000  508.487824\n",
      "2     474.000000  471.771014\n",
      "3     389.027027  386.524667\n",
      "4     504.749999  502.857776\n",
      "...          ...         ...\n",
      "4091  537.333333  526.043224\n",
      "4092  447.514287  443.301968\n",
      "4093  411.999998  406.399731\n",
      "4094  398.499999  404.896029\n",
      "4095  415.749999  418.519705\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.1244\n",
      "Epoch 21/25, Validation Loss: 0.1173\n",
      "          actual   predicted\n",
      "0     425.000004  433.193570\n",
      "1     498.500000  516.264594\n",
      "2     474.000000  475.759998\n",
      "3     389.027027  389.298872\n",
      "4     504.749999  501.903128\n",
      "...          ...         ...\n",
      "4091  537.333333  540.217797\n",
      "4092  447.514287  443.330535\n",
      "4093  411.999998  405.557554\n",
      "4094  398.499999  404.027625\n",
      "4095  415.749999  419.758568\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.1228\n",
      "Epoch 22/25, Validation Loss: 0.1183\n",
      "          actual   predicted\n",
      "0     425.000004  433.860001\n",
      "1     498.500000  514.173637\n",
      "2     474.000000  475.474650\n",
      "3     389.027027  387.439776\n",
      "4     504.749999  501.103550\n",
      "...          ...         ...\n",
      "4091  537.333333  534.325591\n",
      "4092  447.514287  446.319085\n",
      "4093  411.999998  403.084328\n",
      "4094  398.499999  402.166276\n",
      "4095  415.749999  417.362009\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.1234\n",
      "Epoch 23/25, Validation Loss: 0.1161\n",
      "          actual   predicted\n",
      "0     425.000004  436.106496\n",
      "1     498.500000  504.608316\n",
      "2     474.000000  471.952450\n",
      "3     389.027027  385.575404\n",
      "4     504.749999  503.051913\n",
      "...          ...         ...\n",
      "4091  537.333333  524.838201\n",
      "4092  447.514287  445.324108\n",
      "4093  411.999998  404.543832\n",
      "4094  398.499999  403.562484\n",
      "4095  415.749999  417.489850\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.1211\n",
      "Epoch 24/25, Validation Loss: 0.1156\n",
      "          actual   predicted\n",
      "0     425.000004  433.163321\n",
      "1     498.500000  509.380674\n",
      "2     474.000000  474.689217\n",
      "3     389.027027  390.676077\n",
      "4     504.749999  499.395740\n",
      "...          ...         ...\n",
      "4091  537.333333  536.597037\n",
      "4092  447.514287  446.060524\n",
      "4093  411.999998  405.600866\n",
      "4094  398.499999  403.988151\n",
      "4095  415.749999  418.927917\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.1202\n",
      "Epoch 25/25, Validation Loss: 0.1163\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3896e-01,  1.6481e+00,  1.2189e+00, -7.3040e-01, -9.9768e-02,\n",
      "          -8.3099e-02, -1.8816e+00, -5.9109e-01, -2.0547e+00, -2.0547e+00,\n",
      "          -1.4280e+00,  4.1557e+00,  0.0000e+00, -9.0600e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.3963e-01, -1.0391e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.4623e-01,  1.6198e+00,  1.1779e+00, -6.8921e-01, -1.1689e-01,\n",
      "          -8.7525e-02, -1.7402e+00, -5.9109e-01, -2.0932e+00, -2.0932e+00,\n",
      "          -1.0415e-01,  4.1557e+00,  0.0000e+00, -9.0586e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.9923e-01, -9.7336e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.5349e-01,  1.5915e+00,  1.1368e+00, -6.4801e-01, -1.3400e-01,\n",
      "          -9.1950e-02, -1.5988e+00, -5.9109e-01, -2.1317e+00, -2.1317e+00,\n",
      "           1.2197e+00,  4.1557e+00,  0.0000e+00, -9.0573e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.0588e+00, -9.0760e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6076e-01,  1.5632e+00,  1.0958e+00, -6.0681e-01, -1.5112e-01,\n",
      "          -9.6376e-02, -1.4575e+00, -5.9109e-01, -2.1702e+00, -2.1702e+00,\n",
      "           2.5435e+00,  4.1557e+00,  0.0000e+00, -9.0560e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1184e+00, -8.4183e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6802e-01,  1.5349e+00,  1.0548e+00, -5.6561e-01, -1.6824e-01,\n",
      "          -1.0080e-01, -1.3161e+00, -5.9109e-01, -2.2087e+00, -2.2087e+00,\n",
      "           3.8674e+00,  4.1557e+00,  0.0000e+00, -9.0546e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1780e+00, -7.7606e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-6.6942e-01,  2.1811e+00,  1.2599e+00, -1.2637e+00,  2.6910e-01,\n",
      "           9.8832e-04, -6.0919e-01, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0720e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -7.7521e-01, -1.1816e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-1.9069e-01,  1.6999e+00,  3.6163e+00, -1.2338e+00,  4.5965e-02,\n",
      "          -5.3226e-02, -1.3514e+00, -4.8829e-01,  9.1100e-01,  9.1100e-01,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0578e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -8.4576e-01, -1.1314e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 9.4465e-02,  1.2588e+00,  2.2277e+00, -1.1200e+00,  2.9364e-03,\n",
      "          -6.0971e-02, -8.4482e-01, -3.5272e-01,  1.3539e+00,  1.3539e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0413e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -9.1458e-01, -1.0755e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.0253e-01,  1.1187e+00,  1.8504e+00, -1.1149e+00, -9.2903e-03,\n",
      "          -6.9822e-02, -6.0919e-01, -5.0170e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0386e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1980e+00, -7.4471e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.3886e-01,  9.3658e-01,  9.4820e-01, -1.1187e+00, -1.0231e-02,\n",
      "          -6.9822e-02, -2.0230e+00, -5.9109e-01, -1.7273e+00, -1.7273e+00,\n",
      "           3.8674e+00, -7.9865e-01,  0.0000e+00, -9.0372e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0336e+00, -9.6137e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.8916e-01,  8.3876e-01,  8.9899e-01, -1.0463e+00, -3.2803e-02,\n",
      "          -7.4248e-02, -2.0230e+00, -5.0170e-01,  1.8354e+00,  1.8354e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0359e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0943e+00, -8.9111e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.3667e-01,  7.6072e-01,  8.6618e-01, -1.0424e+00, -3.8446e-02,\n",
      "          -7.8673e-02, -1.3161e+00, -3.2292e-01,  1.6428e+00,  1.6428e+00,\n",
      "          -2.7518e+00, -7.9865e-01,  0.0000e+00, -9.0346e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1502e+00, -8.1703e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5624e-01,  8.1171e-01,  8.0056e-01, -9.8904e-01, -4.2208e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0332e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3516e+00, -4.0081e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5344e-01,  8.1795e-01,  7.5135e-01, -8.2501e-01, -4.3149e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0319e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.2470e+00, -6.5866e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.7021e-01,  9.1750e-01,  6.1738e-01, -8.6316e-01, -8.3591e-02,\n",
      "          -9.2688e-02, -1.3161e+00, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           1.9004e-01, -7.9865e-01,  0.0000e+00, -9.0292e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3034e+00, -5.3698e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.9396e-01,  1.0256e+00,  1.1738e+00, -6.0948e-01, -5.2084e-02,\n",
      "          -8.5312e-02, -1.6695e+00, -5.9109e-01,  1.6909e+00,  1.6909e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0259e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3387e+00, -4.4220e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2890e-01,  9.8028e-01,  7.0214e-01, -7.7160e-01, -4.4090e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.7872e+00,  1.7872e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0239e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3664e+00, -3.4705e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.1772e-01,  1.0011e+00,  7.1034e-01, -7.4872e-01, -4.5971e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0225e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3862e+00, -2.5629e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2051e-01,  1.1052e+00,  7.7596e-01, -6.8387e-01, -4.4090e-02,\n",
      "          -8.3099e-02, -6.0919e-01, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "           5.5777e-01, -7.9865e-01,  0.0000e+00, -9.0212e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4001e+00, -1.6443e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.0375e-01,  1.1520e+00,  8.0056e-01, -6.1520e-01, -4.4090e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.5465e+00,  1.5465e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0199e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4080e+00, -7.1849e-02, -1.1547e+00,  1.1547e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.8485]], device='cuda:0')\n",
      "[591.84]\n",
      "           actual   predicted\n",
      "0      425.000004  433.163321\n",
      "1      498.500000  509.380674\n",
      "2      474.000000  474.689217\n",
      "3      389.027027  390.676077\n",
      "4      504.749999  499.395740\n",
      "...           ...         ...\n",
      "18994  552.205127  566.134208\n",
      "18995  401.499998  399.700629\n",
      "18996  482.250000  438.369701\n",
      "18997  445.249999  438.020386\n",
      "18998  575.499998  580.526889\n",
      "\n",
      "[18999 rows x 2 columns]\n",
      "Score (RMSE): 42.0722\n",
      "Score (MAE): 16.0786\n",
      "Score (ME): -1.3055\n",
      "Score (MAPE): 2.8403%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100108, 27) to (100414, 27)\n",
      "training data cutoff:  2023-07-14 01:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([75809, 20, 25]) torch.Size([75809]) torch.Size([75809, 1])\n",
      "Testing data shape: torch.Size([19182, 20, 25]) torch.Size([19182]) torch.Size([19182, 1])\n",
      "Shuffled Training data shape: torch.Size([75992, 20, 25]) torch.Size([75992]) torch.Size([75992, 1])\n",
      "Shuffled Testing data shape: torch.Size([18999, 20, 25]) torch.Size([18999]) torch.Size([18999, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           actual   predicted\n",
      "0      744.666668  725.723445\n",
      "1      657.999997  722.447439\n",
      "2     1745.500006  819.650005\n",
      "3      816.666668  777.487775\n",
      "4      594.249998  714.052790\n",
      "...           ...         ...\n",
      "4091   668.666668  628.642147\n",
      "4092   545.500007  653.026447\n",
      "4093   571.000000  678.383277\n",
      "4094   630.250008  680.202769\n",
      "4095   732.203704  667.055581\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8033\n",
      "Epoch 1/25, Validation Loss: 0.5301\n",
      "           actual   predicted\n",
      "0      744.666668  804.715543\n",
      "1      657.999997  619.710617\n",
      "2     1745.500006  826.039050\n",
      "3      816.666668  875.732501\n",
      "4      594.249998  650.024130\n",
      "...           ...         ...\n",
      "4091   668.666668  633.370339\n",
      "4092   545.500007  625.052210\n",
      "4093   571.000000  619.543134\n",
      "4094   630.250008  788.624336\n",
      "4095   732.203704  718.267229\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.4770\n",
      "Epoch 2/25, Validation Loss: 0.3990\n",
      "           actual   predicted\n",
      "0      744.666668  760.211300\n",
      "1      657.999997  652.316086\n",
      "2     1745.500006  885.745022\n",
      "3      816.666668  921.596713\n",
      "4      594.249998  636.050535\n",
      "...           ...         ...\n",
      "4091   668.666668  636.440265\n",
      "4092   545.500007  596.089857\n",
      "4093   571.000000  598.360274\n",
      "4094   630.250008  744.808811\n",
      "4095   732.203704  744.312179\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.3611\n",
      "Epoch 3/25, Validation Loss: 0.2903\n",
      "           actual    predicted\n",
      "0      744.666668   757.647899\n",
      "1      657.999997   669.886931\n",
      "2     1745.500006  1131.204406\n",
      "3      816.666668   856.409445\n",
      "4      594.249998   640.596682\n",
      "...           ...          ...\n",
      "4091   668.666668   653.775940\n",
      "4092   545.500007   545.948566\n",
      "4093   571.000000   558.036209\n",
      "4094   630.250008   691.972501\n",
      "4095   732.203704   767.629976\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.2601\n",
      "Epoch 4/25, Validation Loss: 0.2063\n",
      "           actual    predicted\n",
      "0      744.666668   760.382918\n",
      "1      657.999997   656.356261\n",
      "2     1745.500006  1505.949249\n",
      "3      816.666668   892.130567\n",
      "4      594.249998   634.051693\n",
      "...           ...          ...\n",
      "4091   668.666668   661.915118\n",
      "4092   545.500007   548.958883\n",
      "4093   571.000000   560.993245\n",
      "4094   630.250008   671.715188\n",
      "4095   732.203704   762.865499\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.1989\n",
      "Epoch 5/25, Validation Loss: 0.1576\n",
      "           actual    predicted\n",
      "0      744.666668   762.022555\n",
      "1      657.999997   668.034533\n",
      "2     1745.500006  1593.948533\n",
      "3      816.666668   855.560687\n",
      "4      594.249998   626.356183\n",
      "...           ...          ...\n",
      "4091   668.666668   671.769008\n",
      "4092   545.500007   549.703532\n",
      "4093   571.000000   569.773714\n",
      "4094   630.250008   654.794338\n",
      "4095   732.203704   754.190879\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.1608\n",
      "Epoch 6/25, Validation Loss: 0.1220\n",
      "           actual    predicted\n",
      "0      744.666668   751.927176\n",
      "1      657.999997   666.994878\n",
      "2     1745.500006  1610.254139\n",
      "3      816.666668   869.330331\n",
      "4      594.249998   618.643954\n",
      "...           ...          ...\n",
      "4091   668.666668   669.723573\n",
      "4092   545.500007   549.480784\n",
      "4093   571.000000   565.080193\n",
      "4094   630.250008   648.249364\n",
      "4095   732.203704   748.331960\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.1431\n",
      "Epoch 7/25, Validation Loss: 0.1133\n",
      "           actual    predicted\n",
      "0      744.666668   760.516877\n",
      "1      657.999997   674.715021\n",
      "2     1745.500006  1620.842233\n",
      "3      816.666668   867.832391\n",
      "4      594.249998   627.519001\n",
      "...           ...          ...\n",
      "4091   668.666668   674.807512\n",
      "4092   545.500007   558.975747\n",
      "4093   571.000000   572.707392\n",
      "4094   630.250008   658.772287\n",
      "4095   732.203704   740.670828\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.1324\n",
      "Epoch 8/25, Validation Loss: 0.1063\n",
      "           actual    predicted\n",
      "0      744.666668   748.293608\n",
      "1      657.999997   669.331967\n",
      "2     1745.500006  1646.987314\n",
      "3      816.666668   866.295405\n",
      "4      594.249998   623.950720\n",
      "...           ...          ...\n",
      "4091   668.666668   667.904167\n",
      "4092   545.500007   554.309601\n",
      "4093   571.000000   568.453304\n",
      "4094   630.250008   663.502800\n",
      "4095   732.203704   735.460070\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.1256\n",
      "Epoch 9/25, Validation Loss: 0.1015\n",
      "           actual    predicted\n",
      "0      744.666668   762.559119\n",
      "1      657.999997   665.985942\n",
      "2     1745.500006  1676.225993\n",
      "3      816.666668   881.093540\n",
      "4      594.249998   614.710504\n",
      "...           ...          ...\n",
      "4091   668.666668   665.536102\n",
      "4092   545.500007   546.228391\n",
      "4093   571.000000   555.294686\n",
      "4094   630.250008   658.814302\n",
      "4095   732.203704   742.057765\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.1196\n",
      "Epoch 10/25, Validation Loss: 0.0991\n",
      "           actual    predicted\n",
      "0      744.666668   750.255620\n",
      "1      657.999997   670.631174\n",
      "2     1745.500006  1635.914662\n",
      "3      816.666668   866.705956\n",
      "4      594.249998   619.467244\n",
      "...           ...          ...\n",
      "4091   668.666668   669.948469\n",
      "4092   545.500007   558.700172\n",
      "4093   571.000000   571.037297\n",
      "4094   630.250008   662.193694\n",
      "4095   732.203704   738.512820\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.1174\n",
      "Epoch 11/25, Validation Loss: 0.0958\n",
      "           actual    predicted\n",
      "0      744.666668   748.855637\n",
      "1      657.999997   664.211301\n",
      "2     1745.500006  1671.355286\n",
      "3      816.666668   873.650449\n",
      "4      594.249998   608.656949\n",
      "...           ...          ...\n",
      "4091   668.666668   655.323302\n",
      "4092   545.500007   551.533032\n",
      "4093   571.000000   563.099648\n",
      "4094   630.250008   647.565526\n",
      "4095   732.203704   720.416074\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.1124\n",
      "Epoch 12/25, Validation Loss: 0.0934\n",
      "           actual    predicted\n",
      "0      744.666668   756.952860\n",
      "1      657.999997   670.443309\n",
      "2     1745.500006  1662.676113\n",
      "3      816.666668   866.393857\n",
      "4      594.249998   616.618034\n",
      "...           ...          ...\n",
      "4091   668.666668   667.176751\n",
      "4092   545.500007   556.382161\n",
      "4093   571.000000   562.911884\n",
      "4094   630.250008   644.524710\n",
      "4095   732.203704   725.433424\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.1100\n",
      "Epoch 13/25, Validation Loss: 0.0920\n",
      "           actual    predicted\n",
      "0      744.666668   750.394299\n",
      "1      657.999997   665.858740\n",
      "2     1745.500006  1663.650292\n",
      "3      816.666668   863.600226\n",
      "4      594.249998   615.461545\n",
      "...           ...          ...\n",
      "4091   668.666668   669.268147\n",
      "4092   545.500007   555.553887\n",
      "4093   571.000000   561.125197\n",
      "4094   630.250008   651.907277\n",
      "4095   732.203704   727.476921\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.1067\n",
      "Epoch 14/25, Validation Loss: 0.0901\n",
      "           actual    predicted\n",
      "0      744.666668   755.103781\n",
      "1      657.999997   661.262007\n",
      "2     1745.500006  1659.606328\n",
      "3      816.666668   866.155914\n",
      "4      594.249998   603.606040\n",
      "...           ...          ...\n",
      "4091   668.666668   659.670123\n",
      "4092   545.500007   551.710202\n",
      "4093   571.000000   557.191935\n",
      "4094   630.250008   645.747839\n",
      "4095   732.203704   723.687924\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.1049\n",
      "Epoch 15/25, Validation Loss: 0.0886\n",
      "           actual    predicted\n",
      "0      744.666668   756.563924\n",
      "1      657.999997   670.517441\n",
      "2     1745.500006  1668.334188\n",
      "3      816.666668   865.436514\n",
      "4      594.249998   614.197290\n",
      "...           ...          ...\n",
      "4091   668.666668   665.899778\n",
      "4092   545.500007   560.292454\n",
      "4093   571.000000   567.404063\n",
      "4094   630.250008   650.952731\n",
      "4095   732.203704   726.637593\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.1029\n",
      "Epoch 16/25, Validation Loss: 0.0875\n",
      "           actual    predicted\n",
      "0      744.666668   761.930576\n",
      "1      657.999997   670.401496\n",
      "2     1745.500006  1672.429839\n",
      "3      816.666668   872.660630\n",
      "4      594.249998   620.210065\n",
      "...           ...          ...\n",
      "4091   668.666668   671.676345\n",
      "4092   545.500007   562.674246\n",
      "4093   571.000000   568.442976\n",
      "4094   630.250008   657.421049\n",
      "4095   732.203704   734.418669\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.1013\n",
      "Epoch 17/25, Validation Loss: 0.0867\n",
      "           actual    predicted\n",
      "0      744.666668   759.287828\n",
      "1      657.999997   673.837677\n",
      "2     1745.500006  1670.886790\n",
      "3      816.666668   855.513078\n",
      "4      594.249998   620.688764\n",
      "...           ...          ...\n",
      "4091   668.666668   669.007595\n",
      "4092   545.500007   561.979893\n",
      "4093   571.000000   570.094899\n",
      "4094   630.250008   651.231744\n",
      "4095   732.203704   719.806060\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0998\n",
      "Epoch 18/25, Validation Loss: 0.0853\n",
      "           actual    predicted\n",
      "0      744.666668   753.959543\n",
      "1      657.999997   668.329663\n",
      "2     1745.500006  1686.059594\n",
      "3      816.666668   870.243675\n",
      "4      594.249998   606.649998\n",
      "...           ...          ...\n",
      "4091   668.666668   659.735661\n",
      "4092   545.500007   549.928640\n",
      "4093   571.000000   557.406964\n",
      "4094   630.250008   646.499786\n",
      "4095   732.203704   720.140822\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0989\n",
      "Epoch 19/25, Validation Loss: 0.0852\n",
      "           actual    predicted\n",
      "0      744.666668   751.115207\n",
      "1      657.999997   675.629677\n",
      "2     1745.500006  1681.963755\n",
      "3      816.666668   861.798038\n",
      "4      594.249998   620.668921\n",
      "...           ...          ...\n",
      "4091   668.666668   668.090290\n",
      "4092   545.500007   566.271323\n",
      "4093   571.000000   573.630571\n",
      "4094   630.250008   648.939514\n",
      "4095   732.203704   722.349092\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0961\n",
      "Epoch 20/25, Validation Loss: 0.0833\n",
      "           actual    predicted\n",
      "0      744.666668   753.639089\n",
      "1      657.999997   668.773253\n",
      "2     1745.500006  1682.096941\n",
      "3      816.666668   860.950335\n",
      "4      594.249998   610.950256\n",
      "...           ...          ...\n",
      "4091   668.666668   665.617351\n",
      "4092   545.500007   559.866286\n",
      "4093   571.000000   562.447170\n",
      "4094   630.250008   643.050676\n",
      "4095   732.203704   722.567535\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0956\n",
      "Epoch 21/25, Validation Loss: 0.0835\n",
      "           actual    predicted\n",
      "0      744.666668   757.279041\n",
      "1      657.999997   676.296796\n",
      "2     1745.500006  1693.980649\n",
      "3      816.666668   866.985375\n",
      "4      594.249998   621.590991\n",
      "...           ...          ...\n",
      "4091   668.666668   669.781596\n",
      "4092   545.500007   563.228460\n",
      "4093   571.000000   570.147211\n",
      "4094   630.250008   651.381563\n",
      "4095   732.203704   727.109405\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0952\n",
      "Epoch 22/25, Validation Loss: 0.0827\n",
      "           actual    predicted\n",
      "0      744.666668   749.949121\n",
      "1      657.999997   669.541262\n",
      "2     1745.500006  1695.247637\n",
      "3      816.666668   869.581141\n",
      "4      594.249998   613.334360\n",
      "...           ...          ...\n",
      "4091   668.666668   666.099159\n",
      "4092   545.500007   557.638244\n",
      "4093   571.000000   561.936471\n",
      "4094   630.250008   639.685112\n",
      "4095   732.203704   718.603543\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0935\n",
      "Epoch 23/25, Validation Loss: 0.0825\n",
      "           actual    predicted\n",
      "0      744.666668   753.198602\n",
      "1      657.999997   674.693630\n",
      "2     1745.500006  1691.468421\n",
      "3      816.666668   857.403140\n",
      "4      594.249998   619.358276\n",
      "...           ...          ...\n",
      "4091   668.666668   668.099564\n",
      "4092   545.500007   564.908773\n",
      "4093   571.000000   568.947190\n",
      "4094   630.250008   651.831043\n",
      "4095   732.203704   723.620178\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0940\n",
      "Epoch 24/25, Validation Loss: 0.0818\n",
      "           actual    predicted\n",
      "0      744.666668   762.771703\n",
      "1      657.999997   670.870375\n",
      "2     1745.500006  1699.444663\n",
      "3      816.666668   877.311315\n",
      "4      594.249998   622.597778\n",
      "...           ...          ...\n",
      "4091   668.666668   671.526940\n",
      "4092   545.500007   567.264221\n",
      "4093   571.000000   566.433150\n",
      "4094   630.250008   651.450305\n",
      "4095   732.203704   733.230050\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0929\n",
      "Epoch 25/25, Validation Loss: 0.0814\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3896e-01,  1.6481e+00,  1.2189e+00, -7.3040e-01, -9.9768e-02,\n",
      "          -8.3099e-02, -1.8816e+00, -5.9109e-01, -2.0547e+00, -2.0547e+00,\n",
      "          -1.4280e+00,  4.1557e+00,  0.0000e+00, -9.0600e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.3963e-01, -1.0391e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.4623e-01,  1.6198e+00,  1.1779e+00, -6.8921e-01, -1.1689e-01,\n",
      "          -8.7525e-02, -1.7402e+00, -5.9109e-01, -2.0932e+00, -2.0932e+00,\n",
      "          -1.0415e-01,  4.1557e+00,  0.0000e+00, -9.0586e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.9923e-01, -9.7336e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.5349e-01,  1.5915e+00,  1.1368e+00, -6.4801e-01, -1.3400e-01,\n",
      "          -9.1950e-02, -1.5988e+00, -5.9109e-01, -2.1317e+00, -2.1317e+00,\n",
      "           1.2197e+00,  4.1557e+00,  0.0000e+00, -9.0573e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.0588e+00, -9.0760e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6076e-01,  1.5632e+00,  1.0958e+00, -6.0681e-01, -1.5112e-01,\n",
      "          -9.6376e-02, -1.4575e+00, -5.9109e-01, -2.1702e+00, -2.1702e+00,\n",
      "           2.5435e+00,  4.1557e+00,  0.0000e+00, -9.0560e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1184e+00, -8.4183e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6802e-01,  1.5349e+00,  1.0548e+00, -5.6561e-01, -1.6824e-01,\n",
      "          -1.0080e-01, -1.3161e+00, -5.9109e-01, -2.2087e+00, -2.2087e+00,\n",
      "           3.8674e+00,  4.1557e+00,  0.0000e+00, -9.0546e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1780e+00, -7.7606e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-6.6942e-01,  2.1811e+00,  1.2599e+00, -1.2637e+00,  2.6910e-01,\n",
      "           9.8832e-04, -6.0919e-01, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0720e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -7.7521e-01, -1.1816e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-1.9069e-01,  1.6999e+00,  3.6163e+00, -1.2338e+00,  4.5965e-02,\n",
      "          -5.3226e-02, -1.3514e+00, -4.8829e-01,  9.1100e-01,  9.1100e-01,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0578e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -8.4576e-01, -1.1314e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 9.4465e-02,  1.2588e+00,  2.2277e+00, -1.1200e+00,  2.9364e-03,\n",
      "          -6.0971e-02, -8.4482e-01, -3.5272e-01,  1.3539e+00,  1.3539e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0413e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -9.1458e-01, -1.0755e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.0253e-01,  1.1187e+00,  1.8504e+00, -1.1149e+00, -9.2903e-03,\n",
      "          -6.9822e-02, -6.0919e-01, -5.0170e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0386e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1980e+00, -7.4471e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.3886e-01,  9.3658e-01,  9.4820e-01, -1.1187e+00, -1.0231e-02,\n",
      "          -6.9822e-02, -2.0230e+00, -5.9109e-01, -1.7273e+00, -1.7273e+00,\n",
      "           3.8674e+00, -7.9865e-01,  0.0000e+00, -9.0372e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0336e+00, -9.6137e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.8916e-01,  8.3876e-01,  8.9899e-01, -1.0463e+00, -3.2803e-02,\n",
      "          -7.4248e-02, -2.0230e+00, -5.0170e-01,  1.8354e+00,  1.8354e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0359e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0943e+00, -8.9111e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.3667e-01,  7.6072e-01,  8.6618e-01, -1.0424e+00, -3.8446e-02,\n",
      "          -7.8673e-02, -1.3161e+00, -3.2292e-01,  1.6428e+00,  1.6428e+00,\n",
      "          -2.7518e+00, -7.9865e-01,  0.0000e+00, -9.0346e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1502e+00, -8.1703e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5624e-01,  8.1171e-01,  8.0056e-01, -9.8904e-01, -4.2208e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0332e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3516e+00, -4.0081e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5344e-01,  8.1795e-01,  7.5135e-01, -8.2501e-01, -4.3149e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0319e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.2470e+00, -6.5866e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.7021e-01,  9.1750e-01,  6.1738e-01, -8.6316e-01, -8.3591e-02,\n",
      "          -9.2688e-02, -1.3161e+00, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           1.9004e-01, -7.9865e-01,  0.0000e+00, -9.0292e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3034e+00, -5.3698e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.9396e-01,  1.0256e+00,  1.1738e+00, -6.0948e-01, -5.2084e-02,\n",
      "          -8.5312e-02, -1.6695e+00, -5.9109e-01,  1.6909e+00,  1.6909e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0259e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3387e+00, -4.4220e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2890e-01,  9.8028e-01,  7.0214e-01, -7.7160e-01, -4.4090e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.7872e+00,  1.7872e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0239e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3664e+00, -3.4705e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.1772e-01,  1.0011e+00,  7.1034e-01, -7.4872e-01, -4.5971e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0225e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3862e+00, -2.5629e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2051e-01,  1.1052e+00,  7.7596e-01, -6.8387e-01, -4.4090e-02,\n",
      "          -8.3099e-02, -6.0919e-01, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "           5.5777e-01, -7.9865e-01,  0.0000e+00, -9.0212e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4001e+00, -1.6443e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.0375e-01,  1.1520e+00,  8.0056e-01, -6.1520e-01, -4.4090e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.5465e+00,  1.5465e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0199e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4080e+00, -7.1849e-02, -1.1547e+00,  1.1547e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.5046]], device='cuda:0')\n",
      "[648.98]\n",
      "            actual    predicted\n",
      "0       744.666668   762.771703\n",
      "1       657.999997   670.870375\n",
      "2      1745.500006  1699.444663\n",
      "3       816.666668   877.311315\n",
      "4       594.249998   622.597778\n",
      "...            ...          ...\n",
      "18994   599.999994   601.191890\n",
      "18995   856.000004   848.447914\n",
      "18996   611.000006   641.332207\n",
      "18997   705.499999   674.593311\n",
      "18998   663.999998   688.991980\n",
      "\n",
      "[18999 rows x 2 columns]\n",
      "Score (RMSE): 74.9080\n",
      "Score (MAE): 38.1087\n",
      "Score (ME): -3.5807\n",
      "Score (MAPE): 4.6523%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100108, 27) to (100414, 27)\n",
      "training data cutoff:  2023-07-14 01:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([75809, 20, 25]) torch.Size([75809]) torch.Size([75809, 1])\n",
      "Testing data shape: torch.Size([19182, 20, 25]) torch.Size([19182]) torch.Size([19182, 1])\n",
      "Shuffled Training data shape: torch.Size([75992, 20, 25]) torch.Size([75992]) torch.Size([75992, 1])\n",
      "Shuffled Testing data shape: torch.Size([18999, 20, 25]) torch.Size([18999]) torch.Size([18999, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     41.050000  35.360410\n",
      "1     42.105000  35.224478\n",
      "2     40.775000  43.247826\n",
      "3     43.880000  38.190056\n",
      "4     41.518793  40.855562\n",
      "...         ...        ...\n",
      "4091  33.437500  33.124023\n",
      "4092  36.932500  34.584519\n",
      "4093  34.807500  33.455353\n",
      "4094  17.625000  31.728195\n",
      "4095  21.982500  32.168895\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8190\n",
      "Epoch 1/25, Validation Loss: 0.3470\n",
      "         actual  predicted\n",
      "0     41.050000  38.368560\n",
      "1     42.105000  40.005658\n",
      "2     40.775000  40.022377\n",
      "3     43.880000  43.651956\n",
      "4     41.518793  42.294115\n",
      "...         ...        ...\n",
      "4091  33.437500  34.898784\n",
      "4092  36.932500  34.622178\n",
      "4093  34.807500  31.262148\n",
      "4094  17.625000  22.481058\n",
      "4095  21.982500  25.676859\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2232\n",
      "Epoch 2/25, Validation Loss: 0.1463\n",
      "         actual  predicted\n",
      "0     41.050000  37.699316\n",
      "1     42.105000  38.741960\n",
      "2     40.775000  39.743981\n",
      "3     43.880000  42.388967\n",
      "4     41.518793  42.485159\n",
      "...         ...        ...\n",
      "4091  33.437500  36.385024\n",
      "4092  36.932500  37.567165\n",
      "4093  34.807500  34.753134\n",
      "4094  17.625000  18.241987\n",
      "4095  21.982500  25.818954\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1209\n",
      "Epoch 3/25, Validation Loss: 0.1008\n",
      "         actual  predicted\n",
      "0     41.050000  37.862635\n",
      "1     42.105000  38.976300\n",
      "2     40.775000  39.842616\n",
      "3     43.880000  42.588677\n",
      "4     41.518793  43.501936\n",
      "...         ...        ...\n",
      "4091  33.437500  35.776752\n",
      "4092  36.932500  37.751260\n",
      "4093  34.807500  35.150269\n",
      "4094  17.625000  17.733018\n",
      "4095  21.982500  25.144656\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0930\n",
      "Epoch 4/25, Validation Loss: 0.0821\n",
      "         actual  predicted\n",
      "0     41.050000  38.426291\n",
      "1     42.105000  39.255808\n",
      "2     40.775000  39.460582\n",
      "3     43.880000  42.573622\n",
      "4     41.518793  43.606289\n",
      "...         ...        ...\n",
      "4091  33.437500  35.020712\n",
      "4092  36.932500  38.681042\n",
      "4093  34.807500  35.512075\n",
      "4094  17.625000  16.607470\n",
      "4095  21.982500  25.238175\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0774\n",
      "Epoch 5/25, Validation Loss: 0.0679\n",
      "         actual  predicted\n",
      "0     41.050000  39.551048\n",
      "1     42.105000  40.316665\n",
      "2     40.775000  40.008359\n",
      "3     43.880000  43.177286\n",
      "4     41.518793  44.411442\n",
      "...         ...        ...\n",
      "4091  33.437500  34.484024\n",
      "4092  36.932500  39.885039\n",
      "4093  34.807500  35.544266\n",
      "4094  17.625000  16.572415\n",
      "4095  21.982500  24.663102\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0658\n",
      "Epoch 6/25, Validation Loss: 0.0587\n",
      "         actual  predicted\n",
      "0     41.050000  39.864279\n",
      "1     42.105000  40.458690\n",
      "2     40.775000  39.960328\n",
      "3     43.880000  42.676401\n",
      "4     41.518793  43.574707\n",
      "...         ...        ...\n",
      "4091  33.437500  34.347121\n",
      "4092  36.932500  39.483737\n",
      "4093  34.807500  35.334752\n",
      "4094  17.625000  16.849998\n",
      "4095  21.982500  24.453441\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0571\n",
      "Epoch 7/25, Validation Loss: 0.0484\n",
      "         actual  predicted\n",
      "0     41.050000  40.300249\n",
      "1     42.105000  40.774498\n",
      "2     40.775000  40.463859\n",
      "3     43.880000  42.962746\n",
      "4     41.518793  43.521484\n",
      "...         ...        ...\n",
      "4091  33.437500  34.406733\n",
      "4092  36.932500  39.050420\n",
      "4093  34.807500  35.447196\n",
      "4094  17.625000  16.336492\n",
      "4095  21.982500  24.444383\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0505\n",
      "Epoch 8/25, Validation Loss: 0.0421\n",
      "         actual  predicted\n",
      "0     41.050000  39.926374\n",
      "1     42.105000  40.661742\n",
      "2     40.775000  40.279746\n",
      "3     43.880000  42.586381\n",
      "4     41.518793  42.969562\n",
      "...         ...        ...\n",
      "4091  33.437500  34.217702\n",
      "4092  36.932500  38.278392\n",
      "4093  34.807500  35.643254\n",
      "4094  17.625000  17.457558\n",
      "4095  21.982500  24.507186\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0459\n",
      "Epoch 9/25, Validation Loss: 0.0369\n",
      "         actual  predicted\n",
      "0     41.050000  40.337182\n",
      "1     42.105000  41.233143\n",
      "2     40.775000  40.775932\n",
      "3     43.880000  43.002875\n",
      "4     41.518793  43.519053\n",
      "...         ...        ...\n",
      "4091  33.437500  34.165481\n",
      "4092  36.932500  37.877050\n",
      "4093  34.807500  35.460160\n",
      "4094  17.625000  17.154228\n",
      "4095  21.982500  23.849743\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0415\n",
      "Epoch 10/25, Validation Loss: 0.0341\n",
      "         actual  predicted\n",
      "0     41.050000  40.086267\n",
      "1     42.105000  41.045344\n",
      "2     40.775000  40.502214\n",
      "3     43.880000  42.905488\n",
      "4     41.518793  43.567545\n",
      "...         ...        ...\n",
      "4091  33.437500  34.052969\n",
      "4092  36.932500  37.651109\n",
      "4093  34.807500  35.771395\n",
      "4094  17.625000  17.928220\n",
      "4095  21.982500  24.665334\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0384\n",
      "Epoch 11/25, Validation Loss: 0.0301\n",
      "         actual  predicted\n",
      "0     41.050000  39.634595\n",
      "1     42.105000  41.072217\n",
      "2     40.775000  40.538322\n",
      "3     43.880000  43.003886\n",
      "4     41.518793  43.352599\n",
      "...         ...        ...\n",
      "4091  33.437500  33.858267\n",
      "4092  36.932500  37.176286\n",
      "4093  34.807500  35.342759\n",
      "4094  17.625000  17.720424\n",
      "4095  21.982500  23.723016\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0356\n",
      "Epoch 12/25, Validation Loss: 0.0280\n",
      "         actual  predicted\n",
      "0     41.050000  39.680203\n",
      "1     42.105000  41.012119\n",
      "2     40.775000  40.463177\n",
      "3     43.880000  42.927198\n",
      "4     41.518793  43.620531\n",
      "...         ...        ...\n",
      "4091  33.437500  33.809930\n",
      "4092  36.932500  37.228639\n",
      "4093  34.807500  35.623268\n",
      "4094  17.625000  17.565259\n",
      "4095  21.982500  24.045150\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0334\n",
      "Epoch 13/25, Validation Loss: 0.0258\n",
      "         actual  predicted\n",
      "0     41.050000  39.373885\n",
      "1     42.105000  40.956427\n",
      "2     40.775000  40.659729\n",
      "3     43.880000  42.884380\n",
      "4     41.518793  43.433030\n",
      "...         ...        ...\n",
      "4091  33.437500  33.967747\n",
      "4092  36.932500  36.957446\n",
      "4093  34.807500  35.736353\n",
      "4094  17.625000  18.317555\n",
      "4095  21.982500  23.868575\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0311\n",
      "Epoch 14/25, Validation Loss: 0.0233\n",
      "         actual  predicted\n",
      "0     41.050000  39.451040\n",
      "1     42.105000  40.895540\n",
      "2     40.775000  40.721716\n",
      "3     43.880000  43.037358\n",
      "4     41.518793  43.468895\n",
      "...         ...        ...\n",
      "4091  33.437500  33.935637\n",
      "4092  36.932500  37.196302\n",
      "4093  34.807500  35.695173\n",
      "4094  17.625000  18.565532\n",
      "4095  21.982500  24.086200\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0295\n",
      "Epoch 15/25, Validation Loss: 0.0222\n",
      "         actual  predicted\n",
      "0     41.050000  39.383310\n",
      "1     42.105000  41.276502\n",
      "2     40.775000  40.799274\n",
      "3     43.880000  43.338768\n",
      "4     41.518793  43.478329\n",
      "...         ...        ...\n",
      "4091  33.437500  34.153709\n",
      "4092  36.932500  36.980380\n",
      "4093  34.807500  35.513925\n",
      "4094  17.625000  18.178777\n",
      "4095  21.982500  23.598598\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0279\n",
      "Epoch 16/25, Validation Loss: 0.0216\n",
      "         actual  predicted\n",
      "0     41.050000  39.443241\n",
      "1     42.105000  41.058845\n",
      "2     40.775000  40.353923\n",
      "3     43.880000  43.155743\n",
      "4     41.518793  43.193616\n",
      "...         ...        ...\n",
      "4091  33.437500  34.079868\n",
      "4092  36.932500  37.193293\n",
      "4093  34.807500  35.565236\n",
      "4094  17.625000  18.330395\n",
      "4095  21.982500  23.837499\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0268\n",
      "Epoch 17/25, Validation Loss: 0.0192\n",
      "         actual  predicted\n",
      "0     41.050000  39.168064\n",
      "1     42.105000  41.055647\n",
      "2     40.775000  40.588584\n",
      "3     43.880000  43.012586\n",
      "4     41.518793  43.087272\n",
      "...         ...        ...\n",
      "4091  33.437500  33.976019\n",
      "4092  36.932500  37.092197\n",
      "4093  34.807500  35.389615\n",
      "4094  17.625000  18.551711\n",
      "4095  21.982500  23.614282\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0256\n",
      "Epoch 18/25, Validation Loss: 0.0182\n",
      "         actual  predicted\n",
      "0     41.050000  39.317940\n",
      "1     42.105000  40.974554\n",
      "2     40.775000  40.484640\n",
      "3     43.880000  43.085216\n",
      "4     41.518793  43.131786\n",
      "...         ...        ...\n",
      "4091  33.437500  34.075224\n",
      "4092  36.932500  37.233420\n",
      "4093  34.807500  35.592240\n",
      "4094  17.625000  18.489948\n",
      "4095  21.982500  23.605547\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0245\n",
      "Epoch 19/25, Validation Loss: 0.0174\n",
      "         actual  predicted\n",
      "0     41.050000  39.545815\n",
      "1     42.105000  41.190094\n",
      "2     40.775000  40.805896\n",
      "3     43.880000  43.463600\n",
      "4     41.518793  43.338131\n",
      "...         ...        ...\n",
      "4091  33.437500  34.099097\n",
      "4092  36.932500  37.492943\n",
      "4093  34.807500  35.609169\n",
      "4094  17.625000  18.122867\n",
      "4095  21.982500  23.421043\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0241\n",
      "Epoch 20/25, Validation Loss: 0.0179\n",
      "         actual  predicted\n",
      "0     41.050000  39.276278\n",
      "1     42.105000  41.229499\n",
      "2     40.775000  40.758516\n",
      "3     43.880000  43.353686\n",
      "4     41.518793  43.203676\n",
      "...         ...        ...\n",
      "4091  33.437500  34.100386\n",
      "4092  36.932500  37.359182\n",
      "4093  34.807500  35.413951\n",
      "4094  17.625000  18.066378\n",
      "4095  21.982500  23.265231\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0227\n",
      "Epoch 21/25, Validation Loss: 0.0163\n",
      "         actual  predicted\n",
      "0     41.050000  39.693154\n",
      "1     42.105000  40.934359\n",
      "2     40.775000  40.595687\n",
      "3     43.880000  43.089885\n",
      "4     41.518793  43.075620\n",
      "...         ...        ...\n",
      "4091  33.437500  34.013511\n",
      "4092  36.932500  37.347400\n",
      "4093  34.807500  35.375630\n",
      "4094  17.625000  18.392386\n",
      "4095  21.982500  23.501233\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0221\n",
      "Epoch 22/25, Validation Loss: 0.0156\n",
      "         actual  predicted\n",
      "0     41.050000  39.265283\n",
      "1     42.105000  41.056988\n",
      "2     40.775000  40.780032\n",
      "3     43.880000  43.144797\n",
      "4     41.518793  43.336439\n",
      "...         ...        ...\n",
      "4091  33.437500  33.987220\n",
      "4092  36.932500  37.289629\n",
      "4093  34.807500  35.514531\n",
      "4094  17.625000  18.354298\n",
      "4095  21.982500  23.279136\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0215\n",
      "Epoch 23/25, Validation Loss: 0.0151\n",
      "         actual  predicted\n",
      "0     41.050000  39.369151\n",
      "1     42.105000  41.173617\n",
      "2     40.775000  40.755972\n",
      "3     43.880000  43.450493\n",
      "4     41.518793  43.178887\n",
      "...         ...        ...\n",
      "4091  33.437500  34.000523\n",
      "4092  36.932500  37.349778\n",
      "4093  34.807500  35.502611\n",
      "4094  17.625000  18.163104\n",
      "4095  21.982500  23.086990\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0208\n",
      "Epoch 24/25, Validation Loss: 0.0146\n",
      "         actual  predicted\n",
      "0     41.050000  39.831991\n",
      "1     42.105000  41.105818\n",
      "2     40.775000  40.594321\n",
      "3     43.880000  43.323907\n",
      "4     41.518793  42.881471\n",
      "...         ...        ...\n",
      "4091  33.437500  33.958709\n",
      "4092  36.932500  37.668263\n",
      "4093  34.807500  35.316355\n",
      "4094  17.625000  18.279099\n",
      "4095  21.982500  23.283430\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0200\n",
      "Epoch 25/25, Validation Loss: 0.0142\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3896e-01,  1.6481e+00,  1.2189e+00, -7.3040e-01, -9.9768e-02,\n",
      "          -8.3099e-02, -1.8816e+00, -5.9109e-01, -2.0547e+00, -2.0547e+00,\n",
      "          -1.4280e+00,  4.1557e+00,  0.0000e+00, -9.0600e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.3963e-01, -1.0391e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.4623e-01,  1.6198e+00,  1.1779e+00, -6.8921e-01, -1.1689e-01,\n",
      "          -8.7525e-02, -1.7402e+00, -5.9109e-01, -2.0932e+00, -2.0932e+00,\n",
      "          -1.0415e-01,  4.1557e+00,  0.0000e+00, -9.0586e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.9923e-01, -9.7336e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.5349e-01,  1.5915e+00,  1.1368e+00, -6.4801e-01, -1.3400e-01,\n",
      "          -9.1950e-02, -1.5988e+00, -5.9109e-01, -2.1317e+00, -2.1317e+00,\n",
      "           1.2197e+00,  4.1557e+00,  0.0000e+00, -9.0573e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.0588e+00, -9.0760e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6076e-01,  1.5632e+00,  1.0958e+00, -6.0681e-01, -1.5112e-01,\n",
      "          -9.6376e-02, -1.4575e+00, -5.9109e-01, -2.1702e+00, -2.1702e+00,\n",
      "           2.5435e+00,  4.1557e+00,  0.0000e+00, -9.0560e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1184e+00, -8.4183e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6802e-01,  1.5349e+00,  1.0548e+00, -5.6561e-01, -1.6824e-01,\n",
      "          -1.0080e-01, -1.3161e+00, -5.9109e-01, -2.2087e+00, -2.2087e+00,\n",
      "           3.8674e+00,  4.1557e+00,  0.0000e+00, -9.0546e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1780e+00, -7.7606e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-6.6942e-01,  2.1811e+00,  1.2599e+00, -1.2637e+00,  2.6910e-01,\n",
      "           9.8832e-04, -6.0919e-01, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0720e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -7.7521e-01, -1.1816e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-1.9069e-01,  1.6999e+00,  3.6163e+00, -1.2338e+00,  4.5965e-02,\n",
      "          -5.3226e-02, -1.3514e+00, -4.8829e-01,  9.1100e-01,  9.1100e-01,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0578e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -8.4576e-01, -1.1314e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 9.4465e-02,  1.2588e+00,  2.2277e+00, -1.1200e+00,  2.9364e-03,\n",
      "          -6.0971e-02, -8.4482e-01, -3.5272e-01,  1.3539e+00,  1.3539e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0413e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -9.1458e-01, -1.0755e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.0253e-01,  1.1187e+00,  1.8504e+00, -1.1149e+00, -9.2903e-03,\n",
      "          -6.9822e-02, -6.0919e-01, -5.0170e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0386e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1980e+00, -7.4471e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.3886e-01,  9.3658e-01,  9.4820e-01, -1.1187e+00, -1.0231e-02,\n",
      "          -6.9822e-02, -2.0230e+00, -5.9109e-01, -1.7273e+00, -1.7273e+00,\n",
      "           3.8674e+00, -7.9865e-01,  0.0000e+00, -9.0372e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0336e+00, -9.6137e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.8916e-01,  8.3876e-01,  8.9899e-01, -1.0463e+00, -3.2803e-02,\n",
      "          -7.4248e-02, -2.0230e+00, -5.0170e-01,  1.8354e+00,  1.8354e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0359e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0943e+00, -8.9111e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.3667e-01,  7.6072e-01,  8.6618e-01, -1.0424e+00, -3.8446e-02,\n",
      "          -7.8673e-02, -1.3161e+00, -3.2292e-01,  1.6428e+00,  1.6428e+00,\n",
      "          -2.7518e+00, -7.9865e-01,  0.0000e+00, -9.0346e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1502e+00, -8.1703e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5624e-01,  8.1171e-01,  8.0056e-01, -9.8904e-01, -4.2208e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0332e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3516e+00, -4.0081e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5344e-01,  8.1795e-01,  7.5135e-01, -8.2501e-01, -4.3149e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0319e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.2470e+00, -6.5866e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.7021e-01,  9.1750e-01,  6.1738e-01, -8.6316e-01, -8.3591e-02,\n",
      "          -9.2688e-02, -1.3161e+00, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           1.9004e-01, -7.9865e-01,  0.0000e+00, -9.0292e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3034e+00, -5.3698e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.9396e-01,  1.0256e+00,  1.1738e+00, -6.0948e-01, -5.2084e-02,\n",
      "          -8.5312e-02, -1.6695e+00, -5.9109e-01,  1.6909e+00,  1.6909e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0259e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3387e+00, -4.4220e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2890e-01,  9.8028e-01,  7.0214e-01, -7.7160e-01, -4.4090e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.7872e+00,  1.7872e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0239e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3664e+00, -3.4705e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.1772e-01,  1.0011e+00,  7.1034e-01, -7.4872e-01, -4.5971e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0225e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3862e+00, -2.5629e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2051e-01,  1.1052e+00,  7.7596e-01, -6.8387e-01, -4.4090e-02,\n",
      "          -8.3099e-02, -6.0919e-01, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "           5.5777e-01, -7.9865e-01,  0.0000e+00, -9.0212e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4001e+00, -1.6443e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.0375e-01,  1.1520e+00,  8.0056e-01, -6.1520e-01, -4.4090e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.5465e+00,  1.5465e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0199e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4080e+00, -7.1849e-02, -1.1547e+00,  1.1547e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[1.0728]], device='cuda:0')\n",
      "[45.83]\n",
      "          actual  predicted\n",
      "0      41.050000  39.831991\n",
      "1      42.105000  41.105818\n",
      "2      40.775000  40.594321\n",
      "3      43.880000  43.323907\n",
      "4      41.518793  42.881471\n",
      "...          ...        ...\n",
      "18994  41.805000  40.981040\n",
      "18995  31.671818  31.554707\n",
      "18996  39.426667  39.513925\n",
      "18997  42.937500  42.123881\n",
      "18998  53.585000  52.250410\n",
      "\n",
      "[18999 rows x 2 columns]\n",
      "Score (RMSE): 1.1496\n",
      "Score (MAE): 0.7237\n",
      "Score (ME): 0.0473\n",
      "Score (MAPE): 1.9815%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100108, 27) to (100414, 27)\n",
      "training data cutoff:  2023-07-14 01:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([75809, 20, 25]) torch.Size([75809]) torch.Size([75809, 1])\n",
      "Testing data shape: torch.Size([19182, 20, 25]) torch.Size([19182]) torch.Size([19182, 1])\n",
      "Shuffled Training data shape: torch.Size([75992, 20, 25]) torch.Size([75992]) torch.Size([75992, 1])\n",
      "Shuffled Testing data shape: torch.Size([18999, 20, 25]) torch.Size([18999]) torch.Size([18999, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual  predicted\n",
      "0     21.815714  23.289255\n",
      "1     21.910000  23.134744\n",
      "2     27.730000  27.503841\n",
      "3     22.987500  22.505728\n",
      "4     23.800000  22.908971\n",
      "...         ...        ...\n",
      "4091  28.027500  27.510415\n",
      "4092  28.710000  27.941751\n",
      "4093  24.004400  26.112477\n",
      "4094  24.745000  22.365962\n",
      "4095  16.891389  21.443838\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.7715\n",
      "Epoch 1/25, Validation Loss: 0.3182\n",
      "         actual  predicted\n",
      "0     21.815714  22.111715\n",
      "1     21.910000  22.139851\n",
      "2     27.730000  26.166353\n",
      "3     22.987500  22.247421\n",
      "4     23.800000  23.340839\n",
      "...         ...        ...\n",
      "4091  28.027500  26.449689\n",
      "4092  28.710000  28.573281\n",
      "4093  24.004400  24.274252\n",
      "4094  24.745000  23.693638\n",
      "4095  16.891389  18.960823\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.2202\n",
      "Epoch 2/25, Validation Loss: 0.1463\n",
      "         actual  predicted\n",
      "0     21.815714  22.382117\n",
      "1     21.910000  22.628567\n",
      "2     27.730000  26.092543\n",
      "3     22.987500  23.332760\n",
      "4     23.800000  23.553260\n",
      "...         ...        ...\n",
      "4091  28.027500  26.338690\n",
      "4092  28.710000  28.802652\n",
      "4093  24.004400  24.542852\n",
      "4094  24.745000  24.071230\n",
      "4095  16.891389  18.524256\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.1144\n",
      "Epoch 3/25, Validation Loss: 0.0986\n",
      "         actual  predicted\n",
      "0     21.815714  22.554912\n",
      "1     21.910000  22.670820\n",
      "2     27.730000  26.246694\n",
      "3     22.987500  23.141465\n",
      "4     23.800000  23.561816\n",
      "...         ...        ...\n",
      "4091  28.027500  26.158334\n",
      "4092  28.710000  28.854063\n",
      "4093  24.004400  24.596388\n",
      "4094  24.745000  24.333773\n",
      "4095  16.891389  18.308640\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.0930\n",
      "Epoch 4/25, Validation Loss: 0.0880\n",
      "         actual  predicted\n",
      "0     21.815714  22.504291\n",
      "1     21.910000  22.479808\n",
      "2     27.730000  26.211221\n",
      "3     22.987500  23.152598\n",
      "4     23.800000  23.775683\n",
      "...         ...        ...\n",
      "4091  28.027500  26.161925\n",
      "4092  28.710000  28.900377\n",
      "4093  24.004400  24.842991\n",
      "4094  24.745000  24.522812\n",
      "4095  16.891389  18.469710\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.0841\n",
      "Epoch 5/25, Validation Loss: 0.0810\n",
      "         actual  predicted\n",
      "0     21.815714  22.560156\n",
      "1     21.910000  22.422810\n",
      "2     27.730000  26.283350\n",
      "3     22.987500  23.172859\n",
      "4     23.800000  23.914478\n",
      "...         ...        ...\n",
      "4091  28.027500  26.211567\n",
      "4092  28.710000  28.939565\n",
      "4093  24.004400  25.149306\n",
      "4094  24.745000  24.734856\n",
      "4095  16.891389  18.538563\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.0782\n",
      "Epoch 6/25, Validation Loss: 0.0750\n",
      "         actual  predicted\n",
      "0     21.815714  22.473847\n",
      "1     21.910000  22.408668\n",
      "2     27.730000  26.026542\n",
      "3     22.987500  23.112612\n",
      "4     23.800000  24.025600\n",
      "...         ...        ...\n",
      "4091  28.027500  26.376639\n",
      "4092  28.710000  28.962282\n",
      "4093  24.004400  25.457726\n",
      "4094  24.745000  24.908754\n",
      "4095  16.891389  18.321350\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.0724\n",
      "Epoch 7/25, Validation Loss: 0.0693\n",
      "         actual  predicted\n",
      "0     21.815714  22.395549\n",
      "1     21.910000  22.431550\n",
      "2     27.730000  26.111638\n",
      "3     22.987500  23.142803\n",
      "4     23.800000  23.963831\n",
      "...         ...        ...\n",
      "4091  28.027500  26.712669\n",
      "4092  28.710000  29.318648\n",
      "4093  24.004400  25.705337\n",
      "4094  24.745000  24.817737\n",
      "4095  16.891389  18.333892\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.0672\n",
      "Epoch 8/25, Validation Loss: 0.0649\n",
      "         actual  predicted\n",
      "0     21.815714  22.333552\n",
      "1     21.910000  22.328070\n",
      "2     27.730000  26.134416\n",
      "3     22.987500  23.113828\n",
      "4     23.800000  23.925245\n",
      "...         ...        ...\n",
      "4091  28.027500  26.984205\n",
      "4092  28.710000  29.403168\n",
      "4093  24.004400  25.709654\n",
      "4094  24.745000  24.714572\n",
      "4095  16.891389  18.419790\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.0629\n",
      "Epoch 9/25, Validation Loss: 0.0618\n",
      "         actual  predicted\n",
      "0     21.815714  22.309511\n",
      "1     21.910000  22.356085\n",
      "2     27.730000  26.012553\n",
      "3     22.987500  23.185203\n",
      "4     23.800000  23.937094\n",
      "...         ...        ...\n",
      "4091  28.027500  27.156373\n",
      "4092  28.710000  29.150888\n",
      "4093  24.004400  25.812415\n",
      "4094  24.745000  24.782877\n",
      "4095  16.891389  18.485467\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.0585\n",
      "Epoch 10/25, Validation Loss: 0.0544\n",
      "         actual  predicted\n",
      "0     21.815714  22.176314\n",
      "1     21.910000  22.167900\n",
      "2     27.730000  26.070827\n",
      "3     22.987500  23.108533\n",
      "4     23.800000  23.857866\n",
      "...         ...        ...\n",
      "4091  28.027500  27.283782\n",
      "4092  28.710000  29.110708\n",
      "4093  24.004400  25.789836\n",
      "4094  24.745000  24.614021\n",
      "4095  16.891389  18.274787\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.0538\n",
      "Epoch 11/25, Validation Loss: 0.0493\n",
      "         actual  predicted\n",
      "0     21.815714  22.103971\n",
      "1     21.910000  22.200356\n",
      "2     27.730000  25.956651\n",
      "3     22.987500  23.231824\n",
      "4     23.800000  23.922477\n",
      "...         ...        ...\n",
      "4091  28.027500  27.258093\n",
      "4092  28.710000  28.768794\n",
      "4093  24.004400  25.978986\n",
      "4094  24.745000  24.682386\n",
      "4095  16.891389  18.321026\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.0488\n",
      "Epoch 12/25, Validation Loss: 0.0436\n",
      "         actual  predicted\n",
      "0     21.815714  22.115866\n",
      "1     21.910000  22.223603\n",
      "2     27.730000  26.055834\n",
      "3     22.987500  23.134927\n",
      "4     23.800000  23.819525\n",
      "...         ...        ...\n",
      "4091  28.027500  27.410571\n",
      "4092  28.710000  29.058691\n",
      "4093  24.004400  26.314964\n",
      "4094  24.745000  24.565989\n",
      "4095  16.891389  18.142931\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.0435\n",
      "Epoch 13/25, Validation Loss: 0.0373\n",
      "         actual  predicted\n",
      "0     21.815714  21.931322\n",
      "1     21.910000  22.098184\n",
      "2     27.730000  26.055898\n",
      "3     22.987500  23.153628\n",
      "4     23.800000  23.920024\n",
      "...         ...        ...\n",
      "4091  28.027500  27.409162\n",
      "4092  28.710000  29.140528\n",
      "4093  24.004400  26.615332\n",
      "4094  24.745000  24.619610\n",
      "4095  16.891389  17.853529\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.0383\n",
      "Epoch 14/25, Validation Loss: 0.0328\n",
      "         actual  predicted\n",
      "0     21.815714  21.951009\n",
      "1     21.910000  22.119880\n",
      "2     27.730000  26.127557\n",
      "3     22.987500  23.202848\n",
      "4     23.800000  23.932678\n",
      "...         ...        ...\n",
      "4091  28.027500  27.461461\n",
      "4092  28.710000  29.144227\n",
      "4093  24.004400  26.613481\n",
      "4094  24.745000  24.649789\n",
      "4095  16.891389  17.768029\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.0350\n",
      "Epoch 15/25, Validation Loss: 0.0287\n",
      "         actual  predicted\n",
      "0     21.815714  21.883588\n",
      "1     21.910000  22.191408\n",
      "2     27.730000  25.889115\n",
      "3     22.987500  23.174957\n",
      "4     23.800000  23.922072\n",
      "...         ...        ...\n",
      "4091  28.027500  27.381753\n",
      "4092  28.710000  28.959552\n",
      "4093  24.004400  26.706543\n",
      "4094  24.745000  24.614450\n",
      "4095  16.891389  17.790174\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.0322\n",
      "Epoch 16/25, Validation Loss: 0.0252\n",
      "         actual  predicted\n",
      "0     21.815714  21.845457\n",
      "1     21.910000  22.201157\n",
      "2     27.730000  26.103022\n",
      "3     22.987500  23.185175\n",
      "4     23.800000  23.911859\n",
      "...         ...        ...\n",
      "4091  28.027500  27.541187\n",
      "4092  28.710000  28.911889\n",
      "4093  24.004400  26.803190\n",
      "4094  24.745000  24.671644\n",
      "4095  16.891389  17.536874\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.0296\n",
      "Epoch 17/25, Validation Loss: 0.0233\n",
      "         actual  predicted\n",
      "0     21.815714  21.795970\n",
      "1     21.910000  22.112456\n",
      "2     27.730000  25.838297\n",
      "3     22.987500  23.206629\n",
      "4     23.800000  23.893229\n",
      "...         ...        ...\n",
      "4091  28.027500  27.380893\n",
      "4092  28.710000  28.708295\n",
      "4093  24.004400  26.088546\n",
      "4094  24.745000  24.651683\n",
      "4095  16.891389  17.537395\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.0278\n",
      "Epoch 18/25, Validation Loss: 0.0215\n",
      "         actual  predicted\n",
      "0     21.815714  21.842601\n",
      "1     21.910000  22.109685\n",
      "2     27.730000  25.908345\n",
      "3     22.987500  23.182639\n",
      "4     23.800000  23.882622\n",
      "...         ...        ...\n",
      "4091  28.027500  27.409073\n",
      "4092  28.710000  28.781380\n",
      "4093  24.004400  25.687101\n",
      "4094  24.745000  24.609881\n",
      "4095  16.891389  17.483328\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.0266\n",
      "Epoch 19/25, Validation Loss: 0.0203\n",
      "         actual  predicted\n",
      "0     21.815714  21.952057\n",
      "1     21.910000  22.216279\n",
      "2     27.730000  26.000091\n",
      "3     22.987500  23.104066\n",
      "4     23.800000  23.818855\n",
      "...         ...        ...\n",
      "4091  28.027500  27.465400\n",
      "4092  28.710000  28.775524\n",
      "4093  24.004400  25.725905\n",
      "4094  24.745000  24.537148\n",
      "4095  16.891389  17.525749\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.0254\n",
      "Epoch 20/25, Validation Loss: 0.0189\n",
      "         actual  predicted\n",
      "0     21.815714  21.847547\n",
      "1     21.910000  22.098179\n",
      "2     27.730000  26.133826\n",
      "3     22.987500  23.162835\n",
      "4     23.800000  23.866211\n",
      "...         ...        ...\n",
      "4091  28.027500  27.617856\n",
      "4092  28.710000  28.791058\n",
      "4093  24.004400  25.819302\n",
      "4094  24.745000  24.606269\n",
      "4095  16.891389  17.247292\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.0241\n",
      "Epoch 21/25, Validation Loss: 0.0181\n",
      "         actual  predicted\n",
      "0     21.815714  21.882863\n",
      "1     21.910000  22.119362\n",
      "2     27.730000  26.189510\n",
      "3     22.987500  23.205140\n",
      "4     23.800000  23.907619\n",
      "...         ...        ...\n",
      "4091  28.027500  27.685859\n",
      "4092  28.710000  28.930859\n",
      "4093  24.004400  25.372458\n",
      "4094  24.745000  24.682982\n",
      "4095  16.891389  17.346759\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.0232\n",
      "Epoch 22/25, Validation Loss: 0.0177\n",
      "         actual  predicted\n",
      "0     21.815714  21.857196\n",
      "1     21.910000  22.060181\n",
      "2     27.730000  26.205811\n",
      "3     22.987500  23.099450\n",
      "4     23.800000  23.825401\n",
      "...         ...        ...\n",
      "4091  28.027500  27.659194\n",
      "4092  28.710000  28.974373\n",
      "4093  24.004400  25.282456\n",
      "4094  24.745000  24.613988\n",
      "4095  16.891389  17.072871\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.0225\n",
      "Epoch 23/25, Validation Loss: 0.0169\n",
      "         actual  predicted\n",
      "0     21.815714  21.899637\n",
      "1     21.910000  22.173821\n",
      "2     27.730000  26.147489\n",
      "3     22.987500  23.149873\n",
      "4     23.800000  23.849790\n",
      "...         ...        ...\n",
      "4091  28.027500  27.495327\n",
      "4092  28.710000  28.781484\n",
      "4093  24.004400  25.102696\n",
      "4094  24.745000  24.647137\n",
      "4095  16.891389  17.149920\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.0216\n",
      "Epoch 24/25, Validation Loss: 0.0160\n",
      "         actual  predicted\n",
      "0     21.815714  21.948953\n",
      "1     21.910000  22.182420\n",
      "2     27.730000  26.307928\n",
      "3     22.987500  23.177741\n",
      "4     23.800000  23.896080\n",
      "...         ...        ...\n",
      "4091  28.027500  27.696756\n",
      "4092  28.710000  28.944396\n",
      "4093  24.004400  25.187323\n",
      "4094  24.745000  24.695169\n",
      "4095  16.891389  17.221565\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.0214\n",
      "Epoch 25/25, Validation Loss: 0.0160\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3896e-01,  1.6481e+00,  1.2189e+00, -7.3040e-01, -9.9768e-02,\n",
      "          -8.3099e-02, -1.8816e+00, -5.9109e-01, -2.0547e+00, -2.0547e+00,\n",
      "          -1.4280e+00,  4.1557e+00,  0.0000e+00, -9.0600e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.3963e-01, -1.0391e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.4623e-01,  1.6198e+00,  1.1779e+00, -6.8921e-01, -1.1689e-01,\n",
      "          -8.7525e-02, -1.7402e+00, -5.9109e-01, -2.0932e+00, -2.0932e+00,\n",
      "          -1.0415e-01,  4.1557e+00,  0.0000e+00, -9.0586e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.9923e-01, -9.7336e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.5349e-01,  1.5915e+00,  1.1368e+00, -6.4801e-01, -1.3400e-01,\n",
      "          -9.1950e-02, -1.5988e+00, -5.9109e-01, -2.1317e+00, -2.1317e+00,\n",
      "           1.2197e+00,  4.1557e+00,  0.0000e+00, -9.0573e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.0588e+00, -9.0760e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6076e-01,  1.5632e+00,  1.0958e+00, -6.0681e-01, -1.5112e-01,\n",
      "          -9.6376e-02, -1.4575e+00, -5.9109e-01, -2.1702e+00, -2.1702e+00,\n",
      "           2.5435e+00,  4.1557e+00,  0.0000e+00, -9.0560e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1184e+00, -8.4183e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6802e-01,  1.5349e+00,  1.0548e+00, -5.6561e-01, -1.6824e-01,\n",
      "          -1.0080e-01, -1.3161e+00, -5.9109e-01, -2.2087e+00, -2.2087e+00,\n",
      "           3.8674e+00,  4.1557e+00,  0.0000e+00, -9.0546e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1780e+00, -7.7606e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-6.6942e-01,  2.1811e+00,  1.2599e+00, -1.2637e+00,  2.6910e-01,\n",
      "           9.8832e-04, -6.0919e-01, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0720e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -7.7521e-01, -1.1816e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-1.9069e-01,  1.6999e+00,  3.6163e+00, -1.2338e+00,  4.5965e-02,\n",
      "          -5.3226e-02, -1.3514e+00, -4.8829e-01,  9.1100e-01,  9.1100e-01,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0578e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -8.4576e-01, -1.1314e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 9.4465e-02,  1.2588e+00,  2.2277e+00, -1.1200e+00,  2.9364e-03,\n",
      "          -6.0971e-02, -8.4482e-01, -3.5272e-01,  1.3539e+00,  1.3539e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0413e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -9.1458e-01, -1.0755e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.0253e-01,  1.1187e+00,  1.8504e+00, -1.1149e+00, -9.2903e-03,\n",
      "          -6.9822e-02, -6.0919e-01, -5.0170e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0386e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1980e+00, -7.4471e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.3886e-01,  9.3658e-01,  9.4820e-01, -1.1187e+00, -1.0231e-02,\n",
      "          -6.9822e-02, -2.0230e+00, -5.9109e-01, -1.7273e+00, -1.7273e+00,\n",
      "           3.8674e+00, -7.9865e-01,  0.0000e+00, -9.0372e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0336e+00, -9.6137e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.8916e-01,  8.3876e-01,  8.9899e-01, -1.0463e+00, -3.2803e-02,\n",
      "          -7.4248e-02, -2.0230e+00, -5.0170e-01,  1.8354e+00,  1.8354e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0359e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0943e+00, -8.9111e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.3667e-01,  7.6072e-01,  8.6618e-01, -1.0424e+00, -3.8446e-02,\n",
      "          -7.8673e-02, -1.3161e+00, -3.2292e-01,  1.6428e+00,  1.6428e+00,\n",
      "          -2.7518e+00, -7.9865e-01,  0.0000e+00, -9.0346e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1502e+00, -8.1703e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5624e-01,  8.1171e-01,  8.0056e-01, -9.8904e-01, -4.2208e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0332e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3516e+00, -4.0081e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5344e-01,  8.1795e-01,  7.5135e-01, -8.2501e-01, -4.3149e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0319e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.2470e+00, -6.5866e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.7021e-01,  9.1750e-01,  6.1738e-01, -8.6316e-01, -8.3591e-02,\n",
      "          -9.2688e-02, -1.3161e+00, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           1.9004e-01, -7.9865e-01,  0.0000e+00, -9.0292e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3034e+00, -5.3698e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.9396e-01,  1.0256e+00,  1.1738e+00, -6.0948e-01, -5.2084e-02,\n",
      "          -8.5312e-02, -1.6695e+00, -5.9109e-01,  1.6909e+00,  1.6909e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0259e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3387e+00, -4.4220e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2890e-01,  9.8028e-01,  7.0214e-01, -7.7160e-01, -4.4090e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.7872e+00,  1.7872e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0239e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3664e+00, -3.4705e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.1772e-01,  1.0011e+00,  7.1034e-01, -7.4872e-01, -4.5971e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0225e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3862e+00, -2.5629e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2051e-01,  1.1052e+00,  7.7596e-01, -6.8387e-01, -4.4090e-02,\n",
      "          -8.3099e-02, -6.0919e-01, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "           5.5777e-01, -7.9865e-01,  0.0000e+00, -9.0212e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4001e+00, -1.6443e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.0375e-01,  1.1520e+00,  8.0056e-01, -6.1520e-01, -4.4090e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.5465e+00,  1.5465e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0199e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4080e+00, -7.1849e-02, -1.1547e+00,  1.1547e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[0.3584]], device='cuda:0')\n",
      "[25.32]\n",
      "          actual  predicted\n",
      "0      21.815714  21.948953\n",
      "1      21.910000  22.182420\n",
      "2      27.730000  26.307928\n",
      "3      22.987500  23.177741\n",
      "4      23.800000  23.896080\n",
      "...          ...        ...\n",
      "18994  25.074000  25.184271\n",
      "18995  22.900000  22.859519\n",
      "18996  26.357500  26.200671\n",
      "18997  30.040000  30.380456\n",
      "18998  20.890000  20.944527\n",
      "\n",
      "[18999 rows x 2 columns]\n",
      "Score (RMSE): 0.4522\n",
      "Score (MAE): 0.2769\n",
      "Score (ME): -0.0800\n",
      "Score (MAPE): 1.1286%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (100108, 27) to (100414, 27)\n",
      "training data cutoff:  2023-07-14 01:00:00\n",
      "date_time_rounded    datetime64[ns]\n",
      "device_id                    object\n",
      "tmp                         float64\n",
      "hum                         float64\n",
      "CO2                         float64\n",
      "VOC                         float64\n",
      "vis                         float64\n",
      "IR                          float64\n",
      "WIFI                        float64\n",
      "BLE                         float64\n",
      "rssi                        float64\n",
      "channel_rssi                float64\n",
      "channel_index               float64\n",
      "spreading_factor            float64\n",
      "bandwidth                   float64\n",
      "f_cnt                       float64\n",
      "isHoliday                   float64\n",
      "isExamTime                  float64\n",
      "weekday_sin                 float64\n",
      "weekday_cos                 float64\n",
      "month_sin                   float64\n",
      "month_cos                   float64\n",
      "time_sin                    float64\n",
      "time_cos                    float64\n",
      "semester_SS23               float64\n",
      "semester_WS22/23            float64\n",
      "semester_WS23/24            float64\n",
      "group                         int32\n",
      "device_id_codes                int8\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([75809, 20, 25]) torch.Size([75809]) torch.Size([75809, 1])\n",
      "Testing data shape: torch.Size([19182, 20, 25]) torch.Size([19182]) torch.Size([19182, 1])\n",
      "Shuffled Training data shape: torch.Size([75992, 20, 25]) torch.Size([75992]) torch.Size([75992, 1])\n",
      "Shuffled Testing data shape: torch.Size([18999, 20, 25]) torch.Size([18999]) torch.Size([18999, 1])\n",
      "same columns\n",
      "same dtypes\n",
      "Dataframe is already saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-ehm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          actual   predicted\n",
      "0       5.749999  255.105675\n",
      "1     125.250003   59.065205\n",
      "2       5.500003  172.926174\n",
      "3       5.250008  173.004920\n",
      "4      16.249997   64.813753\n",
      "...          ...         ...\n",
      "4091    4.999996  517.173482\n",
      "4092   29.249997  -46.125536\n",
      "4093    5.666661  143.058942\n",
      "4094    6.500001  261.011744\n",
      "4095    5.999994   91.978144\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 1/25, Training Loss: 0.8912\n",
      "Epoch 1/25, Validation Loss: 0.9382\n",
      "          actual   predicted\n",
      "0       5.749999  -28.401336\n",
      "1     125.250003  113.270644\n",
      "2       5.500003  412.569175\n",
      "3       5.250008   17.939870\n",
      "4      16.249997  102.842069\n",
      "...          ...         ...\n",
      "4091    4.999996   42.118353\n",
      "4092   29.249997  -59.237233\n",
      "4093    5.666661  -64.935383\n",
      "4094    6.500001   -1.527944\n",
      "4095    5.999994   40.806333\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 2/25, Training Loss: 0.8702\n",
      "Epoch 2/25, Validation Loss: 0.8837\n",
      "          actual   predicted\n",
      "0       5.749999    4.326962\n",
      "1     125.250003   74.961074\n",
      "2       5.500003  223.564835\n",
      "3       5.250008   61.866836\n",
      "4      16.249997   79.739569\n",
      "...          ...         ...\n",
      "4091    4.999996   10.932900\n",
      "4092   29.249997  -55.100766\n",
      "4093    5.666661  -16.447343\n",
      "4094    6.500001   16.064184\n",
      "4095    5.999994   26.264406\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 3/25, Training Loss: 0.8331\n",
      "Epoch 3/25, Validation Loss: 0.8302\n",
      "          actual   predicted\n",
      "0       5.749999   55.456550\n",
      "1     125.250003   94.449239\n",
      "2       5.500003  105.662950\n",
      "3       5.250008  119.965726\n",
      "4      16.249997  106.441550\n",
      "...          ...         ...\n",
      "4091    4.999996   70.723573\n",
      "4092   29.249997  -37.508100\n",
      "4093    5.666661   38.956487\n",
      "4094    6.500001   65.449539\n",
      "4095    5.999994   24.813058\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 4/25, Training Loss: 0.7762\n",
      "Epoch 4/25, Validation Loss: 0.8027\n",
      "          actual   predicted\n",
      "0       5.749999   50.808986\n",
      "1     125.250003  148.322008\n",
      "2       5.500003   37.147122\n",
      "3       5.250008   96.923012\n",
      "4      16.249997  158.659939\n",
      "...          ...         ...\n",
      "4091    4.999996   15.771395\n",
      "4092   29.249997  -88.121410\n",
      "4093    5.666661   -3.954796\n",
      "4094    6.500001   61.267570\n",
      "4095    5.999994  -51.629285\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 5/25, Training Loss: 0.7676\n",
      "Epoch 5/25, Validation Loss: 0.8106\n",
      "          actual   predicted\n",
      "0       5.749999  -32.603791\n",
      "1     125.250003   42.097028\n",
      "2       5.500003  -39.033121\n",
      "3       5.250008   30.327739\n",
      "4      16.249997   72.876260\n",
      "...          ...         ...\n",
      "4091    4.999996  -24.278780\n",
      "4092   29.249997 -141.291838\n",
      "4093    5.666661  -66.471922\n",
      "4094    6.500001   24.482323\n",
      "4095    5.999994 -111.453705\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 6/25, Training Loss: 0.7576\n",
      "Epoch 6/25, Validation Loss: 0.7758\n",
      "          actual  predicted\n",
      "0       5.749999   4.588270\n",
      "1     125.250003  57.730449\n",
      "2       5.500003  -9.964414\n",
      "3       5.250008  55.034573\n",
      "4      16.249997  92.016414\n",
      "...          ...        ...\n",
      "4091    4.999996  32.972836\n",
      "4092   29.249997 -87.404204\n",
      "4093    5.666661 -22.958790\n",
      "4094    6.500001  36.714276\n",
      "4095    5.999994 -62.724447\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 7/25, Training Loss: 0.7528\n",
      "Epoch 7/25, Validation Loss: 0.7686\n",
      "          actual   predicted\n",
      "0       5.749999   68.879257\n",
      "1     125.250003  129.266883\n",
      "2       5.500003   35.605370\n",
      "3       5.250008  130.687875\n",
      "4      16.249997  158.946794\n",
      "...          ...         ...\n",
      "4091    4.999996   31.693200\n",
      "4092   29.249997  -43.276564\n",
      "4093    5.666661   36.714957\n",
      "4094    6.500001   75.736378\n",
      "4095    5.999994  -31.712750\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 8/25, Training Loss: 0.7203\n",
      "Epoch 8/25, Validation Loss: 0.7786\n",
      "          actual   predicted\n",
      "0       5.749999   43.874390\n",
      "1     125.250003  107.283247\n",
      "2       5.500003   22.869656\n",
      "3       5.250008   70.470440\n",
      "4      16.249997  140.033979\n",
      "...          ...         ...\n",
      "4091    4.999996   34.571388\n",
      "4092   29.249997  -38.784124\n",
      "4093    5.666661   14.586677\n",
      "4094    6.500001   59.714410\n",
      "4095    5.999994  -34.743416\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 9/25, Training Loss: 0.6692\n",
      "Epoch 9/25, Validation Loss: 0.7561\n",
      "          actual   predicted\n",
      "0       5.749999   49.450703\n",
      "1     125.250003   82.723512\n",
      "2       5.500003   21.816661\n",
      "3       5.250008   79.278244\n",
      "4      16.249997  107.461114\n",
      "...          ...         ...\n",
      "4091    4.999996    5.407524\n",
      "4092   29.249997  -55.232569\n",
      "4093    5.666661   11.990220\n",
      "4094    6.500001   55.920053\n",
      "4095    5.999994  -47.891664\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 10/25, Training Loss: 0.6796\n",
      "Epoch 10/25, Validation Loss: 0.7542\n",
      "          actual   predicted\n",
      "0       5.749999  102.470062\n",
      "1     125.250003  124.996015\n",
      "2       5.500003   60.047711\n",
      "3       5.250008  134.646989\n",
      "4      16.249997  147.877608\n",
      "...          ...         ...\n",
      "4091    4.999996   45.281107\n",
      "4092   29.249997    2.111827\n",
      "4093    5.666661   70.631363\n",
      "4094    6.500001  105.279211\n",
      "4095    5.999994    3.952611\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 11/25, Training Loss: 0.6991\n",
      "Epoch 11/25, Validation Loss: 0.7757\n",
      "          actual   predicted\n",
      "0       5.749999   66.221881\n",
      "1     125.250003   95.481946\n",
      "2       5.500003   27.471891\n",
      "3       5.250008  108.435731\n",
      "4      16.249997  121.484291\n",
      "...          ...         ...\n",
      "4091    4.999996  -15.422899\n",
      "4092   29.249997  -36.395185\n",
      "4093    5.666661   36.999222\n",
      "4094    6.500001   73.553216\n",
      "4095    5.999994  -25.926209\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 12/25, Training Loss: 0.6220\n",
      "Epoch 12/25, Validation Loss: 0.7163\n",
      "          actual  predicted\n",
      "0       5.749999  40.288772\n",
      "1     125.250003  54.163843\n",
      "2       5.500003   7.216985\n",
      "3       5.250008  72.834742\n",
      "4      16.249997  82.138933\n",
      "...          ...        ...\n",
      "4091    4.999996 -20.065266\n",
      "4092   29.249997 -62.138932\n",
      "4093    5.666661  14.793705\n",
      "4094    6.500001  65.387575\n",
      "4095    5.999994 -42.523424\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 13/25, Training Loss: 0.5911\n",
      "Epoch 13/25, Validation Loss: 0.6062\n",
      "          actual   predicted\n",
      "0       5.749999   86.447856\n",
      "1     125.250003   97.573929\n",
      "2       5.500003   79.275297\n",
      "3       5.250008  118.151663\n",
      "4      16.249997  123.967404\n",
      "...          ...         ...\n",
      "4091    4.999996   22.826894\n",
      "4092   29.249997    1.629613\n",
      "4093    5.666661   68.003853\n",
      "4094    6.500001  109.854766\n",
      "4095    5.999994   30.324887\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 14/25, Training Loss: 0.5575\n",
      "Epoch 14/25, Validation Loss: 0.6323\n",
      "          actual  predicted\n",
      "0       5.749999  40.177581\n",
      "1     125.250003  57.559354\n",
      "2       5.500003  47.007880\n",
      "3       5.250008  79.091433\n",
      "4      16.249997  85.367286\n",
      "...          ...        ...\n",
      "4091    4.999996 -19.533921\n",
      "4092   29.249997 -61.188414\n",
      "4093    5.666661  18.026867\n",
      "4094    6.500001  71.111629\n",
      "4095    5.999994 -11.789608\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 15/25, Training Loss: 0.5576\n",
      "Epoch 15/25, Validation Loss: 0.6781\n",
      "          actual   predicted\n",
      "0       5.749999  -19.456398\n",
      "1     125.250003    5.282170\n",
      "2       5.500003   24.399747\n",
      "3       5.250008   16.326363\n",
      "4      16.249997   33.081365\n",
      "...          ...         ...\n",
      "4091    4.999996  -61.697927\n",
      "4092   29.249997 -106.168105\n",
      "4093    5.666661  -38.571915\n",
      "4094    6.500001    9.573476\n",
      "4095    5.999994  -44.089671\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 16/25, Training Loss: 0.5436\n",
      "Epoch 16/25, Validation Loss: 0.5576\n",
      "          actual   predicted\n",
      "0       5.749999  -58.197595\n",
      "1     125.250003   17.628180\n",
      "2       5.500003   16.208455\n",
      "3       5.250008  -20.656674\n",
      "4      16.249997   54.710937\n",
      "...          ...         ...\n",
      "4091    4.999996  -50.689778\n",
      "4092   29.249997 -109.542243\n",
      "4093    5.666661  -69.523486\n",
      "4094    6.500001    1.051767\n",
      "4095    5.999994  -34.992983\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 17/25, Training Loss: 0.5645\n",
      "Epoch 17/25, Validation Loss: 0.5699\n",
      "          actual  predicted\n",
      "0       5.749999  -3.438011\n",
      "1     125.250003  40.236234\n",
      "2       5.500003  -0.341721\n",
      "3       5.250008  17.188125\n",
      "4      16.249997  73.229968\n",
      "...          ...        ...\n",
      "4091    4.999996   1.667543\n",
      "4092   29.249997 -84.927666\n",
      "4093    5.666661 -33.772190\n",
      "4094    6.500001  68.145177\n",
      "4095    5.999994 -34.867819\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 18/25, Training Loss: 0.4845\n",
      "Epoch 18/25, Validation Loss: 0.5419\n",
      "          actual  predicted\n",
      "0       5.749999   4.282331\n",
      "1     125.250003  58.678670\n",
      "2       5.500003  74.704036\n",
      "3       5.250008  37.049351\n",
      "4      16.249997  85.333445\n",
      "...          ...        ...\n",
      "4091    4.999996 -16.395440\n",
      "4092   29.249997 -59.496180\n",
      "4093    5.666661 -13.644539\n",
      "4094    6.500001  51.376439\n",
      "4095    5.999994  10.163205\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 19/25, Training Loss: 0.4931\n",
      "Epoch 19/25, Validation Loss: 0.6457\n",
      "          actual   predicted\n",
      "0       5.749999   16.240475\n",
      "1     125.250003   86.484225\n",
      "2       5.500003   96.637543\n",
      "3       5.250008   39.715615\n",
      "4      16.249997  116.740573\n",
      "...          ...         ...\n",
      "4091    4.999996    1.154370\n",
      "4092   29.249997  -36.598268\n",
      "4093    5.666661  -11.996667\n",
      "4094    6.500001   54.499568\n",
      "4095    5.999994   25.402485\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 20/25, Training Loss: 0.4614\n",
      "Epoch 20/25, Validation Loss: 0.6692\n",
      "          actual  predicted\n",
      "0       5.749999 -39.652557\n",
      "1     125.250003  62.642568\n",
      "2       5.500003  71.286439\n",
      "3       5.250008 -19.911314\n",
      "4      16.249997  93.479527\n",
      "...          ...        ...\n",
      "4091    4.999996 -19.025406\n",
      "4092   29.249997 -70.137297\n",
      "4093    5.666661 -61.623811\n",
      "4094    6.500001  13.941861\n",
      "4095    5.999994   0.422429\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 21/25, Training Loss: 0.4944\n",
      "Epoch 21/25, Validation Loss: 0.4691\n",
      "          actual  predicted\n",
      "0       5.749999  -1.127340\n",
      "1     125.250003  68.173569\n",
      "2       5.500003  96.034521\n",
      "3       5.250008  25.748683\n",
      "4      16.249997  86.031884\n",
      "...          ...        ...\n",
      "4091    4.999996 -30.782432\n",
      "4092   29.249997 -59.251857\n",
      "4093    5.666661 -33.413936\n",
      "4094    6.500001  14.255911\n",
      "4095    5.999994  19.124366\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 22/25, Training Loss: 0.4770\n",
      "Epoch 22/25, Validation Loss: 0.4394\n",
      "          actual   predicted\n",
      "0       5.749999   19.489972\n",
      "1     125.250003  126.253518\n",
      "2       5.500003   80.284888\n",
      "3       5.250008   27.199857\n",
      "4      16.249997  151.473204\n",
      "...          ...         ...\n",
      "4091    4.999996   54.684240\n",
      "4092   29.249997   -8.762412\n",
      "4093    5.666661  -18.460187\n",
      "4094    6.500001   87.202263\n",
      "4095    5.999994   55.175074\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 23/25, Training Loss: 0.4457\n",
      "Epoch 23/25, Validation Loss: 0.5425\n",
      "          actual  predicted\n",
      "0       5.749999 -28.881269\n",
      "1     125.250003  79.717364\n",
      "2       5.500003  81.048001\n",
      "3       5.250008 -18.683772\n",
      "4      16.249997  95.870549\n",
      "...          ...        ...\n",
      "4091    4.999996 -38.253064\n",
      "4092   29.249997 -65.030254\n",
      "4093    5.666661 -64.295525\n",
      "4094    6.500001 -11.443014\n",
      "4095    5.999994  11.878871\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 24/25, Training Loss: 0.4531\n",
      "Epoch 24/25, Validation Loss: 0.4006\n",
      "          actual   predicted\n",
      "0       5.749999    3.246115\n",
      "1     125.250003  110.025045\n",
      "2       5.500003   88.215751\n",
      "3       5.250008   -6.317197\n",
      "4      16.249997  122.458250\n",
      "...          ...         ...\n",
      "4091    4.999996   17.927481\n",
      "4092   29.249997  -20.812797\n",
      "4093    5.666661  -37.336942\n",
      "4094    6.500001   29.211054\n",
      "4095    5.999994   44.570983\n",
      "\n",
      "[4096 rows x 2 columns]\n",
      "Epoch 25/25, Training Loss: 0.4003\n",
      "Epoch 25/25, Validation Loss: 0.6769\n",
      "loading latest dataframe: data/quarter_hour_26f_20ws_dataframe_v1.parquet\n",
      "input_data: tensor([[[ 4.3896e-01,  1.6481e+00,  1.2189e+00, -7.3040e-01, -9.9768e-02,\n",
      "          -8.3099e-02, -1.8816e+00, -5.9109e-01, -2.0547e+00, -2.0547e+00,\n",
      "          -1.4280e+00,  4.1557e+00,  0.0000e+00, -9.0600e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.3963e-01, -1.0391e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.4623e-01,  1.6198e+00,  1.1779e+00, -6.8921e-01, -1.1689e-01,\n",
      "          -8.7525e-02, -1.7402e+00, -5.9109e-01, -2.0932e+00, -2.0932e+00,\n",
      "          -1.0415e-01,  4.1557e+00,  0.0000e+00, -9.0586e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -9.9923e-01, -9.7336e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.5349e-01,  1.5915e+00,  1.1368e+00, -6.4801e-01, -1.3400e-01,\n",
      "          -9.1950e-02, -1.5988e+00, -5.9109e-01, -2.1317e+00, -2.1317e+00,\n",
      "           1.2197e+00,  4.1557e+00,  0.0000e+00, -9.0573e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.0588e+00, -9.0760e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6076e-01,  1.5632e+00,  1.0958e+00, -6.0681e-01, -1.5112e-01,\n",
      "          -9.6376e-02, -1.4575e+00, -5.9109e-01, -2.1702e+00, -2.1702e+00,\n",
      "           2.5435e+00,  4.1557e+00,  0.0000e+00, -9.0560e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1184e+00, -8.4183e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.6802e-01,  1.5349e+00,  1.0548e+00, -5.6561e-01, -1.6824e-01,\n",
      "          -1.0080e-01, -1.3161e+00, -5.9109e-01, -2.2087e+00, -2.2087e+00,\n",
      "           3.8674e+00,  4.1557e+00,  0.0000e+00, -9.0546e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.9746e+00,  1.3448e-01,\n",
      "          -1.1780e+00, -7.7606e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-6.6942e-01,  2.1811e+00,  1.2599e+00, -1.2637e+00,  2.6910e-01,\n",
      "           9.8832e-04, -6.0919e-01, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0720e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -7.7521e-01, -1.1816e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [-1.9069e-01,  1.6999e+00,  3.6163e+00, -1.2338e+00,  4.5965e-02,\n",
      "          -5.3226e-02, -1.3514e+00, -4.8829e-01,  9.1100e-01,  9.1100e-01,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0578e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -8.4576e-01, -1.1314e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 9.4465e-02,  1.2588e+00,  2.2277e+00, -1.1200e+00,  2.9364e-03,\n",
      "          -6.0971e-02, -8.4482e-01, -3.5272e-01,  1.3539e+00,  1.3539e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0413e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -9.1458e-01, -1.0755e+00, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.0253e-01,  1.1187e+00,  1.8504e+00, -1.1149e+00, -9.2903e-03,\n",
      "          -6.9822e-02, -6.0919e-01, -5.0170e-01,  1.5946e+00,  1.5946e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0386e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1980e+00, -7.4471e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.3886e-01,  9.3658e-01,  9.4820e-01, -1.1187e+00, -1.0231e-02,\n",
      "          -6.9822e-02, -2.0230e+00, -5.9109e-01, -1.7273e+00, -1.7273e+00,\n",
      "           3.8674e+00, -7.9865e-01,  0.0000e+00, -9.0372e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0336e+00, -9.6137e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 2.8916e-01,  8.3876e-01,  8.9899e-01, -1.0463e+00, -3.2803e-02,\n",
      "          -7.4248e-02, -2.0230e+00, -5.0170e-01,  1.8354e+00,  1.8354e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0359e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.0943e+00, -8.9111e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.3667e-01,  7.6072e-01,  8.6618e-01, -1.0424e+00, -3.8446e-02,\n",
      "          -7.8673e-02, -1.3161e+00, -3.2292e-01,  1.6428e+00,  1.6428e+00,\n",
      "          -2.7518e+00, -7.9865e-01,  0.0000e+00, -9.0346e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.1502e+00, -8.1703e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5624e-01,  8.1171e-01,  8.0056e-01, -9.8904e-01, -4.2208e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0332e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3516e+00, -4.0081e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.5344e-01,  8.1795e-01,  7.5135e-01, -8.2501e-01, -4.3149e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.4984e+00,  1.4984e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0319e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.2470e+00, -6.5866e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.7021e-01,  9.1750e-01,  6.1738e-01, -8.6316e-01, -8.3591e-02,\n",
      "          -9.2688e-02, -1.3161e+00, -4.1231e-01,  1.5946e+00,  1.5946e+00,\n",
      "           1.9004e-01, -7.9865e-01,  0.0000e+00, -9.0292e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3034e+00, -5.3698e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 3.9396e-01,  1.0256e+00,  1.1738e+00, -6.0948e-01, -5.2084e-02,\n",
      "          -8.5312e-02, -1.6695e+00, -5.9109e-01,  1.6909e+00,  1.6909e+00,\n",
      "           2.7642e+00, -7.9865e-01,  0.0000e+00, -9.0259e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3387e+00, -4.4220e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2890e-01,  9.8028e-01,  7.0214e-01, -7.7160e-01, -4.4090e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.7872e+00,  1.7872e+00,\n",
      "          -1.6486e+00, -7.9865e-01,  0.0000e+00, -9.0239e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3664e+00, -3.4705e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.1772e-01,  1.0011e+00,  7.1034e-01, -7.4872e-01, -4.5971e-02,\n",
      "          -8.5312e-02, -2.0230e+00, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "          -5.4543e-01, -7.9865e-01,  0.0000e+00, -9.0225e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.3862e+00, -2.5629e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.2051e-01,  1.1052e+00,  7.7596e-01, -6.8387e-01, -4.4090e-02,\n",
      "          -8.3099e-02, -6.0919e-01, -5.9109e-01,  1.5946e+00,  1.5946e+00,\n",
      "           5.5777e-01, -7.9865e-01,  0.0000e+00, -9.0212e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4001e+00, -1.6443e-01, -1.1547e+00,  1.1547e+00,  0.0000e+00],\n",
      "         [ 4.0375e-01,  1.1520e+00,  8.0056e-01, -6.1520e-01, -4.4090e-02,\n",
      "          -8.0886e-02, -2.0230e+00, -5.9109e-01,  1.5465e+00,  1.5465e+00,\n",
      "          -3.8551e+00, -7.9865e-01,  0.0000e+00, -9.0199e-01,  0.0000e+00,\n",
      "          -4.4732e-01, -4.1860e-02,  1.4108e+00, -1.7689e+00,  8.5250e-01,\n",
      "          -1.4080e+00, -7.1849e-02, -1.1547e+00,  1.1547e+00,  0.0000e+00]]])\n",
      "predicted: tensor([[-0.1908]], device='cuda:0')\n",
      "[1.05]\n",
      "           actual   predicted\n",
      "0        5.749999    3.246115\n",
      "1      125.250003  110.025045\n",
      "2        5.500003   88.215751\n",
      "3        5.250008   -6.317197\n",
      "4       16.249997  122.458250\n",
      "...           ...         ...\n",
      "18994   11.500004  140.125868\n",
      "18995  306.800002  196.052966\n",
      "18996   11.500004   29.952770\n",
      "18997   18.500003  270.437842\n",
      "18998  601.818175  837.388467\n",
      "\n",
      "[18999 rows x 2 columns]\n",
      "Score (RMSE): 875.6285\n",
      "Score (MAE): 154.4684\n",
      "Score (ME): 28.8482\n",
      "Score (MAPE): 475.1905%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-ehm\\AppData\\Local\\Temp\\ipykernel_15692\\1996744831.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance_df = performance_df.append({\n"
     ]
    }
   ],
   "source": [
    "# Reload the utils module if you have made changes\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Read existing model performance records\n",
    "performance_df = pd.read_csv('model_performances.csv')\n",
    "\n",
    "# Define LSTM-specific hyperparameters\n",
    "hidden_dim = 256\n",
    "num_layers = 5\n",
    "dropout = 0.3\n",
    "batch_size = 4096\n",
    "learning_rate = 0.00031\n",
    "epochs = 25\n",
    "aggregation_level = 'quarter_hour'\n",
    "y_feature = 'CO2'\n",
    "freq = 15 if aggregation_level == 'quarter_hour' else 30 if aggregation_level == 'half_hour' else 60\n",
    "window_size = 5 * 60 // freq\n",
    "\n",
    "# Loop over each feature to create and evaluate the LSTM model\n",
    "for aggregation_level in ['quarter_hour', 'half_hour', 'hour']:\n",
    "    for y_feature in ['CO2', 'VOC', 'hum', 'tmp', 'vis']:\n",
    "        model, scaler, rmse, mae, me, mape, train_loss, val_loss, n_features = utils.create_multivariate_lstm_model_for_feature(\n",
    "            df, \n",
    "            hidden_dim=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout, \n",
    "            batch_size=batch_size, \n",
    "            learning_rate=learning_rate, \n",
    "            epochs=epochs, \n",
    "            y_feature=y_feature, \n",
    "            aggregation_level=aggregation_level, \n",
    "            window_size=window_size\n",
    "        )\n",
    "        performance_df = performance_df.append({\n",
    "            'model_name': 'multivariate_lstm',\n",
    "            'aggregation_level': aggregation_level,\n",
    "            'y_feature': y_feature,\n",
    "            'n_features': n_features,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'me': me,\n",
    "            'mape': mape,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'num_layers': num_layers,\n",
    "            'dropout': dropout,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epochs': epochs,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'note': 'LSTM model with device_id embedding'\n",
    "        }, ignore_index=True)\n",
    "        performance_df.to_csv('model_performances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device_ids = torch.tensor([0])\n",
    "\n",
    "device = utils.get_device()\n",
    "\n",
    "model = utils.load_transformer_model(y_feature=\"CO2\", model_name = 'transformer_multivariate_quarter_hour_26f', device=device)\n",
    "scaler = utils.load_scaler(y_feature=\"CO2\", model_name = 'transformer_multivariate_quarter_hour_26f')\n",
    "\n",
    "real_data = utils.load_dataframe(model_name='transformer_multivariate_quarter_hour_26f')\n",
    "pd.set_option('display.max_columns', None)\n",
    "real_data.iloc[7:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = real_data.iloc[7:27].drop(columns=['device_id', 'date_time_rounded'])\n",
    "\n",
    "data_df_scaled = scaler.transform(data_df)\n",
    "\n",
    "input_data = torch.tensor(data_df_scaled, dtype=torch.float32).view(-1, 20, data_df_scaled.shape[1])\n",
    "print(input_data.shape)\n",
    "device_ids = torch.tensor([0])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_data.to(device), device_ids.to(device))\n",
    "    print(output)\n",
    "\n",
    "    prediction = output.cpu().numpy().reshape(-1, 1)\n",
    "    print(prediction)\n",
    "    print(prediction.shape)\n",
    "    zeroes_for_scaler = np.zeros((prediction.shape[0], 25))\n",
    "\n",
    "    zeroes_for_scaler[:, 2] = prediction  # Insert predicted values into the correct column\n",
    "    print(zeroes_for_scaler)\n",
    "    inverse_transformed = scaler.inverse_transform(zeroes_for_scaler)\n",
    "    predicted_unscaled = inverse_transformed[:, 2].round(2)\n",
    "    print(predicted_unscaled)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
