{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('hka-aqm-am/' + f.removeprefix('._'), skiprows=1, sep=';', engine='python') for f in os.listdir('hka-aqm-am/')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended data shape from (395686, 27) to (401930, 27)\n",
      "training data cutoff:  2023-07-14 03:45:00\n",
      "device_id              int8\n",
      "tmp                 float64\n",
      "hum                 float64\n",
      "CO2                 float64\n",
      "VOC                 float64\n",
      "vis                 float64\n",
      "IR                  float64\n",
      "WIFI                float64\n",
      "BLE                 float64\n",
      "rssi                float64\n",
      "channel_rssi        float64\n",
      "channel_index       float64\n",
      "spreading_factor    float64\n",
      "bandwidth           float64\n",
      "f_cnt               float64\n",
      "isHoliday           float64\n",
      "isExamTime          float64\n",
      "weekday_sin         float64\n",
      "weekday_cos         float64\n",
      "month_sin           float64\n",
      "month_cos           float64\n",
      "time_sin            float64\n",
      "time_cos            float64\n",
      "semester_SS23       float64\n",
      "semester_WS22/23    float64\n",
      "semester_WS23/24    float64\n",
      "group                 int64\n",
      "dtype: object\n",
      "Creating sequences...\n",
      "Training data shape: torch.Size([316477, 20, 25]) torch.Size([316477]) torch.Size([316477, 1])\n",
      "Testing data shape: torch.Size([79441, 20, 25]) torch.Size([79441]) torch.Size([79441, 1])\n",
      "Combined data shape: torch.Size([395918, 20, 25]) torch.Size([395918]) torch.Size([395918, 1])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 189\u001b[0m\n\u001b[1;32m    185\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, test_dataset, train_loader, test_loader, scaler, y_test, full_preprocessed_df_unscaled, y_feature_scaler_index\n\u001b[0;32m--> 189\u001b[0m train_dataset, test_dataset, train_loader, test_loader, scaler, y_test, full_preprocessed_df_unscaled, y_feature_scaler_index \u001b[38;5;241m=\u001b[39m get_data_for_multivarate_sequential_forecast(df, y_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCO2\u001b[39m\u001b[38;5;124m'\u001b[39m, window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, aggregation_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquarter_hour\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# full_preprocessed_df_unscaled = get_data_for_multivarate_sequential_forecast(df, y_feature='CO2', window_size=20, aggregation_level='quarter_hour')\u001b[39;00m\n\u001b[1;32m    192\u001b[0m full_preprocessed_df_unscaled\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 159\u001b[0m, in \u001b[0;36mget_data_for_multivarate_sequential_forecast\u001b[0;34m(df, y_feature, window_size, aggregation_level, batch_size, clean_data, drop_columns, extend_training_data)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Shuffle the combined dataset\u001b[39;00m\n\u001b[1;32m    158\u001b[0m combined_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(x_combined, device_ids_combined, y_combined)\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_dataset\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    160\u001b[0m combined_loader \u001b[38;5;241m=\u001b[39m DataLoader(combined_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(combined_dataset), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_loader\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def get_data_for_multivarate_sequential_forecast(df: pd.DataFrame, y_feature: str='CO2', window_size: int=5, aggregation_level: str = 'half_hour', batch_size: int=utils.get_batch_size(), clean_data: bool=True, drop_columns: list=[], extend_training_data: bool=True) -> np.array:\n",
    "    df_cpy = utils.clean_df(df, clean_data)\n",
    "\n",
    "    # Get list of holidays and exams at HKA and add features for them\n",
    "    holiday_list, exam_dates = utils.get_list_of_special_dates()\n",
    "    df_cpy['isHoliday'] = df_cpy['date_time'].dt.date.isin(holiday_list).astype(int)\n",
    "    df_cpy['isExamTime'] = df_cpy['date_time'].dt.date.isin(exam_dates).astype(int)\n",
    "\n",
    "    # round date_time by half hour, group by date_time_rounded and device_id and take the mean of the other columns\n",
    "    if aggregation_level == \"hour\":\n",
    "        freq='60T'\n",
    "        df_cpy['date_time_rounded'] = df_cpy['date_time'].dt.round('60T')\n",
    "    elif aggregation_level == \"half_hour\":\n",
    "        freq='30T'\n",
    "        df_cpy['date_time_rounded'] = df_cpy['date_time'].dt.round('30T')\n",
    "    elif aggregation_level == \"quarter_hour\":\n",
    "        freq='15T'\n",
    "        df_cpy['date_time_rounded'] = df_cpy['date_time'].dt.round('15T')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid aggregation_level. Please choose one of 'hour', 'half_hour', or 'quarter_hour'.\")\n",
    "\n",
    "    # encode cyclic features\n",
    "    df_cpy['weekday_sin'] = np.sin(2 * np.pi * df_cpy['date_time_rounded'].dt.weekday / 7)\n",
    "    df_cpy['weekday_cos'] = np.cos(2 * np.pi * df_cpy['date_time_rounded'].dt.weekday / 7)\n",
    "    df_cpy['month_sin'] = np.sin(2 * np.pi * df_cpy['date_time_rounded'].dt.month / 12)\n",
    "    df_cpy['month_cos'] = np.cos(2 * np.pi * df_cpy['date_time_rounded'].dt.month / 12)\n",
    "    df_cpy['time_sin'] = np.sin(2 * np.pi * (df_cpy['date_time_rounded'].dt.hour * 3600 + df_cpy['date_time'].dt.minute * 60) / 86400.0)\n",
    "    df_cpy['time_cos'] = np.cos(2 * np.pi * (df_cpy['date_time_rounded'].dt.hour * 3600 + df_cpy['date_time'].dt.minute * 60) / 86400.0)\n",
    "    df_cpy['semester'] = 'WS22/23'\n",
    "    df_cpy.loc[df_cpy['date_time_rounded'] >= '2023-03-01', 'semester'] = 'SS23'\n",
    "    df_cpy.loc[df_cpy['date_time_rounded'] >= '2023-09-01', 'semester'] = 'WS23/24'\n",
    "    df_cpy = pd.get_dummies(df_cpy, columns=['semester'])\n",
    "\n",
    "    df_cpy = df_cpy.groupby(['device_id', 'date_time_rounded']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    df_cpy_extended = pd.DataFrame()\n",
    "    \n",
    "    def fill_consecutive_nans(device_df: pd.DataFrame, window_size: int):\n",
    "        max_consecutive_nans = window_size // 4\n",
    "        device_df = device_df.sort_values('date_time_rounded')\n",
    "        device_df['row_contains_no_nan'] = device_df.notna().all(axis=1).astype(int)\n",
    "        device_df['group'] = device_df['row_contains_no_nan'].cumsum()\n",
    "        reduced_df = deepcopy(device_df[device_df['row_contains_no_nan'] == False])\n",
    "        nan_count_in_group = reduced_df.groupby('group')['row_contains_no_nan'].transform('count')\n",
    "        #reduced_df['nan_count_in_group'] = np.where(reduced_df[y_feature].isna(), nan_count_in_group, 0)\n",
    "        #reduced_df.loc[reduced_df.isna().any(axis=1), 'nan_count_in_group'] = nan_count_in_group\n",
    "        device_df.loc[device_df.isna().any(axis=1), 'nan_count_in_group'] = nan_count_in_group.loc[device_df.isna().any(axis=1)]\n",
    "\n",
    "    \n",
    "        #device_df = pd.merge(device_df, reduced_df[['date_time_rounded', 'nan_count_in_group']], on='date_time_rounded', how='left')\n",
    "        device_df['nan_count_in_group'] = device_df['nan_count_in_group'].fillna(0)\n",
    "        device_df['device_id'] = device_df['device_id'].fillna(method='ffill')\n",
    "        device_df = device_df[device_df['nan_count_in_group'] <= max_consecutive_nans]\n",
    "        device_df.drop(columns=['row_contains_no_nan', 'group', 'nan_count_in_group'], inplace=True)\n",
    "        device_df[[col for col in device_df.columns if col != 'date_time_rounded']] = device_df[[col for col in device_df.columns if col != 'date_time_rounded']].interpolate(method='linear', axis=0)\n",
    "        \n",
    "        return device_df\n",
    "\n",
    "    \n",
    "    if extend_training_data:\n",
    "        for device_id in df_cpy['device_id'].unique():\n",
    "            device_data = df_cpy[df_cpy['device_id'] == device_id]\n",
    "            min_date = device_data['date_time_rounded'].min()\n",
    "            max_date = device_data['date_time_rounded'].max()\n",
    "            date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
    "            device_df = pd.DataFrame(date_range, columns=['date_time_rounded'])\n",
    "            #device_df['device_id'] = device_id\n",
    "            merged_df = pd.merge(device_df, device_data, on='date_time_rounded', how='left')\n",
    "            merged_df_filled = fill_consecutive_nans(merged_df, window_size)\n",
    "\n",
    "            df_cpy_extended = pd.concat([df_cpy_extended, merged_df_filled], ignore_index=True)\n",
    "        \n",
    "        print(f'extended data shape from {df_cpy.shape} to {df_cpy_extended.shape}')\n",
    "\n",
    "        df_cpy = deepcopy(df_cpy_extended)\n",
    "    \n",
    "    \n",
    "    # the data is now in the state that it needs to be so that it can be used as the context data at later predictions\n",
    "    full_preprocessed_df_unscaled = deepcopy(df_cpy)\n",
    "    \n",
    "    # create 'consecutive_data_point' thats 1 if the previous data point is <freq> before the current data point and device_id is the same, else 0\n",
    "    df_cpy['consecutive_data_point'] = (df_cpy['date_time_rounded'] - df_cpy['date_time_rounded'].shift(1)).dt.total_seconds() == pd.to_timedelta(freq).total_seconds()\n",
    "    df_cpy['consecutive_data_point'] = df_cpy['consecutive_data_point'].astype(int)\n",
    "    \n",
    "    # Identify changes and resets (when the value is '0' or there's a change in 'device_id')\n",
    "    df_cpy['reset'] = (df_cpy['consecutive_data_point'] == 0) | (df_cpy['device_id'] != df_cpy['device_id'].shift(1))\n",
    "\n",
    "    # Create a group identifier that increments every time a reset occurs\n",
    "    df_cpy['group'] = df_cpy['reset'].cumsum()\n",
    "\n",
    "    # Calculate cumulative sum of \"1\"s within each group\n",
    "    df_cpy['consecutive_data_points'] = df_cpy.groupby(['device_id', 'group'])['consecutive_data_point'].cumsum() - df_cpy['consecutive_data_point']\n",
    "    df_cpy['group_size'] = df_cpy.groupby(['device_id', 'group'])['consecutive_data_point'].transform('count')\n",
    "\n",
    "    df_cpy = df_cpy[df_cpy['group_size'] > window_size]\n",
    "\n",
    "    # You may want to drop the 'reset' and 'group' columns if they are no longer needed\n",
    "    df_cpy.drop(['reset', 'consecutive_data_point', 'consecutive_data_points', 'group_size'], axis=1, inplace=True)\n",
    "    \n",
    "    threshold_date = df_cpy.sort_values('date_time_rounded', ascending=True)['date_time_rounded'].quantile(0.8)\n",
    "    print('training data cutoff: ', threshold_date)\n",
    "\n",
    "    df_cpy['device_id'] = df_cpy['device_id'].astype('category').cat.codes\n",
    "\n",
    "    df_train = deepcopy(df_cpy[df_cpy['date_time_rounded'] < threshold_date])\n",
    "    df_test = deepcopy(df_cpy[df_cpy['date_time_rounded'] >= threshold_date])\n",
    "\n",
    "    # Create the scaler instance\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Get the columns to scale\n",
    "    columns_to_scale = [col for col in df_train.columns if col not in ['date_time_rounded', 'device_id', 'group']]\n",
    "\n",
    "    # Fit on training data and transform both training and test data\n",
    "    df_train[columns_to_scale] = scaler.fit_transform(df_train[columns_to_scale])\n",
    "    df_test[columns_to_scale] = scaler.transform(df_test[columns_to_scale])\n",
    "    y_feature_scaler_index = columns_to_scale.index(y_feature)\n",
    "\n",
    "    # drop unconvertible columns\n",
    "    df_train.drop(['date_time_rounded'], axis=1, inplace=True)\n",
    "    df_test.drop(['date_time_rounded'], axis=1, inplace=True)\n",
    "    \n",
    "    print(df_train.dtypes)\n",
    "\n",
    "    def to_sequences(seq_size: int, obs: pd.DataFrame):\n",
    "        x = []\n",
    "        y = []\n",
    "        device_ids = []\n",
    "        for g_id in obs['group'].unique():\n",
    "            group_df = obs[obs['group'] == g_id]\n",
    "            feature_values = group_df[f'{y_feature}'].tolist()\n",
    "            for i in range(len(group_df) - seq_size):\n",
    "                window = group_df[i:(i + seq_size)]\n",
    "                after_window = feature_values[i + seq_size]\n",
    "                x.append(window.drop(columns=['device_id', 'group']).values)\n",
    "                device_ids.append(window['device_id'].values[-1])\n",
    "                y.append(after_window)\n",
    "        feature_count = x[0].shape[1]\n",
    "        return (torch.tensor(np.array(x), dtype=torch.float32).view(-1, seq_size, feature_count),\n",
    "                torch.tensor(device_ids, dtype=torch.long),\n",
    "                torch.tensor(y, dtype=torch.float32).view(-1, 1))\n",
    "\n",
    "    print(\"Creating sequences...\")\n",
    "    x_train, train_device_ids, y_train = to_sequences(window_size, df_train)\n",
    "    x_test, test_device_ids, y_test = to_sequences(window_size, df_test)\n",
    "\n",
    "    print(\"Training data shape:\", x_train.shape, train_device_ids.shape,y_train.shape)\n",
    "    print(\"Testing data shape:\", x_test.shape, test_device_ids.shape, y_test.shape)\n",
    "\n",
    "    # combining the datasets again and shuffle them. This is because we cant shuffle them before because of the context. Maybe we can, but im too layzy to think about it.\n",
    "    x_combined = torch.cat((x_train, x_test), dim=0)\n",
    "    device_ids_combined = torch.cat((train_device_ids, test_device_ids), dim=0)\n",
    "    y_combined = torch.cat((y_train, y_test), dim=0)\n",
    "\n",
    "    # Shuffle the combined dataset\n",
    "    combined_dataset = TensorDataset(x_combined, device_ids_combined, y_combined)\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=len(combined_dataset), shuffle=True)\n",
    "\n",
    "    # Get shuffled data from the loader\n",
    "    for batch in combined_loader:\n",
    "        x_combined_shuffled, device_ids_combined_shuffled, y_combined_shuffled = batch\n",
    "\n",
    "    # Split back into training and test sets (80:20 split)\n",
    "    split_index = int(0.8 * len(x_combined_shuffled))\n",
    "    x_train_shuffled, x_test_shuffled = x_combined_shuffled[:split_index], x_combined_shuffled[split_index:]\n",
    "    train_device_ids_shuffled, test_device_ids_shuffled = device_ids_combined_shuffled[:split_index], device_ids_combined_shuffled[split_index:]\n",
    "    y_train_shuffled, y_test_shuffled = y_combined_shuffled[:split_index], y_combined_shuffled[split_index:]\n",
    "\n",
    "    print(\"Shuffled Training data shape:\", x_train_shuffled.shape, train_device_ids_shuffled.shape, y_train_shuffled.shape)\n",
    "    print(\"Shuffled Testing data shape:\", x_test_shuffled.shape, test_device_ids_shuffled.shape, y_test_shuffled.shape)\n",
    "\n",
    "    # Setup data loaders for batch\n",
    "    # train_dataset = TensorDataset(x_train, train_device_ids, y_train)\n",
    "    train_dataset = TensorDataset(x_train_shuffled, train_device_ids_shuffled, y_train_shuffled)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "    # test_dataset = TensorDataset(x_test, test_device_ids, y_test)\n",
    "    test_dataset = TensorDataset(x_test_shuffled, test_device_ids_shuffled, y_test_shuffled)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    return train_dataset, test_dataset, train_loader, test_loader, scaler, y_test, full_preprocessed_df_unscaled, y_feature_scaler_index\n",
    "\n",
    "train_dataset, test_dataset, train_loader, test_loader, scaler, y_test, full_preprocessed_df_unscaled, y_feature_scaler_index = get_data_for_multivarate_sequential_forecast(df, y_feature='CO2', window_size=20, aggregation_level='quarter_hour')\n",
    "\n",
    "# full_preprocessed_df_unscaled = get_data_for_multivarate_sequential_forecast(df, y_feature='CO2', window_size=20, aggregation_level='quarter_hour')\n",
    "full_preprocessed_df_unscaled.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
